diff -uNr gcc-11.2.0.orig/gcc/config/i386/i386.c gcc-11.2.0/gcc/config/i386/i386.c
--- gcc-11.2.0.orig/gcc/config/i386/i386.c	2021-07-28 01:55:07.072284343 -0500
+++ gcc-11.2.0/gcc/config/i386/i386.c	2021-08-06 00:34:01.876709574 -0500
@@ -23518,6 +23518,9 @@
 #undef TARGET_ADDRESS_COST
 #define TARGET_ADDRESS_COST ix86_address_cost
 
+#undef TARGET_CLEAR_BY_PIECES
+#define TARGET_CLEAR_BY_PIECES builtin_memset_read_str
+
 #undef TARGET_FLAGS_REGNUM
 #define TARGET_FLAGS_REGNUM FLAGS_REG
 #undef TARGET_FIXED_CONDITION_CODE_REGS
diff -uNr gcc-11.2.0.orig/gcc/config/i386/i386.c.orig gcc-11.2.0/gcc/config/i386/i386.c.orig
--- gcc-11.2.0.orig/gcc/config/i386/i386.c.orig	1969-12-31 18:00:00.000000000 -0600
+++ gcc-11.2.0/gcc/config/i386/i386.c.orig	2021-07-28 01:55:07.072284343 -0500
@@ -0,0 +1,23896 @@
+/* Subroutines used for code generation on IA-32.
+   Copyright (C) 1988-2021 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation; either version 3, or (at your option)
+any later version.
+
+GCC is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING3.  If not see
+<http://www.gnu.org/licenses/>.  */
+
+#define IN_TARGET_CODE 1
+
+#include "config.h"
+#include "system.h"
+#include "coretypes.h"
+#include "backend.h"
+#include "rtl.h"
+#include "tree.h"
+#include "memmodel.h"
+#include "gimple.h"
+#include "cfghooks.h"
+#include "cfgloop.h"
+#include "df.h"
+#include "tm_p.h"
+#include "stringpool.h"
+#include "expmed.h"
+#include "optabs.h"
+#include "regs.h"
+#include "emit-rtl.h"
+#include "recog.h"
+#include "cgraph.h"
+#include "diagnostic.h"
+#include "cfgbuild.h"
+#include "alias.h"
+#include "fold-const.h"
+#include "attribs.h"
+#include "calls.h"
+#include "stor-layout.h"
+#include "varasm.h"
+#include "output.h"
+#include "insn-attr.h"
+#include "flags.h"
+#include "except.h"
+#include "explow.h"
+#include "expr.h"
+#include "cfgrtl.h"
+#include "common/common-target.h"
+#include "langhooks.h"
+#include "reload.h"
+#include "gimplify.h"
+#include "dwarf2.h"
+#include "tm-constrs.h"
+#include "cselib.h"
+#include "sched-int.h"
+#include "opts.h"
+#include "tree-pass.h"
+#include "context.h"
+#include "pass_manager.h"
+#include "target-globals.h"
+#include "gimple-iterator.h"
+#include "tree-vectorizer.h"
+#include "shrink-wrap.h"
+#include "builtins.h"
+#include "rtl-iter.h"
+#include "tree-iterator.h"
+#include "dbgcnt.h"
+#include "case-cfn-macros.h"
+#include "dojump.h"
+#include "fold-const-call.h"
+#include "tree-vrp.h"
+#include "tree-ssanames.h"
+#include "selftest.h"
+#include "selftest-rtl.h"
+#include "print-rtl.h"
+#include "intl.h"
+#include "ifcvt.h"
+#include "symbol-summary.h"
+#include "ipa-prop.h"
+#include "ipa-fnsummary.h"
+#include "wide-int-bitmask.h"
+#include "tree-vector-builder.h"
+#include "debug.h"
+#include "dwarf2out.h"
+#include "i386-options.h"
+#include "i386-builtins.h"
+#include "i386-expand.h"
+#include "i386-features.h"
+#include "function-abi.h"
+
+/* This file should be included last.  */
+#include "target-def.h"
+
+static rtx legitimize_dllimport_symbol (rtx, bool);
+static rtx legitimize_pe_coff_extern_decl (rtx, bool);
+static void ix86_print_operand_address_as (FILE *, rtx, addr_space_t, bool);
+static void ix86_emit_restore_reg_using_pop (rtx);
+
+
+#ifndef CHECK_STACK_LIMIT
+#define CHECK_STACK_LIMIT (-1)
+#endif
+
+/* Return index of given mode in mult and division cost tables.  */
+#define MODE_INDEX(mode)					\
+  ((mode) == QImode ? 0						\
+   : (mode) == HImode ? 1					\
+   : (mode) == SImode ? 2					\
+   : (mode) == DImode ? 3					\
+   : 4)
+
+
+/* Set by -mtune.  */
+const struct processor_costs *ix86_tune_cost = NULL;
+
+/* Set by -mtune or -Os.  */
+const struct processor_costs *ix86_cost = NULL;
+
+/* In case the average insn count for single function invocation is
+   lower than this constant, emit fast (but longer) prologue and
+   epilogue code.  */
+#define FAST_PROLOGUE_INSN_COUNT 20
+
+/* Names for 8 (low), 8 (high), and 16-bit registers, respectively.  */
+static const char *const qi_reg_name[] = QI_REGISTER_NAMES;
+static const char *const qi_high_reg_name[] = QI_HIGH_REGISTER_NAMES;
+static const char *const hi_reg_name[] = HI_REGISTER_NAMES;
+
+/* Array of the smallest class containing reg number REGNO, indexed by
+   REGNO.  Used by REGNO_REG_CLASS in i386.h.  */
+
+enum reg_class const regclass_map[FIRST_PSEUDO_REGISTER] =
+{
+  /* ax, dx, cx, bx */
+  AREG, DREG, CREG, BREG,
+  /* si, di, bp, sp */
+  SIREG, DIREG, NON_Q_REGS, NON_Q_REGS,
+  /* FP registers */
+  FP_TOP_REG, FP_SECOND_REG, FLOAT_REGS, FLOAT_REGS,
+  FLOAT_REGS, FLOAT_REGS, FLOAT_REGS, FLOAT_REGS,
+  /* arg pointer, flags, fpsr, frame */
+  NON_Q_REGS, NO_REGS, NO_REGS, NON_Q_REGS,
+  /* SSE registers */
+  SSE_FIRST_REG, SSE_REGS, SSE_REGS, SSE_REGS,
+  SSE_REGS, SSE_REGS, SSE_REGS, SSE_REGS,
+  /* MMX registers */
+  MMX_REGS, MMX_REGS, MMX_REGS, MMX_REGS,
+  MMX_REGS, MMX_REGS, MMX_REGS, MMX_REGS,
+  /* REX registers */
+  GENERAL_REGS, GENERAL_REGS, GENERAL_REGS, GENERAL_REGS,
+  GENERAL_REGS, GENERAL_REGS, GENERAL_REGS, GENERAL_REGS,
+  /* SSE REX registers */
+  SSE_REGS, SSE_REGS, SSE_REGS, SSE_REGS,
+  SSE_REGS, SSE_REGS, SSE_REGS, SSE_REGS,
+  /* AVX-512 SSE registers */
+  ALL_SSE_REGS, ALL_SSE_REGS, ALL_SSE_REGS, ALL_SSE_REGS,
+  ALL_SSE_REGS, ALL_SSE_REGS, ALL_SSE_REGS, ALL_SSE_REGS,
+  ALL_SSE_REGS, ALL_SSE_REGS, ALL_SSE_REGS, ALL_SSE_REGS,
+  ALL_SSE_REGS, ALL_SSE_REGS, ALL_SSE_REGS, ALL_SSE_REGS,
+  /* Mask registers.  */
+  ALL_MASK_REGS, MASK_REGS, MASK_REGS, MASK_REGS,
+  MASK_REGS, MASK_REGS, MASK_REGS, MASK_REGS
+};
+
+/* The "default" register map used in 32bit mode.  */
+
+int const dbx_register_map[FIRST_PSEUDO_REGISTER] =
+{
+  /* general regs */
+  0, 2, 1, 3, 6, 7, 4, 5,
+  /* fp regs */
+  12, 13, 14, 15, 16, 17, 18, 19,
+  /* arg, flags, fpsr, frame */
+  IGNORED_DWARF_REGNUM, IGNORED_DWARF_REGNUM,
+  IGNORED_DWARF_REGNUM, IGNORED_DWARF_REGNUM,
+  /* SSE */
+  21, 22, 23, 24, 25, 26, 27, 28,
+  /* MMX */
+  29, 30, 31, 32, 33, 34, 35, 36,
+  /* extended integer registers */
+  INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM,
+  INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM,
+  /* extended sse registers */
+  INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM,
+  INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM,
+  /* AVX-512 registers 16-23 */
+  INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM,
+  INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM,
+  /* AVX-512 registers 24-31 */
+  INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM,
+  INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM,
+  /* Mask registers */
+  93, 94, 95, 96, 97, 98, 99, 100
+};
+
+/* The "default" register map used in 64bit mode.  */
+
+int const dbx64_register_map[FIRST_PSEUDO_REGISTER] =
+{
+  /* general regs */
+  0, 1, 2, 3, 4, 5, 6, 7,
+  /* fp regs */
+  33, 34, 35, 36, 37, 38, 39, 40,
+  /* arg, flags, fpsr, frame */
+  IGNORED_DWARF_REGNUM, IGNORED_DWARF_REGNUM,
+  IGNORED_DWARF_REGNUM, IGNORED_DWARF_REGNUM,
+  /* SSE */
+  17, 18, 19, 20, 21, 22, 23, 24,
+  /* MMX */
+  41, 42, 43, 44, 45, 46, 47, 48,
+  /* extended integer registers */
+  8, 9, 10, 11, 12, 13, 14, 15,
+  /* extended SSE registers */
+  25, 26, 27, 28, 29, 30, 31, 32,
+  /* AVX-512 registers 16-23 */
+  67, 68, 69, 70, 71, 72, 73, 74,
+  /* AVX-512 registers 24-31 */
+  75, 76, 77, 78, 79, 80, 81, 82,
+  /* Mask registers */
+  118, 119, 120, 121, 122, 123, 124, 125
+};
+
+/* Define the register numbers to be used in Dwarf debugging information.
+   The SVR4 reference port C compiler uses the following register numbers
+   in its Dwarf output code:
+	0 for %eax (gcc regno = 0)
+	1 for %ecx (gcc regno = 2)
+	2 for %edx (gcc regno = 1)
+	3 for %ebx (gcc regno = 3)
+	4 for %esp (gcc regno = 7)
+	5 for %ebp (gcc regno = 6)
+	6 for %esi (gcc regno = 4)
+	7 for %edi (gcc regno = 5)
+   The following three DWARF register numbers are never generated by
+   the SVR4 C compiler or by the GNU compilers, but SDB on x86/svr4
+   believed these numbers have these meanings.
+	8  for %eip    (no gcc equivalent)
+	9  for %eflags (gcc regno = 17)
+	10 for %trapno (no gcc equivalent)
+   It is not at all clear how we should number the FP stack registers
+   for the x86 architecture.  If the version of SDB on x86/svr4 were
+   a bit less brain dead with respect to floating-point then we would
+   have a precedent to follow with respect to DWARF register numbers
+   for x86 FP registers, but the SDB on x86/svr4 was so completely
+   broken with respect to FP registers that it is hardly worth thinking
+   of it as something to strive for compatibility with.
+   The version of x86/svr4 SDB I had does (partially)
+   seem to believe that DWARF register number 11 is associated with
+   the x86 register %st(0), but that's about all.  Higher DWARF
+   register numbers don't seem to be associated with anything in
+   particular, and even for DWARF regno 11, SDB only seemed to under-
+   stand that it should say that a variable lives in %st(0) (when
+   asked via an `=' command) if we said it was in DWARF regno 11,
+   but SDB still printed garbage when asked for the value of the
+   variable in question (via a `/' command).
+   (Also note that the labels SDB printed for various FP stack regs
+   when doing an `x' command were all wrong.)
+   Note that these problems generally don't affect the native SVR4
+   C compiler because it doesn't allow the use of -O with -g and
+   because when it is *not* optimizing, it allocates a memory
+   location for each floating-point variable, and the memory
+   location is what gets described in the DWARF AT_location
+   attribute for the variable in question.
+   Regardless of the severe mental illness of the x86/svr4 SDB, we
+   do something sensible here and we use the following DWARF
+   register numbers.  Note that these are all stack-top-relative
+   numbers.
+	11 for %st(0) (gcc regno = 8)
+	12 for %st(1) (gcc regno = 9)
+	13 for %st(2) (gcc regno = 10)
+	14 for %st(3) (gcc regno = 11)
+	15 for %st(4) (gcc regno = 12)
+	16 for %st(5) (gcc regno = 13)
+	17 for %st(6) (gcc regno = 14)
+	18 for %st(7) (gcc regno = 15)
+*/
+int const svr4_dbx_register_map[FIRST_PSEUDO_REGISTER] =
+{
+  /* general regs */
+  0, 2, 1, 3, 6, 7, 5, 4,
+  /* fp regs */
+  11, 12, 13, 14, 15, 16, 17, 18,
+  /* arg, flags, fpsr, frame */
+  IGNORED_DWARF_REGNUM, 9,
+  IGNORED_DWARF_REGNUM, IGNORED_DWARF_REGNUM,
+  /* SSE registers */
+  21, 22, 23, 24, 25, 26, 27, 28,
+  /* MMX registers */
+  29, 30, 31, 32, 33, 34, 35, 36,
+  /* extended integer registers */
+  INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM,
+  INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM,
+  /* extended sse registers */
+  INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM,
+  INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM,
+  /* AVX-512 registers 16-23 */
+  INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM,
+  INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM,
+  /* AVX-512 registers 24-31 */
+  INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM,
+  INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM, INVALID_REGNUM,
+  /* Mask registers */
+  93, 94, 95, 96, 97, 98, 99, 100
+};
+
+/* Define parameter passing and return registers.  */
+
+static int const x86_64_int_parameter_registers[6] =
+{
+  DI_REG, SI_REG, DX_REG, CX_REG, R8_REG, R9_REG
+};
+
+static int const x86_64_ms_abi_int_parameter_registers[4] =
+{
+  CX_REG, DX_REG, R8_REG, R9_REG
+};
+
+static int const x86_64_int_return_registers[4] =
+{
+  AX_REG, DX_REG, DI_REG, SI_REG
+};
+
+/* Define the structure for the machine field in struct function.  */
+
+struct GTY(()) stack_local_entry {
+  unsigned short mode;
+  unsigned short n;
+  rtx rtl;
+  struct stack_local_entry *next;
+};
+
+/* Which cpu are we scheduling for.  */
+enum attr_cpu ix86_schedule;
+
+/* Which cpu are we optimizing for.  */
+enum processor_type ix86_tune;
+
+/* Which instruction set architecture to use.  */
+enum processor_type ix86_arch;
+
+/* True if processor has SSE prefetch instruction.  */
+unsigned char ix86_prefetch_sse;
+
+/* Preferred alignment for stack boundary in bits.  */
+unsigned int ix86_preferred_stack_boundary;
+
+/* Alignment for incoming stack boundary in bits specified at
+   command line.  */
+unsigned int ix86_user_incoming_stack_boundary;
+
+/* Default alignment for incoming stack boundary in bits.  */
+unsigned int ix86_default_incoming_stack_boundary;
+
+/* Alignment for incoming stack boundary in bits.  */
+unsigned int ix86_incoming_stack_boundary;
+
+/* Calling abi specific va_list type nodes.  */
+tree sysv_va_list_type_node;
+tree ms_va_list_type_node;
+
+/* Prefix built by ASM_GENERATE_INTERNAL_LABEL.  */
+char internal_label_prefix[16];
+int internal_label_prefix_len;
+
+/* Fence to use after loop using movnt.  */
+tree x86_mfence;
+
+/* Register class used for passing given 64bit part of the argument.
+   These represent classes as documented by the PS ABI, with the exception
+   of SSESF, SSEDF classes, that are basically SSE class, just gcc will
+   use SF or DFmode move instead of DImode to avoid reformatting penalties.
+
+   Similarly we play games with INTEGERSI_CLASS to use cheaper SImode moves
+   whenever possible (upper half does contain padding).  */
+enum x86_64_reg_class
+  {
+    X86_64_NO_CLASS,
+    X86_64_INTEGER_CLASS,
+    X86_64_INTEGERSI_CLASS,
+    X86_64_SSE_CLASS,
+    X86_64_SSESF_CLASS,
+    X86_64_SSEDF_CLASS,
+    X86_64_SSEUP_CLASS,
+    X86_64_X87_CLASS,
+    X86_64_X87UP_CLASS,
+    X86_64_COMPLEX_X87_CLASS,
+    X86_64_MEMORY_CLASS
+  };
+
+#define MAX_CLASSES 8
+
+/* Table of constants used by fldpi, fldln2, etc....  */
+static REAL_VALUE_TYPE ext_80387_constants_table [5];
+static bool ext_80387_constants_init;
+
+
+static rtx ix86_function_value (const_tree, const_tree, bool);
+static bool ix86_function_value_regno_p (const unsigned int);
+static unsigned int ix86_function_arg_boundary (machine_mode,
+						const_tree);
+static rtx ix86_static_chain (const_tree, bool);
+static int ix86_function_regparm (const_tree, const_tree);
+static void ix86_compute_frame_layout (void);
+static tree ix86_canonical_va_list_type (tree);
+static unsigned int split_stack_prologue_scratch_regno (void);
+static bool i386_asm_output_addr_const_extra (FILE *, rtx);
+
+static bool ix86_can_inline_p (tree, tree);
+static unsigned int ix86_minimum_incoming_stack_boundary (bool);
+
+
+/* Whether -mtune= or -march= were specified */
+int ix86_tune_defaulted;
+int ix86_arch_specified;
+
+/* Return true if a red-zone is in use.  We can't use red-zone when
+   there are local indirect jumps, like "indirect_jump" or "tablejump",
+   which jumps to another place in the function, since "call" in the
+   indirect thunk pushes the return address onto stack, destroying
+   red-zone.
+
+   TODO: If we can reserve the first 2 WORDs, for PUSH and, another
+   for CALL, in red-zone, we can allow local indirect jumps with
+   indirect thunk.  */
+
+bool
+ix86_using_red_zone (void)
+{
+  return (TARGET_RED_ZONE
+	  && !TARGET_64BIT_MS_ABI
+	  && (!cfun->machine->has_local_indirect_jump
+	      || cfun->machine->indirect_branch_type == indirect_branch_keep));
+}
+
+/* Return true, if profiling code should be emitted before
+   prologue. Otherwise it returns false.
+   Note: For x86 with "hotfix" it is sorried.  */
+static bool
+ix86_profile_before_prologue (void)
+{
+  return flag_fentry != 0;
+}
+
+/* Update register usage after having seen the compiler flags.  */
+
+static void
+ix86_conditional_register_usage (void)
+{
+  int i, c_mask;
+
+  /* If there are no caller-saved registers, preserve all registers.
+     except fixed_regs and registers used for function return value
+     since aggregate_value_p checks call_used_regs[regno] on return
+     value.  */
+  if (cfun && cfun->machine->no_caller_saved_registers)
+    for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
+      if (!fixed_regs[i] && !ix86_function_value_regno_p (i))
+	call_used_regs[i] = 0;
+
+  /* For 32-bit targets, disable the REX registers.  */
+  if (! TARGET_64BIT)
+    {
+      for (i = FIRST_REX_INT_REG; i <= LAST_REX_INT_REG; i++)
+	CLEAR_HARD_REG_BIT (accessible_reg_set, i);
+      for (i = FIRST_REX_SSE_REG; i <= LAST_REX_SSE_REG; i++)
+	CLEAR_HARD_REG_BIT (accessible_reg_set, i);
+      for (i = FIRST_EXT_REX_SSE_REG; i <= LAST_EXT_REX_SSE_REG; i++)
+	CLEAR_HARD_REG_BIT (accessible_reg_set, i);
+    }
+
+  /*  See the definition of CALL_USED_REGISTERS in i386.h.  */
+  c_mask = CALL_USED_REGISTERS_MASK (TARGET_64BIT_MS_ABI);
+  
+  CLEAR_HARD_REG_SET (reg_class_contents[(int)CLOBBERED_REGS]);
+
+  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
+    {
+      /* Set/reset conditionally defined registers from
+	 CALL_USED_REGISTERS initializer.  */
+      if (call_used_regs[i] > 1)
+	call_used_regs[i] = !!(call_used_regs[i] & c_mask);
+
+      /* Calculate registers of CLOBBERED_REGS register set
+	 as call used registers from GENERAL_REGS register set.  */
+      if (TEST_HARD_REG_BIT (reg_class_contents[(int)GENERAL_REGS], i)
+	  && call_used_regs[i])
+	SET_HARD_REG_BIT (reg_class_contents[(int)CLOBBERED_REGS], i);
+    }
+
+  /* If MMX is disabled, disable the registers.  */
+  if (! TARGET_MMX)
+    accessible_reg_set &= ~reg_class_contents[MMX_REGS];
+
+  /* If SSE is disabled, disable the registers.  */
+  if (! TARGET_SSE)
+    accessible_reg_set &= ~reg_class_contents[ALL_SSE_REGS];
+
+  /* If the FPU is disabled, disable the registers.  */
+  if (! (TARGET_80387 || TARGET_FLOAT_RETURNS_IN_80387))
+    accessible_reg_set &= ~reg_class_contents[FLOAT_REGS];
+
+  /* If AVX512F is disabled, disable the registers.  */
+  if (! TARGET_AVX512F)
+    {
+      for (i = FIRST_EXT_REX_SSE_REG; i <= LAST_EXT_REX_SSE_REG; i++)
+	CLEAR_HARD_REG_BIT (accessible_reg_set, i);
+
+      accessible_reg_set &= ~reg_class_contents[ALL_MASK_REGS];
+    }
+}
+
+/* Canonicalize a comparison from one we don't have to one we do have.  */
+
+static void
+ix86_canonicalize_comparison (int *code, rtx *op0, rtx *op1,
+			      bool op0_preserve_value)
+{
+  /* The order of operands in x87 ficom compare is forced by combine in
+     simplify_comparison () function. Float operator is treated as RTX_OBJ
+     with a precedence over other operators and is always put in the first
+     place. Swap condition and operands to match ficom instruction.  */
+  if (!op0_preserve_value
+      && GET_CODE (*op0) == FLOAT && MEM_P (XEXP (*op0, 0)) && REG_P (*op1))
+    {
+      enum rtx_code scode = swap_condition ((enum rtx_code) *code);
+
+      /* We are called only for compares that are split to SAHF instruction.
+	 Ensure that we have setcc/jcc insn for the swapped condition.  */
+      if (ix86_fp_compare_code_to_integer (scode) != UNKNOWN)
+	{
+	  std::swap (*op0, *op1);
+	  *code = (int) scode;
+	}
+    }
+}
+
+
+/* Hook to determine if one function can safely inline another.  */
+
+static bool
+ix86_can_inline_p (tree caller, tree callee)
+{
+  tree caller_tree = DECL_FUNCTION_SPECIFIC_TARGET (caller);
+  tree callee_tree = DECL_FUNCTION_SPECIFIC_TARGET (callee);
+
+  /* Changes of those flags can be tolerated for always inlines. Lets hope
+     user knows what he is doing.  */
+  const unsigned HOST_WIDE_INT always_inline_safe_mask
+	 = (MASK_USE_8BIT_IDIV | MASK_ACCUMULATE_OUTGOING_ARGS
+	    | MASK_NO_ALIGN_STRINGOPS | MASK_AVX256_SPLIT_UNALIGNED_LOAD
+	    | MASK_AVX256_SPLIT_UNALIGNED_STORE | MASK_CLD
+	    | MASK_NO_FANCY_MATH_387 | MASK_IEEE_FP | MASK_INLINE_ALL_STRINGOPS
+	    | MASK_INLINE_STRINGOPS_DYNAMICALLY | MASK_RECIP | MASK_STACK_PROBE
+	    | MASK_STV | MASK_TLS_DIRECT_SEG_REFS | MASK_VZEROUPPER
+	    | MASK_NO_PUSH_ARGS | MASK_OMIT_LEAF_FRAME_POINTER);
+
+
+  if (!callee_tree)
+    callee_tree = target_option_default_node;
+  if (!caller_tree)
+    caller_tree = target_option_default_node;
+  if (callee_tree == caller_tree)
+    return true;
+
+  struct cl_target_option *caller_opts = TREE_TARGET_OPTION (caller_tree);
+  struct cl_target_option *callee_opts = TREE_TARGET_OPTION (callee_tree);
+  bool ret = false;
+  bool always_inline
+    = (DECL_DISREGARD_INLINE_LIMITS (callee)
+       && lookup_attribute ("always_inline",
+			    DECL_ATTRIBUTES (callee)));
+
+  cgraph_node *callee_node = cgraph_node::get (callee);
+  /* Callee's isa options should be a subset of the caller's, i.e. a SSE4
+     function can inline a SSE2 function but a SSE2 function can't inline
+     a SSE4 function.  */
+  if (((caller_opts->x_ix86_isa_flags & callee_opts->x_ix86_isa_flags)
+       != callee_opts->x_ix86_isa_flags)
+      || ((caller_opts->x_ix86_isa_flags2 & callee_opts->x_ix86_isa_flags2)
+	  != callee_opts->x_ix86_isa_flags2))
+    ret = false;
+
+  /* See if we have the same non-isa options.  */
+  else if ((!always_inline
+	    && caller_opts->x_target_flags != callee_opts->x_target_flags)
+	   || (caller_opts->x_target_flags & ~always_inline_safe_mask)
+	       != (callee_opts->x_target_flags & ~always_inline_safe_mask))
+    ret = false;
+
+  /* See if arch, tune, etc. are the same.  */
+  else if (caller_opts->arch != callee_opts->arch)
+    ret = false;
+
+  else if (!always_inline && caller_opts->tune != callee_opts->tune)
+    ret = false;
+
+  else if (caller_opts->x_ix86_fpmath != callee_opts->x_ix86_fpmath
+	   /* If the calle doesn't use FP expressions differences in
+	      ix86_fpmath can be ignored.  We are called from FEs
+	      for multi-versioning call optimization, so beware of
+	      ipa_fn_summaries not available.  */
+	   && (! ipa_fn_summaries
+	       || ipa_fn_summaries->get (callee_node) == NULL
+	       || ipa_fn_summaries->get (callee_node)->fp_expressions))
+    ret = false;
+
+  else if (!always_inline
+	   && caller_opts->branch_cost != callee_opts->branch_cost)
+    ret = false;
+
+  else
+    ret = true;
+
+  return ret;
+}
+
+/* Return true if this goes in large data/bss.  */
+
+static bool
+ix86_in_large_data_p (tree exp)
+{
+  if (ix86_cmodel != CM_MEDIUM && ix86_cmodel != CM_MEDIUM_PIC)
+    return false;
+
+  if (exp == NULL_TREE)
+    return false;
+
+  /* Functions are never large data.  */
+  if (TREE_CODE (exp) == FUNCTION_DECL)
+    return false;
+
+  /* Automatic variables are never large data.  */
+  if (VAR_P (exp) && !is_global_var (exp))
+    return false;
+
+  if (VAR_P (exp) && DECL_SECTION_NAME (exp))
+    {
+      const char *section = DECL_SECTION_NAME (exp);
+      if (strcmp (section, ".ldata") == 0
+	  || strcmp (section, ".lbss") == 0)
+	return true;
+      return false;
+    }
+  else
+    {
+      HOST_WIDE_INT size = int_size_in_bytes (TREE_TYPE (exp));
+
+      /* If this is an incomplete type with size 0, then we can't put it
+	 in data because it might be too big when completed.  Also,
+	 int_size_in_bytes returns -1 if size can vary or is larger than
+	 an integer in which case also it is safer to assume that it goes in
+	 large data.  */
+      if (size <= 0 || size > ix86_section_threshold)
+	return true;
+    }
+
+  return false;
+}
+
+/* i386-specific section flag to mark large sections.  */
+#define SECTION_LARGE SECTION_MACH_DEP
+
+/* Switch to the appropriate section for output of DECL.
+   DECL is either a `VAR_DECL' node or a constant of some sort.
+   RELOC indicates whether forming the initial value of DECL requires
+   link-time relocations.  */
+
+ATTRIBUTE_UNUSED static section *
+x86_64_elf_select_section (tree decl, int reloc,
+			   unsigned HOST_WIDE_INT align)
+{
+  if (ix86_in_large_data_p (decl))
+    {
+      const char *sname = NULL;
+      unsigned int flags = SECTION_WRITE | SECTION_LARGE;
+      switch (categorize_decl_for_section (decl, reloc))
+	{
+	case SECCAT_DATA:
+	  sname = ".ldata";
+	  break;
+	case SECCAT_DATA_REL:
+	  sname = ".ldata.rel";
+	  break;
+	case SECCAT_DATA_REL_LOCAL:
+	  sname = ".ldata.rel.local";
+	  break;
+	case SECCAT_DATA_REL_RO:
+	  sname = ".ldata.rel.ro";
+	  break;
+	case SECCAT_DATA_REL_RO_LOCAL:
+	  sname = ".ldata.rel.ro.local";
+	  break;
+	case SECCAT_BSS:
+	  sname = ".lbss";
+	  flags |= SECTION_BSS;
+	  break;
+	case SECCAT_RODATA:
+	case SECCAT_RODATA_MERGE_STR:
+	case SECCAT_RODATA_MERGE_STR_INIT:
+	case SECCAT_RODATA_MERGE_CONST:
+	  sname = ".lrodata";
+	  flags &= ~SECTION_WRITE;
+	  break;
+	case SECCAT_SRODATA:
+	case SECCAT_SDATA:
+	case SECCAT_SBSS:
+	  gcc_unreachable ();
+	case SECCAT_TEXT:
+	case SECCAT_TDATA:
+	case SECCAT_TBSS:
+	  /* We don't split these for medium model.  Place them into
+	     default sections and hope for best.  */
+	  break;
+	}
+      if (sname)
+	{
+	  /* We might get called with string constants, but get_named_section
+	     doesn't like them as they are not DECLs.  Also, we need to set
+	     flags in that case.  */
+	  if (!DECL_P (decl))
+	    return get_section (sname, flags, NULL);
+	  return get_named_section (decl, sname, reloc);
+	}
+    }
+  return default_elf_select_section (decl, reloc, align);
+}
+
+/* Select a set of attributes for section NAME based on the properties
+   of DECL and whether or not RELOC indicates that DECL's initializer
+   might contain runtime relocations.  */
+
+static unsigned int ATTRIBUTE_UNUSED
+x86_64_elf_section_type_flags (tree decl, const char *name, int reloc)
+{
+  unsigned int flags = default_section_type_flags (decl, name, reloc);
+
+  if (ix86_in_large_data_p (decl))
+    flags |= SECTION_LARGE;
+
+  if (decl == NULL_TREE
+      && (strcmp (name, ".ldata.rel.ro") == 0
+	  || strcmp (name, ".ldata.rel.ro.local") == 0))
+    flags |= SECTION_RELRO;
+
+  if (strcmp (name, ".lbss") == 0
+      || strncmp (name, ".lbss.", sizeof (".lbss.") - 1) == 0
+      || strncmp (name, ".gnu.linkonce.lb.",
+		  sizeof (".gnu.linkonce.lb.") - 1) == 0)
+    flags |= SECTION_BSS;
+
+  return flags;
+}
+
+/* Build up a unique section name, expressed as a
+   STRING_CST node, and assign it to DECL_SECTION_NAME (decl).
+   RELOC indicates whether the initial value of EXP requires
+   link-time relocations.  */
+
+static void ATTRIBUTE_UNUSED
+x86_64_elf_unique_section (tree decl, int reloc)
+{
+  if (ix86_in_large_data_p (decl))
+    {
+      const char *prefix = NULL;
+      /* We only need to use .gnu.linkonce if we don't have COMDAT groups.  */
+      bool one_only = DECL_COMDAT_GROUP (decl) && !HAVE_COMDAT_GROUP;
+
+      switch (categorize_decl_for_section (decl, reloc))
+	{
+	case SECCAT_DATA:
+	case SECCAT_DATA_REL:
+	case SECCAT_DATA_REL_LOCAL:
+	case SECCAT_DATA_REL_RO:
+	case SECCAT_DATA_REL_RO_LOCAL:
+          prefix = one_only ? ".ld" : ".ldata";
+	  break;
+	case SECCAT_BSS:
+          prefix = one_only ? ".lb" : ".lbss";
+	  break;
+	case SECCAT_RODATA:
+	case SECCAT_RODATA_MERGE_STR:
+	case SECCAT_RODATA_MERGE_STR_INIT:
+	case SECCAT_RODATA_MERGE_CONST:
+          prefix = one_only ? ".lr" : ".lrodata";
+	  break;
+	case SECCAT_SRODATA:
+	case SECCAT_SDATA:
+	case SECCAT_SBSS:
+	  gcc_unreachable ();
+	case SECCAT_TEXT:
+	case SECCAT_TDATA:
+	case SECCAT_TBSS:
+	  /* We don't split these for medium model.  Place them into
+	     default sections and hope for best.  */
+	  break;
+	}
+      if (prefix)
+	{
+	  const char *name, *linkonce;
+	  char *string;
+
+	  name = IDENTIFIER_POINTER (DECL_ASSEMBLER_NAME (decl));
+	  name = targetm.strip_name_encoding (name);
+
+	  /* If we're using one_only, then there needs to be a .gnu.linkonce
+     	     prefix to the section name.  */
+	  linkonce = one_only ? ".gnu.linkonce" : "";
+
+	  string = ACONCAT ((linkonce, prefix, ".", name, NULL));
+
+	  set_decl_section_name (decl, string);
+	  return;
+	}
+    }
+  default_unique_section (decl, reloc);
+}
+
+#ifdef COMMON_ASM_OP
+
+#ifndef LARGECOMM_SECTION_ASM_OP
+#define LARGECOMM_SECTION_ASM_OP "\t.largecomm\t"
+#endif
+
+/* This says how to output assembler code to declare an
+   uninitialized external linkage data object.
+
+   For medium model x86-64 we need to use LARGECOMM_SECTION_ASM_OP opcode for
+   large objects.  */
+void
+x86_elf_aligned_decl_common (FILE *file, tree decl,
+			const char *name, unsigned HOST_WIDE_INT size,
+			int align)
+{
+  if ((ix86_cmodel == CM_MEDIUM || ix86_cmodel == CM_MEDIUM_PIC)
+      && size > (unsigned int)ix86_section_threshold)
+    {
+      switch_to_section (get_named_section (decl, ".lbss", 0));
+      fputs (LARGECOMM_SECTION_ASM_OP, file);
+    }
+  else
+    fputs (COMMON_ASM_OP, file);
+  assemble_name (file, name);
+  fprintf (file, "," HOST_WIDE_INT_PRINT_UNSIGNED ",%u\n",
+	   size, align / BITS_PER_UNIT);
+}
+#endif
+
+/* Utility function for targets to use in implementing
+   ASM_OUTPUT_ALIGNED_BSS.  */
+
+void
+x86_output_aligned_bss (FILE *file, tree decl, const char *name,
+		       	unsigned HOST_WIDE_INT size, int align)
+{
+  if ((ix86_cmodel == CM_MEDIUM || ix86_cmodel == CM_MEDIUM_PIC)
+      && size > (unsigned int)ix86_section_threshold)
+    switch_to_section (get_named_section (decl, ".lbss", 0));
+  else
+    switch_to_section (bss_section);
+  ASM_OUTPUT_ALIGN (file, floor_log2 (align / BITS_PER_UNIT));
+#ifdef ASM_DECLARE_OBJECT_NAME
+  last_assemble_variable_decl = decl;
+  ASM_DECLARE_OBJECT_NAME (file, name, decl);
+#else
+  /* Standard thing is just output label for the object.  */
+  ASM_OUTPUT_LABEL (file, name);
+#endif /* ASM_DECLARE_OBJECT_NAME */
+  ASM_OUTPUT_SKIP (file, size ? size : 1);
+}
+
+/* Decide whether we must probe the stack before any space allocation
+   on this target.  It's essentially TARGET_STACK_PROBE except when
+   -fstack-check causes the stack to be already probed differently.  */
+
+bool
+ix86_target_stack_probe (void)
+{
+  /* Do not probe the stack twice if static stack checking is enabled.  */
+  if (flag_stack_check == STATIC_BUILTIN_STACK_CHECK)
+    return false;
+
+  return TARGET_STACK_PROBE;
+}
+
+/* Decide whether we can make a sibling call to a function.  DECL is the
+   declaration of the function being targeted by the call and EXP is the
+   CALL_EXPR representing the call.  */
+
+static bool
+ix86_function_ok_for_sibcall (tree decl, tree exp)
+{
+  tree type, decl_or_type;
+  rtx a, b;
+  bool bind_global = decl && !targetm.binds_local_p (decl);
+
+  if (ix86_function_naked (current_function_decl))
+    return false;
+
+  /* Sibling call isn't OK if there are no caller-saved registers
+     since all registers must be preserved before return.  */
+  if (cfun->machine->no_caller_saved_registers)
+    return false;
+
+  /* If we are generating position-independent code, we cannot sibcall
+     optimize direct calls to global functions, as the PLT requires
+     %ebx be live. (Darwin does not have a PLT.)  */
+  if (!TARGET_MACHO
+      && !TARGET_64BIT
+      && flag_pic
+      && flag_plt
+      && bind_global)
+    return false;
+
+  /* If we need to align the outgoing stack, then sibcalling would
+     unalign the stack, which may break the called function.  */
+  if (ix86_minimum_incoming_stack_boundary (true)
+      < PREFERRED_STACK_BOUNDARY)
+    return false;
+
+  if (decl)
+    {
+      decl_or_type = decl;
+      type = TREE_TYPE (decl);
+    }
+  else
+    {
+      /* We're looking at the CALL_EXPR, we need the type of the function.  */
+      type = CALL_EXPR_FN (exp);		/* pointer expression */
+      type = TREE_TYPE (type);			/* pointer type */
+      type = TREE_TYPE (type);			/* function type */
+      decl_or_type = type;
+    }
+
+  /* If outgoing reg parm stack space changes, we cannot do sibcall.  */
+  if ((OUTGOING_REG_PARM_STACK_SPACE (type)
+       != OUTGOING_REG_PARM_STACK_SPACE (TREE_TYPE (current_function_decl)))
+      || (REG_PARM_STACK_SPACE (decl_or_type)
+	  != REG_PARM_STACK_SPACE (current_function_decl)))
+    {
+      maybe_complain_about_tail_call (exp,
+				      "inconsistent size of stack space"
+				      " allocated for arguments which are"
+				      " passed in registers");
+      return false;
+    }
+
+  /* Check that the return value locations are the same.  Like
+     if we are returning floats on the 80387 register stack, we cannot
+     make a sibcall from a function that doesn't return a float to a
+     function that does or, conversely, from a function that does return
+     a float to a function that doesn't; the necessary stack adjustment
+     would not be executed.  This is also the place we notice
+     differences in the return value ABI.  Note that it is ok for one
+     of the functions to have void return type as long as the return
+     value of the other is passed in a register.  */
+  a = ix86_function_value (TREE_TYPE (exp), decl_or_type, false);
+  b = ix86_function_value (TREE_TYPE (DECL_RESULT (cfun->decl)),
+			   cfun->decl, false);
+  if (STACK_REG_P (a) || STACK_REG_P (b))
+    {
+      if (!rtx_equal_p (a, b))
+	return false;
+    }
+  else if (VOID_TYPE_P (TREE_TYPE (DECL_RESULT (cfun->decl))))
+    ;
+  else if (!rtx_equal_p (a, b))
+    return false;
+
+  if (TARGET_64BIT)
+    {
+      /* The SYSV ABI has more call-clobbered registers;
+	 disallow sibcalls from MS to SYSV.  */
+      if (cfun->machine->call_abi == MS_ABI
+	  && ix86_function_type_abi (type) == SYSV_ABI)
+	return false;
+    }
+  else
+    {
+      /* If this call is indirect, we'll need to be able to use a
+	 call-clobbered register for the address of the target function.
+	 Make sure that all such registers are not used for passing
+	 parameters.  Note that DLLIMPORT functions and call to global
+	 function via GOT slot are indirect.  */
+      if (!decl
+	  || (bind_global && flag_pic && !flag_plt)
+	  || (TARGET_DLLIMPORT_DECL_ATTRIBUTES && DECL_DLLIMPORT_P (decl))
+	  || flag_force_indirect_call)
+	{
+	  /* Check if regparm >= 3 since arg_reg_available is set to
+	     false if regparm == 0.  If regparm is 1 or 2, there is
+	     always a call-clobbered register available.
+
+	     ??? The symbol indirect call doesn't need a call-clobbered
+	     register.  But we don't know if this is a symbol indirect
+	     call or not here.  */
+	  if (ix86_function_regparm (type, decl) >= 3
+	      && !cfun->machine->arg_reg_available)
+	    return false;
+	}
+    }
+
+  /* Otherwise okay.  That also includes certain types of indirect calls.  */
+  return true;
+}
+
+/* This function determines from TYPE the calling-convention.  */
+
+unsigned int
+ix86_get_callcvt (const_tree type)
+{
+  unsigned int ret = 0;
+  bool is_stdarg;
+  tree attrs;
+
+  if (TARGET_64BIT)
+    return IX86_CALLCVT_CDECL;
+
+  attrs = TYPE_ATTRIBUTES (type);
+  if (attrs != NULL_TREE)
+    {
+      if (lookup_attribute ("cdecl", attrs))
+	ret |= IX86_CALLCVT_CDECL;
+      else if (lookup_attribute ("stdcall", attrs))
+	ret |= IX86_CALLCVT_STDCALL;
+      else if (lookup_attribute ("fastcall", attrs))
+	ret |= IX86_CALLCVT_FASTCALL;
+      else if (lookup_attribute ("thiscall", attrs))
+	ret |= IX86_CALLCVT_THISCALL;
+
+      /* Regparam isn't allowed for thiscall and fastcall.  */
+      if ((ret & (IX86_CALLCVT_THISCALL | IX86_CALLCVT_FASTCALL)) == 0)
+	{
+	  if (lookup_attribute ("regparm", attrs))
+	    ret |= IX86_CALLCVT_REGPARM;
+	  if (lookup_attribute ("sseregparm", attrs))
+	    ret |= IX86_CALLCVT_SSEREGPARM;
+	}
+
+      if (IX86_BASE_CALLCVT(ret) != 0)
+	return ret;
+    }
+
+  is_stdarg = stdarg_p (type);
+  if (TARGET_RTD && !is_stdarg)
+    return IX86_CALLCVT_STDCALL | ret;
+
+  if (ret != 0
+      || is_stdarg
+      || TREE_CODE (type) != METHOD_TYPE
+      || ix86_function_type_abi (type) != MS_ABI)
+    return IX86_CALLCVT_CDECL | ret;
+
+  return IX86_CALLCVT_THISCALL;
+}
+
+/* Return 0 if the attributes for two types are incompatible, 1 if they
+   are compatible, and 2 if they are nearly compatible (which causes a
+   warning to be generated).  */
+
+static int
+ix86_comp_type_attributes (const_tree type1, const_tree type2)
+{
+  unsigned int ccvt1, ccvt2;
+
+  if (TREE_CODE (type1) != FUNCTION_TYPE
+      && TREE_CODE (type1) != METHOD_TYPE)
+    return 1;
+
+  ccvt1 = ix86_get_callcvt (type1);
+  ccvt2 = ix86_get_callcvt (type2);
+  if (ccvt1 != ccvt2)
+    return 0;
+  if (ix86_function_regparm (type1, NULL)
+      != ix86_function_regparm (type2, NULL))
+    return 0;
+
+  return 1;
+}
+
+/* Return the regparm value for a function with the indicated TYPE and DECL.
+   DECL may be NULL when calling function indirectly
+   or considering a libcall.  */
+
+static int
+ix86_function_regparm (const_tree type, const_tree decl)
+{
+  tree attr;
+  int regparm;
+  unsigned int ccvt;
+
+  if (TARGET_64BIT)
+    return (ix86_function_type_abi (type) == SYSV_ABI
+	    ? X86_64_REGPARM_MAX : X86_64_MS_REGPARM_MAX);
+  ccvt = ix86_get_callcvt (type);
+  regparm = ix86_regparm;
+
+  if ((ccvt & IX86_CALLCVT_REGPARM) != 0)
+    {
+      attr = lookup_attribute ("regparm", TYPE_ATTRIBUTES (type));
+      if (attr)
+	{
+	  regparm = TREE_INT_CST_LOW (TREE_VALUE (TREE_VALUE (attr)));
+	  return regparm;
+	}
+    }
+  else if ((ccvt & IX86_CALLCVT_FASTCALL) != 0)
+    return 2;
+  else if ((ccvt & IX86_CALLCVT_THISCALL) != 0)
+    return 1;
+
+  /* Use register calling convention for local functions when possible.  */
+  if (decl
+      && TREE_CODE (decl) == FUNCTION_DECL)
+    {
+      cgraph_node *target = cgraph_node::get (decl);
+      if (target)
+	target = target->function_symbol ();
+
+      /* Caller and callee must agree on the calling convention, so
+	 checking here just optimize means that with
+	 __attribute__((optimize (...))) caller could use regparm convention
+	 and callee not, or vice versa.  Instead look at whether the callee
+	 is optimized or not.  */
+      if (target && opt_for_fn (target->decl, optimize)
+	  && !(profile_flag && !flag_fentry))
+	{
+	  if (target->local && target->can_change_signature)
+	    {
+	      int local_regparm, globals = 0, regno;
+
+	      /* Make sure no regparm register is taken by a
+		 fixed register variable.  */
+	      for (local_regparm = 0; local_regparm < REGPARM_MAX;
+		   local_regparm++)
+		if (fixed_regs[local_regparm])
+		  break;
+
+	      /* We don't want to use regparm(3) for nested functions as
+		 these use a static chain pointer in the third argument.  */
+	      if (local_regparm == 3 && DECL_STATIC_CHAIN (target->decl))
+		local_regparm = 2;
+
+	      /* Save a register for the split stack.  */
+	      if (flag_split_stack)
+		{
+		  if (local_regparm == 3)
+		    local_regparm = 2;
+		  else if (local_regparm == 2
+			   && DECL_STATIC_CHAIN (target->decl))
+		    local_regparm = 1;
+		}
+
+	      /* Each fixed register usage increases register pressure,
+		 so less registers should be used for argument passing.
+		 This functionality can be overriden by an explicit
+		 regparm value.  */
+	      for (regno = AX_REG; regno <= DI_REG; regno++)
+		if (fixed_regs[regno])
+		  globals++;
+
+	      local_regparm
+		= globals < local_regparm ? local_regparm - globals : 0;
+
+	      if (local_regparm > regparm)
+		regparm = local_regparm;
+	    }
+	}
+    }
+
+  return regparm;
+}
+
+/* Return 1 or 2, if we can pass up to SSE_REGPARM_MAX SFmode (1) and
+   DFmode (2) arguments in SSE registers for a function with the
+   indicated TYPE and DECL.  DECL may be NULL when calling function
+   indirectly or considering a libcall.  Return -1 if any FP parameter
+   should be rejected by error.  This is used in siutation we imply SSE
+   calling convetion but the function is called from another function with
+   SSE disabled. Otherwise return 0.  */
+
+static int
+ix86_function_sseregparm (const_tree type, const_tree decl, bool warn)
+{
+  gcc_assert (!TARGET_64BIT);
+
+  /* Use SSE registers to pass SFmode and DFmode arguments if requested
+     by the sseregparm attribute.  */
+  if (TARGET_SSEREGPARM
+      || (type && lookup_attribute ("sseregparm", TYPE_ATTRIBUTES (type))))
+    {
+      if (!TARGET_SSE)
+	{
+	  if (warn)
+	    {
+	      if (decl)
+		error ("calling %qD with attribute sseregparm without "
+		       "SSE/SSE2 enabled", decl);
+	      else
+		error ("calling %qT with attribute sseregparm without "
+		       "SSE/SSE2 enabled", type);
+	    }
+	  return 0;
+	}
+
+      return 2;
+    }
+
+  if (!decl)
+    return 0;
+
+  cgraph_node *target = cgraph_node::get (decl);
+  if (target)
+    target = target->function_symbol ();
+
+  /* For local functions, pass up to SSE_REGPARM_MAX SFmode
+     (and DFmode for SSE2) arguments in SSE registers.  */
+  if (target
+      /* TARGET_SSE_MATH */
+      && (target_opts_for_fn (target->decl)->x_ix86_fpmath & FPMATH_SSE)
+      && opt_for_fn (target->decl, optimize)
+      && !(profile_flag && !flag_fentry))
+    {
+      if (target->local && target->can_change_signature)
+	{
+	  /* Refuse to produce wrong code when local function with SSE enabled
+	     is called from SSE disabled function.
+	     FIXME: We need a way to detect these cases cross-ltrans partition
+	     and avoid using SSE calling conventions on local functions called
+	     from function with SSE disabled.  For now at least delay the
+	     warning until we know we are going to produce wrong code.
+	     See PR66047  */
+	  if (!TARGET_SSE && warn)
+	    return -1;
+	  return TARGET_SSE2_P (target_opts_for_fn (target->decl)
+				->x_ix86_isa_flags) ? 2 : 1;
+	}
+    }
+
+  return 0;
+}
+
+/* Return true if EAX is live at the start of the function.  Used by
+   ix86_expand_prologue to determine if we need special help before
+   calling allocate_stack_worker.  */
+
+static bool
+ix86_eax_live_at_start_p (void)
+{
+  /* Cheat.  Don't bother working forward from ix86_function_regparm
+     to the function type to whether an actual argument is located in
+     eax.  Instead just look at cfg info, which is still close enough
+     to correct at this point.  This gives false positives for broken
+     functions that might use uninitialized data that happens to be
+     allocated in eax, but who cares?  */
+  return REGNO_REG_SET_P (df_get_live_out (ENTRY_BLOCK_PTR_FOR_FN (cfun)), 0);
+}
+
+static bool
+ix86_keep_aggregate_return_pointer (tree fntype)
+{
+  tree attr;
+
+  if (!TARGET_64BIT)
+    {
+      attr = lookup_attribute ("callee_pop_aggregate_return",
+			       TYPE_ATTRIBUTES (fntype));
+      if (attr)
+	return (TREE_INT_CST_LOW (TREE_VALUE (TREE_VALUE (attr))) == 0);
+
+      /* For 32-bit MS-ABI the default is to keep aggregate
+         return pointer.  */
+      if (ix86_function_type_abi (fntype) == MS_ABI)
+	return true;
+    }
+  return KEEP_AGGREGATE_RETURN_POINTER != 0;
+}
+
+/* Value is the number of bytes of arguments automatically
+   popped when returning from a subroutine call.
+   FUNDECL is the declaration node of the function (as a tree),
+   FUNTYPE is the data type of the function (as a tree),
+   or for a library call it is an identifier node for the subroutine name.
+   SIZE is the number of bytes of arguments passed on the stack.
+
+   On the 80386, the RTD insn may be used to pop them if the number
+     of args is fixed, but if the number is variable then the caller
+     must pop them all.  RTD can't be used for library calls now
+     because the library is compiled with the Unix compiler.
+   Use of RTD is a selectable option, since it is incompatible with
+   standard Unix calling sequences.  If the option is not selected,
+   the caller must always pop the args.
+
+   The attribute stdcall is equivalent to RTD on a per module basis.  */
+
+static poly_int64
+ix86_return_pops_args (tree fundecl, tree funtype, poly_int64 size)
+{
+  unsigned int ccvt;
+
+  /* None of the 64-bit ABIs pop arguments.  */
+  if (TARGET_64BIT)
+    return 0;
+
+  ccvt = ix86_get_callcvt (funtype);
+
+  if ((ccvt & (IX86_CALLCVT_STDCALL | IX86_CALLCVT_FASTCALL
+	       | IX86_CALLCVT_THISCALL)) != 0
+      && ! stdarg_p (funtype))
+    return size;
+
+  /* Lose any fake structure return argument if it is passed on the stack.  */
+  if (aggregate_value_p (TREE_TYPE (funtype), fundecl)
+      && !ix86_keep_aggregate_return_pointer (funtype))
+    {
+      int nregs = ix86_function_regparm (funtype, fundecl);
+      if (nregs == 0)
+	return GET_MODE_SIZE (Pmode);
+    }
+
+  return 0;
+}
+
+/* Implement the TARGET_LEGITIMATE_COMBINED_INSN hook.  */
+
+static bool
+ix86_legitimate_combined_insn (rtx_insn *insn)
+{
+  int i;
+
+  /* Check operand constraints in case hard registers were propagated
+     into insn pattern.  This check prevents combine pass from
+     generating insn patterns with invalid hard register operands.
+     These invalid insns can eventually confuse reload to error out
+     with a spill failure.  See also PRs 46829 and 46843.  */
+
+  gcc_assert (INSN_CODE (insn) >= 0);
+
+  extract_insn (insn);
+  preprocess_constraints (insn);
+
+  int n_operands = recog_data.n_operands;
+  int n_alternatives = recog_data.n_alternatives;
+  for (i = 0; i < n_operands; i++)
+    {
+      rtx op = recog_data.operand[i];
+      machine_mode mode = GET_MODE (op);
+      const operand_alternative *op_alt;
+      int offset = 0;
+      bool win;
+      int j;
+
+      /* A unary operator may be accepted by the predicate, but it
+	 is irrelevant for matching constraints.  */
+      if (UNARY_P (op))
+	op = XEXP (op, 0);
+
+      if (SUBREG_P (op))
+	{
+	  if (REG_P (SUBREG_REG (op))
+	      && REGNO (SUBREG_REG (op)) < FIRST_PSEUDO_REGISTER)
+	    offset = subreg_regno_offset (REGNO (SUBREG_REG (op)),
+					  GET_MODE (SUBREG_REG (op)),
+					  SUBREG_BYTE (op),
+					  GET_MODE (op));
+	  op = SUBREG_REG (op);
+	}
+
+      if (!(REG_P (op) && HARD_REGISTER_P (op)))
+	continue;
+
+      op_alt = recog_op_alt;
+
+      /* Operand has no constraints, anything is OK.  */
+      win = !n_alternatives;
+
+      alternative_mask preferred = get_preferred_alternatives (insn);
+      for (j = 0; j < n_alternatives; j++, op_alt += n_operands)
+	{
+	  if (!TEST_BIT (preferred, j))
+	    continue;
+	  if (op_alt[i].anything_ok
+	      || (op_alt[i].matches != -1
+		  && operands_match_p
+		  (recog_data.operand[i],
+		   recog_data.operand[op_alt[i].matches]))
+	      || reg_fits_class_p (op, op_alt[i].cl, offset, mode))
+	    {
+	      win = true;
+	      break;
+	    }
+	}
+
+      if (!win)
+	return false;
+    }
+
+  return true;
+}
+
+/* Implement the TARGET_ASAN_SHADOW_OFFSET hook.  */
+
+static unsigned HOST_WIDE_INT
+ix86_asan_shadow_offset (void)
+{
+  return SUBTARGET_SHADOW_OFFSET;
+}
+
+/* Argument support functions.  */
+
+/* Return true when register may be used to pass function parameters.  */
+bool
+ix86_function_arg_regno_p (int regno)
+{
+  int i;
+  enum calling_abi call_abi;
+  const int *parm_regs;
+
+  if (TARGET_SSE && SSE_REGNO_P (regno)
+      && regno < FIRST_SSE_REG + SSE_REGPARM_MAX)
+    return true;
+
+   if (!TARGET_64BIT)
+     return (regno < REGPARM_MAX
+	     || (TARGET_MMX && MMX_REGNO_P (regno)
+		 && regno < FIRST_MMX_REG + MMX_REGPARM_MAX));
+
+  /* TODO: The function should depend on current function ABI but
+     builtins.c would need updating then. Therefore we use the
+     default ABI.  */
+  call_abi = ix86_cfun_abi ();
+
+  /* RAX is used as hidden argument to va_arg functions.  */
+  if (call_abi == SYSV_ABI && regno == AX_REG)
+    return true;
+
+  if (call_abi == MS_ABI)
+    parm_regs = x86_64_ms_abi_int_parameter_registers;
+  else
+    parm_regs = x86_64_int_parameter_registers;
+
+  for (i = 0; i < (call_abi == MS_ABI
+		   ? X86_64_MS_REGPARM_MAX : X86_64_REGPARM_MAX); i++)
+    if (regno == parm_regs[i])
+      return true;
+  return false;
+}
+
+/* Return if we do not know how to pass ARG solely in registers.  */
+
+static bool
+ix86_must_pass_in_stack (const function_arg_info &arg)
+{
+  if (must_pass_in_stack_var_size_or_pad (arg))
+    return true;
+
+  /* For 32-bit, we want TImode aggregates to go on the stack.  But watch out!
+     The layout_type routine is crafty and tries to trick us into passing
+     currently unsupported vector types on the stack by using TImode.  */
+  return (!TARGET_64BIT && arg.mode == TImode
+	  && arg.type && TREE_CODE (arg.type) != VECTOR_TYPE);
+}
+
+/* It returns the size, in bytes, of the area reserved for arguments passed
+   in registers for the function represented by fndecl dependent to the used
+   abi format.  */
+int
+ix86_reg_parm_stack_space (const_tree fndecl)
+{
+  enum calling_abi call_abi = SYSV_ABI;
+  if (fndecl != NULL_TREE && TREE_CODE (fndecl) == FUNCTION_DECL)
+    call_abi = ix86_function_abi (fndecl);
+  else
+    call_abi = ix86_function_type_abi (fndecl);
+  if (TARGET_64BIT && call_abi == MS_ABI)
+    return 32;
+  return 0;
+}
+
+/* We add this as a workaround in order to use libc_has_function
+   hook in i386.md.  */
+bool
+ix86_libc_has_function (enum function_class fn_class)
+{
+  return targetm.libc_has_function (fn_class, NULL_TREE);
+}
+
+/* Returns value SYSV_ABI, MS_ABI dependent on fntype,
+   specifying the call abi used.  */
+enum calling_abi
+ix86_function_type_abi (const_tree fntype)
+{
+  enum calling_abi abi = ix86_abi;
+
+  if (fntype == NULL_TREE || TYPE_ATTRIBUTES (fntype) == NULL_TREE)
+    return abi;
+
+  if (abi == SYSV_ABI
+      && lookup_attribute ("ms_abi", TYPE_ATTRIBUTES (fntype)))
+    {
+      static int warned;
+      if (TARGET_X32 && !warned)
+	{
+	  error ("X32 does not support %<ms_abi%> attribute");
+	  warned = 1;
+	}
+
+      abi = MS_ABI;
+    }
+  else if (abi == MS_ABI
+	   && lookup_attribute ("sysv_abi", TYPE_ATTRIBUTES (fntype)))
+    abi = SYSV_ABI;
+
+  return abi;
+}
+
+enum calling_abi
+ix86_function_abi (const_tree fndecl)
+{
+  return fndecl ? ix86_function_type_abi (TREE_TYPE (fndecl)) : ix86_abi;
+}
+
+/* Returns value SYSV_ABI, MS_ABI dependent on cfun,
+   specifying the call abi used.  */
+enum calling_abi
+ix86_cfun_abi (void)
+{
+  return cfun ? cfun->machine->call_abi : ix86_abi;
+}
+
+bool
+ix86_function_ms_hook_prologue (const_tree fn)
+{
+  if (fn && lookup_attribute ("ms_hook_prologue", DECL_ATTRIBUTES (fn)))
+    {
+      if (decl_function_context (fn) != NULL_TREE)
+	error_at (DECL_SOURCE_LOCATION (fn),
+		  "%<ms_hook_prologue%> attribute is not compatible "
+		  "with nested function");
+      else
+        return true;
+    }
+  return false;
+}
+
+bool
+ix86_function_naked (const_tree fn)
+{
+  if (fn && lookup_attribute ("naked", DECL_ATTRIBUTES (fn)))
+    return true;
+
+  return false;
+}
+
+/* Write the extra assembler code needed to declare a function properly.  */
+
+void
+ix86_asm_output_function_label (FILE *asm_out_file, const char *fname,
+				tree decl)
+{
+  bool is_ms_hook = ix86_function_ms_hook_prologue (decl);
+
+  if (cfun)
+    cfun->machine->function_label_emitted = true;
+
+  if (is_ms_hook)
+    {
+      int i, filler_count = (TARGET_64BIT ? 32 : 16);
+      unsigned int filler_cc = 0xcccccccc;
+
+      for (i = 0; i < filler_count; i += 4)
+        fprintf (asm_out_file, ASM_LONG " %#x\n", filler_cc);
+    }
+
+#ifdef SUBTARGET_ASM_UNWIND_INIT
+  SUBTARGET_ASM_UNWIND_INIT (asm_out_file);
+#endif
+
+  ASM_OUTPUT_LABEL (asm_out_file, fname);
+
+  /* Output magic byte marker, if hot-patch attribute is set.  */
+  if (is_ms_hook)
+    {
+      if (TARGET_64BIT)
+	{
+	  /* leaq [%rsp + 0], %rsp  */
+	  fputs (ASM_BYTE "0x48, 0x8d, 0xa4, 0x24, 0x00, 0x00, 0x00, 0x00\n",
+		 asm_out_file);
+	}
+      else
+	{
+          /* movl.s %edi, %edi
+	     push   %ebp
+	     movl.s %esp, %ebp */
+	  fputs (ASM_BYTE "0x8b, 0xff, 0x55, 0x8b, 0xec\n", asm_out_file);
+	}
+    }
+}
+
+/* Implementation of call abi switching target hook. Specific to FNDECL
+   the specific call register sets are set.  See also
+   ix86_conditional_register_usage for more details.  */
+void
+ix86_call_abi_override (const_tree fndecl)
+{
+  cfun->machine->call_abi = ix86_function_abi (fndecl);
+}
+
+/* Return 1 if pseudo register should be created and used to hold
+   GOT address for PIC code.  */
+bool
+ix86_use_pseudo_pic_reg (void)
+{
+  if ((TARGET_64BIT
+       && (ix86_cmodel == CM_SMALL_PIC
+	   || TARGET_PECOFF))
+      || !flag_pic)
+    return false;
+  return true;
+}
+
+/* Initialize large model PIC register.  */
+
+static void
+ix86_init_large_pic_reg (unsigned int tmp_regno)
+{
+  rtx_code_label *label;
+  rtx tmp_reg;
+
+  gcc_assert (Pmode == DImode);
+  label = gen_label_rtx ();
+  emit_label (label);
+  LABEL_PRESERVE_P (label) = 1;
+  tmp_reg = gen_rtx_REG (Pmode, tmp_regno);
+  gcc_assert (REGNO (pic_offset_table_rtx) != tmp_regno);
+  emit_insn (gen_set_rip_rex64 (pic_offset_table_rtx,
+				label));
+  emit_insn (gen_set_got_offset_rex64 (tmp_reg, label));
+  emit_insn (gen_add2_insn (pic_offset_table_rtx, tmp_reg));
+  const char *name = LABEL_NAME (label);
+  PUT_CODE (label, NOTE);
+  NOTE_KIND (label) = NOTE_INSN_DELETED_LABEL;
+  NOTE_DELETED_LABEL_NAME (label) = name;
+}
+
+/* Create and initialize PIC register if required.  */
+static void
+ix86_init_pic_reg (void)
+{
+  edge entry_edge;
+  rtx_insn *seq;
+
+  if (!ix86_use_pseudo_pic_reg ())
+    return;
+
+  start_sequence ();
+
+  if (TARGET_64BIT)
+    {
+      if (ix86_cmodel == CM_LARGE_PIC)
+	ix86_init_large_pic_reg (R11_REG);
+      else
+	emit_insn (gen_set_got_rex64 (pic_offset_table_rtx));
+    }
+  else
+    {
+      /*  If there is future mcount call in the function it is more profitable
+	  to emit SET_GOT into ABI defined REAL_PIC_OFFSET_TABLE_REGNUM.  */
+      rtx reg = crtl->profile
+		? gen_rtx_REG (Pmode, REAL_PIC_OFFSET_TABLE_REGNUM)
+		: pic_offset_table_rtx;
+      rtx_insn *insn = emit_insn (gen_set_got (reg));
+      RTX_FRAME_RELATED_P (insn) = 1;
+      if (crtl->profile)
+        emit_move_insn (pic_offset_table_rtx, reg);
+      add_reg_note (insn, REG_CFA_FLUSH_QUEUE, NULL_RTX);
+    }
+
+  seq = get_insns ();
+  end_sequence ();
+
+  entry_edge = single_succ_edge (ENTRY_BLOCK_PTR_FOR_FN (cfun));
+  insert_insn_on_edge (seq, entry_edge);
+  commit_one_edge_insertion (entry_edge);
+}
+
+/* Initialize a variable CUM of type CUMULATIVE_ARGS
+   for a call to a function whose data type is FNTYPE.
+   For a library call, FNTYPE is 0.  */
+
+void
+init_cumulative_args (CUMULATIVE_ARGS *cum,  /* Argument info to initialize */
+		      tree fntype,	/* tree ptr for function decl */
+		      rtx libname,	/* SYMBOL_REF of library name or 0 */
+		      tree fndecl,
+		      int caller)
+{
+  struct cgraph_node *local_info_node = NULL;
+  struct cgraph_node *target = NULL;
+
+  /* Set silent_p to false to raise an error for invalid calls when
+     expanding function body.  */
+  cfun->machine->silent_p = false;
+
+  memset (cum, 0, sizeof (*cum));
+
+  if (fndecl)
+    {
+      target = cgraph_node::get (fndecl);
+      if (target)
+	{
+	  target = target->function_symbol ();
+	  local_info_node = cgraph_node::local_info_node (target->decl);
+	  cum->call_abi = ix86_function_abi (target->decl);
+	}
+      else
+	cum->call_abi = ix86_function_abi (fndecl);
+    }
+  else
+    cum->call_abi = ix86_function_type_abi (fntype);
+
+  cum->caller = caller;
+
+  /* Set up the number of registers to use for passing arguments.  */
+  cum->nregs = ix86_regparm;
+  if (TARGET_64BIT)
+    {
+      cum->nregs = (cum->call_abi == SYSV_ABI
+                   ? X86_64_REGPARM_MAX
+                   : X86_64_MS_REGPARM_MAX);
+    }
+  if (TARGET_SSE)
+    {
+      cum->sse_nregs = SSE_REGPARM_MAX;
+      if (TARGET_64BIT)
+        {
+          cum->sse_nregs = (cum->call_abi == SYSV_ABI
+                           ? X86_64_SSE_REGPARM_MAX
+                           : X86_64_MS_SSE_REGPARM_MAX);
+        }
+    }
+  if (TARGET_MMX)
+    cum->mmx_nregs = MMX_REGPARM_MAX;
+  cum->warn_avx512f = true;
+  cum->warn_avx = true;
+  cum->warn_sse = true;
+  cum->warn_mmx = true;
+
+  /* Because type might mismatch in between caller and callee, we need to
+     use actual type of function for local calls.
+     FIXME: cgraph_analyze can be told to actually record if function uses
+     va_start so for local functions maybe_vaarg can be made aggressive
+     helping K&R code.
+     FIXME: once typesytem is fixed, we won't need this code anymore.  */
+  if (local_info_node && local_info_node->local
+      && local_info_node->can_change_signature)
+    fntype = TREE_TYPE (target->decl);
+  cum->stdarg = stdarg_p (fntype);
+  cum->maybe_vaarg = (fntype
+		      ? (!prototype_p (fntype) || stdarg_p (fntype))
+		      : !libname);
+
+  cum->decl = fndecl;
+
+  cum->warn_empty = !warn_abi || cum->stdarg;
+  if (!cum->warn_empty && fntype)
+    {
+      function_args_iterator iter;
+      tree argtype;
+      bool seen_empty_type = false;
+      FOREACH_FUNCTION_ARGS (fntype, argtype, iter)
+	{
+	  if (argtype == error_mark_node || VOID_TYPE_P (argtype))
+	    break;
+	  if (TYPE_EMPTY_P (argtype))
+	    seen_empty_type = true;
+	  else if (seen_empty_type)
+	    {
+	      cum->warn_empty = true;
+	      break;
+	    }
+	}
+    }
+
+  if (!TARGET_64BIT)
+    {
+      /* If there are variable arguments, then we won't pass anything
+         in registers in 32-bit mode. */
+      if (stdarg_p (fntype))
+	{
+	  cum->nregs = 0;
+	  /* Since in 32-bit, variable arguments are always passed on
+	     stack, there is scratch register available for indirect
+	     sibcall.  */
+	  cfun->machine->arg_reg_available = true;
+	  cum->sse_nregs = 0;
+	  cum->mmx_nregs = 0;
+	  cum->warn_avx512f = false;
+	  cum->warn_avx = false;
+	  cum->warn_sse = false;
+	  cum->warn_mmx = false;
+	  return;
+	}
+
+      /* Use ecx and edx registers if function has fastcall attribute,
+	 else look for regparm information.  */
+      if (fntype)
+	{
+	  unsigned int ccvt = ix86_get_callcvt (fntype);
+	  if ((ccvt & IX86_CALLCVT_THISCALL) != 0)
+	    {
+	      cum->nregs = 1;
+	      cum->fastcall = 1; /* Same first register as in fastcall.  */
+	    }
+	  else if ((ccvt & IX86_CALLCVT_FASTCALL) != 0)
+	    {
+	      cum->nregs = 2;
+	      cum->fastcall = 1;
+	    }
+	  else
+	    cum->nregs = ix86_function_regparm (fntype, fndecl);
+	}
+
+      /* Set up the number of SSE registers used for passing SFmode
+	 and DFmode arguments.  Warn for mismatching ABI.  */
+      cum->float_in_sse = ix86_function_sseregparm (fntype, fndecl, true);
+    }
+
+  cfun->machine->arg_reg_available = (cum->nregs > 0);
+}
+
+/* Return the "natural" mode for TYPE.  In most cases, this is just TYPE_MODE.
+   But in the case of vector types, it is some vector mode.
+
+   When we have only some of our vector isa extensions enabled, then there
+   are some modes for which vector_mode_supported_p is false.  For these
+   modes, the generic vector support in gcc will choose some non-vector mode
+   in order to implement the type.  By computing the natural mode, we'll
+   select the proper ABI location for the operand and not depend on whatever
+   the middle-end decides to do with these vector types.
+
+   The midde-end can't deal with the vector types > 16 bytes.  In this
+   case, we return the original mode and warn ABI change if CUM isn't
+   NULL. 
+
+   If INT_RETURN is true, warn ABI change if the vector mode isn't
+   available for function return value.  */
+
+static machine_mode
+type_natural_mode (const_tree type, const CUMULATIVE_ARGS *cum,
+		   bool in_return)
+{
+  machine_mode mode = TYPE_MODE (type);
+
+  if (TREE_CODE (type) == VECTOR_TYPE && !VECTOR_MODE_P (mode))
+    {
+      HOST_WIDE_INT size = int_size_in_bytes (type);
+      if ((size == 8 || size == 16 || size == 32 || size == 64)
+	  /* ??? Generic code allows us to create width 1 vectors.  Ignore.  */
+	  && TYPE_VECTOR_SUBPARTS (type) > 1)
+	{
+	  machine_mode innermode = TYPE_MODE (TREE_TYPE (type));
+
+	  /* There are no XFmode vector modes.  */
+	  if (innermode == XFmode)
+	    return mode;
+
+	  if (TREE_CODE (TREE_TYPE (type)) == REAL_TYPE)
+	    mode = MIN_MODE_VECTOR_FLOAT;
+	  else
+	    mode = MIN_MODE_VECTOR_INT;
+
+	  /* Get the mode which has this inner mode and number of units.  */
+	  FOR_EACH_MODE_FROM (mode, mode)
+	    if (GET_MODE_NUNITS (mode) == TYPE_VECTOR_SUBPARTS (type)
+		&& GET_MODE_INNER (mode) == innermode)
+	      {
+		if (size == 64 && !TARGET_AVX512F && !TARGET_IAMCU)
+		  {
+		    static bool warnedavx512f;
+		    static bool warnedavx512f_ret;
+
+		    if (cum && cum->warn_avx512f && !warnedavx512f)
+		      {
+			if (warning (OPT_Wpsabi, "AVX512F vector argument "
+				     "without AVX512F enabled changes the ABI"))
+			  warnedavx512f = true;
+		      }
+		    else if (in_return && !warnedavx512f_ret)
+		      {
+			if (warning (OPT_Wpsabi, "AVX512F vector return "
+				     "without AVX512F enabled changes the ABI"))
+			  warnedavx512f_ret = true;
+		      }
+
+		    return TYPE_MODE (type);
+		  }
+		else if (size == 32 && !TARGET_AVX && !TARGET_IAMCU)
+		  {
+		    static bool warnedavx;
+		    static bool warnedavx_ret;
+
+		    if (cum && cum->warn_avx && !warnedavx)
+		      {
+			if (warning (OPT_Wpsabi, "AVX vector argument "
+				     "without AVX enabled changes the ABI"))
+			  warnedavx = true;
+		      }
+		    else if (in_return && !warnedavx_ret)
+		      {
+			if (warning (OPT_Wpsabi, "AVX vector return "
+				     "without AVX enabled changes the ABI"))
+			  warnedavx_ret = true;
+		      }
+
+		    return TYPE_MODE (type);
+		  }
+		else if (((size == 8 && TARGET_64BIT) || size == 16)
+			 && !TARGET_SSE
+			 && !TARGET_IAMCU)
+		  {
+		    static bool warnedsse;
+		    static bool warnedsse_ret;
+
+		    if (cum && cum->warn_sse && !warnedsse)
+		      {
+			if (warning (OPT_Wpsabi, "SSE vector argument "
+				     "without SSE enabled changes the ABI"))
+			  warnedsse = true;
+		      }
+		    else if (!TARGET_64BIT && in_return && !warnedsse_ret)
+		      {
+			if (warning (OPT_Wpsabi, "SSE vector return "
+				     "without SSE enabled changes the ABI"))
+			  warnedsse_ret = true;
+		      }
+		  }
+		else if ((size == 8 && !TARGET_64BIT)
+			 && (!cfun
+			     || cfun->machine->func_type == TYPE_NORMAL)
+			 && !TARGET_MMX
+			 && !TARGET_IAMCU)
+		  {
+		    static bool warnedmmx;
+		    static bool warnedmmx_ret;
+
+		    if (cum && cum->warn_mmx && !warnedmmx)
+		      {
+			if (warning (OPT_Wpsabi, "MMX vector argument "
+				     "without MMX enabled changes the ABI"))
+			  warnedmmx = true;
+		      }
+		    else if (in_return && !warnedmmx_ret)
+		      {
+			if (warning (OPT_Wpsabi, "MMX vector return "
+				     "without MMX enabled changes the ABI"))
+			  warnedmmx_ret = true;
+		      }
+		  }
+		return mode;
+	      }
+
+	  gcc_unreachable ();
+	}
+    }
+
+  return mode;
+}
+
+/* We want to pass a value in REGNO whose "natural" mode is MODE.  However,
+   this may not agree with the mode that the type system has chosen for the
+   register, which is ORIG_MODE.  If ORIG_MODE is not BLKmode, then we can
+   go ahead and use it.  Otherwise we have to build a PARALLEL instead.  */
+
+static rtx
+gen_reg_or_parallel (machine_mode mode, machine_mode orig_mode,
+		     unsigned int regno)
+{
+  rtx tmp;
+
+  if (orig_mode != BLKmode)
+    tmp = gen_rtx_REG (orig_mode, regno);
+  else
+    {
+      tmp = gen_rtx_REG (mode, regno);
+      tmp = gen_rtx_EXPR_LIST (VOIDmode, tmp, const0_rtx);
+      tmp = gen_rtx_PARALLEL (orig_mode, gen_rtvec (1, tmp));
+    }
+
+  return tmp;
+}
+
+/* x86-64 register passing implementation.  See x86-64 ABI for details.  Goal
+   of this code is to classify each 8bytes of incoming argument by the register
+   class and assign registers accordingly.  */
+
+/* Return the union class of CLASS1 and CLASS2.
+   See the x86-64 PS ABI for details.  */
+
+static enum x86_64_reg_class
+merge_classes (enum x86_64_reg_class class1, enum x86_64_reg_class class2)
+{
+  /* Rule #1: If both classes are equal, this is the resulting class.  */
+  if (class1 == class2)
+    return class1;
+
+  /* Rule #2: If one of the classes is NO_CLASS, the resulting class is
+     the other class.  */
+  if (class1 == X86_64_NO_CLASS)
+    return class2;
+  if (class2 == X86_64_NO_CLASS)
+    return class1;
+
+  /* Rule #3: If one of the classes is MEMORY, the result is MEMORY.  */
+  if (class1 == X86_64_MEMORY_CLASS || class2 == X86_64_MEMORY_CLASS)
+    return X86_64_MEMORY_CLASS;
+
+  /* Rule #4: If one of the classes is INTEGER, the result is INTEGER.  */
+  if ((class1 == X86_64_INTEGERSI_CLASS && class2 == X86_64_SSESF_CLASS)
+      || (class2 == X86_64_INTEGERSI_CLASS && class1 == X86_64_SSESF_CLASS))
+    return X86_64_INTEGERSI_CLASS;
+  if (class1 == X86_64_INTEGER_CLASS || class1 == X86_64_INTEGERSI_CLASS
+      || class2 == X86_64_INTEGER_CLASS || class2 == X86_64_INTEGERSI_CLASS)
+    return X86_64_INTEGER_CLASS;
+
+  /* Rule #5: If one of the classes is X87, X87UP, or COMPLEX_X87 class,
+     MEMORY is used.  */
+  if (class1 == X86_64_X87_CLASS
+      || class1 == X86_64_X87UP_CLASS
+      || class1 == X86_64_COMPLEX_X87_CLASS
+      || class2 == X86_64_X87_CLASS
+      || class2 == X86_64_X87UP_CLASS
+      || class2 == X86_64_COMPLEX_X87_CLASS)
+    return X86_64_MEMORY_CLASS;
+
+  /* Rule #6: Otherwise class SSE is used.  */
+  return X86_64_SSE_CLASS;
+}
+
+/* Classify the argument of type TYPE and mode MODE.
+   CLASSES will be filled by the register class used to pass each word
+   of the operand.  The number of words is returned.  In case the parameter
+   should be passed in memory, 0 is returned. As a special case for zero
+   sized containers, classes[0] will be NO_CLASS and 1 is returned.
+
+   BIT_OFFSET is used internally for handling records and specifies offset
+   of the offset in bits modulo 512 to avoid overflow cases.
+
+   See the x86-64 PS ABI for details.
+*/
+
+static int
+classify_argument (machine_mode mode, const_tree type,
+		   enum x86_64_reg_class classes[MAX_CLASSES], int bit_offset)
+{
+  HOST_WIDE_INT bytes
+    = mode == BLKmode ? int_size_in_bytes (type) : (int) GET_MODE_SIZE (mode);
+  int words = CEIL (bytes + (bit_offset % 64) / 8, UNITS_PER_WORD);
+
+  /* Variable sized entities are always passed/returned in memory.  */
+  if (bytes < 0)
+    return 0;
+
+  if (mode != VOIDmode)
+    {
+      /* The value of "named" doesn't matter.  */
+      function_arg_info arg (const_cast<tree> (type), mode, /*named=*/true);
+      if (targetm.calls.must_pass_in_stack (arg))
+	return 0;
+    }
+
+  if (type && AGGREGATE_TYPE_P (type))
+    {
+      int i;
+      tree field;
+      enum x86_64_reg_class subclasses[MAX_CLASSES];
+
+      /* On x86-64 we pass structures larger than 64 bytes on the stack.  */
+      if (bytes > 64)
+	return 0;
+
+      for (i = 0; i < words; i++)
+	classes[i] = X86_64_NO_CLASS;
+
+      /* Zero sized arrays or structures are NO_CLASS.  We return 0 to
+	 signalize memory class, so handle it as special case.  */
+      if (!words)
+	{
+	  classes[0] = X86_64_NO_CLASS;
+	  return 1;
+	}
+
+      /* Classify each field of record and merge classes.  */
+      switch (TREE_CODE (type))
+	{
+	case RECORD_TYPE:
+	  /* And now merge the fields of structure.  */
+	  for (field = TYPE_FIELDS (type); field; field = DECL_CHAIN (field))
+	    {
+	      if (TREE_CODE (field) == FIELD_DECL)
+		{
+		  int num;
+
+		  if (TREE_TYPE (field) == error_mark_node)
+		    continue;
+
+		  /* Bitfields are always classified as integer.  Handle them
+		     early, since later code would consider them to be
+		     misaligned integers.  */
+		  if (DECL_BIT_FIELD (field))
+		    {
+		      for (i = (int_bit_position (field)
+				+ (bit_offset % 64)) / 8 / 8;
+			   i < ((int_bit_position (field) + (bit_offset % 64))
+			        + tree_to_shwi (DECL_SIZE (field))
+				+ 63) / 8 / 8; i++)
+			classes[i]
+			  = merge_classes (X86_64_INTEGER_CLASS, classes[i]);
+		    }
+		  else
+		    {
+		      int pos;
+
+		      type = TREE_TYPE (field);
+
+		      /* Flexible array member is ignored.  */
+		      if (TYPE_MODE (type) == BLKmode
+			  && TREE_CODE (type) == ARRAY_TYPE
+			  && TYPE_SIZE (type) == NULL_TREE
+			  && TYPE_DOMAIN (type) != NULL_TREE
+			  && (TYPE_MAX_VALUE (TYPE_DOMAIN (type))
+			      == NULL_TREE))
+			{
+			  static bool warned;
+
+			  if (!warned && warn_psabi)
+			    {
+			      warned = true;
+			      inform (input_location,
+				      "the ABI of passing struct with"
+				      " a flexible array member has"
+				      " changed in GCC 4.4");
+			    }
+			  continue;
+			}
+		      num = classify_argument (TYPE_MODE (type), type,
+					       subclasses,
+					       (int_bit_position (field)
+						+ bit_offset) % 512);
+		      if (!num)
+			return 0;
+		      pos = (int_bit_position (field)
+			     + (bit_offset % 64)) / 8 / 8;
+		      for (i = 0; i < num && (i + pos) < words; i++)
+			classes[i + pos]
+			  = merge_classes (subclasses[i], classes[i + pos]);
+		    }
+		}
+	    }
+	  break;
+
+	case ARRAY_TYPE:
+	  /* Arrays are handled as small records.  */
+	  {
+	    int num;
+	    num = classify_argument (TYPE_MODE (TREE_TYPE (type)),
+				     TREE_TYPE (type), subclasses, bit_offset);
+	    if (!num)
+	      return 0;
+
+	    /* The partial classes are now full classes.  */
+	    if (subclasses[0] == X86_64_SSESF_CLASS && bytes != 4)
+	      subclasses[0] = X86_64_SSE_CLASS;
+	    if (subclasses[0] == X86_64_INTEGERSI_CLASS
+		&& !((bit_offset % 64) == 0 && bytes == 4))
+	      subclasses[0] = X86_64_INTEGER_CLASS;
+
+	    for (i = 0; i < words; i++)
+	      classes[i] = subclasses[i % num];
+
+	    break;
+	  }
+	case UNION_TYPE:
+	case QUAL_UNION_TYPE:
+	  /* Unions are similar to RECORD_TYPE but offset is always 0.
+	     */
+	  for (field = TYPE_FIELDS (type); field; field = DECL_CHAIN (field))
+	    {
+	      if (TREE_CODE (field) == FIELD_DECL)
+		{
+		  int num;
+
+		  if (TREE_TYPE (field) == error_mark_node)
+		    continue;
+
+		  num = classify_argument (TYPE_MODE (TREE_TYPE (field)),
+					   TREE_TYPE (field), subclasses,
+					   bit_offset);
+		  if (!num)
+		    return 0;
+		  for (i = 0; i < num && i < words; i++)
+		    classes[i] = merge_classes (subclasses[i], classes[i]);
+		}
+	    }
+	  break;
+
+	default:
+	  gcc_unreachable ();
+	}
+
+      if (words > 2)
+	{
+	  /* When size > 16 bytes, if the first one isn't
+	     X86_64_SSE_CLASS or any other ones aren't
+	     X86_64_SSEUP_CLASS, everything should be passed in
+	     memory.  */
+	  if (classes[0] != X86_64_SSE_CLASS)
+	      return 0;
+
+	  for (i = 1; i < words; i++)
+	    if (classes[i] != X86_64_SSEUP_CLASS)
+	      return 0;
+	}
+
+      /* Final merger cleanup.  */
+      for (i = 0; i < words; i++)
+	{
+	  /* If one class is MEMORY, everything should be passed in
+	     memory.  */
+	  if (classes[i] == X86_64_MEMORY_CLASS)
+	    return 0;
+
+	  /* The X86_64_SSEUP_CLASS should be always preceded by
+	     X86_64_SSE_CLASS or X86_64_SSEUP_CLASS.  */
+	  if (classes[i] == X86_64_SSEUP_CLASS
+	      && classes[i - 1] != X86_64_SSE_CLASS
+	      && classes[i - 1] != X86_64_SSEUP_CLASS)
+	    {
+	      /* The first one should never be X86_64_SSEUP_CLASS.  */
+	      gcc_assert (i != 0);
+	      classes[i] = X86_64_SSE_CLASS;
+	    }
+
+	  /*  If X86_64_X87UP_CLASS isn't preceded by X86_64_X87_CLASS,
+	       everything should be passed in memory.  */
+	  if (classes[i] == X86_64_X87UP_CLASS
+	      && (classes[i - 1] != X86_64_X87_CLASS))
+	    {
+	      static bool warned;
+
+	      /* The first one should never be X86_64_X87UP_CLASS.  */
+	      gcc_assert (i != 0);
+	      if (!warned && warn_psabi)
+		{
+		  warned = true;
+		  inform (input_location,
+			  "the ABI of passing union with %<long double%>"
+			  " has changed in GCC 4.4");
+		}
+	      return 0;
+	    }
+	}
+      return words;
+    }
+
+  /* Compute alignment needed.  We align all types to natural boundaries with
+     exception of XFmode that is aligned to 64bits.  */
+  if (mode != VOIDmode && mode != BLKmode)
+    {
+      int mode_alignment = GET_MODE_BITSIZE (mode);
+
+      if (mode == XFmode)
+	mode_alignment = 128;
+      else if (mode == XCmode)
+	mode_alignment = 256;
+      if (COMPLEX_MODE_P (mode))
+	mode_alignment /= 2;
+      /* Misaligned fields are always returned in memory.  */
+      if (bit_offset % mode_alignment)
+	return 0;
+    }
+
+  /* for V1xx modes, just use the base mode */
+  if (VECTOR_MODE_P (mode) && mode != V1DImode && mode != V1TImode
+      && GET_MODE_UNIT_SIZE (mode) == bytes)
+    mode = GET_MODE_INNER (mode);
+
+  /* Classification of atomic types.  */
+  switch (mode)
+    {
+    case E_SDmode:
+    case E_DDmode:
+      classes[0] = X86_64_SSE_CLASS;
+      return 1;
+    case E_TDmode:
+      classes[0] = X86_64_SSE_CLASS;
+      classes[1] = X86_64_SSEUP_CLASS;
+      return 2;
+    case E_DImode:
+    case E_SImode:
+    case E_HImode:
+    case E_QImode:
+    case E_CSImode:
+    case E_CHImode:
+    case E_CQImode:
+      {
+	int size = bit_offset + (int) GET_MODE_BITSIZE (mode);
+
+	/* Analyze last 128 bits only.  */
+	size = (size - 1) & 0x7f;
+
+	if (size < 32)
+	  {
+	    classes[0] = X86_64_INTEGERSI_CLASS;
+	    return 1;
+	  }
+	else if (size < 64)
+	  {
+	    classes[0] = X86_64_INTEGER_CLASS;
+	    return 1;
+	  }
+	else if (size < 64+32)
+	  {
+	    classes[0] = X86_64_INTEGER_CLASS;
+	    classes[1] = X86_64_INTEGERSI_CLASS;
+	    return 2;
+	  }
+	else if (size < 64+64)
+	  {
+	    classes[0] = classes[1] = X86_64_INTEGER_CLASS;
+	    return 2;
+	  }
+	else
+	  gcc_unreachable ();
+      }
+    case E_CDImode:
+    case E_TImode:
+      classes[0] = classes[1] = X86_64_INTEGER_CLASS;
+      return 2;
+    case E_COImode:
+    case E_OImode:
+      /* OImode shouldn't be used directly.  */
+      gcc_unreachable ();
+    case E_CTImode:
+      return 0;
+    case E_SFmode:
+      if (!(bit_offset % 64))
+	classes[0] = X86_64_SSESF_CLASS;
+      else
+	classes[0] = X86_64_SSE_CLASS;
+      return 1;
+    case E_DFmode:
+      classes[0] = X86_64_SSEDF_CLASS;
+      return 1;
+    case E_XFmode:
+      classes[0] = X86_64_X87_CLASS;
+      classes[1] = X86_64_X87UP_CLASS;
+      return 2;
+    case E_TFmode:
+      classes[0] = X86_64_SSE_CLASS;
+      classes[1] = X86_64_SSEUP_CLASS;
+      return 2;
+    case E_SCmode:
+      classes[0] = X86_64_SSE_CLASS;
+      if (!(bit_offset % 64))
+	return 1;
+      else
+	{
+	  static bool warned;
+
+	  if (!warned && warn_psabi)
+	    {
+	      warned = true;
+	      inform (input_location,
+		      "the ABI of passing structure with %<complex float%>"
+		      " member has changed in GCC 4.4");
+	    }
+	  classes[1] = X86_64_SSESF_CLASS;
+	  return 2;
+	}
+    case E_DCmode:
+      classes[0] = X86_64_SSEDF_CLASS;
+      classes[1] = X86_64_SSEDF_CLASS;
+      return 2;
+    case E_XCmode:
+      classes[0] = X86_64_COMPLEX_X87_CLASS;
+      return 1;
+    case E_TCmode:
+      /* This modes is larger than 16 bytes.  */
+      return 0;
+    case E_V8SFmode:
+    case E_V8SImode:
+    case E_V32QImode:
+    case E_V16HImode:
+    case E_V4DFmode:
+    case E_V4DImode:
+      classes[0] = X86_64_SSE_CLASS;
+      classes[1] = X86_64_SSEUP_CLASS;
+      classes[2] = X86_64_SSEUP_CLASS;
+      classes[3] = X86_64_SSEUP_CLASS;
+      return 4;
+    case E_V8DFmode:
+    case E_V16SFmode:
+    case E_V8DImode:
+    case E_V16SImode:
+    case E_V32HImode:
+    case E_V64QImode:
+      classes[0] = X86_64_SSE_CLASS;
+      classes[1] = X86_64_SSEUP_CLASS;
+      classes[2] = X86_64_SSEUP_CLASS;
+      classes[3] = X86_64_SSEUP_CLASS;
+      classes[4] = X86_64_SSEUP_CLASS;
+      classes[5] = X86_64_SSEUP_CLASS;
+      classes[6] = X86_64_SSEUP_CLASS;
+      classes[7] = X86_64_SSEUP_CLASS;
+      return 8;
+    case E_V4SFmode:
+    case E_V4SImode:
+    case E_V16QImode:
+    case E_V8HImode:
+    case E_V2DFmode:
+    case E_V2DImode:
+      classes[0] = X86_64_SSE_CLASS;
+      classes[1] = X86_64_SSEUP_CLASS;
+      return 2;
+    case E_V1TImode:
+    case E_V1DImode:
+    case E_V2SFmode:
+    case E_V2SImode:
+    case E_V4HImode:
+    case E_V8QImode:
+      classes[0] = X86_64_SSE_CLASS;
+      return 1;
+    case E_BLKmode:
+    case E_VOIDmode:
+      return 0;
+    default:
+      gcc_assert (VECTOR_MODE_P (mode));
+
+      if (bytes > 16)
+	return 0;
+
+      gcc_assert (GET_MODE_CLASS (GET_MODE_INNER (mode)) == MODE_INT);
+
+      if (bit_offset + GET_MODE_BITSIZE (mode) <= 32)
+	classes[0] = X86_64_INTEGERSI_CLASS;
+      else
+	classes[0] = X86_64_INTEGER_CLASS;
+      classes[1] = X86_64_INTEGER_CLASS;
+      return 1 + (bytes > 8);
+    }
+}
+
+/* Examine the argument and return set number of register required in each
+   class.  Return true iff parameter should be passed in memory.  */
+
+static bool
+examine_argument (machine_mode mode, const_tree type, int in_return,
+		  int *int_nregs, int *sse_nregs)
+{
+  enum x86_64_reg_class regclass[MAX_CLASSES];
+  int n = classify_argument (mode, type, regclass, 0);
+
+  *int_nregs = 0;
+  *sse_nregs = 0;
+
+  if (!n)
+    return true;
+  for (n--; n >= 0; n--)
+    switch (regclass[n])
+      {
+      case X86_64_INTEGER_CLASS:
+      case X86_64_INTEGERSI_CLASS:
+	(*int_nregs)++;
+	break;
+      case X86_64_SSE_CLASS:
+      case X86_64_SSESF_CLASS:
+      case X86_64_SSEDF_CLASS:
+	(*sse_nregs)++;
+	break;
+      case X86_64_NO_CLASS:
+      case X86_64_SSEUP_CLASS:
+	break;
+      case X86_64_X87_CLASS:
+      case X86_64_X87UP_CLASS:
+      case X86_64_COMPLEX_X87_CLASS:
+	if (!in_return)
+	  return true;
+	break;
+      case X86_64_MEMORY_CLASS:
+	gcc_unreachable ();
+      }
+
+  return false;
+}
+
+/* Construct container for the argument used by GCC interface.  See
+   FUNCTION_ARG for the detailed description.  */
+
+static rtx
+construct_container (machine_mode mode, machine_mode orig_mode,
+		     const_tree type, int in_return, int nintregs, int nsseregs,
+		     const int *intreg, int sse_regno)
+{
+  /* The following variables hold the static issued_error state.  */
+  static bool issued_sse_arg_error;
+  static bool issued_sse_ret_error;
+  static bool issued_x87_ret_error;
+
+  machine_mode tmpmode;
+  int bytes
+    = mode == BLKmode ? int_size_in_bytes (type) : (int) GET_MODE_SIZE (mode);
+  enum x86_64_reg_class regclass[MAX_CLASSES];
+  int n;
+  int i;
+  int nexps = 0;
+  int needed_sseregs, needed_intregs;
+  rtx exp[MAX_CLASSES];
+  rtx ret;
+
+  n = classify_argument (mode, type, regclass, 0);
+  if (!n)
+    return NULL;
+  if (examine_argument (mode, type, in_return, &needed_intregs,
+			&needed_sseregs))
+    return NULL;
+  if (needed_intregs > nintregs || needed_sseregs > nsseregs)
+    return NULL;
+
+  /* We allowed the user to turn off SSE for kernel mode.  Don't crash if
+     some less clueful developer tries to use floating-point anyway.  */
+  if (needed_sseregs && !TARGET_SSE)
+    {
+      /* Return early if we shouldn't raise an error for invalid
+	 calls.  */
+      if (cfun != NULL && cfun->machine->silent_p)
+	return NULL;
+      if (in_return)
+	{
+	  if (!issued_sse_ret_error)
+	    {
+	      error ("SSE register return with SSE disabled");
+	      issued_sse_ret_error = true;
+	    }
+	}
+      else if (!issued_sse_arg_error)
+	{
+	  error ("SSE register argument with SSE disabled");
+	  issued_sse_arg_error = true;
+	}
+      return NULL;
+    }
+
+  /* Likewise, error if the ABI requires us to return values in the
+     x87 registers and the user specified -mno-80387.  */
+  if (!TARGET_FLOAT_RETURNS_IN_80387 && in_return)
+    for (i = 0; i < n; i++)
+      if (regclass[i] == X86_64_X87_CLASS
+	  || regclass[i] == X86_64_X87UP_CLASS
+	  || regclass[i] == X86_64_COMPLEX_X87_CLASS)
+	{
+	  /* Return early if we shouldn't raise an error for invalid
+	     calls.  */
+	  if (cfun != NULL && cfun->machine->silent_p)
+	    return NULL;
+	  if (!issued_x87_ret_error)
+	    {
+	      error ("x87 register return with x87 disabled");
+	      issued_x87_ret_error = true;
+	    }
+	  return NULL;
+	}
+
+  /* First construct simple cases.  Avoid SCmode, since we want to use
+     single register to pass this type.  */
+  if (n == 1 && mode != SCmode)
+    switch (regclass[0])
+      {
+      case X86_64_INTEGER_CLASS:
+      case X86_64_INTEGERSI_CLASS:
+	return gen_rtx_REG (mode, intreg[0]);
+      case X86_64_SSE_CLASS:
+      case X86_64_SSESF_CLASS:
+      case X86_64_SSEDF_CLASS:
+	if (mode != BLKmode)
+	  return gen_reg_or_parallel (mode, orig_mode,
+				      GET_SSE_REGNO (sse_regno));
+	break;
+      case X86_64_X87_CLASS:
+      case X86_64_COMPLEX_X87_CLASS:
+	return gen_rtx_REG (mode, FIRST_STACK_REG);
+      case X86_64_NO_CLASS:
+	/* Zero sized array, struct or class.  */
+	return NULL;
+      default:
+	gcc_unreachable ();
+      }
+  if (n == 2
+      && regclass[0] == X86_64_SSE_CLASS
+      && regclass[1] == X86_64_SSEUP_CLASS
+      && mode != BLKmode)
+    return gen_reg_or_parallel (mode, orig_mode,
+				GET_SSE_REGNO (sse_regno));
+  if (n == 4
+      && regclass[0] == X86_64_SSE_CLASS
+      && regclass[1] == X86_64_SSEUP_CLASS
+      && regclass[2] == X86_64_SSEUP_CLASS
+      && regclass[3] == X86_64_SSEUP_CLASS
+      && mode != BLKmode)
+    return gen_reg_or_parallel (mode, orig_mode,
+				GET_SSE_REGNO (sse_regno));
+  if (n == 8
+      && regclass[0] == X86_64_SSE_CLASS
+      && regclass[1] == X86_64_SSEUP_CLASS
+      && regclass[2] == X86_64_SSEUP_CLASS
+      && regclass[3] == X86_64_SSEUP_CLASS
+      && regclass[4] == X86_64_SSEUP_CLASS
+      && regclass[5] == X86_64_SSEUP_CLASS
+      && regclass[6] == X86_64_SSEUP_CLASS
+      && regclass[7] == X86_64_SSEUP_CLASS
+      && mode != BLKmode)
+    return gen_reg_or_parallel (mode, orig_mode,
+				GET_SSE_REGNO (sse_regno));
+  if (n == 2
+      && regclass[0] == X86_64_X87_CLASS
+      && regclass[1] == X86_64_X87UP_CLASS)
+    return gen_rtx_REG (XFmode, FIRST_STACK_REG);
+
+  if (n == 2
+      && regclass[0] == X86_64_INTEGER_CLASS
+      && regclass[1] == X86_64_INTEGER_CLASS
+      && (mode == CDImode || mode == TImode || mode == BLKmode)
+      && intreg[0] + 1 == intreg[1])
+    {
+      if (mode == BLKmode)
+	{
+	  /* Use TImode for BLKmode values in 2 integer registers.  */
+	  exp[0] = gen_rtx_EXPR_LIST (VOIDmode,
+				      gen_rtx_REG (TImode, intreg[0]),
+				      GEN_INT (0));
+	  ret = gen_rtx_PARALLEL (mode, rtvec_alloc (1));
+	  XVECEXP (ret, 0, 0) = exp[0];
+	  return ret;
+	}
+      else
+	return gen_rtx_REG (mode, intreg[0]);
+    }
+
+  /* Otherwise figure out the entries of the PARALLEL.  */
+  for (i = 0; i < n; i++)
+    {
+      int pos;
+
+      switch (regclass[i])
+        {
+	  case X86_64_NO_CLASS:
+	    break;
+	  case X86_64_INTEGER_CLASS:
+	  case X86_64_INTEGERSI_CLASS:
+	    /* Merge TImodes on aligned occasions here too.  */
+	    if (i * 8 + 8 > bytes)
+	      {
+		unsigned int tmpbits = (bytes - i * 8) * BITS_PER_UNIT;
+		if (!int_mode_for_size (tmpbits, 0).exists (&tmpmode))
+		  /* We've requested 24 bytes we
+		     don't have mode for.  Use DImode.  */
+		  tmpmode = DImode;
+	      }
+	    else if (regclass[i] == X86_64_INTEGERSI_CLASS)
+	      tmpmode = SImode;
+	    else
+	      tmpmode = DImode;
+	    exp [nexps++]
+	      = gen_rtx_EXPR_LIST (VOIDmode,
+				   gen_rtx_REG (tmpmode, *intreg),
+				   GEN_INT (i*8));
+	    intreg++;
+	    break;
+	  case X86_64_SSESF_CLASS:
+	    exp [nexps++]
+	      = gen_rtx_EXPR_LIST (VOIDmode,
+				   gen_rtx_REG (SFmode,
+						GET_SSE_REGNO (sse_regno)),
+				   GEN_INT (i*8));
+	    sse_regno++;
+	    break;
+	  case X86_64_SSEDF_CLASS:
+	    exp [nexps++]
+	      = gen_rtx_EXPR_LIST (VOIDmode,
+				   gen_rtx_REG (DFmode,
+						GET_SSE_REGNO (sse_regno)),
+				   GEN_INT (i*8));
+	    sse_regno++;
+	    break;
+	  case X86_64_SSE_CLASS:
+	    pos = i;
+	    switch (n)
+	      {
+	      case 1:
+		tmpmode = DImode;
+		break;
+	      case 2:
+		if (i == 0 && regclass[1] == X86_64_SSEUP_CLASS)
+		  {
+		    tmpmode = TImode;
+		    i++;
+		  }
+		else
+		  tmpmode = DImode;
+		break;
+	      case 4:
+		gcc_assert (i == 0
+			    && regclass[1] == X86_64_SSEUP_CLASS
+			    && regclass[2] == X86_64_SSEUP_CLASS
+			    && regclass[3] == X86_64_SSEUP_CLASS);
+		tmpmode = OImode;
+		i += 3;
+		break;
+	      case 8:
+		gcc_assert (i == 0
+			    && regclass[1] == X86_64_SSEUP_CLASS
+			    && regclass[2] == X86_64_SSEUP_CLASS
+			    && regclass[3] == X86_64_SSEUP_CLASS
+			    && regclass[4] == X86_64_SSEUP_CLASS
+			    && regclass[5] == X86_64_SSEUP_CLASS
+			    && regclass[6] == X86_64_SSEUP_CLASS
+			    && regclass[7] == X86_64_SSEUP_CLASS);
+		tmpmode = XImode;
+		i += 7;
+		break;
+	      default:
+		gcc_unreachable ();
+	      }
+	    exp [nexps++]
+	      = gen_rtx_EXPR_LIST (VOIDmode,
+				   gen_rtx_REG (tmpmode,
+						GET_SSE_REGNO (sse_regno)),
+				   GEN_INT (pos*8));
+	    sse_regno++;
+	    break;
+	  default:
+	    gcc_unreachable ();
+	}
+    }
+
+  /* Empty aligned struct, union or class.  */
+  if (nexps == 0)
+    return NULL;
+
+  ret =  gen_rtx_PARALLEL (mode, rtvec_alloc (nexps));
+  for (i = 0; i < nexps; i++)
+    XVECEXP (ret, 0, i) = exp [i];
+  return ret;
+}
+
+/* Update the data in CUM to advance over an argument of mode MODE
+   and data type TYPE.  (TYPE is null for libcalls where that information
+   may not be available.)
+
+   Return a number of integer regsiters advanced over.  */
+
+static int
+function_arg_advance_32 (CUMULATIVE_ARGS *cum, machine_mode mode,
+			 const_tree type, HOST_WIDE_INT bytes,
+			 HOST_WIDE_INT words)
+{
+  int res = 0;
+  bool error_p = false;
+
+  if (TARGET_IAMCU)
+    {
+      /* Intel MCU psABI passes scalars and aggregates no larger than 8
+	 bytes in registers.  */
+      if (!VECTOR_MODE_P (mode) && bytes <= 8)
+	goto pass_in_reg;
+      return res;
+    }
+
+  switch (mode)
+    {
+    default:
+      break;
+
+    case E_BLKmode:
+      if (bytes < 0)
+	break;
+      /* FALLTHRU */
+
+    case E_DImode:
+    case E_SImode:
+    case E_HImode:
+    case E_QImode:
+pass_in_reg:
+      cum->words += words;
+      cum->nregs -= words;
+      cum->regno += words;
+      if (cum->nregs >= 0)
+	res = words;
+      if (cum->nregs <= 0)
+	{
+	  cum->nregs = 0;
+	  cfun->machine->arg_reg_available = false;
+	  cum->regno = 0;
+	}
+      break;
+
+    case E_OImode:
+      /* OImode shouldn't be used directly.  */
+      gcc_unreachable ();
+
+    case E_DFmode:
+      if (cum->float_in_sse == -1)
+	error_p = true;
+      if (cum->float_in_sse < 2)
+	break;
+      /* FALLTHRU */
+    case E_SFmode:
+      if (cum->float_in_sse == -1)
+	error_p = true;
+      if (cum->float_in_sse < 1)
+	break;
+      /* FALLTHRU */
+
+    case E_V8SFmode:
+    case E_V8SImode:
+    case E_V64QImode:
+    case E_V32HImode:
+    case E_V16SImode:
+    case E_V8DImode:
+    case E_V16SFmode:
+    case E_V8DFmode:
+    case E_V32QImode:
+    case E_V16HImode:
+    case E_V4DFmode:
+    case E_V4DImode:
+    case E_TImode:
+    case E_V16QImode:
+    case E_V8HImode:
+    case E_V4SImode:
+    case E_V2DImode:
+    case E_V4SFmode:
+    case E_V2DFmode:
+      if (!type || !AGGREGATE_TYPE_P (type))
+	{
+	  cum->sse_words += words;
+	  cum->sse_nregs -= 1;
+	  cum->sse_regno += 1;
+	  if (cum->sse_nregs <= 0)
+	    {
+	      cum->sse_nregs = 0;
+	      cum->sse_regno = 0;
+	    }
+	}
+      break;
+
+    case E_V8QImode:
+    case E_V4HImode:
+    case E_V2SImode:
+    case E_V2SFmode:
+    case E_V1TImode:
+    case E_V1DImode:
+      if (!type || !AGGREGATE_TYPE_P (type))
+	{
+	  cum->mmx_words += words;
+	  cum->mmx_nregs -= 1;
+	  cum->mmx_regno += 1;
+	  if (cum->mmx_nregs <= 0)
+	    {
+	      cum->mmx_nregs = 0;
+	      cum->mmx_regno = 0;
+	    }
+	}
+      break;
+    }
+  if (error_p)
+    {
+      cum->float_in_sse = 0;
+      error ("calling %qD with SSE calling convention without "
+	     "SSE/SSE2 enabled", cum->decl);
+      sorry ("this is a GCC bug that can be worked around by adding "
+	     "attribute used to function called");
+    }
+
+  return res;
+}
+
+static int
+function_arg_advance_64 (CUMULATIVE_ARGS *cum, machine_mode mode,
+			 const_tree type, HOST_WIDE_INT words, bool named)
+{
+  int int_nregs, sse_nregs;
+
+  /* Unnamed 512 and 256bit vector mode parameters are passed on stack.  */
+  if (!named && (VALID_AVX512F_REG_MODE (mode)
+		 || VALID_AVX256_REG_MODE (mode)))
+    return 0;
+
+  if (!examine_argument (mode, type, 0, &int_nregs, &sse_nregs)
+      && sse_nregs <= cum->sse_nregs && int_nregs <= cum->nregs)
+    {
+      cum->nregs -= int_nregs;
+      cum->sse_nregs -= sse_nregs;
+      cum->regno += int_nregs;
+      cum->sse_regno += sse_nregs;
+      return int_nregs;
+    }
+  else
+    {
+      int align = ix86_function_arg_boundary (mode, type) / BITS_PER_WORD;
+      cum->words = ROUND_UP (cum->words, align);
+      cum->words += words;
+      return 0;
+    }
+}
+
+static int
+function_arg_advance_ms_64 (CUMULATIVE_ARGS *cum, HOST_WIDE_INT bytes,
+			    HOST_WIDE_INT words)
+{
+  /* Otherwise, this should be passed indirect.  */
+  gcc_assert (bytes == 1 || bytes == 2 || bytes == 4 || bytes == 8);
+
+  cum->words += words;
+  if (cum->nregs > 0)
+    {
+      cum->nregs -= 1;
+      cum->regno += 1;
+      return 1;
+    }
+  return 0;
+}
+
+/* Update the data in CUM to advance over argument ARG.  */
+
+static void
+ix86_function_arg_advance (cumulative_args_t cum_v,
+			   const function_arg_info &arg)
+{
+  CUMULATIVE_ARGS *cum = get_cumulative_args (cum_v);
+  machine_mode mode = arg.mode;
+  HOST_WIDE_INT bytes, words;
+  int nregs;
+
+  /* The argument of interrupt handler is a special case and is
+     handled in ix86_function_arg.  */
+  if (!cum->caller && cfun->machine->func_type != TYPE_NORMAL)
+    return;
+
+  bytes = arg.promoted_size_in_bytes ();
+  words = CEIL (bytes, UNITS_PER_WORD);
+
+  if (arg.type)
+    mode = type_natural_mode (arg.type, NULL, false);
+
+  if (TARGET_64BIT)
+    {
+      enum calling_abi call_abi = cum ? cum->call_abi : ix86_abi;
+
+      if (call_abi == MS_ABI)
+	nregs = function_arg_advance_ms_64 (cum, bytes, words);
+      else
+	nregs = function_arg_advance_64 (cum, mode, arg.type, words,
+					 arg.named);
+    }
+  else
+    nregs = function_arg_advance_32 (cum, mode, arg.type, bytes, words);
+
+  if (!nregs)
+    {
+      /* Track if there are outgoing arguments on stack.  */
+      if (cum->caller)
+	cfun->machine->outgoing_args_on_stack = true;
+    }
+}
+
+/* Define where to put the arguments to a function.
+   Value is zero to push the argument on the stack,
+   or a hard register in which to store the argument.
+
+   MODE is the argument's machine mode.
+   TYPE is the data type of the argument (as a tree).
+    This is null for libcalls where that information may
+    not be available.
+   CUM is a variable of type CUMULATIVE_ARGS which gives info about
+    the preceding args and about the function being called.
+   NAMED is nonzero if this argument is a named parameter
+    (otherwise it is an extra parameter matching an ellipsis).  */
+
+static rtx
+function_arg_32 (CUMULATIVE_ARGS *cum, machine_mode mode,
+		 machine_mode orig_mode, const_tree type,
+		 HOST_WIDE_INT bytes, HOST_WIDE_INT words)
+{
+  bool error_p = false;
+
+  /* Avoid the AL settings for the Unix64 ABI.  */
+  if (mode == VOIDmode)
+    return constm1_rtx;
+
+  if (TARGET_IAMCU)
+    {
+      /* Intel MCU psABI passes scalars and aggregates no larger than 8
+	 bytes in registers.  */
+      if (!VECTOR_MODE_P (mode) && bytes <= 8)
+	goto pass_in_reg;
+      return NULL_RTX;
+    }
+
+  switch (mode)
+    {
+    default:
+      break;
+
+    case E_BLKmode:
+      if (bytes < 0)
+	break;
+      /* FALLTHRU */
+    case E_DImode:
+    case E_SImode:
+    case E_HImode:
+    case E_QImode:
+pass_in_reg:
+      if (words <= cum->nregs)
+	{
+	  int regno = cum->regno;
+
+	  /* Fastcall allocates the first two DWORD (SImode) or
+            smaller arguments to ECX and EDX if it isn't an
+            aggregate type .  */
+	  if (cum->fastcall)
+	    {
+	      if (mode == BLKmode
+		  || mode == DImode
+		  || (type && AGGREGATE_TYPE_P (type)))
+	        break;
+
+	      /* ECX not EAX is the first allocated register.  */
+	      if (regno == AX_REG)
+		regno = CX_REG;
+	    }
+	  return gen_rtx_REG (mode, regno);
+	}
+      break;
+
+    case E_DFmode:
+      if (cum->float_in_sse == -1)
+	error_p = true;
+      if (cum->float_in_sse < 2)
+	break;
+      /* FALLTHRU */
+    case E_SFmode:
+      if (cum->float_in_sse == -1)
+	error_p = true;
+      if (cum->float_in_sse < 1)
+	break;
+      /* FALLTHRU */
+    case E_TImode:
+      /* In 32bit, we pass TImode in xmm registers.  */
+    case E_V16QImode:
+    case E_V8HImode:
+    case E_V4SImode:
+    case E_V2DImode:
+    case E_V4SFmode:
+    case E_V2DFmode:
+      if (!type || !AGGREGATE_TYPE_P (type))
+	{
+	  if (cum->sse_nregs)
+	    return gen_reg_or_parallel (mode, orig_mode,
+				        cum->sse_regno + FIRST_SSE_REG);
+	}
+      break;
+
+    case E_OImode:
+    case E_XImode:
+      /* OImode and XImode shouldn't be used directly.  */
+      gcc_unreachable ();
+
+    case E_V64QImode:
+    case E_V32HImode:
+    case E_V16SImode:
+    case E_V8DImode:
+    case E_V16SFmode:
+    case E_V8DFmode:
+    case E_V8SFmode:
+    case E_V8SImode:
+    case E_V32QImode:
+    case E_V16HImode:
+    case E_V4DFmode:
+    case E_V4DImode:
+      if (!type || !AGGREGATE_TYPE_P (type))
+	{
+	  if (cum->sse_nregs)
+	    return gen_reg_or_parallel (mode, orig_mode,
+				        cum->sse_regno + FIRST_SSE_REG);
+	}
+      break;
+
+    case E_V8QImode:
+    case E_V4HImode:
+    case E_V2SImode:
+    case E_V2SFmode:
+    case E_V1TImode:
+    case E_V1DImode:
+      if (!type || !AGGREGATE_TYPE_P (type))
+	{
+	  if (cum->mmx_nregs)
+	    return gen_reg_or_parallel (mode, orig_mode,
+				        cum->mmx_regno + FIRST_MMX_REG);
+	}
+      break;
+    }
+  if (error_p)
+    {
+      cum->float_in_sse = 0;
+      error ("calling %qD with SSE calling convention without "
+	     "SSE/SSE2 enabled", cum->decl);
+      sorry ("this is a GCC bug that can be worked around by adding "
+	     "attribute used to function called");
+    }
+
+  return NULL_RTX;
+}
+
+static rtx
+function_arg_64 (const CUMULATIVE_ARGS *cum, machine_mode mode,
+		 machine_mode orig_mode, const_tree type, bool named)
+{
+  /* Handle a hidden AL argument containing number of registers
+     for varargs x86-64 functions.  */
+  if (mode == VOIDmode)
+    return GEN_INT (cum->maybe_vaarg
+		    ? (cum->sse_nregs < 0
+		       ? X86_64_SSE_REGPARM_MAX
+		       : cum->sse_regno)
+		    : -1);
+
+  switch (mode)
+    {
+    default:
+      break;
+
+    case E_V8SFmode:
+    case E_V8SImode:
+    case E_V32QImode:
+    case E_V16HImode:
+    case E_V4DFmode:
+    case E_V4DImode:
+    case E_V16SFmode:
+    case E_V16SImode:
+    case E_V64QImode:
+    case E_V32HImode:
+    case E_V8DFmode:
+    case E_V8DImode:
+      /* Unnamed 256 and 512bit vector mode parameters are passed on stack.  */
+      if (!named)
+	return NULL;
+      break;
+    }
+
+  return construct_container (mode, orig_mode, type, 0, cum->nregs,
+			      cum->sse_nregs,
+			      &x86_64_int_parameter_registers [cum->regno],
+			      cum->sse_regno);
+}
+
+static rtx
+function_arg_ms_64 (const CUMULATIVE_ARGS *cum, machine_mode mode,
+		    machine_mode orig_mode, bool named, const_tree type,
+		    HOST_WIDE_INT bytes)
+{
+  unsigned int regno;
+
+  /* We need to add clobber for MS_ABI->SYSV ABI calls in expand_call.
+     We use value of -2 to specify that current function call is MSABI.  */
+  if (mode == VOIDmode)
+    return GEN_INT (-2);
+
+  /* If we've run out of registers, it goes on the stack.  */
+  if (cum->nregs == 0)
+    return NULL_RTX;
+
+  regno = x86_64_ms_abi_int_parameter_registers[cum->regno];
+
+  /* Only floating point modes are passed in anything but integer regs.  */
+  if (TARGET_SSE && (mode == SFmode || mode == DFmode))
+    {
+      if (named)
+	{
+	  if (type == NULL_TREE || !AGGREGATE_TYPE_P (type))
+	    regno = cum->regno + FIRST_SSE_REG;
+	}
+      else
+	{
+	  rtx t1, t2;
+
+	  /* Unnamed floating parameters are passed in both the
+	     SSE and integer registers.  */
+	  t1 = gen_rtx_REG (mode, cum->regno + FIRST_SSE_REG);
+	  t2 = gen_rtx_REG (mode, regno);
+	  t1 = gen_rtx_EXPR_LIST (VOIDmode, t1, const0_rtx);
+	  t2 = gen_rtx_EXPR_LIST (VOIDmode, t2, const0_rtx);
+	  return gen_rtx_PARALLEL (mode, gen_rtvec (2, t1, t2));
+	}
+    }
+  /* Handle aggregated types passed in register.  */
+  if (orig_mode == BLKmode)
+    {
+      if (bytes > 0 && bytes <= 8)
+        mode = (bytes > 4 ? DImode : SImode);
+      if (mode == BLKmode)
+        mode = DImode;
+    }
+
+  return gen_reg_or_parallel (mode, orig_mode, regno);
+}
+
+/* Return where to put the arguments to a function.
+   Return zero to push the argument on the stack, or a hard register in which to store the argument.
+
+   ARG describes the argument while CUM gives information about the
+   preceding args and about the function being called.  */
+
+static rtx
+ix86_function_arg (cumulative_args_t cum_v, const function_arg_info &arg)
+{
+  CUMULATIVE_ARGS *cum = get_cumulative_args (cum_v);
+  machine_mode mode = arg.mode;
+  HOST_WIDE_INT bytes, words;
+  rtx reg;
+
+  if (!cum->caller && cfun->machine->func_type != TYPE_NORMAL)
+    {
+      gcc_assert (arg.type != NULL_TREE);
+      if (POINTER_TYPE_P (arg.type))
+	{
+	  /* This is the pointer argument.  */
+	  gcc_assert (TYPE_MODE (arg.type) == Pmode);
+	  /* It is at -WORD(AP) in the current frame in interrupt and
+	     exception handlers.  */
+	  reg = plus_constant (Pmode, arg_pointer_rtx, -UNITS_PER_WORD);
+	}
+      else
+	{
+	  gcc_assert (cfun->machine->func_type == TYPE_EXCEPTION
+		      && TREE_CODE (arg.type) == INTEGER_TYPE
+		      && TYPE_MODE (arg.type) == word_mode);
+	  /* The error code is the word-mode integer argument at
+	     -2 * WORD(AP) in the current frame of the exception
+	     handler.  */
+	  reg = gen_rtx_MEM (word_mode,
+			     plus_constant (Pmode,
+					    arg_pointer_rtx,
+					    -2 * UNITS_PER_WORD));
+	}
+      return reg;
+    }
+
+  bytes = arg.promoted_size_in_bytes ();
+  words = CEIL (bytes, UNITS_PER_WORD);
+
+  /* To simplify the code below, represent vector types with a vector mode
+     even if MMX/SSE are not active.  */
+  if (arg.type && TREE_CODE (arg.type) == VECTOR_TYPE)
+    mode = type_natural_mode (arg.type, cum, false);
+
+  if (TARGET_64BIT)
+    {
+      enum calling_abi call_abi = cum ? cum->call_abi : ix86_abi;
+
+      if (call_abi == MS_ABI)
+	reg = function_arg_ms_64 (cum, mode, arg.mode, arg.named,
+				  arg.type, bytes);
+      else
+	reg = function_arg_64 (cum, mode, arg.mode, arg.type, arg.named);
+    }
+  else
+    reg = function_arg_32 (cum, mode, arg.mode, arg.type, bytes, words);
+
+  /* Track if there are outgoing arguments on stack.  */
+  if (reg == NULL_RTX && cum->caller)
+    cfun->machine->outgoing_args_on_stack = true;
+
+  return reg;
+}
+
+/* A C expression that indicates when an argument must be passed by
+   reference.  If nonzero for an argument, a copy of that argument is
+   made in memory and a pointer to the argument is passed instead of
+   the argument itself.  The pointer is passed in whatever way is
+   appropriate for passing a pointer to that type.  */
+
+static bool
+ix86_pass_by_reference (cumulative_args_t cum_v, const function_arg_info &arg)
+{
+  CUMULATIVE_ARGS *cum = get_cumulative_args (cum_v);
+
+  if (TARGET_64BIT)
+    {
+      enum calling_abi call_abi = cum ? cum->call_abi : ix86_abi;
+
+      /* See Windows x64 Software Convention.  */
+      if (call_abi == MS_ABI)
+	{
+	  HOST_WIDE_INT msize = GET_MODE_SIZE (arg.mode);
+
+	  if (tree type = arg.type)
+	    {
+	      /* Arrays are passed by reference.  */
+	      if (TREE_CODE (type) == ARRAY_TYPE)
+		return true;
+
+	      if (RECORD_OR_UNION_TYPE_P (type))
+		{
+		  /* Structs/unions of sizes other than 8, 16, 32, or 64 bits
+		     are passed by reference.  */
+		  msize = int_size_in_bytes (type);
+		}
+	    }
+
+	  /* __m128 is passed by reference.  */
+	  return msize != 1 && msize != 2 && msize != 4 && msize != 8;
+	}
+      else if (arg.type && int_size_in_bytes (arg.type) == -1)
+	return true;
+    }
+
+  return false;
+}
+
+/* Return true when TYPE should be 128bit aligned for 32bit argument
+   passing ABI.  XXX: This function is obsolete and is only used for
+   checking psABI compatibility with previous versions of GCC.  */
+
+static bool
+ix86_compat_aligned_value_p (const_tree type)
+{
+  machine_mode mode = TYPE_MODE (type);
+  if (((TARGET_SSE && SSE_REG_MODE_P (mode))
+       || mode == TDmode
+       || mode == TFmode
+       || mode == TCmode)
+      && (!TYPE_USER_ALIGN (type) || TYPE_ALIGN (type) > 128))
+    return true;
+  if (TYPE_ALIGN (type) < 128)
+    return false;
+
+  if (AGGREGATE_TYPE_P (type))
+    {
+      /* Walk the aggregates recursively.  */
+      switch (TREE_CODE (type))
+	{
+	case RECORD_TYPE:
+	case UNION_TYPE:
+	case QUAL_UNION_TYPE:
+	  {
+	    tree field;
+
+	    /* Walk all the structure fields.  */
+	    for (field = TYPE_FIELDS (type); field; field = DECL_CHAIN (field))
+	      {
+		if (TREE_CODE (field) == FIELD_DECL
+		    && ix86_compat_aligned_value_p (TREE_TYPE (field)))
+		  return true;
+	      }
+	    break;
+	  }
+
+	case ARRAY_TYPE:
+	  /* Just for use if some languages passes arrays by value.  */
+	  if (ix86_compat_aligned_value_p (TREE_TYPE (type)))
+	    return true;
+	  break;
+
+	default:
+	  gcc_unreachable ();
+	}
+    }
+  return false;
+}
+
+/* Return the alignment boundary for MODE and TYPE with alignment ALIGN.
+   XXX: This function is obsolete and is only used for checking psABI
+   compatibility with previous versions of GCC.  */
+
+static unsigned int
+ix86_compat_function_arg_boundary (machine_mode mode,
+				   const_tree type, unsigned int align)
+{
+  /* In 32bit, only _Decimal128 and __float128 are aligned to their
+     natural boundaries.  */
+  if (!TARGET_64BIT && mode != TDmode && mode != TFmode)
+    {
+      /* i386 ABI defines all arguments to be 4 byte aligned.  We have to
+	 make an exception for SSE modes since these require 128bit
+	 alignment.
+
+	 The handling here differs from field_alignment.  ICC aligns MMX
+	 arguments to 4 byte boundaries, while structure fields are aligned
+	 to 8 byte boundaries.  */
+      if (!type)
+	{
+	  if (!(TARGET_SSE && SSE_REG_MODE_P (mode)))
+	    align = PARM_BOUNDARY;
+	}
+      else
+	{
+	  if (!ix86_compat_aligned_value_p (type))
+	    align = PARM_BOUNDARY;
+	}
+    }
+  if (align > BIGGEST_ALIGNMENT)
+    align = BIGGEST_ALIGNMENT;
+  return align;
+}
+
+/* Return true when TYPE should be 128bit aligned for 32bit argument
+   passing ABI.  */
+
+static bool
+ix86_contains_aligned_value_p (const_tree type)
+{
+  machine_mode mode = TYPE_MODE (type);
+
+  if (mode == XFmode || mode == XCmode)
+    return false;
+
+  if (TYPE_ALIGN (type) < 128)
+    return false;
+
+  if (AGGREGATE_TYPE_P (type))
+    {
+      /* Walk the aggregates recursively.  */
+      switch (TREE_CODE (type))
+	{
+	case RECORD_TYPE:
+	case UNION_TYPE:
+	case QUAL_UNION_TYPE:
+	  {
+	    tree field;
+
+	    /* Walk all the structure fields.  */
+	    for (field = TYPE_FIELDS (type);
+		 field;
+		 field = DECL_CHAIN (field))
+	      {
+		if (TREE_CODE (field) == FIELD_DECL
+		    && ix86_contains_aligned_value_p (TREE_TYPE (field)))
+		  return true;
+	      }
+	    break;
+	  }
+
+	case ARRAY_TYPE:
+	  /* Just for use if some languages passes arrays by value.  */
+	  if (ix86_contains_aligned_value_p (TREE_TYPE (type)))
+	    return true;
+	  break;
+
+	default:
+	  gcc_unreachable ();
+	}
+    }
+  else
+    return TYPE_ALIGN (type) >= 128;
+
+  return false;
+}
+
+/* Gives the alignment boundary, in bits, of an argument with the
+   specified mode and type.  */
+
+static unsigned int
+ix86_function_arg_boundary (machine_mode mode, const_tree type)
+{
+  unsigned int align;
+  if (type)
+    {
+      /* Since the main variant type is used for call, we convert it to
+	 the main variant type.  */
+      type = TYPE_MAIN_VARIANT (type);
+      align = TYPE_ALIGN (type);
+      if (TYPE_EMPTY_P (type))
+	return PARM_BOUNDARY;
+    }
+  else
+    align = GET_MODE_ALIGNMENT (mode);
+  if (align < PARM_BOUNDARY)
+    align = PARM_BOUNDARY;
+  else
+    {
+      static bool warned;
+      unsigned int saved_align = align;
+
+      if (!TARGET_64BIT)
+	{
+	  /* i386 ABI defines XFmode arguments to be 4 byte aligned.  */
+	  if (!type)
+	    {
+	      if (mode == XFmode || mode == XCmode)
+		align = PARM_BOUNDARY;
+	    }
+	  else if (!ix86_contains_aligned_value_p (type))
+	    align = PARM_BOUNDARY;
+
+	  if (align < 128)
+	    align = PARM_BOUNDARY;
+	}
+
+      if (warn_psabi
+	  && !warned
+	  && align != ix86_compat_function_arg_boundary (mode, type,
+							 saved_align))
+	{
+	  warned = true;
+	  inform (input_location,
+		  "the ABI for passing parameters with %d-byte"
+		  " alignment has changed in GCC 4.6",
+		  align / BITS_PER_UNIT);
+	}
+    }
+
+  return align;
+}
+
+/* Return true if N is a possible register number of function value.  */
+
+static bool
+ix86_function_value_regno_p (const unsigned int regno)
+{
+  switch (regno)
+    {
+    case AX_REG:
+      return true;
+    case DX_REG:
+      return (!TARGET_64BIT || ix86_cfun_abi () != MS_ABI);
+    case DI_REG:
+    case SI_REG:
+      return TARGET_64BIT && ix86_cfun_abi () != MS_ABI;
+
+      /* Complex values are returned in %st(0)/%st(1) pair.  */
+    case ST0_REG:
+    case ST1_REG:
+      /* TODO: The function should depend on current function ABI but
+       builtins.c would need updating then. Therefore we use the
+       default ABI.  */
+      if (TARGET_64BIT && ix86_cfun_abi () == MS_ABI)
+	return false;
+      return TARGET_FLOAT_RETURNS_IN_80387;
+
+      /* Complex values are returned in %xmm0/%xmm1 pair.  */
+    case XMM0_REG:
+    case XMM1_REG:
+      return TARGET_SSE;
+
+    case MM0_REG:
+      if (TARGET_MACHO || TARGET_64BIT)
+	return false;
+      return TARGET_MMX;
+    }
+
+  return false;
+}
+
+/* Check whether the register REGNO should be zeroed on X86.
+   When ALL_SSE_ZEROED is true, all SSE registers have been zeroed
+   together, no need to zero it again.
+   When NEED_ZERO_MMX is true, MMX registers should be cleared.  */
+
+static bool
+zero_call_used_regno_p (const unsigned int regno,
+			bool all_sse_zeroed,
+			bool need_zero_mmx)
+{
+  return GENERAL_REGNO_P (regno)
+	 || (!all_sse_zeroed && SSE_REGNO_P (regno))
+	 || MASK_REGNO_P (regno)
+	 || (need_zero_mmx && MMX_REGNO_P (regno));
+}
+
+/* Return the machine_mode that is used to zero register REGNO.  */
+
+static machine_mode
+zero_call_used_regno_mode (const unsigned int regno)
+{
+  /* NB: We only need to zero the lower 32 bits for integer registers
+     and the lower 128 bits for vector registers since destination are
+     zero-extended to the full register width.  */
+  if (GENERAL_REGNO_P (regno))
+    return SImode;
+  else if (SSE_REGNO_P (regno))
+    return V4SFmode;
+  else if (MASK_REGNO_P (regno))
+    return HImode;
+  else if (MMX_REGNO_P (regno))
+    return V4HImode;
+  else
+    gcc_unreachable ();
+}
+
+/* Generate a rtx to zero all vector registers together if possible,
+   otherwise, return NULL.  */
+
+static rtx
+zero_all_vector_registers (HARD_REG_SET need_zeroed_hardregs)
+{
+  if (!TARGET_AVX)
+    return NULL;
+
+  for (unsigned int regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)
+    if ((IN_RANGE (regno, FIRST_SSE_REG, LAST_SSE_REG)
+	 || (TARGET_64BIT
+	     && (REX_SSE_REGNO_P (regno)
+		 || (TARGET_AVX512F && EXT_REX_SSE_REGNO_P (regno)))))
+	&& !TEST_HARD_REG_BIT (need_zeroed_hardregs, regno))
+      return NULL;
+
+  return gen_avx_vzeroall ();
+}
+
+/* Generate insns to zero all st registers together.
+   Return true when zeroing instructions are generated.
+   Assume the number of st registers that are zeroed is num_of_st,
+   we will emit the following sequence to zero them together:
+		  fldz;		\
+		  fldz;		\
+		  ...
+		  fldz;		\
+		  fstp %%st(0);	\
+		  fstp %%st(0);	\
+		  ...
+		  fstp %%st(0);
+   i.e., num_of_st fldz followed by num_of_st fstp to clear the stack
+   mark stack slots empty.
+
+   How to compute the num_of_st:
+   There is no direct mapping from stack registers to hard register
+   numbers.  If one stack register needs to be cleared, we don't know
+   where in the stack the value remains.  So, if any stack register
+   needs to be cleared, the whole stack should be cleared.  However,
+   x87 stack registers that hold the return value should be excluded.
+   x87 returns in the top (two for complex values) register, so
+   num_of_st should be 7/6 when x87 returns, otherwise it will be 8.  */
+
+
+static bool
+zero_all_st_registers (HARD_REG_SET need_zeroed_hardregs)
+{
+
+  /* If the FPU is disabled, no need to zero all st registers.  */
+  if (! (TARGET_80387 || TARGET_FLOAT_RETURNS_IN_80387))
+    return false;
+
+  unsigned int num_of_st = 0;
+  for (unsigned int regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)
+    if ((STACK_REGNO_P (regno) || MMX_REGNO_P (regno))
+	&& TEST_HARD_REG_BIT (need_zeroed_hardregs, regno))
+      {
+	num_of_st++;
+	break;
+      }
+
+  if (num_of_st == 0)
+    return false;
+
+  bool return_with_x87 = false;
+  return_with_x87 = (crtl->return_rtx
+		     && (STACK_REG_P (crtl->return_rtx)));
+
+  bool complex_return = false;
+  complex_return = (crtl->return_rtx
+		    && COMPLEX_MODE_P (GET_MODE (crtl->return_rtx)));
+
+  if (return_with_x87)
+    if (complex_return)
+      num_of_st = 6;
+    else
+      num_of_st = 7;
+  else
+    num_of_st = 8;
+
+  rtx st_reg = gen_rtx_REG (XFmode, FIRST_STACK_REG);
+  for (unsigned int i = 0; i < num_of_st; i++)
+    emit_insn (gen_rtx_SET (st_reg, CONST0_RTX (XFmode)));
+
+  for (unsigned int i = 0; i < num_of_st; i++)
+    {
+      rtx insn;
+      insn = emit_insn (gen_rtx_SET (st_reg, st_reg));
+      add_reg_note (insn, REG_DEAD, st_reg);
+    }
+  return true;
+}
+
+
+/* When the routine exit in MMX mode, if any ST register needs
+   to be zeroed, we should clear all MMX registers except the
+   RET_MMX_REGNO that holds the return value.  */
+static bool
+zero_all_mm_registers (HARD_REG_SET need_zeroed_hardregs,
+		       unsigned int ret_mmx_regno)
+{
+  bool need_zero_all_mm = false;
+  for (unsigned int regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)
+    if (STACK_REGNO_P (regno)
+	&& TEST_HARD_REG_BIT (need_zeroed_hardregs, regno))
+      {
+	need_zero_all_mm = true;
+	break;
+      }
+
+  if (!need_zero_all_mm)
+    return false;
+
+  rtx zero_mmx = NULL_RTX;
+  machine_mode mode = V4HImode;
+  for (unsigned int regno = FIRST_MMX_REG; regno <= LAST_MMX_REG; regno++)
+    if (regno != ret_mmx_regno)
+      {
+	rtx reg = gen_rtx_REG (mode, regno);
+	if (zero_mmx == NULL_RTX)
+	  {
+	    zero_mmx = reg;
+	    emit_insn (gen_rtx_SET (reg, CONST0_RTX (mode)));
+	  }
+	else
+	  emit_move_insn (reg, zero_mmx);
+      }
+  return true;
+}
+
+/* TARGET_ZERO_CALL_USED_REGS.  */
+/* Generate a sequence of instructions that zero registers specified by
+   NEED_ZEROED_HARDREGS.  Return the ZEROED_HARDREGS that are actually
+   zeroed.  */
+static HARD_REG_SET
+ix86_zero_call_used_regs (HARD_REG_SET need_zeroed_hardregs)
+{
+  HARD_REG_SET zeroed_hardregs;
+  bool all_sse_zeroed = false;
+  bool all_st_zeroed = false;
+  bool all_mm_zeroed = false;
+
+  CLEAR_HARD_REG_SET (zeroed_hardregs);
+
+  /* first, let's see whether we can zero all vector registers together.  */
+  rtx zero_all_vec_insn = zero_all_vector_registers (need_zeroed_hardregs);
+  if (zero_all_vec_insn)
+    {
+      emit_insn (zero_all_vec_insn);
+      all_sse_zeroed = true;
+    }
+
+  /* mm/st registers are shared registers set, we should follow the following
+     rules to clear them:
+			MMX exit mode	      x87 exit mode
+	-------------|----------------------|---------------
+	uses x87 reg | clear all MMX	    | clear all x87
+	uses MMX reg | clear individual MMX | clear all x87
+	x87 + MMX    | clear all MMX	    | clear all x87
+
+     first, we should decide which mode (MMX mode or x87 mode) the function
+     exit with.  */
+
+  bool exit_with_mmx_mode = (crtl->return_rtx
+			     && (MMX_REG_P (crtl->return_rtx)));
+
+  if (!exit_with_mmx_mode)
+    /* x87 exit mode, we should zero all st registers together.  */
+    {
+      all_st_zeroed = zero_all_st_registers (need_zeroed_hardregs);
+      if (all_st_zeroed)
+	SET_HARD_REG_BIT (zeroed_hardregs, FIRST_STACK_REG);
+    }
+  else
+    /* MMX exit mode, check whether we can zero all mm registers.  */
+    {
+      unsigned int exit_mmx_regno = REGNO (crtl->return_rtx);
+      all_mm_zeroed = zero_all_mm_registers (need_zeroed_hardregs,
+					     exit_mmx_regno);
+      if (all_mm_zeroed)
+	for (unsigned int regno = FIRST_MMX_REG; regno <= LAST_MMX_REG; regno++)
+	  if (regno != exit_mmx_regno)
+	    SET_HARD_REG_BIT (zeroed_hardregs, regno);
+    }
+
+  /* Now, generate instructions to zero all the other registers.  */
+
+  rtx zero_gpr = NULL_RTX;
+  rtx zero_vector = NULL_RTX;
+  rtx zero_mask = NULL_RTX;
+  rtx zero_mmx = NULL_RTX;
+
+  for (unsigned int regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)
+    {
+      if (!TEST_HARD_REG_BIT (need_zeroed_hardregs, regno))
+	continue;
+      if (!zero_call_used_regno_p (regno, all_sse_zeroed,
+				   exit_with_mmx_mode && !all_mm_zeroed))
+	continue;
+
+      SET_HARD_REG_BIT (zeroed_hardregs, regno);
+
+      rtx reg, tmp, zero_rtx;
+      machine_mode mode = zero_call_used_regno_mode (regno);
+
+      reg = gen_rtx_REG (mode, regno);
+      zero_rtx = CONST0_RTX (mode);
+
+      if (mode == SImode)
+	if (zero_gpr == NULL_RTX)
+	  {
+	    zero_gpr = reg;
+	    tmp = gen_rtx_SET (reg, zero_rtx);
+	    if (!TARGET_USE_MOV0 || optimize_insn_for_size_p ())
+	      {
+		rtx clob = gen_rtx_CLOBBER (VOIDmode,
+					    gen_rtx_REG (CCmode,
+							 FLAGS_REG));
+		tmp = gen_rtx_PARALLEL (VOIDmode, gen_rtvec (2,
+							     tmp,
+							     clob));
+	      }
+	    emit_insn (tmp);
+	  }
+	else
+	  emit_move_insn (reg, zero_gpr);
+      else if (mode == V4SFmode)
+	if (zero_vector == NULL_RTX)
+	  {
+	    zero_vector = reg;
+	    tmp = gen_rtx_SET (reg, zero_rtx);
+	    emit_insn (tmp);
+	  }
+	else
+	  emit_move_insn (reg, zero_vector);
+      else if (mode == HImode)
+	if (zero_mask == NULL_RTX)
+	  {
+	    zero_mask = reg;
+	    tmp = gen_rtx_SET (reg, zero_rtx);
+	    emit_insn (tmp);
+	  }
+	else
+	  emit_move_insn (reg, zero_mask);
+      else if (mode == V4HImode)
+	if (zero_mmx == NULL_RTX)
+	  {
+	    zero_mmx = reg;
+	    tmp = gen_rtx_SET (reg, zero_rtx);
+	    emit_insn (tmp);
+	  }
+	else
+	  emit_move_insn (reg, zero_mmx);
+      else
+	gcc_unreachable ();
+    }
+  return zeroed_hardregs;
+}
+
+/* Define how to find the value returned by a function.
+   VALTYPE is the data type of the value (as a tree).
+   If the precise function being called is known, FUNC is its FUNCTION_DECL;
+   otherwise, FUNC is 0.  */
+
+static rtx
+function_value_32 (machine_mode orig_mode, machine_mode mode,
+		   const_tree fntype, const_tree fn)
+{
+  unsigned int regno;
+
+  /* 8-byte vector modes in %mm0. See ix86_return_in_memory for where
+     we normally prevent this case when mmx is not available.  However
+     some ABIs may require the result to be returned like DImode.  */
+  if (VECTOR_MODE_P (mode) && GET_MODE_SIZE (mode) == 8)
+    regno = FIRST_MMX_REG;
+
+  /* 16-byte vector modes in %xmm0.  See ix86_return_in_memory for where
+     we prevent this case when sse is not available.  However some ABIs
+     may require the result to be returned like integer TImode.  */
+  else if (mode == TImode
+	   || (VECTOR_MODE_P (mode) && GET_MODE_SIZE (mode) == 16))
+    regno = FIRST_SSE_REG;
+
+  /* 32-byte vector modes in %ymm0.   */
+  else if (VECTOR_MODE_P (mode) && GET_MODE_SIZE (mode) == 32)
+    regno = FIRST_SSE_REG;
+
+  /* 64-byte vector modes in %zmm0.   */
+  else if (VECTOR_MODE_P (mode) && GET_MODE_SIZE (mode) == 64)
+    regno = FIRST_SSE_REG;
+
+  /* Floating point return values in %st(0) (unless -mno-fp-ret-in-387).  */
+  else if (X87_FLOAT_MODE_P (mode) && TARGET_FLOAT_RETURNS_IN_80387)
+    regno = FIRST_FLOAT_REG;
+  else
+    /* Most things go in %eax.  */
+    regno = AX_REG;
+
+  /* Override FP return register with %xmm0 for local functions when
+     SSE math is enabled or for functions with sseregparm attribute.  */
+  if ((fn || fntype) && (mode == SFmode || mode == DFmode))
+    {
+      int sse_level = ix86_function_sseregparm (fntype, fn, false);
+      if (sse_level == -1)
+	{
+	  error ("calling %qD with SSE calling convention without "
+		 "SSE/SSE2 enabled", fn);
+	  sorry ("this is a GCC bug that can be worked around by adding "
+		 "attribute used to function called");
+	}
+      else if ((sse_level >= 1 && mode == SFmode)
+	       || (sse_level == 2 && mode == DFmode))
+	regno = FIRST_SSE_REG;
+    }
+
+  /* OImode shouldn't be used directly.  */
+  gcc_assert (mode != OImode);
+
+  return gen_rtx_REG (orig_mode, regno);
+}
+
+static rtx
+function_value_64 (machine_mode orig_mode, machine_mode mode,
+		   const_tree valtype)
+{
+  rtx ret;
+
+  /* Handle libcalls, which don't provide a type node.  */
+  if (valtype == NULL)
+    {
+      unsigned int regno;
+
+      switch (mode)
+	{
+	case E_SFmode:
+	case E_SCmode:
+	case E_DFmode:
+	case E_DCmode:
+	case E_TFmode:
+	case E_SDmode:
+	case E_DDmode:
+	case E_TDmode:
+	  regno = FIRST_SSE_REG;
+	  break;
+	case E_XFmode:
+	case E_XCmode:
+	  regno = FIRST_FLOAT_REG;
+	  break;
+	case E_TCmode:
+	  return NULL;
+	default:
+	  regno = AX_REG;
+	}
+
+      return gen_rtx_REG (mode, regno);
+    }
+  else if (POINTER_TYPE_P (valtype))
+    {
+      /* Pointers are always returned in word_mode.  */
+      mode = word_mode;
+    }
+
+  ret = construct_container (mode, orig_mode, valtype, 1,
+			     X86_64_REGPARM_MAX, X86_64_SSE_REGPARM_MAX,
+			     x86_64_int_return_registers, 0);
+
+  /* For zero sized structures, construct_container returns NULL, but we
+     need to keep rest of compiler happy by returning meaningful value.  */
+  if (!ret)
+    ret = gen_rtx_REG (orig_mode, AX_REG);
+
+  return ret;
+}
+
+static rtx
+function_value_ms_32 (machine_mode orig_mode, machine_mode mode,
+		      const_tree fntype, const_tree fn, const_tree valtype)
+{
+  unsigned int regno;
+
+  /* Floating point return values in %st(0)
+     (unless -mno-fp-ret-in-387 or aggregate type of up to 8 bytes).  */
+  if (X87_FLOAT_MODE_P (mode) && TARGET_FLOAT_RETURNS_IN_80387
+	   && (GET_MODE_SIZE (mode) > 8
+	       || valtype == NULL_TREE || !AGGREGATE_TYPE_P (valtype)))
+  {
+    regno = FIRST_FLOAT_REG;
+    return gen_rtx_REG (orig_mode, regno);
+  }
+  else
+    return function_value_32(orig_mode, mode, fntype,fn);
+}
+
+static rtx
+function_value_ms_64 (machine_mode orig_mode, machine_mode mode,
+		      const_tree valtype)
+{
+  unsigned int regno = AX_REG;
+
+  if (TARGET_SSE)
+    {
+      switch (GET_MODE_SIZE (mode))
+	{
+	case 16:
+	  if (valtype != NULL_TREE
+	      && !VECTOR_INTEGER_TYPE_P (valtype)
+	      && !VECTOR_INTEGER_TYPE_P (valtype)
+	      && !INTEGRAL_TYPE_P (valtype)
+	      && !VECTOR_FLOAT_TYPE_P (valtype))
+	    break;
+	  if ((SCALAR_INT_MODE_P (mode) || VECTOR_MODE_P (mode))
+	      && !COMPLEX_MODE_P (mode))
+	    regno = FIRST_SSE_REG;
+	  break;
+	case 8:
+	case 4:
+	  if (valtype != NULL_TREE && AGGREGATE_TYPE_P (valtype))
+	    break;
+	  if (mode == SFmode || mode == DFmode)
+	    regno = FIRST_SSE_REG;
+	  break;
+	default:
+	  break;
+        }
+    }
+  return gen_rtx_REG (orig_mode, regno);
+}
+
+static rtx
+ix86_function_value_1 (const_tree valtype, const_tree fntype_or_decl,
+		       machine_mode orig_mode, machine_mode mode)
+{
+  const_tree fn, fntype;
+
+  fn = NULL_TREE;
+  if (fntype_or_decl && DECL_P (fntype_or_decl))
+    fn = fntype_or_decl;
+  fntype = fn ? TREE_TYPE (fn) : fntype_or_decl;
+  
+  if (ix86_function_type_abi (fntype) == MS_ABI)
+    {
+      if (TARGET_64BIT)
+	return function_value_ms_64 (orig_mode, mode, valtype);
+      else
+	return function_value_ms_32 (orig_mode, mode, fntype, fn, valtype);
+    }
+  else if (TARGET_64BIT)
+    return function_value_64 (orig_mode, mode, valtype);
+  else
+    return function_value_32 (orig_mode, mode, fntype, fn);
+}
+
+static rtx
+ix86_function_value (const_tree valtype, const_tree fntype_or_decl, bool)
+{
+  machine_mode mode, orig_mode;
+
+  orig_mode = TYPE_MODE (valtype);
+  mode = type_natural_mode (valtype, NULL, true);
+  return ix86_function_value_1 (valtype, fntype_or_decl, orig_mode, mode);
+}
+
+/* Pointer function arguments and return values are promoted to
+   word_mode for normal functions.  */
+
+static machine_mode
+ix86_promote_function_mode (const_tree type, machine_mode mode,
+			    int *punsignedp, const_tree fntype,
+			    int for_return)
+{
+  if (cfun->machine->func_type == TYPE_NORMAL
+      && type != NULL_TREE
+      && POINTER_TYPE_P (type))
+    {
+      *punsignedp = POINTERS_EXTEND_UNSIGNED;
+      return word_mode;
+    }
+  return default_promote_function_mode (type, mode, punsignedp, fntype,
+					for_return);
+}
+
+/* Return true if a structure, union or array with MODE containing FIELD
+   should be accessed using BLKmode.  */
+
+static bool
+ix86_member_type_forces_blk (const_tree field, machine_mode mode)
+{
+  /* Union with XFmode must be in BLKmode.  */
+  return (mode == XFmode
+	  && (TREE_CODE (DECL_FIELD_CONTEXT (field)) == UNION_TYPE
+	      || TREE_CODE (DECL_FIELD_CONTEXT (field)) == QUAL_UNION_TYPE));
+}
+
+rtx
+ix86_libcall_value (machine_mode mode)
+{
+  return ix86_function_value_1 (NULL, NULL, mode, mode);
+}
+
+/* Return true iff type is returned in memory.  */
+
+static bool
+ix86_return_in_memory (const_tree type, const_tree fntype ATTRIBUTE_UNUSED)
+{
+  const machine_mode mode = type_natural_mode (type, NULL, true);
+  HOST_WIDE_INT size;
+
+  if (TARGET_64BIT)
+    {
+      if (ix86_function_type_abi (fntype) == MS_ABI)
+	{
+	  size = int_size_in_bytes (type);
+
+	  /* __m128 is returned in xmm0.  */
+	  if ((!type || VECTOR_INTEGER_TYPE_P (type)
+	       || INTEGRAL_TYPE_P (type)
+	       || VECTOR_FLOAT_TYPE_P (type))
+	      && (SCALAR_INT_MODE_P (mode) || VECTOR_MODE_P (mode))
+	      && !COMPLEX_MODE_P (mode)
+	      && (GET_MODE_SIZE (mode) == 16 || size == 16))
+	    return false;
+
+	  /* Otherwise, the size must be exactly in [1248]. */
+	  return size != 1 && size != 2 && size != 4 && size != 8;
+	}
+      else
+	{
+	  int needed_intregs, needed_sseregs;
+
+	  return examine_argument (mode, type, 1,
+				   &needed_intregs, &needed_sseregs);
+	}
+    }
+  else
+    {
+      size = int_size_in_bytes (type);
+
+      /* Intel MCU psABI returns scalars and aggregates no larger than 8
+	 bytes in registers.  */
+      if (TARGET_IAMCU)
+	return VECTOR_MODE_P (mode) || size < 0 || size > 8;
+
+      if (mode == BLKmode)
+	return true;
+
+      if (MS_AGGREGATE_RETURN && AGGREGATE_TYPE_P (type) && size <= 8)
+	return false;
+
+      if (VECTOR_MODE_P (mode) || mode == TImode)
+	{
+	  /* User-created vectors small enough to fit in EAX.  */
+	  if (size < 8)
+	    return false;
+
+	  /* Unless ABI prescibes otherwise,
+	     MMX/3dNow values are returned in MM0 if available.  */
+	     
+	  if (size == 8)
+	    return TARGET_VECT8_RETURNS || !TARGET_MMX;
+
+	  /* SSE values are returned in XMM0 if available.  */
+	  if (size == 16)
+	    return !TARGET_SSE;
+
+	  /* AVX values are returned in YMM0 if available.  */
+	  if (size == 32)
+	    return !TARGET_AVX;
+
+	  /* AVX512F values are returned in ZMM0 if available.  */
+	  if (size == 64)
+	    return !TARGET_AVX512F;
+	}
+
+      if (mode == XFmode)
+	return false;
+
+      if (size > 12)
+	return true;
+
+      /* OImode shouldn't be used directly.  */
+      gcc_assert (mode != OImode);
+
+      return false;
+    }
+}
+
+
+/* Create the va_list data type.  */
+
+static tree
+ix86_build_builtin_va_list_64 (void)
+{
+  tree f_gpr, f_fpr, f_ovf, f_sav, record, type_decl;
+
+  record = lang_hooks.types.make_type (RECORD_TYPE);
+  type_decl = build_decl (BUILTINS_LOCATION,
+			  TYPE_DECL, get_identifier ("__va_list_tag"), record);
+
+  f_gpr = build_decl (BUILTINS_LOCATION,
+		      FIELD_DECL, get_identifier ("gp_offset"),
+		      unsigned_type_node);
+  f_fpr = build_decl (BUILTINS_LOCATION,
+		      FIELD_DECL, get_identifier ("fp_offset"),
+		      unsigned_type_node);
+  f_ovf = build_decl (BUILTINS_LOCATION,
+		      FIELD_DECL, get_identifier ("overflow_arg_area"),
+		      ptr_type_node);
+  f_sav = build_decl (BUILTINS_LOCATION,
+		      FIELD_DECL, get_identifier ("reg_save_area"),
+		      ptr_type_node);
+
+  va_list_gpr_counter_field = f_gpr;
+  va_list_fpr_counter_field = f_fpr;
+
+  DECL_FIELD_CONTEXT (f_gpr) = record;
+  DECL_FIELD_CONTEXT (f_fpr) = record;
+  DECL_FIELD_CONTEXT (f_ovf) = record;
+  DECL_FIELD_CONTEXT (f_sav) = record;
+
+  TYPE_STUB_DECL (record) = type_decl;
+  TYPE_NAME (record) = type_decl;
+  TYPE_FIELDS (record) = f_gpr;
+  DECL_CHAIN (f_gpr) = f_fpr;
+  DECL_CHAIN (f_fpr) = f_ovf;
+  DECL_CHAIN (f_ovf) = f_sav;
+
+  layout_type (record);
+
+  TYPE_ATTRIBUTES (record) = tree_cons (get_identifier ("sysv_abi va_list"),
+					NULL_TREE, TYPE_ATTRIBUTES (record));
+
+  /* The correct type is an array type of one element.  */
+  return build_array_type (record, build_index_type (size_zero_node));
+}
+
+/* Setup the builtin va_list data type and for 64-bit the additional
+   calling convention specific va_list data types.  */
+
+static tree
+ix86_build_builtin_va_list (void)
+{
+  if (TARGET_64BIT)
+    {
+      /* Initialize ABI specific va_list builtin types.
+
+	 In lto1, we can encounter two va_list types:
+	 - one as a result of the type-merge across TUs, and
+	 - the one constructed here.
+	 These two types will not have the same TYPE_MAIN_VARIANT, and therefore
+	 a type identity check in canonical_va_list_type based on
+	 TYPE_MAIN_VARIANT (which we used to have) will not work.
+	 Instead, we tag each va_list_type_node with its unique attribute, and
+	 look for the attribute in the type identity check in
+	 canonical_va_list_type.
+
+	 Tagging sysv_va_list_type_node directly with the attribute is
+	 problematic since it's a array of one record, which will degrade into a
+	 pointer to record when used as parameter (see build_va_arg comments for
+	 an example), dropping the attribute in the process.  So we tag the
+	 record instead.  */
+
+      /* For SYSV_ABI we use an array of one record.  */
+      sysv_va_list_type_node = ix86_build_builtin_va_list_64 ();
+	
+      /* For MS_ABI we use plain pointer to argument area.  */
+      tree char_ptr_type = build_pointer_type (char_type_node);
+      tree attr = tree_cons (get_identifier ("ms_abi va_list"), NULL_TREE,
+			     TYPE_ATTRIBUTES (char_ptr_type));
+      ms_va_list_type_node = build_type_attribute_variant (char_ptr_type, attr);
+
+      return ((ix86_abi == MS_ABI)
+	      ? ms_va_list_type_node
+	      : sysv_va_list_type_node);
+    }
+  else
+    {
+      /* For i386 we use plain pointer to argument area.  */
+      return build_pointer_type (char_type_node);
+    }
+}
+
+/* Worker function for TARGET_SETUP_INCOMING_VARARGS.  */
+
+static void
+setup_incoming_varargs_64 (CUMULATIVE_ARGS *cum)
+{
+  rtx save_area, mem;
+  alias_set_type set;
+  int i, max;
+
+  /* GPR size of varargs save area.  */
+  if (cfun->va_list_gpr_size)
+    ix86_varargs_gpr_size = X86_64_REGPARM_MAX * UNITS_PER_WORD;
+  else
+    ix86_varargs_gpr_size = 0;
+
+  /* FPR size of varargs save area.  We don't need it if we don't pass
+     anything in SSE registers.  */
+  if (TARGET_SSE && cfun->va_list_fpr_size)
+    ix86_varargs_fpr_size = X86_64_SSE_REGPARM_MAX * 16;
+  else
+    ix86_varargs_fpr_size = 0;
+
+  if (! ix86_varargs_gpr_size && ! ix86_varargs_fpr_size)
+    return;
+
+  save_area = frame_pointer_rtx;
+  set = get_varargs_alias_set ();
+
+  max = cum->regno + cfun->va_list_gpr_size / UNITS_PER_WORD;
+  if (max > X86_64_REGPARM_MAX)
+    max = X86_64_REGPARM_MAX;
+
+  for (i = cum->regno; i < max; i++)
+    {
+      mem = gen_rtx_MEM (word_mode,
+			 plus_constant (Pmode, save_area, i * UNITS_PER_WORD));
+      MEM_NOTRAP_P (mem) = 1;
+      set_mem_alias_set (mem, set);
+      emit_move_insn (mem,
+		      gen_rtx_REG (word_mode,
+				   x86_64_int_parameter_registers[i]));
+    }
+
+  if (ix86_varargs_fpr_size)
+    {
+      machine_mode smode;
+      rtx_code_label *label;
+      rtx test;
+
+      /* Now emit code to save SSE registers.  The AX parameter contains number
+	 of SSE parameter registers used to call this function, though all we
+	 actually check here is the zero/non-zero status.  */
+
+      label = gen_label_rtx ();
+      test = gen_rtx_EQ (VOIDmode, gen_rtx_REG (QImode, AX_REG), const0_rtx);
+      emit_jump_insn (gen_cbranchqi4 (test, XEXP (test, 0), XEXP (test, 1),
+				      label));
+
+      /* ??? If !TARGET_SSE_TYPELESS_STORES, would we perform better if
+	 we used movdqa (i.e. TImode) instead?  Perhaps even better would
+	 be if we could determine the real mode of the data, via a hook
+	 into pass_stdarg.  Ignore all that for now.  */
+      smode = V4SFmode;
+      if (crtl->stack_alignment_needed < GET_MODE_ALIGNMENT (smode))
+	crtl->stack_alignment_needed = GET_MODE_ALIGNMENT (smode);
+
+      max = cum->sse_regno + cfun->va_list_fpr_size / 16;
+      if (max > X86_64_SSE_REGPARM_MAX)
+	max = X86_64_SSE_REGPARM_MAX;
+
+      for (i = cum->sse_regno; i < max; ++i)
+	{
+	  mem = plus_constant (Pmode, save_area,
+			       i * 16 + ix86_varargs_gpr_size);
+	  mem = gen_rtx_MEM (smode, mem);
+	  MEM_NOTRAP_P (mem) = 1;
+	  set_mem_alias_set (mem, set);
+	  set_mem_align (mem, GET_MODE_ALIGNMENT (smode));
+
+	  emit_move_insn (mem, gen_rtx_REG (smode, GET_SSE_REGNO (i)));
+	}
+
+      emit_label (label);
+    }
+}
+
+static void
+setup_incoming_varargs_ms_64 (CUMULATIVE_ARGS *cum)
+{
+  alias_set_type set = get_varargs_alias_set ();
+  int i;
+
+  /* Reset to zero, as there might be a sysv vaarg used
+     before.  */
+  ix86_varargs_gpr_size = 0;
+  ix86_varargs_fpr_size = 0;
+
+  for (i = cum->regno; i < X86_64_MS_REGPARM_MAX; i++)
+    {
+      rtx reg, mem;
+
+      mem = gen_rtx_MEM (Pmode,
+			 plus_constant (Pmode, virtual_incoming_args_rtx,
+					i * UNITS_PER_WORD));
+      MEM_NOTRAP_P (mem) = 1;
+      set_mem_alias_set (mem, set);
+
+      reg = gen_rtx_REG (Pmode, x86_64_ms_abi_int_parameter_registers[i]);
+      emit_move_insn (mem, reg);
+    }
+}
+
+static void
+ix86_setup_incoming_varargs (cumulative_args_t cum_v,
+			     const function_arg_info &arg,
+			     int *, int no_rtl)
+{
+  CUMULATIVE_ARGS *cum = get_cumulative_args (cum_v);
+  CUMULATIVE_ARGS next_cum;
+  tree fntype;
+
+  /* This argument doesn't appear to be used anymore.  Which is good,
+     because the old code here didn't suppress rtl generation.  */
+  gcc_assert (!no_rtl);
+
+  if (!TARGET_64BIT)
+    return;
+
+  fntype = TREE_TYPE (current_function_decl);
+
+  /* For varargs, we do not want to skip the dummy va_dcl argument.
+     For stdargs, we do want to skip the last named argument.  */
+  next_cum = *cum;
+  if (stdarg_p (fntype))
+    ix86_function_arg_advance (pack_cumulative_args (&next_cum), arg);
+
+  if (cum->call_abi == MS_ABI)
+    setup_incoming_varargs_ms_64 (&next_cum);
+  else
+    setup_incoming_varargs_64 (&next_cum);
+}
+
+/* Checks if TYPE is of kind va_list char *.  */
+
+static bool
+is_va_list_char_pointer (tree type)
+{
+  tree canonic;
+
+  /* For 32-bit it is always true.  */
+  if (!TARGET_64BIT)
+    return true;
+  canonic = ix86_canonical_va_list_type (type);
+  return (canonic == ms_va_list_type_node
+          || (ix86_abi == MS_ABI && canonic == va_list_type_node));
+}
+
+/* Implement va_start.  */
+
+static void
+ix86_va_start (tree valist, rtx nextarg)
+{
+  HOST_WIDE_INT words, n_gpr, n_fpr;
+  tree f_gpr, f_fpr, f_ovf, f_sav;
+  tree gpr, fpr, ovf, sav, t;
+  tree type;
+  rtx ovf_rtx;
+
+  if (flag_split_stack
+      && cfun->machine->split_stack_varargs_pointer == NULL_RTX)
+    {
+      unsigned int scratch_regno;
+
+      /* When we are splitting the stack, we can't refer to the stack
+	 arguments using internal_arg_pointer, because they may be on
+	 the old stack.  The split stack prologue will arrange to
+	 leave a pointer to the old stack arguments in a scratch
+	 register, which we here copy to a pseudo-register.  The split
+	 stack prologue can't set the pseudo-register directly because
+	 it (the prologue) runs before any registers have been saved.  */
+
+      scratch_regno = split_stack_prologue_scratch_regno ();
+      if (scratch_regno != INVALID_REGNUM)
+	{
+	  rtx reg;
+	  rtx_insn *seq;
+
+	  reg = gen_reg_rtx (Pmode);
+	  cfun->machine->split_stack_varargs_pointer = reg;
+
+	  start_sequence ();
+	  emit_move_insn (reg, gen_rtx_REG (Pmode, scratch_regno));
+	  seq = get_insns ();
+	  end_sequence ();
+
+	  push_topmost_sequence ();
+	  emit_insn_after (seq, entry_of_function ());
+	  pop_topmost_sequence ();
+	}
+    }
+
+  /* Only 64bit target needs something special.  */
+  if (is_va_list_char_pointer (TREE_TYPE (valist)))
+    {
+      if (cfun->machine->split_stack_varargs_pointer == NULL_RTX)
+	std_expand_builtin_va_start (valist, nextarg);
+      else
+	{
+	  rtx va_r, next;
+
+	  va_r = expand_expr (valist, NULL_RTX, VOIDmode, EXPAND_WRITE);
+	  next = expand_binop (ptr_mode, add_optab,
+			       cfun->machine->split_stack_varargs_pointer,
+			       crtl->args.arg_offset_rtx,
+			       NULL_RTX, 0, OPTAB_LIB_WIDEN);
+	  convert_move (va_r, next, 0);
+	}
+      return;
+    }
+
+  f_gpr = TYPE_FIELDS (TREE_TYPE (sysv_va_list_type_node));
+  f_fpr = DECL_CHAIN (f_gpr);
+  f_ovf = DECL_CHAIN (f_fpr);
+  f_sav = DECL_CHAIN (f_ovf);
+
+  valist = build_simple_mem_ref (valist);
+  TREE_TYPE (valist) = TREE_TYPE (sysv_va_list_type_node);
+  /* The following should be folded into the MEM_REF offset.  */
+  gpr = build3 (COMPONENT_REF, TREE_TYPE (f_gpr), unshare_expr (valist),
+		f_gpr, NULL_TREE);
+  fpr = build3 (COMPONENT_REF, TREE_TYPE (f_fpr), unshare_expr (valist),
+		f_fpr, NULL_TREE);
+  ovf = build3 (COMPONENT_REF, TREE_TYPE (f_ovf), unshare_expr (valist),
+		f_ovf, NULL_TREE);
+  sav = build3 (COMPONENT_REF, TREE_TYPE (f_sav), unshare_expr (valist),
+		f_sav, NULL_TREE);
+
+  /* Count number of gp and fp argument registers used.  */
+  words = crtl->args.info.words;
+  n_gpr = crtl->args.info.regno;
+  n_fpr = crtl->args.info.sse_regno;
+
+  if (cfun->va_list_gpr_size)
+    {
+      type = TREE_TYPE (gpr);
+      t = build2 (MODIFY_EXPR, type,
+		  gpr, build_int_cst (type, n_gpr * 8));
+      TREE_SIDE_EFFECTS (t) = 1;
+      expand_expr (t, const0_rtx, VOIDmode, EXPAND_NORMAL);
+    }
+
+  if (TARGET_SSE && cfun->va_list_fpr_size)
+    {
+      type = TREE_TYPE (fpr);
+      t = build2 (MODIFY_EXPR, type, fpr,
+		  build_int_cst (type, n_fpr * 16 + 8*X86_64_REGPARM_MAX));
+      TREE_SIDE_EFFECTS (t) = 1;
+      expand_expr (t, const0_rtx, VOIDmode, EXPAND_NORMAL);
+    }
+
+  /* Find the overflow area.  */
+  type = TREE_TYPE (ovf);
+  if (cfun->machine->split_stack_varargs_pointer == NULL_RTX)
+    ovf_rtx = crtl->args.internal_arg_pointer;
+  else
+    ovf_rtx = cfun->machine->split_stack_varargs_pointer;
+  t = make_tree (type, ovf_rtx);
+  if (words != 0)
+    t = fold_build_pointer_plus_hwi (t, words * UNITS_PER_WORD);
+
+  t = build2 (MODIFY_EXPR, type, ovf, t);
+  TREE_SIDE_EFFECTS (t) = 1;
+  expand_expr (t, const0_rtx, VOIDmode, EXPAND_NORMAL);
+
+  if (ix86_varargs_gpr_size || ix86_varargs_fpr_size)
+    {
+      /* Find the register save area.
+	 Prologue of the function save it right above stack frame.  */
+      type = TREE_TYPE (sav);
+      t = make_tree (type, frame_pointer_rtx);
+      if (!ix86_varargs_gpr_size)
+	t = fold_build_pointer_plus_hwi (t, -8 * X86_64_REGPARM_MAX);
+
+      t = build2 (MODIFY_EXPR, type, sav, t);
+      TREE_SIDE_EFFECTS (t) = 1;
+      expand_expr (t, const0_rtx, VOIDmode, EXPAND_NORMAL);
+    }
+}
+
+/* Implement va_arg.  */
+
+static tree
+ix86_gimplify_va_arg (tree valist, tree type, gimple_seq *pre_p,
+		      gimple_seq *post_p)
+{
+  static const int intreg[6] = { 0, 1, 2, 3, 4, 5 };
+  tree f_gpr, f_fpr, f_ovf, f_sav;
+  tree gpr, fpr, ovf, sav, t;
+  int size, rsize;
+  tree lab_false, lab_over = NULL_TREE;
+  tree addr, t2;
+  rtx container;
+  int indirect_p = 0;
+  tree ptrtype;
+  machine_mode nat_mode;
+  unsigned int arg_boundary;
+  unsigned int type_align;
+
+  /* Only 64bit target needs something special.  */
+  if (is_va_list_char_pointer (TREE_TYPE (valist)))
+    return std_gimplify_va_arg_expr (valist, type, pre_p, post_p);
+
+  f_gpr = TYPE_FIELDS (TREE_TYPE (sysv_va_list_type_node));
+  f_fpr = DECL_CHAIN (f_gpr);
+  f_ovf = DECL_CHAIN (f_fpr);
+  f_sav = DECL_CHAIN (f_ovf);
+
+  gpr = build3 (COMPONENT_REF, TREE_TYPE (f_gpr),
+		valist, f_gpr, NULL_TREE);
+
+  fpr = build3 (COMPONENT_REF, TREE_TYPE (f_fpr), valist, f_fpr, NULL_TREE);
+  ovf = build3 (COMPONENT_REF, TREE_TYPE (f_ovf), valist, f_ovf, NULL_TREE);
+  sav = build3 (COMPONENT_REF, TREE_TYPE (f_sav), valist, f_sav, NULL_TREE);
+
+  indirect_p = pass_va_arg_by_reference (type);
+  if (indirect_p)
+    type = build_pointer_type (type);
+  size = arg_int_size_in_bytes (type);
+  rsize = CEIL (size, UNITS_PER_WORD);
+
+  nat_mode = type_natural_mode (type, NULL, false);
+  switch (nat_mode)
+    {
+    case E_V8SFmode:
+    case E_V8SImode:
+    case E_V32QImode:
+    case E_V16HImode:
+    case E_V4DFmode:
+    case E_V4DImode:
+    case E_V16SFmode:
+    case E_V16SImode:
+    case E_V64QImode:
+    case E_V32HImode:
+    case E_V8DFmode:
+    case E_V8DImode:
+      /* Unnamed 256 and 512bit vector mode parameters are passed on stack.  */
+      if (!TARGET_64BIT_MS_ABI)
+	{
+	  container = NULL;
+	  break;
+	}
+      /* FALLTHRU */
+
+    default:
+      container = construct_container (nat_mode, TYPE_MODE (type),
+				       type, 0, X86_64_REGPARM_MAX,
+				       X86_64_SSE_REGPARM_MAX, intreg,
+				       0);
+      break;
+    }
+
+  /* Pull the value out of the saved registers.  */
+
+  addr = create_tmp_var (ptr_type_node, "addr");
+  type_align = TYPE_ALIGN (type);
+
+  if (container)
+    {
+      int needed_intregs, needed_sseregs;
+      bool need_temp;
+      tree int_addr, sse_addr;
+
+      lab_false = create_artificial_label (UNKNOWN_LOCATION);
+      lab_over = create_artificial_label (UNKNOWN_LOCATION);
+
+      examine_argument (nat_mode, type, 0, &needed_intregs, &needed_sseregs);
+
+      need_temp = (!REG_P (container)
+		   && ((needed_intregs && TYPE_ALIGN (type) > 64)
+		       || TYPE_ALIGN (type) > 128));
+
+      /* In case we are passing structure, verify that it is consecutive block
+         on the register save area.  If not we need to do moves.  */
+      if (!need_temp && !REG_P (container))
+	{
+	  /* Verify that all registers are strictly consecutive  */
+	  if (SSE_REGNO_P (REGNO (XEXP (XVECEXP (container, 0, 0), 0))))
+	    {
+	      int i;
+
+	      for (i = 0; i < XVECLEN (container, 0) && !need_temp; i++)
+		{
+		  rtx slot = XVECEXP (container, 0, i);
+		  if (REGNO (XEXP (slot, 0)) != FIRST_SSE_REG + (unsigned int) i
+		      || INTVAL (XEXP (slot, 1)) != i * 16)
+		    need_temp = true;
+		}
+	    }
+	  else
+	    {
+	      int i;
+
+	      for (i = 0; i < XVECLEN (container, 0) && !need_temp; i++)
+		{
+		  rtx slot = XVECEXP (container, 0, i);
+		  if (REGNO (XEXP (slot, 0)) != (unsigned int) i
+		      || INTVAL (XEXP (slot, 1)) != i * 8)
+		    need_temp = true;
+		}
+	    }
+	}
+      if (!need_temp)
+	{
+	  int_addr = addr;
+	  sse_addr = addr;
+	}
+      else
+	{
+	  int_addr = create_tmp_var (ptr_type_node, "int_addr");
+	  sse_addr = create_tmp_var (ptr_type_node, "sse_addr");
+	}
+
+      /* First ensure that we fit completely in registers.  */
+      if (needed_intregs)
+	{
+	  t = build_int_cst (TREE_TYPE (gpr),
+			     (X86_64_REGPARM_MAX - needed_intregs + 1) * 8);
+	  t = build2 (GE_EXPR, boolean_type_node, gpr, t);
+	  t2 = build1 (GOTO_EXPR, void_type_node, lab_false);
+	  t = build3 (COND_EXPR, void_type_node, t, t2, NULL_TREE);
+	  gimplify_and_add (t, pre_p);
+	}
+      if (needed_sseregs)
+	{
+	  t = build_int_cst (TREE_TYPE (fpr),
+			     (X86_64_SSE_REGPARM_MAX - needed_sseregs + 1) * 16
+			     + X86_64_REGPARM_MAX * 8);
+	  t = build2 (GE_EXPR, boolean_type_node, fpr, t);
+	  t2 = build1 (GOTO_EXPR, void_type_node, lab_false);
+	  t = build3 (COND_EXPR, void_type_node, t, t2, NULL_TREE);
+	  gimplify_and_add (t, pre_p);
+	}
+
+      /* Compute index to start of area used for integer regs.  */
+      if (needed_intregs)
+	{
+	  /* int_addr = gpr + sav; */
+	  t = fold_build_pointer_plus (sav, gpr);
+	  gimplify_assign (int_addr, t, pre_p);
+	}
+      if (needed_sseregs)
+	{
+	  /* sse_addr = fpr + sav; */
+	  t = fold_build_pointer_plus (sav, fpr);
+	  gimplify_assign (sse_addr, t, pre_p);
+	}
+      if (need_temp)
+	{
+	  int i, prev_size = 0;
+	  tree temp = create_tmp_var (type, "va_arg_tmp");
+
+	  /* addr = &temp; */
+	  t = build1 (ADDR_EXPR, build_pointer_type (type), temp);
+	  gimplify_assign (addr, t, pre_p);
+
+	  for (i = 0; i < XVECLEN (container, 0); i++)
+	    {
+	      rtx slot = XVECEXP (container, 0, i);
+	      rtx reg = XEXP (slot, 0);
+	      machine_mode mode = GET_MODE (reg);
+	      tree piece_type;
+	      tree addr_type;
+	      tree daddr_type;
+	      tree src_addr, src;
+	      int src_offset;
+	      tree dest_addr, dest;
+	      int cur_size = GET_MODE_SIZE (mode);
+
+	      gcc_assert (prev_size <= INTVAL (XEXP (slot, 1)));
+	      prev_size = INTVAL (XEXP (slot, 1));
+	      if (prev_size + cur_size > size)
+		{
+		  cur_size = size - prev_size;
+		  unsigned int nbits = cur_size * BITS_PER_UNIT;
+		  if (!int_mode_for_size (nbits, 1).exists (&mode))
+		    mode = QImode;
+		}
+	      piece_type = lang_hooks.types.type_for_mode (mode, 1);
+	      if (mode == GET_MODE (reg))
+		addr_type = build_pointer_type (piece_type);
+	      else
+		addr_type = build_pointer_type_for_mode (piece_type, ptr_mode,
+							 true);
+	      daddr_type = build_pointer_type_for_mode (piece_type, ptr_mode,
+							true);
+
+	      if (SSE_REGNO_P (REGNO (reg)))
+		{
+		  src_addr = sse_addr;
+		  src_offset = (REGNO (reg) - FIRST_SSE_REG) * 16;
+		}
+	      else
+		{
+		  src_addr = int_addr;
+		  src_offset = REGNO (reg) * 8;
+		}
+	      src_addr = fold_convert (addr_type, src_addr);
+	      src_addr = fold_build_pointer_plus_hwi (src_addr, src_offset);
+
+	      dest_addr = fold_convert (daddr_type, addr);
+	      dest_addr = fold_build_pointer_plus_hwi (dest_addr, prev_size);
+	      if (cur_size == GET_MODE_SIZE (mode))
+		{
+		  src = build_va_arg_indirect_ref (src_addr);
+		  dest = build_va_arg_indirect_ref (dest_addr);
+
+		  gimplify_assign (dest, src, pre_p);
+		}
+	      else
+		{
+		  tree copy
+		    = build_call_expr (builtin_decl_implicit (BUILT_IN_MEMCPY),
+				       3, dest_addr, src_addr,
+				       size_int (cur_size));
+		  gimplify_and_add (copy, pre_p);
+		}
+	      prev_size += cur_size;
+	    }
+	}
+
+      if (needed_intregs)
+	{
+	  t = build2 (PLUS_EXPR, TREE_TYPE (gpr), gpr,
+		      build_int_cst (TREE_TYPE (gpr), needed_intregs * 8));
+	  gimplify_assign (gpr, t, pre_p);
+	  /* The GPR save area guarantees only 8-byte alignment.  */
+	  if (!need_temp)
+	    type_align = MIN (type_align, 64);
+	}
+
+      if (needed_sseregs)
+	{
+	  t = build2 (PLUS_EXPR, TREE_TYPE (fpr), fpr,
+		      build_int_cst (TREE_TYPE (fpr), needed_sseregs * 16));
+	  gimplify_assign (unshare_expr (fpr), t, pre_p);
+	}
+
+      gimple_seq_add_stmt (pre_p, gimple_build_goto (lab_over));
+
+      gimple_seq_add_stmt (pre_p, gimple_build_label (lab_false));
+    }
+
+  /* ... otherwise out of the overflow area.  */
+
+  /* When we align parameter on stack for caller, if the parameter
+     alignment is beyond MAX_SUPPORTED_STACK_ALIGNMENT, it will be
+     aligned at MAX_SUPPORTED_STACK_ALIGNMENT.  We will match callee
+     here with caller.  */
+  arg_boundary = ix86_function_arg_boundary (VOIDmode, type);
+  if ((unsigned int) arg_boundary > MAX_SUPPORTED_STACK_ALIGNMENT)
+    arg_boundary = MAX_SUPPORTED_STACK_ALIGNMENT;
+
+  /* Care for on-stack alignment if needed.  */
+  if (arg_boundary <= 64 || size == 0)
+    t = ovf;
+ else
+    {
+      HOST_WIDE_INT align = arg_boundary / 8;
+      t = fold_build_pointer_plus_hwi (ovf, align - 1);
+      t = build2 (BIT_AND_EXPR, TREE_TYPE (t), t,
+		  build_int_cst (TREE_TYPE (t), -align));
+    }
+
+  gimplify_expr (&t, pre_p, NULL, is_gimple_val, fb_rvalue);
+  gimplify_assign (addr, t, pre_p);
+
+  t = fold_build_pointer_plus_hwi (t, rsize * UNITS_PER_WORD);
+  gimplify_assign (unshare_expr (ovf), t, pre_p);
+
+  if (container)
+    gimple_seq_add_stmt (pre_p, gimple_build_label (lab_over));
+
+  type = build_aligned_type (type, type_align);
+  ptrtype = build_pointer_type_for_mode (type, ptr_mode, true);
+  addr = fold_convert (ptrtype, addr);
+
+  if (indirect_p)
+    addr = build_va_arg_indirect_ref (addr);
+  return build_va_arg_indirect_ref (addr);
+}
+
+/* Return true if OPNUM's MEM should be matched
+   in movabs* patterns.  */
+
+bool
+ix86_check_movabs (rtx insn, int opnum)
+{
+  rtx set, mem;
+
+  set = PATTERN (insn);
+  if (GET_CODE (set) == PARALLEL)
+    set = XVECEXP (set, 0, 0);
+  gcc_assert (GET_CODE (set) == SET);
+  mem = XEXP (set, opnum);
+  while (SUBREG_P (mem))
+    mem = SUBREG_REG (mem);
+  gcc_assert (MEM_P (mem));
+  return volatile_ok || !MEM_VOLATILE_P (mem);
+}
+
+/* Return false if INSN contains a MEM with a non-default address space.  */
+bool
+ix86_check_no_addr_space (rtx insn)
+{
+  subrtx_var_iterator::array_type array;
+  FOR_EACH_SUBRTX_VAR (iter, array, PATTERN (insn), ALL)
+    {
+      rtx x = *iter;
+      if (MEM_P (x) && !ADDR_SPACE_GENERIC_P (MEM_ADDR_SPACE (x)))
+	return false;
+    }
+  return true;
+}
+
+/* Initialize the table of extra 80387 mathematical constants.  */
+
+static void
+init_ext_80387_constants (void)
+{
+  static const char * cst[5] =
+  {
+    "0.3010299956639811952256464283594894482",  /* 0: fldlg2  */
+    "0.6931471805599453094286904741849753009",  /* 1: fldln2  */
+    "1.4426950408889634073876517827983434472",  /* 2: fldl2e  */
+    "3.3219280948873623478083405569094566090",  /* 3: fldl2t  */
+    "3.1415926535897932385128089594061862044",  /* 4: fldpi   */
+  };
+  int i;
+
+  for (i = 0; i < 5; i++)
+    {
+      real_from_string (&ext_80387_constants_table[i], cst[i]);
+      /* Ensure each constant is rounded to XFmode precision.  */
+      real_convert (&ext_80387_constants_table[i],
+		    XFmode, &ext_80387_constants_table[i]);
+    }
+
+  ext_80387_constants_init = 1;
+}
+
+/* Return non-zero if the constant is something that
+   can be loaded with a special instruction.  */
+
+int
+standard_80387_constant_p (rtx x)
+{
+  machine_mode mode = GET_MODE (x);
+
+  const REAL_VALUE_TYPE *r;
+
+  if (!(CONST_DOUBLE_P (x) && X87_FLOAT_MODE_P (mode)))
+    return -1;
+
+  if (x == CONST0_RTX (mode))
+    return 1;
+  if (x == CONST1_RTX (mode))
+    return 2;
+
+  r = CONST_DOUBLE_REAL_VALUE (x);
+
+  /* For XFmode constants, try to find a special 80387 instruction when
+     optimizing for size or on those CPUs that benefit from them.  */
+  if (mode == XFmode
+      && (optimize_function_for_size_p (cfun) || TARGET_EXT_80387_CONSTANTS))
+    {
+      int i;
+
+      if (! ext_80387_constants_init)
+	init_ext_80387_constants ();
+
+      for (i = 0; i < 5; i++)
+        if (real_identical (r, &ext_80387_constants_table[i]))
+	  return i + 3;
+    }
+
+  /* Load of the constant -0.0 or -1.0 will be split as
+     fldz;fchs or fld1;fchs sequence.  */
+  if (real_isnegzero (r))
+    return 8;
+  if (real_identical (r, &dconstm1))
+    return 9;
+
+  return 0;
+}
+
+/* Return the opcode of the special instruction to be used to load
+   the constant X.  */
+
+const char *
+standard_80387_constant_opcode (rtx x)
+{
+  switch (standard_80387_constant_p (x))
+    {
+    case 1:
+      return "fldz";
+    case 2:
+      return "fld1";
+    case 3:
+      return "fldlg2";
+    case 4:
+      return "fldln2";
+    case 5:
+      return "fldl2e";
+    case 6:
+      return "fldl2t";
+    case 7:
+      return "fldpi";
+    case 8:
+    case 9:
+      return "#";
+    default:
+      gcc_unreachable ();
+    }
+}
+
+/* Return the CONST_DOUBLE representing the 80387 constant that is
+   loaded by the specified special instruction.  The argument IDX
+   matches the return value from standard_80387_constant_p.  */
+
+rtx
+standard_80387_constant_rtx (int idx)
+{
+  int i;
+
+  if (! ext_80387_constants_init)
+    init_ext_80387_constants ();
+
+  switch (idx)
+    {
+    case 3:
+    case 4:
+    case 5:
+    case 6:
+    case 7:
+      i = idx - 3;
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  return const_double_from_real_value (ext_80387_constants_table[i],
+				       XFmode);
+}
+
+/* Return 1 if X is all bits 0 and 2 if X is all bits 1
+   in supported SSE/AVX vector mode.  */
+
+int
+standard_sse_constant_p (rtx x, machine_mode pred_mode)
+{
+  machine_mode mode;
+
+  if (!TARGET_SSE)
+    return 0;
+
+  mode = GET_MODE (x);
+
+  if (x == const0_rtx || const0_operand (x, mode))
+    return 1;
+
+  if (x == constm1_rtx || vector_all_ones_operand (x, mode))
+    {
+      /* VOIDmode integer constant, get mode from the predicate.  */
+      if (mode == VOIDmode)
+	mode = pred_mode;
+
+      switch (GET_MODE_SIZE (mode))
+	{
+	case 64:
+	  if (TARGET_AVX512F)
+	    return 2;
+	  break;
+	case 32:
+	  if (TARGET_AVX2)
+	    return 2;
+	  break;
+	case 16:
+	  if (TARGET_SSE2)
+	    return 2;
+	  break;
+	case 0:
+	  /* VOIDmode */
+	  gcc_unreachable ();
+	default:
+	  break;
+	}
+    }
+
+  return 0;
+}
+
+/* Return the opcode of the special instruction to be used to load
+   the constant operands[1] into operands[0].  */
+
+const char *
+standard_sse_constant_opcode (rtx_insn *insn, rtx *operands)
+{
+  machine_mode mode;
+  rtx x = operands[1];
+
+  gcc_assert (TARGET_SSE);
+
+  mode = GET_MODE (x);
+
+  if (x == const0_rtx || const0_operand (x, mode))
+    {
+      switch (get_attr_mode (insn))
+	{
+	case MODE_TI:
+	  if (!EXT_REX_SSE_REG_P (operands[0]))
+	    return "%vpxor\t%0, %d0";
+	  /* FALLTHRU */
+	case MODE_XI:
+	case MODE_OI:
+	  if (EXT_REX_SSE_REG_P (operands[0]))
+	    return (TARGET_AVX512VL
+		    ? "vpxord\t%x0, %x0, %x0"
+		    : "vpxord\t%g0, %g0, %g0");
+	  return "vpxor\t%x0, %x0, %x0";
+
+	case MODE_V2DF:
+	  if (!EXT_REX_SSE_REG_P (operands[0]))
+	    return "%vxorpd\t%0, %d0";
+	  /* FALLTHRU */
+	case MODE_V8DF:
+	case MODE_V4DF:
+	  if (!EXT_REX_SSE_REG_P (operands[0]))
+	    return "vxorpd\t%x0, %x0, %x0";
+	  else if (TARGET_AVX512DQ)
+	    return (TARGET_AVX512VL
+		    ? "vxorpd\t%x0, %x0, %x0"
+		    : "vxorpd\t%g0, %g0, %g0");
+	  else
+	    return (TARGET_AVX512VL
+		    ? "vpxorq\t%x0, %x0, %x0"
+		    : "vpxorq\t%g0, %g0, %g0");
+
+	case MODE_V4SF:
+	  if (!EXT_REX_SSE_REG_P (operands[0]))
+	    return "%vxorps\t%0, %d0";
+	  /* FALLTHRU */
+	case MODE_V16SF:
+	case MODE_V8SF:
+	  if (!EXT_REX_SSE_REG_P (operands[0]))
+	    return "vxorps\t%x0, %x0, %x0";
+	  else if (TARGET_AVX512DQ)
+	    return (TARGET_AVX512VL
+		    ? "vxorps\t%x0, %x0, %x0"
+		    : "vxorps\t%g0, %g0, %g0");
+	  else
+	    return (TARGET_AVX512VL
+		    ? "vpxord\t%x0, %x0, %x0"
+		    : "vpxord\t%g0, %g0, %g0");
+
+	default:
+	  gcc_unreachable ();
+	}
+    }
+  else if (x == constm1_rtx || vector_all_ones_operand (x, mode))
+    {
+      enum attr_mode insn_mode = get_attr_mode (insn);
+      
+      switch (insn_mode)
+	{
+	case MODE_XI:
+	case MODE_V8DF:
+	case MODE_V16SF:
+	  gcc_assert (TARGET_AVX512F);
+	  return "vpternlogd\t{$0xFF, %g0, %g0, %g0|%g0, %g0, %g0, 0xFF}";
+
+	case MODE_OI:
+	case MODE_V4DF:
+	case MODE_V8SF:
+	  gcc_assert (TARGET_AVX2);
+	  /* FALLTHRU */
+	case MODE_TI:
+	case MODE_V2DF:
+	case MODE_V4SF:
+	  gcc_assert (TARGET_SSE2);
+	  if (!EXT_REX_SSE_REG_P (operands[0]))
+	    return (TARGET_AVX
+		    ? "vpcmpeqd\t%0, %0, %0"
+		    : "pcmpeqd\t%0, %0");
+	  else if (TARGET_AVX512VL)
+	    return "vpternlogd\t{$0xFF, %0, %0, %0|%0, %0, %0, 0xFF}";
+	  else
+	    return "vpternlogd\t{$0xFF, %g0, %g0, %g0|%g0, %g0, %g0, 0xFF}";
+
+	default:
+	  gcc_unreachable ();
+	}
+   }
+
+  gcc_unreachable ();
+}
+
+/* Returns true if INSN can be transformed from a memory load
+   to a supported FP constant load.  */
+
+bool
+ix86_standard_x87sse_constant_load_p (const rtx_insn *insn, rtx dst)
+{
+  rtx src = find_constant_src (insn);
+
+  gcc_assert (REG_P (dst));
+
+  if (src == NULL
+      || (SSE_REGNO_P (REGNO (dst))
+	  && standard_sse_constant_p (src, GET_MODE (dst)) != 1)
+      || (STACK_REGNO_P (REGNO (dst))
+	   && standard_80387_constant_p (src) < 1))
+    return false;
+
+  return true;
+}
+
+/* Predicate for pre-reload splitters with associated instructions,
+   which can match any time before the split1 pass (usually combine),
+   then are unconditionally split in that pass and should not be
+   matched again afterwards.  */
+
+bool
+ix86_pre_reload_split (void)
+{
+  return (can_create_pseudo_p ()
+	  && !(cfun->curr_properties & PROP_rtl_split_insns));
+}
+
+/* Return the opcode of the TYPE_SSEMOV instruction.  To move from
+   or to xmm16-xmm31/ymm16-ymm31 registers, we either require
+   TARGET_AVX512VL or it is a register to register move which can
+   be done with zmm register move. */
+
+static const char *
+ix86_get_ssemov (rtx *operands, unsigned size,
+		 enum attr_mode insn_mode, machine_mode mode)
+{
+  char buf[128];
+  bool misaligned_p = (misaligned_operand (operands[0], mode)
+		       || misaligned_operand (operands[1], mode));
+  bool evex_reg_p = (size == 64
+		     || EXT_REX_SSE_REG_P (operands[0])
+		     || EXT_REX_SSE_REG_P (operands[1]));
+  machine_mode scalar_mode;
+
+  const char *opcode = NULL;
+  enum
+    {
+      opcode_int,
+      opcode_float,
+      opcode_double
+    } type = opcode_int;
+
+  switch (insn_mode)
+    {
+    case MODE_V16SF:
+    case MODE_V8SF:
+    case MODE_V4SF:
+      scalar_mode = E_SFmode;
+      type = opcode_float;
+      break;
+    case MODE_V8DF:
+    case MODE_V4DF:
+    case MODE_V2DF:
+      scalar_mode = E_DFmode;
+      type = opcode_double;
+      break;
+    case MODE_XI:
+    case MODE_OI:
+    case MODE_TI:
+      scalar_mode = GET_MODE_INNER (mode);
+      break;
+    default:
+      gcc_unreachable ();
+    }
+
+  /* NB: To move xmm16-xmm31/ymm16-ymm31 registers without AVX512VL,
+     we can only use zmm register move without memory operand.  */
+  if (evex_reg_p
+      && !TARGET_AVX512VL
+      && GET_MODE_SIZE (mode) < 64)
+    {
+      /* NB: Even though ix86_hard_regno_mode_ok doesn't allow
+	 xmm16-xmm31 nor ymm16-ymm31 in 128/256 bit modes when
+	 AVX512VL is disabled, LRA can still generate reg to
+	 reg moves with xmm16-xmm31 and ymm16-ymm31 in 128/256 bit
+	 modes.  */
+      if (memory_operand (operands[0], mode)
+	  || memory_operand (operands[1], mode))
+	gcc_unreachable ();
+      size = 64;
+      switch (type)
+	{
+	case opcode_int:
+	  opcode = misaligned_p ? "vmovdqu32" : "vmovdqa32";
+	  break;
+	case opcode_float:
+	  opcode = misaligned_p ? "vmovups" : "vmovaps";
+	  break;
+	case opcode_double:
+	  opcode = misaligned_p ? "vmovupd" : "vmovapd";
+	  break;
+	}
+    }
+  else if (SCALAR_FLOAT_MODE_P (scalar_mode))
+    {
+      switch (scalar_mode)
+	{
+	case E_SFmode:
+	  opcode = misaligned_p ? "%vmovups" : "%vmovaps";
+	  break;
+	case E_DFmode:
+	  opcode = misaligned_p ? "%vmovupd" : "%vmovapd";
+	  break;
+	case E_TFmode:
+	  if (evex_reg_p)
+	    opcode = misaligned_p ? "vmovdqu64" : "vmovdqa64";
+	  else
+	    opcode = misaligned_p ? "%vmovdqu" : "%vmovdqa";
+	  break;
+	default:
+	  gcc_unreachable ();
+	}
+    }
+  else if (SCALAR_INT_MODE_P (scalar_mode))
+    {
+      switch (scalar_mode)
+	{
+	case E_QImode:
+	  if (evex_reg_p)
+	    opcode = (misaligned_p
+		      ? (TARGET_AVX512BW
+			 ? "vmovdqu8"
+			 : "vmovdqu64")
+		      : "vmovdqa64");
+	  else
+	    opcode = (misaligned_p
+		      ? (TARGET_AVX512BW
+			 ? "vmovdqu8"
+			 : "%vmovdqu")
+		      : "%vmovdqa");
+	  break;
+	case E_HImode:
+	  if (evex_reg_p)
+	    opcode = (misaligned_p
+		      ? (TARGET_AVX512BW
+			 ? "vmovdqu16"
+			 : "vmovdqu64")
+		      : "vmovdqa64");
+	  else
+	    opcode = (misaligned_p
+		      ? (TARGET_AVX512BW
+			 ? "vmovdqu16"
+			 : "%vmovdqu")
+		      : "%vmovdqa");
+	  break;
+	case E_SImode:
+	  if (evex_reg_p)
+	    opcode = misaligned_p ? "vmovdqu32" : "vmovdqa32";
+	  else
+	    opcode = misaligned_p ? "%vmovdqu" : "%vmovdqa";
+	  break;
+	case E_DImode:
+	case E_TImode:
+	case E_OImode:
+	  if (evex_reg_p)
+	    opcode = misaligned_p ? "vmovdqu64" : "vmovdqa64";
+	  else
+	    opcode = misaligned_p ? "%vmovdqu" : "%vmovdqa";
+	  break;
+	case E_XImode:
+	  opcode = misaligned_p ? "vmovdqu64" : "vmovdqa64";
+	  break;
+	default:
+	  gcc_unreachable ();
+	}
+    }
+  else
+    gcc_unreachable ();
+
+  switch (size)
+    {
+    case 64:
+      snprintf (buf, sizeof (buf), "%s\t{%%g1, %%g0|%%g0, %%g1}",
+		opcode);
+      break;
+    case 32:
+      snprintf (buf, sizeof (buf), "%s\t{%%t1, %%t0|%%t0, %%t1}",
+		opcode);
+      break;
+    case 16:
+      snprintf (buf, sizeof (buf), "%s\t{%%x1, %%x0|%%x0, %%x1}",
+		opcode);
+      break;
+    default:
+      gcc_unreachable ();
+    }
+  output_asm_insn (buf, operands);
+  return "";
+}
+
+/* Return the template of the TYPE_SSEMOV instruction to move
+   operands[1] into operands[0].  */
+
+const char *
+ix86_output_ssemov (rtx_insn *insn, rtx *operands)
+{
+  machine_mode mode = GET_MODE (operands[0]);
+  if (get_attr_type (insn) != TYPE_SSEMOV
+      || mode != GET_MODE (operands[1]))
+    gcc_unreachable ();
+
+  enum attr_mode insn_mode = get_attr_mode (insn);
+
+  switch (insn_mode)
+    {
+    case MODE_XI:
+    case MODE_V8DF:
+    case MODE_V16SF:
+      return ix86_get_ssemov (operands, 64, insn_mode, mode);
+
+    case MODE_OI:
+    case MODE_V4DF:
+    case MODE_V8SF:
+      return ix86_get_ssemov (operands, 32, insn_mode, mode);
+
+    case MODE_TI:
+    case MODE_V2DF:
+    case MODE_V4SF:
+      return ix86_get_ssemov (operands, 16, insn_mode, mode);
+
+    case MODE_DI:
+      /* Handle broken assemblers that require movd instead of movq. */
+      if (!HAVE_AS_IX86_INTERUNIT_MOVQ
+	  && (GENERAL_REG_P (operands[0])
+	      || GENERAL_REG_P (operands[1])))
+	return "%vmovd\t{%1, %0|%0, %1}";
+      else
+	return "%vmovq\t{%1, %0|%0, %1}";
+
+    case MODE_SI:
+      return "%vmovd\t{%1, %0|%0, %1}";
+
+    case MODE_DF:
+      if (TARGET_AVX && REG_P (operands[0]) && REG_P (operands[1]))
+	return "vmovsd\t{%d1, %0|%0, %d1}";
+      else
+	return "%vmovsd\t{%1, %0|%0, %1}";
+
+    case MODE_SF:
+      if (TARGET_AVX && REG_P (operands[0]) && REG_P (operands[1]))
+	return "vmovss\t{%d1, %0|%0, %d1}";
+      else
+	return "%vmovss\t{%1, %0|%0, %1}";
+
+    case MODE_V1DF:
+      gcc_assert (!TARGET_AVX);
+      return "movlpd\t{%1, %0|%0, %1}";
+
+    case MODE_V2SF:
+      if (TARGET_AVX && REG_P (operands[0]))
+	return "vmovlps\t{%1, %d0|%d0, %1}";
+      else
+	return "%vmovlps\t{%1, %0|%0, %1}";
+
+    default:
+      gcc_unreachable ();
+    }
+}
+
+/* Returns true if OP contains a symbol reference */
+
+bool
+symbolic_reference_mentioned_p (rtx op)
+{
+  const char *fmt;
+  int i;
+
+  if (GET_CODE (op) == SYMBOL_REF || GET_CODE (op) == LABEL_REF)
+    return true;
+
+  fmt = GET_RTX_FORMAT (GET_CODE (op));
+  for (i = GET_RTX_LENGTH (GET_CODE (op)) - 1; i >= 0; i--)
+    {
+      if (fmt[i] == 'E')
+	{
+	  int j;
+
+	  for (j = XVECLEN (op, i) - 1; j >= 0; j--)
+	    if (symbolic_reference_mentioned_p (XVECEXP (op, i, j)))
+	      return true;
+	}
+
+      else if (fmt[i] == 'e' && symbolic_reference_mentioned_p (XEXP (op, i)))
+	return true;
+    }
+
+  return false;
+}
+
+/* Return true if it is appropriate to emit `ret' instructions in the
+   body of a function.  Do this only if the epilogue is simple, needing a
+   couple of insns.  Prior to reloading, we can't tell how many registers
+   must be saved, so return false then.  Return false if there is no frame
+   marker to de-allocate.  */
+
+bool
+ix86_can_use_return_insn_p (void)
+{
+  if (ix86_function_ms_hook_prologue (current_function_decl))
+    return false;
+
+  if (ix86_function_naked (current_function_decl))
+    return false;
+
+  /* Don't use `ret' instruction in interrupt handler.  */
+  if (! reload_completed
+      || frame_pointer_needed
+      || cfun->machine->func_type != TYPE_NORMAL)
+    return 0;
+
+  /* Don't allow more than 32k pop, since that's all we can do
+     with one instruction.  */
+  if (crtl->args.pops_args && crtl->args.size >= 32768)
+    return 0;
+
+  struct ix86_frame &frame = cfun->machine->frame;
+  return (frame.stack_pointer_offset == UNITS_PER_WORD
+	  && (frame.nregs + frame.nsseregs) == 0);
+}
+
+/* Return stack frame size.  get_frame_size () returns used stack slots
+   during compilation, which may be optimized out later.  If stack frame
+   is needed, stack_frame_required should be true.  */
+
+static HOST_WIDE_INT
+ix86_get_frame_size (void)
+{
+  if (cfun->machine->stack_frame_required)
+    return get_frame_size ();
+  else
+    return 0;
+}
+
+/* Value should be nonzero if functions must have frame pointers.
+   Zero means the frame pointer need not be set up (and parms may
+   be accessed via the stack pointer) in functions that seem suitable.  */
+
+static bool
+ix86_frame_pointer_required (void)
+{
+  /* If we accessed previous frames, then the generated code expects
+     to be able to access the saved ebp value in our frame.  */
+  if (cfun->machine->accesses_prev_frame)
+    return true;
+
+  /* Several x86 os'es need a frame pointer for other reasons,
+     usually pertaining to setjmp.  */
+  if (SUBTARGET_FRAME_POINTER_REQUIRED)
+    return true;
+
+  /* For older 32-bit runtimes setjmp requires valid frame-pointer.  */
+  if (TARGET_32BIT_MS_ABI && cfun->calls_setjmp)
+    return true;
+
+  /* Win64 SEH, very large frames need a frame-pointer as maximum stack
+     allocation is 4GB.  */
+  if (TARGET_64BIT_MS_ABI && ix86_get_frame_size () > SEH_MAX_FRAME_SIZE)
+    return true;
+
+  /* SSE saves require frame-pointer when stack is misaligned.  */
+  if (TARGET_64BIT_MS_ABI && ix86_incoming_stack_boundary < 128)
+    return true;
+  
+  /* In ix86_option_override_internal, TARGET_OMIT_LEAF_FRAME_POINTER
+     turns off the frame pointer by default.  Turn it back on now if
+     we've not got a leaf function.  */
+  if (TARGET_OMIT_LEAF_FRAME_POINTER
+      && (!crtl->is_leaf
+	  || ix86_current_function_calls_tls_descriptor))
+    return true;
+
+  /* Several versions of mcount for the x86 assumes that there is a
+     frame, so we cannot allow profiling without a frame pointer.  */
+  if (crtl->profile && !flag_fentry)
+    return true;
+
+  return false;
+}
+
+/* Record that the current function accesses previous call frames.  */
+
+void
+ix86_setup_frame_addresses (void)
+{
+  cfun->machine->accesses_prev_frame = 1;
+}
+
+#ifndef USE_HIDDEN_LINKONCE
+# if defined(HAVE_GAS_HIDDEN) && (SUPPORTS_ONE_ONLY - 0)
+#  define USE_HIDDEN_LINKONCE 1
+# else
+#  define USE_HIDDEN_LINKONCE 0
+# endif
+#endif
+
+/* Label count for call and return thunks.  It is used to make unique
+   labels in call and return thunks.  */
+static int indirectlabelno;
+
+/* True if call thunk function is needed.  */
+static bool indirect_thunk_needed = false;
+
+/* Bit masks of integer registers, which contain branch target, used
+   by call thunk functions.  */
+static int indirect_thunks_used;
+
+/* True if return thunk function is needed.  */
+static bool indirect_return_needed = false;
+
+/* True if return thunk function via CX is needed.  */
+static bool indirect_return_via_cx;
+
+#ifndef INDIRECT_LABEL
+# define INDIRECT_LABEL "LIND"
+#endif
+
+/* Indicate what prefix is needed for an indirect branch.  */
+enum indirect_thunk_prefix
+{
+  indirect_thunk_prefix_none,
+  indirect_thunk_prefix_nt
+};
+
+/* Return the prefix needed for an indirect branch INSN.  */
+
+enum indirect_thunk_prefix
+indirect_thunk_need_prefix (rtx_insn *insn)
+{
+  enum indirect_thunk_prefix need_prefix;
+  if ((cfun->machine->indirect_branch_type
+	    == indirect_branch_thunk_extern)
+	   && ix86_notrack_prefixed_insn_p (insn))
+    {
+      /* NOTRACK prefix is only used with external thunk so that it
+	 can be properly updated to support CET at run-time.  */
+      need_prefix = indirect_thunk_prefix_nt;
+    }
+  else
+    need_prefix = indirect_thunk_prefix_none;
+  return need_prefix;
+}
+
+/* Fills in the label name that should be used for the indirect thunk.  */
+
+static void
+indirect_thunk_name (char name[32], unsigned int regno,
+		     enum indirect_thunk_prefix need_prefix,
+		     bool ret_p)
+{
+  if (regno != INVALID_REGNUM && regno != CX_REG && ret_p)
+    gcc_unreachable ();
+
+  if (USE_HIDDEN_LINKONCE)
+    {
+      const char *prefix;
+
+      if (need_prefix == indirect_thunk_prefix_nt
+	  && regno != INVALID_REGNUM)
+	{
+	  /* NOTRACK prefix is only used with external thunk via
+	     register so that NOTRACK prefix can be added to indirect
+	     branch via register to support CET at run-time.  */
+	  prefix = "_nt";
+	}
+      else
+	prefix = "";
+
+      const char *ret = ret_p ? "return" : "indirect";
+
+      if (regno != INVALID_REGNUM)
+	{
+	  const char *reg_prefix;
+	  if (LEGACY_INT_REGNO_P (regno))
+	    reg_prefix = TARGET_64BIT ? "r" : "e";
+	  else
+	    reg_prefix = "";
+	  sprintf (name, "__x86_%s_thunk%s_%s%s",
+		   ret, prefix, reg_prefix, reg_names[regno]);
+	}
+      else
+	sprintf (name, "__x86_%s_thunk%s", ret, prefix);
+    }
+  else
+    {
+      if (regno != INVALID_REGNUM)
+	ASM_GENERATE_INTERNAL_LABEL (name, "LITR", regno);
+      else
+	{
+	  if (ret_p)
+	    ASM_GENERATE_INTERNAL_LABEL (name, "LRT", 0);
+	  else
+	    ASM_GENERATE_INTERNAL_LABEL (name, "LIT", 0);
+	}
+    }
+}
+
+/* Output a call and return thunk for indirect branch.  If REGNO != -1,
+   the function address is in REGNO and the call and return thunk looks like:
+
+	call	L2
+   L1:
+	pause
+	lfence
+	jmp	L1
+   L2:
+	mov	%REG, (%sp)
+	ret
+
+   Otherwise, the function address is on the top of stack and the
+   call and return thunk looks like:
+
+	call L2
+  L1:
+	pause
+	lfence
+	jmp L1
+  L2:
+	lea WORD_SIZE(%sp), %sp
+	ret
+ */
+
+static void
+output_indirect_thunk (unsigned int regno)
+{
+  char indirectlabel1[32];
+  char indirectlabel2[32];
+
+  ASM_GENERATE_INTERNAL_LABEL (indirectlabel1, INDIRECT_LABEL,
+			       indirectlabelno++);
+  ASM_GENERATE_INTERNAL_LABEL (indirectlabel2, INDIRECT_LABEL,
+			       indirectlabelno++);
+
+  /* Call */
+  fputs ("\tcall\t", asm_out_file);
+  assemble_name_raw (asm_out_file, indirectlabel2);
+  fputc ('\n', asm_out_file);
+
+  ASM_OUTPUT_INTERNAL_LABEL (asm_out_file, indirectlabel1);
+
+  /* AMD and Intel CPUs prefer each a different instruction as loop filler.
+     Usage of both pause + lfence is compromise solution.  */
+  fprintf (asm_out_file, "\tpause\n\tlfence\n");
+
+  /* Jump.  */
+  fputs ("\tjmp\t", asm_out_file);
+  assemble_name_raw (asm_out_file, indirectlabel1);
+  fputc ('\n', asm_out_file);
+
+  ASM_OUTPUT_INTERNAL_LABEL (asm_out_file, indirectlabel2);
+
+  /* The above call insn pushed a word to stack.  Adjust CFI info.  */
+  if (flag_asynchronous_unwind_tables && dwarf2out_do_frame ())
+    {
+      if (! dwarf2out_do_cfi_asm ())
+	{
+	  dw_cfi_ref xcfi = ggc_cleared_alloc<dw_cfi_node> ();
+	  xcfi->dw_cfi_opc = DW_CFA_advance_loc4;
+	  xcfi->dw_cfi_oprnd1.dw_cfi_addr = ggc_strdup (indirectlabel2);
+	  vec_safe_push (cfun->fde->dw_fde_cfi, xcfi);
+	}
+      dw_cfi_ref xcfi = ggc_cleared_alloc<dw_cfi_node> ();
+      xcfi->dw_cfi_opc = DW_CFA_def_cfa_offset;
+      xcfi->dw_cfi_oprnd1.dw_cfi_offset = 2 * UNITS_PER_WORD;
+      vec_safe_push (cfun->fde->dw_fde_cfi, xcfi);
+      dwarf2out_emit_cfi (xcfi);
+    }
+
+  if (regno != INVALID_REGNUM)
+    {
+      /* MOV.  */
+      rtx xops[2];
+      xops[0] = gen_rtx_MEM (word_mode, stack_pointer_rtx);
+      xops[1] = gen_rtx_REG (word_mode, regno);
+      output_asm_insn ("mov\t{%1, %0|%0, %1}", xops);
+    }
+  else
+    {
+      /* LEA.  */
+      rtx xops[2];
+      xops[0] = stack_pointer_rtx;
+      xops[1] = plus_constant (Pmode, stack_pointer_rtx, UNITS_PER_WORD);
+      output_asm_insn ("lea\t{%E1, %0|%0, %E1}", xops);
+    }
+
+  fputs ("\tret\n", asm_out_file);
+}
+
+/* Output a funtion with a call and return thunk for indirect branch.
+   If REGNO != INVALID_REGNUM, the function address is in REGNO.
+   Otherwise, the function address is on the top of stack.  Thunk is
+   used for function return if RET_P is true.  */
+
+static void
+output_indirect_thunk_function (enum indirect_thunk_prefix need_prefix,
+				unsigned int regno, bool ret_p)
+{
+  char name[32];
+  tree decl;
+
+  /* Create __x86_indirect_thunk.  */
+  indirect_thunk_name (name, regno, need_prefix, ret_p);
+  decl = build_decl (BUILTINS_LOCATION, FUNCTION_DECL,
+		     get_identifier (name),
+		     build_function_type_list (void_type_node, NULL_TREE));
+  DECL_RESULT (decl) = build_decl (BUILTINS_LOCATION, RESULT_DECL,
+				   NULL_TREE, void_type_node);
+  TREE_PUBLIC (decl) = 1;
+  TREE_STATIC (decl) = 1;
+  DECL_IGNORED_P (decl) = 1;
+
+#if TARGET_MACHO
+  if (TARGET_MACHO)
+    {
+      switch_to_section (darwin_sections[picbase_thunk_section]);
+      fputs ("\t.weak_definition\t", asm_out_file);
+      assemble_name (asm_out_file, name);
+      fputs ("\n\t.private_extern\t", asm_out_file);
+      assemble_name (asm_out_file, name);
+      putc ('\n', asm_out_file);
+      ASM_OUTPUT_LABEL (asm_out_file, name);
+      DECL_WEAK (decl) = 1;
+    }
+  else
+#endif
+    if (USE_HIDDEN_LINKONCE)
+      {
+	cgraph_node::create (decl)->set_comdat_group (DECL_ASSEMBLER_NAME (decl));
+
+	targetm.asm_out.unique_section (decl, 0);
+	switch_to_section (get_named_section (decl, NULL, 0));
+
+	targetm.asm_out.globalize_label (asm_out_file, name);
+	fputs ("\t.hidden\t", asm_out_file);
+	assemble_name (asm_out_file, name);
+	putc ('\n', asm_out_file);
+	ASM_DECLARE_FUNCTION_NAME (asm_out_file, name, decl);
+      }
+    else
+      {
+	switch_to_section (text_section);
+	ASM_OUTPUT_LABEL (asm_out_file, name);
+      }
+
+  DECL_INITIAL (decl) = make_node (BLOCK);
+  current_function_decl = decl;
+  allocate_struct_function (decl, false);
+  init_function_start (decl);
+  /* We're about to hide the function body from callees of final_* by
+     emitting it directly; tell them we're a thunk, if they care.  */
+  cfun->is_thunk = true;
+  first_function_block_is_cold = false;
+  /* Make sure unwind info is emitted for the thunk if needed.  */
+  final_start_function (emit_barrier (), asm_out_file, 1);
+
+  output_indirect_thunk (regno);
+
+  final_end_function ();
+  init_insn_lengths ();
+  free_after_compilation (cfun);
+  set_cfun (NULL);
+  current_function_decl = NULL;
+}
+
+static int pic_labels_used;
+
+/* Fills in the label name that should be used for a pc thunk for
+   the given register.  */
+
+static void
+get_pc_thunk_name (char name[32], unsigned int regno)
+{
+  gcc_assert (!TARGET_64BIT);
+
+  if (USE_HIDDEN_LINKONCE)
+    sprintf (name, "__x86.get_pc_thunk.%s", reg_names[regno]);
+  else
+    ASM_GENERATE_INTERNAL_LABEL (name, "LPR", regno);
+}
+
+
+/* This function generates code for -fpic that loads %ebx with
+   the return address of the caller and then returns.  */
+
+static void
+ix86_code_end (void)
+{
+  rtx xops[2];
+  unsigned int regno;
+
+  if (indirect_return_needed)
+    output_indirect_thunk_function (indirect_thunk_prefix_none,
+				    INVALID_REGNUM, true);
+  if (indirect_return_via_cx)
+    output_indirect_thunk_function (indirect_thunk_prefix_none,
+				    CX_REG, true);
+  if (indirect_thunk_needed)
+    output_indirect_thunk_function (indirect_thunk_prefix_none,
+				    INVALID_REGNUM, false);
+
+  for (regno = FIRST_REX_INT_REG; regno <= LAST_REX_INT_REG; regno++)
+    {
+      unsigned int i = regno - FIRST_REX_INT_REG + LAST_INT_REG + 1;
+      if ((indirect_thunks_used & (1 << i)))
+	output_indirect_thunk_function (indirect_thunk_prefix_none,
+					regno, false);
+    }
+
+  for (regno = FIRST_INT_REG; regno <= LAST_INT_REG; regno++)
+    {
+      char name[32];
+      tree decl;
+
+      if ((indirect_thunks_used & (1 << regno)))
+	output_indirect_thunk_function (indirect_thunk_prefix_none,
+					regno, false);
+
+      if (!(pic_labels_used & (1 << regno)))
+	continue;
+
+      get_pc_thunk_name (name, regno);
+
+      decl = build_decl (BUILTINS_LOCATION, FUNCTION_DECL,
+			 get_identifier (name),
+			 build_function_type_list (void_type_node, NULL_TREE));
+      DECL_RESULT (decl) = build_decl (BUILTINS_LOCATION, RESULT_DECL,
+				       NULL_TREE, void_type_node);
+      TREE_PUBLIC (decl) = 1;
+      TREE_STATIC (decl) = 1;
+      DECL_IGNORED_P (decl) = 1;
+
+#if TARGET_MACHO
+      if (TARGET_MACHO)
+	{
+	  switch_to_section (darwin_sections[picbase_thunk_section]);
+	  fputs ("\t.weak_definition\t", asm_out_file);
+	  assemble_name (asm_out_file, name);
+	  fputs ("\n\t.private_extern\t", asm_out_file);
+	  assemble_name (asm_out_file, name);
+	  putc ('\n', asm_out_file);
+	  ASM_OUTPUT_LABEL (asm_out_file, name);
+	  DECL_WEAK (decl) = 1;
+	}
+      else
+#endif
+      if (USE_HIDDEN_LINKONCE)
+	{
+	  cgraph_node::create (decl)->set_comdat_group (DECL_ASSEMBLER_NAME (decl));
+
+	  targetm.asm_out.unique_section (decl, 0);
+	  switch_to_section (get_named_section (decl, NULL, 0));
+
+	  targetm.asm_out.globalize_label (asm_out_file, name);
+	  fputs ("\t.hidden\t", asm_out_file);
+	  assemble_name (asm_out_file, name);
+	  putc ('\n', asm_out_file);
+	  ASM_DECLARE_FUNCTION_NAME (asm_out_file, name, decl);
+	}
+      else
+	{
+	  switch_to_section (text_section);
+	  ASM_OUTPUT_LABEL (asm_out_file, name);
+	}
+
+      DECL_INITIAL (decl) = make_node (BLOCK);
+      current_function_decl = decl;
+      allocate_struct_function (decl, false);
+      init_function_start (decl);
+      /* We're about to hide the function body from callees of final_* by
+	 emitting it directly; tell them we're a thunk, if they care.  */
+      cfun->is_thunk = true;
+      first_function_block_is_cold = false;
+      /* Make sure unwind info is emitted for the thunk if needed.  */
+      final_start_function (emit_barrier (), asm_out_file, 1);
+
+      /* Pad stack IP move with 4 instructions (two NOPs count
+	 as one instruction).  */
+      if (TARGET_PAD_SHORT_FUNCTION)
+	{
+	  int i = 8;
+
+	  while (i--)
+	    fputs ("\tnop\n", asm_out_file);
+	}
+
+      xops[0] = gen_rtx_REG (Pmode, regno);
+      xops[1] = gen_rtx_MEM (Pmode, stack_pointer_rtx);
+      output_asm_insn ("mov%z0\t{%1, %0|%0, %1}", xops);
+      output_asm_insn ("%!ret", NULL);
+      final_end_function ();
+      init_insn_lengths ();
+      free_after_compilation (cfun);
+      set_cfun (NULL);
+      current_function_decl = NULL;
+    }
+
+  if (flag_split_stack)
+    file_end_indicate_split_stack ();
+}
+
+/* Emit code for the SET_GOT patterns.  */
+
+const char *
+output_set_got (rtx dest, rtx label)
+{
+  rtx xops[3];
+
+  xops[0] = dest;
+
+  if (TARGET_VXWORKS_RTP && flag_pic)
+    {
+      /* Load (*VXWORKS_GOTT_BASE) into the PIC register.  */
+      xops[2] = gen_rtx_MEM (Pmode,
+			     gen_rtx_SYMBOL_REF (Pmode, VXWORKS_GOTT_BASE));
+      output_asm_insn ("mov{l}\t{%2, %0|%0, %2}", xops);
+
+      /* Load (*VXWORKS_GOTT_BASE)[VXWORKS_GOTT_INDEX] into the PIC register.
+	 Use %P and a local symbol in order to print VXWORKS_GOTT_INDEX as
+	 an unadorned address.  */
+      xops[2] = gen_rtx_SYMBOL_REF (Pmode, VXWORKS_GOTT_INDEX);
+      SYMBOL_REF_FLAGS (xops[2]) |= SYMBOL_FLAG_LOCAL;
+      output_asm_insn ("mov{l}\t{%P2(%0), %0|%0, DWORD PTR %P2[%0]}", xops);
+      return "";
+    }
+
+  xops[1] = gen_rtx_SYMBOL_REF (Pmode, GOT_SYMBOL_NAME);
+
+  if (flag_pic)
+    {
+      char name[32];
+      get_pc_thunk_name (name, REGNO (dest));
+      pic_labels_used |= 1 << REGNO (dest);
+
+      xops[2] = gen_rtx_SYMBOL_REF (Pmode, ggc_strdup (name));
+      xops[2] = gen_rtx_MEM (QImode, xops[2]);
+      output_asm_insn ("%!call\t%X2", xops);
+
+#if TARGET_MACHO
+      /* Output the Mach-O "canonical" pic base label name ("Lxx$pb") here.
+         This is what will be referenced by the Mach-O PIC subsystem.  */
+      if (machopic_should_output_picbase_label () || !label)
+	ASM_OUTPUT_LABEL (asm_out_file, MACHOPIC_FUNCTION_BASE_NAME);
+
+      /* When we are restoring the pic base at the site of a nonlocal label,
+         and we decided to emit the pic base above, we will still output a
+         local label used for calculating the correction offset (even though
+         the offset will be 0 in that case).  */
+      if (label)
+        targetm.asm_out.internal_label (asm_out_file, "L",
+					   CODE_LABEL_NUMBER (label));
+#endif
+    }
+  else
+    {
+      if (TARGET_MACHO)
+	/* We don't need a pic base, we're not producing pic.  */
+	gcc_unreachable ();
+
+      xops[2] = gen_rtx_LABEL_REF (Pmode, label ? label : gen_label_rtx ());
+      output_asm_insn ("mov%z0\t{%2, %0|%0, %2}", xops);
+      targetm.asm_out.internal_label (asm_out_file, "L",
+				      CODE_LABEL_NUMBER (XEXP (xops[2], 0)));
+    }
+
+  if (!TARGET_MACHO)
+    output_asm_insn ("add%z0\t{%1, %0|%0, %1}", xops);
+
+  return "";
+}
+
+/* Generate an "push" pattern for input ARG.  */
+
+rtx
+gen_push (rtx arg)
+{
+  struct machine_function *m = cfun->machine;
+
+  if (m->fs.cfa_reg == stack_pointer_rtx)
+    m->fs.cfa_offset += UNITS_PER_WORD;
+  m->fs.sp_offset += UNITS_PER_WORD;
+
+  if (REG_P (arg) && GET_MODE (arg) != word_mode)
+    arg = gen_rtx_REG (word_mode, REGNO (arg));
+
+  return gen_rtx_SET (gen_rtx_MEM (word_mode,
+				   gen_rtx_PRE_DEC (Pmode,
+						    stack_pointer_rtx)),
+		      arg);
+}
+
+/* Generate an "pop" pattern for input ARG.  */
+
+rtx
+gen_pop (rtx arg)
+{
+  if (REG_P (arg) && GET_MODE (arg) != word_mode)
+    arg = gen_rtx_REG (word_mode, REGNO (arg));
+
+  return gen_rtx_SET (arg,
+		      gen_rtx_MEM (word_mode,
+				   gen_rtx_POST_INC (Pmode,
+						     stack_pointer_rtx)));
+}
+
+/* Return >= 0 if there is an unused call-clobbered register available
+   for the entire function.  */
+
+static unsigned int
+ix86_select_alt_pic_regnum (void)
+{
+  if (ix86_use_pseudo_pic_reg ())
+    return INVALID_REGNUM;
+
+  if (crtl->is_leaf
+      && !crtl->profile
+      && !ix86_current_function_calls_tls_descriptor)
+    {
+      int i, drap;
+      /* Can't use the same register for both PIC and DRAP.  */
+      if (crtl->drap_reg)
+	drap = REGNO (crtl->drap_reg);
+      else
+	drap = -1;
+      for (i = 2; i >= 0; --i)
+        if (i != drap && !df_regs_ever_live_p (i))
+	  return i;
+    }
+
+  return INVALID_REGNUM;
+}
+
+/* Return true if REGNO is used by the epilogue.  */
+
+bool
+ix86_epilogue_uses (int regno)
+{
+  /* If there are no caller-saved registers, we preserve all registers,
+     except for MMX and x87 registers which aren't supported when saving
+     and restoring registers.  Don't explicitly save SP register since
+     it is always preserved.  */
+  return (epilogue_completed
+	  && cfun->machine->no_caller_saved_registers
+	  && !fixed_regs[regno]
+	  && !STACK_REGNO_P (regno)
+	  && !MMX_REGNO_P (regno));
+}
+
+/* Return nonzero if register REGNO can be used as a scratch register
+   in peephole2.  */
+
+static bool
+ix86_hard_regno_scratch_ok (unsigned int regno)
+{
+  /* If there are no caller-saved registers, we can't use any register
+     as a scratch register after epilogue and use REGNO as scratch
+     register only if it has been used before to avoid saving and
+     restoring it.  */
+  return (!cfun->machine->no_caller_saved_registers
+	  || (!epilogue_completed
+	      && df_regs_ever_live_p (regno)));
+}
+
+/* Return TRUE if we need to save REGNO.  */
+
+bool
+ix86_save_reg (unsigned int regno, bool maybe_eh_return, bool ignore_outlined)
+{
+  /* If there are no caller-saved registers, we preserve all registers,
+     except for MMX and x87 registers which aren't supported when saving
+     and restoring registers.  Don't explicitly save SP register since
+     it is always preserved.  */
+  if (cfun->machine->no_caller_saved_registers)
+    {
+      /* Don't preserve registers used for function return value.  */
+      rtx reg = crtl->return_rtx;
+      if (reg)
+	{
+	  unsigned int i = REGNO (reg);
+	  unsigned int nregs = REG_NREGS (reg);
+	  while (nregs-- > 0)
+	    if ((i + nregs) == regno)
+	      return false;
+	}
+
+      return (df_regs_ever_live_p (regno)
+	      && !fixed_regs[regno]
+	      && !STACK_REGNO_P (regno)
+	      && !MMX_REGNO_P (regno)
+	      && (regno != HARD_FRAME_POINTER_REGNUM
+		  || !frame_pointer_needed));
+    }
+
+  if (regno == REAL_PIC_OFFSET_TABLE_REGNUM
+      && pic_offset_table_rtx)
+    {
+      if (ix86_use_pseudo_pic_reg ())
+	{
+	  /* REAL_PIC_OFFSET_TABLE_REGNUM used by call to
+	  _mcount in prologue.  */
+	  if (!TARGET_64BIT && flag_pic && crtl->profile)
+	    return true;
+	}
+      else if (df_regs_ever_live_p (REAL_PIC_OFFSET_TABLE_REGNUM)
+	       || crtl->profile
+	       || crtl->calls_eh_return
+	       || crtl->uses_const_pool
+	       || cfun->has_nonlocal_label)
+        return ix86_select_alt_pic_regnum () == INVALID_REGNUM;
+    }
+
+  if (crtl->calls_eh_return && maybe_eh_return)
+    {
+      unsigned i;
+      for (i = 0; ; i++)
+	{
+	  unsigned test = EH_RETURN_DATA_REGNO (i);
+	  if (test == INVALID_REGNUM)
+	    break;
+	  if (test == regno)
+	    return true;
+	}
+    }
+
+  if (ignore_outlined && cfun->machine->call_ms2sysv)
+    {
+      unsigned count = cfun->machine->call_ms2sysv_extra_regs
+		       + xlogue_layout::MIN_REGS;
+      if (xlogue_layout::is_stub_managed_reg (regno, count))
+	return false;
+    }
+
+  if (crtl->drap_reg
+      && regno == REGNO (crtl->drap_reg)
+      && !cfun->machine->no_drap_save_restore)
+    return true;
+
+  return (df_regs_ever_live_p (regno)
+	  && !call_used_or_fixed_reg_p (regno)
+	  && (regno != HARD_FRAME_POINTER_REGNUM || !frame_pointer_needed));
+}
+
+/* Return number of saved general prupose registers.  */
+
+static int
+ix86_nsaved_regs (void)
+{
+  int nregs = 0;
+  int regno;
+
+  for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)
+    if (GENERAL_REGNO_P (regno) && ix86_save_reg (regno, true, true))
+      nregs ++;
+  return nregs;
+}
+
+/* Return number of saved SSE registers.  */
+
+static int
+ix86_nsaved_sseregs (void)
+{
+  int nregs = 0;
+  int regno;
+
+  if (!TARGET_64BIT_MS_ABI)
+    return 0;
+  for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)
+    if (SSE_REGNO_P (regno) && ix86_save_reg (regno, true, true))
+      nregs ++;
+  return nregs;
+}
+
+/* Given FROM and TO register numbers, say whether this elimination is
+   allowed.  If stack alignment is needed, we can only replace argument
+   pointer with hard frame pointer, or replace frame pointer with stack
+   pointer.  Otherwise, frame pointer elimination is automatically
+   handled and all other eliminations are valid.  */
+
+static bool
+ix86_can_eliminate (const int from, const int to)
+{
+  if (stack_realign_fp)
+    return ((from == ARG_POINTER_REGNUM
+	     && to == HARD_FRAME_POINTER_REGNUM)
+	    || (from == FRAME_POINTER_REGNUM
+		&& to == STACK_POINTER_REGNUM));
+  else
+    return to == STACK_POINTER_REGNUM ? !frame_pointer_needed : true;
+}
+
+/* Return the offset between two registers, one to be eliminated, and the other
+   its replacement, at the start of a routine.  */
+
+HOST_WIDE_INT
+ix86_initial_elimination_offset (int from, int to)
+{
+  struct ix86_frame &frame = cfun->machine->frame;
+
+  if (from == ARG_POINTER_REGNUM && to == HARD_FRAME_POINTER_REGNUM)
+    return frame.hard_frame_pointer_offset;
+  else if (from == FRAME_POINTER_REGNUM
+	   && to == HARD_FRAME_POINTER_REGNUM)
+    return frame.hard_frame_pointer_offset - frame.frame_pointer_offset;
+  else
+    {
+      gcc_assert (to == STACK_POINTER_REGNUM);
+
+      if (from == ARG_POINTER_REGNUM)
+	return frame.stack_pointer_offset;
+
+      gcc_assert (from == FRAME_POINTER_REGNUM);
+      return frame.stack_pointer_offset - frame.frame_pointer_offset;
+    }
+}
+
+/* Emits a warning for unsupported msabi to sysv pro/epilogues.  */
+void warn_once_call_ms2sysv_xlogues (const char *feature)
+{
+  static bool warned_once = false;
+  if (!warned_once)
+    {
+      warning (0, "%<-mcall-ms2sysv-xlogues%> is not compatible with %s",
+	       feature);
+      warned_once = true;
+    }
+}
+
+/* Return the probing interval for -fstack-clash-protection.  */
+
+static HOST_WIDE_INT
+get_probe_interval (void)
+{
+  if (flag_stack_clash_protection)
+    return (HOST_WIDE_INT_1U
+	    << param_stack_clash_protection_probe_interval);
+  else
+    return (HOST_WIDE_INT_1U << STACK_CHECK_PROBE_INTERVAL_EXP);
+}
+
+/* When using -fsplit-stack, the allocation routines set a field in
+   the TCB to the bottom of the stack plus this much space, measured
+   in bytes.  */
+
+#define SPLIT_STACK_AVAILABLE 256
+
+/* Fill structure ix86_frame about frame of currently computed function.  */
+
+static void
+ix86_compute_frame_layout (void)
+{
+  struct ix86_frame *frame = &cfun->machine->frame;
+  struct machine_function *m = cfun->machine;
+  unsigned HOST_WIDE_INT stack_alignment_needed;
+  HOST_WIDE_INT offset;
+  unsigned HOST_WIDE_INT preferred_alignment;
+  HOST_WIDE_INT size = ix86_get_frame_size ();
+  HOST_WIDE_INT to_allocate;
+
+  /* m->call_ms2sysv is initially enabled in ix86_expand_call for all 64-bit
+   * ms_abi functions that call a sysv function.  We now need to prune away
+   * cases where it should be disabled.  */
+  if (TARGET_64BIT && m->call_ms2sysv)
+    {
+      gcc_assert (TARGET_64BIT_MS_ABI);
+      gcc_assert (TARGET_CALL_MS2SYSV_XLOGUES);
+      gcc_assert (!TARGET_SEH);
+      gcc_assert (TARGET_SSE);
+      gcc_assert (!ix86_using_red_zone ());
+
+      if (crtl->calls_eh_return)
+	{
+	  gcc_assert (!reload_completed);
+	  m->call_ms2sysv = false;
+	  warn_once_call_ms2sysv_xlogues ("__builtin_eh_return");
+	}
+
+      else if (ix86_static_chain_on_stack)
+	{
+	  gcc_assert (!reload_completed);
+	  m->call_ms2sysv = false;
+	  warn_once_call_ms2sysv_xlogues ("static call chains");
+	}
+
+      /* Finally, compute which registers the stub will manage.  */
+      else
+	{
+	  unsigned count = xlogue_layout::count_stub_managed_regs ();
+	  m->call_ms2sysv_extra_regs = count - xlogue_layout::MIN_REGS;
+	  m->call_ms2sysv_pad_in = 0;
+	}
+    }
+
+  frame->nregs = ix86_nsaved_regs ();
+  frame->nsseregs = ix86_nsaved_sseregs ();
+
+  /* 64-bit MS ABI seem to require stack alignment to be always 16,
+     except for function prologues, leaf functions and when the defult
+     incoming stack boundary is overriden at command line or via
+     force_align_arg_pointer attribute.
+
+     Darwin's ABI specifies 128b alignment for both 32 and  64 bit variants
+     at call sites, including profile function calls.
+ */
+  if (((TARGET_64BIT_MS_ABI || TARGET_MACHO)
+        && crtl->preferred_stack_boundary < 128)
+      && (!crtl->is_leaf || cfun->calls_alloca != 0
+	  || ix86_current_function_calls_tls_descriptor
+	  || (TARGET_MACHO && crtl->profile)
+	  || ix86_incoming_stack_boundary < 128))
+    {
+      crtl->preferred_stack_boundary = 128;
+      crtl->stack_alignment_needed = 128;
+    }
+
+  stack_alignment_needed = crtl->stack_alignment_needed / BITS_PER_UNIT;
+  preferred_alignment = crtl->preferred_stack_boundary / BITS_PER_UNIT;
+
+  gcc_assert (!size || stack_alignment_needed);
+  gcc_assert (preferred_alignment >= STACK_BOUNDARY / BITS_PER_UNIT);
+  gcc_assert (preferred_alignment <= stack_alignment_needed);
+
+  /* The only ABI saving SSE regs should be 64-bit ms_abi.  */
+  gcc_assert (TARGET_64BIT || !frame->nsseregs);
+  if (TARGET_64BIT && m->call_ms2sysv)
+    {
+      gcc_assert (stack_alignment_needed >= 16);
+      gcc_assert (!frame->nsseregs);
+    }
+
+  /* For SEH we have to limit the amount of code movement into the prologue.
+     At present we do this via a BLOCKAGE, at which point there's very little
+     scheduling that can be done, which means that there's very little point
+     in doing anything except PUSHs.  */
+  if (TARGET_SEH)
+    m->use_fast_prologue_epilogue = false;
+  else if (!optimize_bb_for_size_p (ENTRY_BLOCK_PTR_FOR_FN (cfun)))
+    {
+      int count = frame->nregs;
+      struct cgraph_node *node = cgraph_node::get (current_function_decl);
+
+      /* The fast prologue uses move instead of push to save registers.  This
+         is significantly longer, but also executes faster as modern hardware
+         can execute the moves in parallel, but can't do that for push/pop.
+
+	 Be careful about choosing what prologue to emit:  When function takes
+	 many instructions to execute we may use slow version as well as in
+	 case function is known to be outside hot spot (this is known with
+	 feedback only).  Weight the size of function by number of registers
+	 to save as it is cheap to use one or two push instructions but very
+	 slow to use many of them.
+
+	 Calling this hook multiple times with the same frame requirements
+	 must produce the same layout, since the RA might otherwise be
+	 unable to reach a fixed point or might fail its final sanity checks.
+	 This means that once we've assumed that a function does or doesn't
+	 have a particular size, we have to stick to that assumption
+	 regardless of how the function has changed since.  */
+      if (count)
+	count = (count - 1) * FAST_PROLOGUE_INSN_COUNT;
+      if (node->frequency < NODE_FREQUENCY_NORMAL
+	  || (flag_branch_probabilities
+	      && node->frequency < NODE_FREQUENCY_HOT))
+	m->use_fast_prologue_epilogue = false;
+      else
+	{
+	  if (count != frame->expensive_count)
+	    {
+	      frame->expensive_count = count;
+	      frame->expensive_p = expensive_function_p (count);
+	    }
+	  m->use_fast_prologue_epilogue = !frame->expensive_p;
+	}
+    }
+
+  frame->save_regs_using_mov
+    = TARGET_PROLOGUE_USING_MOVE && m->use_fast_prologue_epilogue;
+
+  /* Skip return address and error code in exception handler.  */
+  offset = INCOMING_FRAME_SP_OFFSET;
+
+  /* Skip pushed static chain.  */
+  if (ix86_static_chain_on_stack)
+    offset += UNITS_PER_WORD;
+
+  /* Skip saved base pointer.  */
+  if (frame_pointer_needed)
+    offset += UNITS_PER_WORD;
+  frame->hfp_save_offset = offset;
+
+  /* The traditional frame pointer location is at the top of the frame.  */
+  frame->hard_frame_pointer_offset = offset;
+
+  /* Register save area */
+  offset += frame->nregs * UNITS_PER_WORD;
+  frame->reg_save_offset = offset;
+
+  /* Calculate the size of the va-arg area (not including padding, if any).  */
+  frame->va_arg_size = ix86_varargs_gpr_size + ix86_varargs_fpr_size;
+
+  /* Also adjust stack_realign_offset for the largest alignment of
+     stack slot actually used.  */
+  if (stack_realign_fp
+      || (cfun->machine->max_used_stack_alignment != 0
+	  && (offset % cfun->machine->max_used_stack_alignment) != 0))
+    {
+      /* We may need a 16-byte aligned stack for the remainder of the
+	 register save area, but the stack frame for the local function
+	 may require a greater alignment if using AVX/2/512.  In order
+	 to avoid wasting space, we first calculate the space needed for
+	 the rest of the register saves, add that to the stack pointer,
+	 and then realign the stack to the boundary of the start of the
+	 frame for the local function.  */
+      HOST_WIDE_INT space_needed = 0;
+      HOST_WIDE_INT sse_reg_space_needed = 0;
+
+      if (TARGET_64BIT)
+	{
+	  if (m->call_ms2sysv)
+	    {
+	      m->call_ms2sysv_pad_in = 0;
+	      space_needed = xlogue_layout::get_instance ().get_stack_space_used ();
+	    }
+
+	  else if (frame->nsseregs)
+	    /* The only ABI that has saved SSE registers (Win64) also has a
+	       16-byte aligned default stack.  However, many programs violate
+	       the ABI, and Wine64 forces stack realignment to compensate.  */
+	    space_needed = frame->nsseregs * 16;
+
+	  sse_reg_space_needed = space_needed = ROUND_UP (space_needed, 16);
+
+	  /* 64-bit frame->va_arg_size should always be a multiple of 16, but
+	     rounding to be pedantic.  */
+	  space_needed = ROUND_UP (space_needed + frame->va_arg_size, 16);
+	}
+      else
+	space_needed = frame->va_arg_size;
+
+      /* Record the allocation size required prior to the realignment AND.  */
+      frame->stack_realign_allocate = space_needed;
+
+      /* The re-aligned stack starts at frame->stack_realign_offset.  Values
+	 before this point are not directly comparable with values below
+	 this point.  Use sp_valid_at to determine if the stack pointer is
+	 valid for a given offset, fp_valid_at for the frame pointer, or
+	 choose_baseaddr to have a base register chosen for you.
+
+	 Note that the result of (frame->stack_realign_offset
+	 & (stack_alignment_needed - 1)) may not equal zero.  */
+      offset = ROUND_UP (offset + space_needed, stack_alignment_needed);
+      frame->stack_realign_offset = offset - space_needed;
+      frame->sse_reg_save_offset = frame->stack_realign_offset
+							+ sse_reg_space_needed;
+    }
+  else
+    {
+      frame->stack_realign_offset = offset;
+
+      if (TARGET_64BIT && m->call_ms2sysv)
+	{
+	  m->call_ms2sysv_pad_in = !!(offset & UNITS_PER_WORD);
+	  offset += xlogue_layout::get_instance ().get_stack_space_used ();
+	}
+
+      /* Align and set SSE register save area.  */
+      else if (frame->nsseregs)
+	{
+	  /* If the incoming stack boundary is at least 16 bytes, or DRAP is
+	     required and the DRAP re-alignment boundary is at least 16 bytes,
+	     then we want the SSE register save area properly aligned.  */
+	  if (ix86_incoming_stack_boundary >= 128
+		  || (stack_realign_drap && stack_alignment_needed >= 16))
+	    offset = ROUND_UP (offset, 16);
+	  offset += frame->nsseregs * 16;
+	}
+      frame->sse_reg_save_offset = offset;
+      offset += frame->va_arg_size;
+    }
+
+  /* Align start of frame for local function.  When a function call
+     is removed, it may become a leaf function.  But if argument may
+     be passed on stack, we need to align the stack when there is no
+     tail call.  */
+  if (m->call_ms2sysv
+      || frame->va_arg_size != 0
+      || size != 0
+      || !crtl->is_leaf
+      || (!crtl->tail_call_emit
+	  && cfun->machine->outgoing_args_on_stack)
+      || cfun->calls_alloca
+      || ix86_current_function_calls_tls_descriptor)
+    offset = ROUND_UP (offset, stack_alignment_needed);
+
+  /* Frame pointer points here.  */
+  frame->frame_pointer_offset = offset;
+
+  offset += size;
+
+  /* Add outgoing arguments area.  Can be skipped if we eliminated
+     all the function calls as dead code.
+     Skipping is however impossible when function calls alloca.  Alloca
+     expander assumes that last crtl->outgoing_args_size
+     of stack frame are unused.  */
+  if (ACCUMULATE_OUTGOING_ARGS
+      && (!crtl->is_leaf || cfun->calls_alloca
+	  || ix86_current_function_calls_tls_descriptor))
+    {
+      offset += crtl->outgoing_args_size;
+      frame->outgoing_arguments_size = crtl->outgoing_args_size;
+    }
+  else
+    frame->outgoing_arguments_size = 0;
+
+  /* Align stack boundary.  Only needed if we're calling another function
+     or using alloca.  */
+  if (!crtl->is_leaf || cfun->calls_alloca
+      || ix86_current_function_calls_tls_descriptor)
+    offset = ROUND_UP (offset, preferred_alignment);
+
+  /* We've reached end of stack frame.  */
+  frame->stack_pointer_offset = offset;
+
+  /* Size prologue needs to allocate.  */
+  to_allocate = offset - frame->sse_reg_save_offset;
+
+  if ((!to_allocate && frame->nregs <= 1)
+      || (TARGET_64BIT && to_allocate >= HOST_WIDE_INT_C (0x80000000))
+       /* If static stack checking is enabled and done with probes,
+	  the registers need to be saved before allocating the frame.  */
+      || flag_stack_check == STATIC_BUILTIN_STACK_CHECK
+      /* If stack clash probing needs a loop, then it needs a
+	 scratch register.  But the returned register is only guaranteed
+	 to be safe to use after register saves are complete.  So if
+	 stack clash protections are enabled and the allocated frame is
+	 larger than the probe interval, then use pushes to save
+	 callee saved registers.  */
+      || (flag_stack_clash_protection && to_allocate > get_probe_interval ()))
+    frame->save_regs_using_mov = false;
+
+  if (ix86_using_red_zone ()
+      && crtl->sp_is_unchanging
+      && crtl->is_leaf
+      && !ix86_pc_thunk_call_expanded
+      && !ix86_current_function_calls_tls_descriptor)
+    {
+      frame->red_zone_size = to_allocate;
+      if (frame->save_regs_using_mov)
+	frame->red_zone_size += frame->nregs * UNITS_PER_WORD;
+      if (frame->red_zone_size > RED_ZONE_SIZE - RED_ZONE_RESERVE)
+	frame->red_zone_size = RED_ZONE_SIZE - RED_ZONE_RESERVE;
+    }
+  else
+    frame->red_zone_size = 0;
+  frame->stack_pointer_offset -= frame->red_zone_size;
+
+  /* The SEH frame pointer location is near the bottom of the frame.
+     This is enforced by the fact that the difference between the
+     stack pointer and the frame pointer is limited to 240 bytes in
+     the unwind data structure.  */
+  if (TARGET_SEH)
+    {
+      /* Force the frame pointer to point at or below the lowest register save
+	 area, see the SEH code in config/i386/winnt.c for the rationale.  */
+      frame->hard_frame_pointer_offset = frame->sse_reg_save_offset;
+
+      /* If we can leave the frame pointer where it is, do so; however return
+	 the establisher frame for __builtin_frame_address (0) or else if the
+	 frame overflows the SEH maximum frame size.
+
+	 Note that the value returned by __builtin_frame_address (0) is quite
+	 constrained, because setjmp is piggybacked on the SEH machinery with
+	 recent versions of MinGW:
+
+	  #    elif defined(__SEH__)
+	  #     if defined(__aarch64__) || defined(_ARM64_)
+	  #      define setjmp(BUF) _setjmp((BUF), __builtin_sponentry())
+	  #     elif (__MINGW_GCC_VERSION < 40702)
+	  #      define setjmp(BUF) _setjmp((BUF), mingw_getsp())
+	  #     else
+	  #      define setjmp(BUF) _setjmp((BUF), __builtin_frame_address (0))
+	  #     endif
+
+	 and the second argument passed to _setjmp, if not null, is forwarded
+	 to the TargetFrame parameter of RtlUnwindEx by longjmp (after it has
+	 built an ExceptionRecord on the fly describing the setjmp buffer).  */
+      const HOST_WIDE_INT diff
+	= frame->stack_pointer_offset - frame->hard_frame_pointer_offset;
+      if (diff <= 255 && !crtl->accesses_prior_frames)
+	{
+	  /* The resulting diff will be a multiple of 16 lower than 255,
+	     i.e. at most 240 as required by the unwind data structure.  */
+	  frame->hard_frame_pointer_offset += (diff & 15);
+	}
+      else if (diff <= SEH_MAX_FRAME_SIZE && !crtl->accesses_prior_frames)
+	{
+	  /* Ideally we'd determine what portion of the local stack frame
+	     (within the constraint of the lowest 240) is most heavily used.
+	     But without that complication, simply bias the frame pointer
+	     by 128 bytes so as to maximize the amount of the local stack
+	     frame that is addressable with 8-bit offsets.  */
+	  frame->hard_frame_pointer_offset = frame->stack_pointer_offset - 128;
+	}
+      else
+	frame->hard_frame_pointer_offset = frame->hfp_save_offset;
+    }
+}
+
+/* This is semi-inlined memory_address_length, but simplified
+   since we know that we're always dealing with reg+offset, and
+   to avoid having to create and discard all that rtl.  */
+
+static inline int
+choose_baseaddr_len (unsigned int regno, HOST_WIDE_INT offset)
+{
+  int len = 4;
+
+  if (offset == 0)
+    {
+      /* EBP and R13 cannot be encoded without an offset.  */
+      len = (regno == BP_REG || regno == R13_REG);
+    }
+  else if (IN_RANGE (offset, -128, 127))
+    len = 1;
+
+  /* ESP and R12 must be encoded with a SIB byte.  */
+  if (regno == SP_REG || regno == R12_REG)
+    len++;
+
+  return len;
+}
+
+/* Determine if the stack pointer is valid for accessing the CFA_OFFSET in
+   the frame save area.  The register is saved at CFA - CFA_OFFSET.  */
+
+static bool
+sp_valid_at (HOST_WIDE_INT cfa_offset)
+{
+  const struct machine_frame_state &fs = cfun->machine->fs;
+  if (fs.sp_realigned && cfa_offset <= fs.sp_realigned_offset)
+    {
+      /* Validate that the cfa_offset isn't in a "no-man's land".  */
+      gcc_assert (cfa_offset <= fs.sp_realigned_fp_last);
+      return false;
+    }
+  return fs.sp_valid;
+}
+
+/* Determine if the frame pointer is valid for accessing the CFA_OFFSET in
+   the frame save area.  The register is saved at CFA - CFA_OFFSET.  */
+
+static inline bool
+fp_valid_at (HOST_WIDE_INT cfa_offset)
+{
+  const struct machine_frame_state &fs = cfun->machine->fs;
+  if (fs.sp_realigned && cfa_offset > fs.sp_realigned_fp_last)
+    {
+      /* Validate that the cfa_offset isn't in a "no-man's land".  */
+      gcc_assert (cfa_offset >= fs.sp_realigned_offset);
+      return false;
+    }
+  return fs.fp_valid;
+}
+
+/* Choose a base register based upon alignment requested, speed and/or
+   size.  */
+
+static void
+choose_basereg (HOST_WIDE_INT cfa_offset, rtx &base_reg,
+		HOST_WIDE_INT &base_offset,
+		unsigned int align_reqested, unsigned int *align)
+{
+  const struct machine_function *m = cfun->machine;
+  unsigned int hfp_align;
+  unsigned int drap_align;
+  unsigned int sp_align;
+  bool hfp_ok  = fp_valid_at (cfa_offset);
+  bool drap_ok = m->fs.drap_valid;
+  bool sp_ok   = sp_valid_at (cfa_offset);
+
+  hfp_align = drap_align = sp_align = INCOMING_STACK_BOUNDARY;
+
+  /* Filter out any registers that don't meet the requested alignment
+     criteria.  */
+  if (align_reqested)
+    {
+      if (m->fs.realigned)
+	hfp_align = drap_align = sp_align = crtl->stack_alignment_needed;
+      /* SEH unwind code does do not currently support REG_CFA_EXPRESSION
+	 notes (which we would need to use a realigned stack pointer),
+	 so disable on SEH targets.  */
+      else if (m->fs.sp_realigned)
+	sp_align = crtl->stack_alignment_needed;
+
+      hfp_ok = hfp_ok && hfp_align >= align_reqested;
+      drap_ok = drap_ok && drap_align >= align_reqested;
+      sp_ok = sp_ok && sp_align >= align_reqested;
+    }
+
+  if (m->use_fast_prologue_epilogue)
+    {
+      /* Choose the base register most likely to allow the most scheduling
+         opportunities.  Generally FP is valid throughout the function,
+         while DRAP must be reloaded within the epilogue.  But choose either
+         over the SP due to increased encoding size.  */
+
+      if (hfp_ok)
+	{
+	  base_reg = hard_frame_pointer_rtx;
+	  base_offset = m->fs.fp_offset - cfa_offset;
+	}
+      else if (drap_ok)
+	{
+	  base_reg = crtl->drap_reg;
+	  base_offset = 0 - cfa_offset;
+	}
+      else if (sp_ok)
+	{
+	  base_reg = stack_pointer_rtx;
+	  base_offset = m->fs.sp_offset - cfa_offset;
+	}
+    }
+  else
+    {
+      HOST_WIDE_INT toffset;
+      int len = 16, tlen;
+
+      /* Choose the base register with the smallest address encoding.
+         With a tie, choose FP > DRAP > SP.  */
+      if (sp_ok)
+	{
+	  base_reg = stack_pointer_rtx;
+	  base_offset = m->fs.sp_offset - cfa_offset;
+          len = choose_baseaddr_len (STACK_POINTER_REGNUM, base_offset);
+	}
+      if (drap_ok)
+	{
+	  toffset = 0 - cfa_offset;
+	  tlen = choose_baseaddr_len (REGNO (crtl->drap_reg), toffset);
+	  if (tlen <= len)
+	    {
+	      base_reg = crtl->drap_reg;
+	      base_offset = toffset;
+	      len = tlen;
+	    }
+	}
+      if (hfp_ok)
+	{
+	  toffset = m->fs.fp_offset - cfa_offset;
+	  tlen = choose_baseaddr_len (HARD_FRAME_POINTER_REGNUM, toffset);
+	  if (tlen <= len)
+	    {
+	      base_reg = hard_frame_pointer_rtx;
+	      base_offset = toffset;
+	    }
+	}
+    }
+
+    /* Set the align return value.  */
+    if (align)
+      {
+	if (base_reg == stack_pointer_rtx)
+	  *align = sp_align;
+	else if (base_reg == crtl->drap_reg)
+	  *align = drap_align;
+	else if (base_reg == hard_frame_pointer_rtx)
+	  *align = hfp_align;
+      }
+}
+
+/* Return an RTX that points to CFA_OFFSET within the stack frame and
+   the alignment of address.  If ALIGN is non-null, it should point to
+   an alignment value (in bits) that is preferred or zero and will
+   recieve the alignment of the base register that was selected,
+   irrespective of rather or not CFA_OFFSET is a multiple of that
+   alignment value.  If it is possible for the base register offset to be
+   non-immediate then SCRATCH_REGNO should specify a scratch register to
+   use.
+
+   The valid base registers are taken from CFUN->MACHINE->FS.  */
+
+static rtx
+choose_baseaddr (HOST_WIDE_INT cfa_offset, unsigned int *align,
+		 unsigned int scratch_regno = INVALID_REGNUM)
+{
+  rtx base_reg = NULL;
+  HOST_WIDE_INT base_offset = 0;
+
+  /* If a specific alignment is requested, try to get a base register
+     with that alignment first.  */
+  if (align && *align)
+    choose_basereg (cfa_offset, base_reg, base_offset, *align, align);
+
+  if (!base_reg)
+    choose_basereg (cfa_offset, base_reg, base_offset, 0, align);
+
+  gcc_assert (base_reg != NULL);
+
+  rtx base_offset_rtx = GEN_INT (base_offset);
+
+  if (!x86_64_immediate_operand (base_offset_rtx, Pmode))
+    {
+      gcc_assert (scratch_regno != INVALID_REGNUM);
+
+      rtx scratch_reg = gen_rtx_REG (Pmode, scratch_regno);
+      emit_move_insn (scratch_reg, base_offset_rtx);
+
+      return gen_rtx_PLUS (Pmode, base_reg, scratch_reg);
+    }
+
+  return plus_constant (Pmode, base_reg, base_offset);
+}
+
+/* Emit code to save registers in the prologue.  */
+
+static void
+ix86_emit_save_regs (void)
+{
+  unsigned int regno;
+  rtx_insn *insn;
+
+  for (regno = FIRST_PSEUDO_REGISTER - 1; regno-- > 0; )
+    if (GENERAL_REGNO_P (regno) && ix86_save_reg (regno, true, true))
+      {
+	insn = emit_insn (gen_push (gen_rtx_REG (word_mode, regno)));
+	RTX_FRAME_RELATED_P (insn) = 1;
+      }
+}
+
+/* Emit a single register save at CFA - CFA_OFFSET.  */
+
+static void
+ix86_emit_save_reg_using_mov (machine_mode mode, unsigned int regno,
+			      HOST_WIDE_INT cfa_offset)
+{
+  struct machine_function *m = cfun->machine;
+  rtx reg = gen_rtx_REG (mode, regno);
+  rtx mem, addr, base, insn;
+  unsigned int align = GET_MODE_ALIGNMENT (mode);
+
+  addr = choose_baseaddr (cfa_offset, &align);
+  mem = gen_frame_mem (mode, addr);
+
+  /* The location aligment depends upon the base register.  */
+  align = MIN (GET_MODE_ALIGNMENT (mode), align);
+  gcc_assert (! (cfa_offset & (align / BITS_PER_UNIT - 1)));
+  set_mem_align (mem, align);
+
+  insn = emit_insn (gen_rtx_SET (mem, reg));
+  RTX_FRAME_RELATED_P (insn) = 1;
+
+  base = addr;
+  if (GET_CODE (base) == PLUS)
+    base = XEXP (base, 0);
+  gcc_checking_assert (REG_P (base));
+
+  /* When saving registers into a re-aligned local stack frame, avoid
+     any tricky guessing by dwarf2out.  */
+  if (m->fs.realigned)
+    {
+      gcc_checking_assert (stack_realign_drap);
+
+      if (regno == REGNO (crtl->drap_reg))
+	{
+	  /* A bit of a hack.  We force the DRAP register to be saved in
+	     the re-aligned stack frame, which provides us with a copy
+	     of the CFA that will last past the prologue.  Install it.  */
+	  gcc_checking_assert (cfun->machine->fs.fp_valid);
+	  addr = plus_constant (Pmode, hard_frame_pointer_rtx,
+				cfun->machine->fs.fp_offset - cfa_offset);
+	  mem = gen_rtx_MEM (mode, addr);
+	  add_reg_note (insn, REG_CFA_DEF_CFA, mem);
+	}
+      else
+	{
+	  /* The frame pointer is a stable reference within the
+	     aligned frame.  Use it.  */
+	  gcc_checking_assert (cfun->machine->fs.fp_valid);
+	  addr = plus_constant (Pmode, hard_frame_pointer_rtx,
+				cfun->machine->fs.fp_offset - cfa_offset);
+	  mem = gen_rtx_MEM (mode, addr);
+	  add_reg_note (insn, REG_CFA_EXPRESSION, gen_rtx_SET (mem, reg));
+	}
+    }
+
+  else if (base == stack_pointer_rtx && m->fs.sp_realigned
+	   && cfa_offset >= m->fs.sp_realigned_offset)
+    {
+      gcc_checking_assert (stack_realign_fp);
+      add_reg_note (insn, REG_CFA_EXPRESSION, gen_rtx_SET (mem, reg));
+    }
+
+  /* The memory may not be relative to the current CFA register,
+     which means that we may need to generate a new pattern for
+     use by the unwind info.  */
+  else if (base != m->fs.cfa_reg)
+    {
+      addr = plus_constant (Pmode, m->fs.cfa_reg,
+			    m->fs.cfa_offset - cfa_offset);
+      mem = gen_rtx_MEM (mode, addr);
+      add_reg_note (insn, REG_CFA_OFFSET, gen_rtx_SET (mem, reg));
+    }
+}
+
+/* Emit code to save registers using MOV insns.
+   First register is stored at CFA - CFA_OFFSET.  */
+static void
+ix86_emit_save_regs_using_mov (HOST_WIDE_INT cfa_offset)
+{
+  unsigned int regno;
+
+  for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)
+    if (GENERAL_REGNO_P (regno) && ix86_save_reg (regno, true, true))
+      {
+        ix86_emit_save_reg_using_mov (word_mode, regno, cfa_offset);
+	cfa_offset -= UNITS_PER_WORD;
+      }
+}
+
+/* Emit code to save SSE registers using MOV insns.
+   First register is stored at CFA - CFA_OFFSET.  */
+static void
+ix86_emit_save_sse_regs_using_mov (HOST_WIDE_INT cfa_offset)
+{
+  unsigned int regno;
+
+  for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)
+    if (SSE_REGNO_P (regno) && ix86_save_reg (regno, true, true))
+      {
+	ix86_emit_save_reg_using_mov (V4SFmode, regno, cfa_offset);
+	cfa_offset -= GET_MODE_SIZE (V4SFmode);
+      }
+}
+
+static GTY(()) rtx queued_cfa_restores;
+
+/* Add a REG_CFA_RESTORE REG note to INSN or queue them until next stack
+   manipulation insn.  The value is on the stack at CFA - CFA_OFFSET.
+   Don't add the note if the previously saved value will be left untouched
+   within stack red-zone till return, as unwinders can find the same value
+   in the register and on the stack.  */
+
+static void
+ix86_add_cfa_restore_note (rtx_insn *insn, rtx reg, HOST_WIDE_INT cfa_offset)
+{
+  if (!crtl->shrink_wrapped
+      && cfa_offset <= cfun->machine->fs.red_zone_offset)
+    return;
+
+  if (insn)
+    {
+      add_reg_note (insn, REG_CFA_RESTORE, reg);
+      RTX_FRAME_RELATED_P (insn) = 1;
+    }
+  else
+    queued_cfa_restores
+      = alloc_reg_note (REG_CFA_RESTORE, reg, queued_cfa_restores);
+}
+
+/* Add queued REG_CFA_RESTORE notes if any to INSN.  */
+
+static void
+ix86_add_queued_cfa_restore_notes (rtx insn)
+{
+  rtx last;
+  if (!queued_cfa_restores)
+    return;
+  for (last = queued_cfa_restores; XEXP (last, 1); last = XEXP (last, 1))
+    ;
+  XEXP (last, 1) = REG_NOTES (insn);
+  REG_NOTES (insn) = queued_cfa_restores;
+  queued_cfa_restores = NULL_RTX;
+  RTX_FRAME_RELATED_P (insn) = 1;
+}
+
+/* Expand prologue or epilogue stack adjustment.
+   The pattern exist to put a dependency on all ebp-based memory accesses.
+   STYLE should be negative if instructions should be marked as frame related,
+   zero if %r11 register is live and cannot be freely used and positive
+   otherwise.  */
+
+static rtx
+pro_epilogue_adjust_stack (rtx dest, rtx src, rtx offset,
+			   int style, bool set_cfa)
+{
+  struct machine_function *m = cfun->machine;
+  rtx addend = offset;
+  rtx insn;
+  bool add_frame_related_expr = false;
+
+  if (!x86_64_immediate_operand (offset, Pmode))
+    {
+      /* r11 is used by indirect sibcall return as well, set before the
+	 epilogue and used after the epilogue.  */
+      if (style)
+        addend = gen_rtx_REG (Pmode, R11_REG);
+      else
+	{
+	  gcc_assert (src != hard_frame_pointer_rtx
+		      && dest != hard_frame_pointer_rtx);
+	  addend = hard_frame_pointer_rtx;
+	}
+      emit_insn (gen_rtx_SET (addend, offset));
+      if (style < 0)
+	add_frame_related_expr = true;
+    }
+
+  insn = emit_insn (gen_pro_epilogue_adjust_stack_add
+		    (Pmode, dest, src, addend));
+  if (style >= 0)
+    ix86_add_queued_cfa_restore_notes (insn);
+
+  if (set_cfa)
+    {
+      rtx r;
+
+      gcc_assert (m->fs.cfa_reg == src);
+      m->fs.cfa_offset += INTVAL (offset);
+      m->fs.cfa_reg = dest;
+
+      r = gen_rtx_PLUS (Pmode, src, offset);
+      r = gen_rtx_SET (dest, r);
+      add_reg_note (insn, REG_CFA_ADJUST_CFA, r);
+      RTX_FRAME_RELATED_P (insn) = 1;
+    }
+  else if (style < 0)
+    {
+      RTX_FRAME_RELATED_P (insn) = 1;
+      if (add_frame_related_expr)
+	{
+	  rtx r = gen_rtx_PLUS (Pmode, src, offset);
+	  r = gen_rtx_SET (dest, r);
+	  add_reg_note (insn, REG_FRAME_RELATED_EXPR, r);
+	}
+    }
+
+  if (dest == stack_pointer_rtx)
+    {
+      HOST_WIDE_INT ooffset = m->fs.sp_offset;
+      bool valid = m->fs.sp_valid;
+      bool realigned = m->fs.sp_realigned;
+
+      if (src == hard_frame_pointer_rtx)
+	{
+	  valid = m->fs.fp_valid;
+	  realigned = false;
+	  ooffset = m->fs.fp_offset;
+	}
+      else if (src == crtl->drap_reg)
+	{
+	  valid = m->fs.drap_valid;
+	  realigned = false;
+	  ooffset = 0;
+	}
+      else
+	{
+	  /* Else there are two possibilities: SP itself, which we set
+	     up as the default above.  Or EH_RETURN_STACKADJ_RTX, which is
+	     taken care of this by hand along the eh_return path.  */
+	  gcc_checking_assert (src == stack_pointer_rtx
+			       || offset == const0_rtx);
+	}
+
+      m->fs.sp_offset = ooffset - INTVAL (offset);
+      m->fs.sp_valid = valid;
+      m->fs.sp_realigned = realigned;
+    }
+  return insn;
+}
+
+/* Find an available register to be used as dynamic realign argument
+   pointer regsiter.  Such a register will be written in prologue and
+   used in begin of body, so it must not be
+	1. parameter passing register.
+	2. GOT pointer.
+   We reuse static-chain register if it is available.  Otherwise, we
+   use DI for i386 and R13 for x86-64.  We chose R13 since it has
+   shorter encoding.
+
+   Return: the regno of chosen register.  */
+
+static unsigned int
+find_drap_reg (void)
+{
+  tree decl = cfun->decl;
+
+  /* Always use callee-saved register if there are no caller-saved
+     registers.  */
+  if (TARGET_64BIT)
+    {
+      /* Use R13 for nested function or function need static chain.
+	 Since function with tail call may use any caller-saved
+	 registers in epilogue, DRAP must not use caller-saved
+	 register in such case.  */
+      if (DECL_STATIC_CHAIN (decl)
+	  || cfun->machine->no_caller_saved_registers
+	  || crtl->tail_call_emit)
+	return R13_REG;
+
+      return R10_REG;
+    }
+  else
+    {
+      /* Use DI for nested function or function need static chain.
+	 Since function with tail call may use any caller-saved
+	 registers in epilogue, DRAP must not use caller-saved
+	 register in such case.  */
+      if (DECL_STATIC_CHAIN (decl)
+	  || cfun->machine->no_caller_saved_registers
+	  || crtl->tail_call_emit)
+	return DI_REG;
+
+      /* Reuse static chain register if it isn't used for parameter
+         passing.  */
+      if (ix86_function_regparm (TREE_TYPE (decl), decl) <= 2)
+	{
+	  unsigned int ccvt = ix86_get_callcvt (TREE_TYPE (decl));
+	  if ((ccvt & (IX86_CALLCVT_FASTCALL | IX86_CALLCVT_THISCALL)) == 0)
+	    return CX_REG;
+	}
+      return DI_REG;
+    }
+}
+
+/* Return minimum incoming stack alignment.  */
+
+static unsigned int
+ix86_minimum_incoming_stack_boundary (bool sibcall)
+{
+  unsigned int incoming_stack_boundary;
+
+  /* Stack of interrupt handler is aligned to 128 bits in 64bit mode.  */
+  if (cfun->machine->func_type != TYPE_NORMAL)
+    incoming_stack_boundary = TARGET_64BIT ? 128 : MIN_STACK_BOUNDARY;
+  /* Prefer the one specified at command line. */
+  else if (ix86_user_incoming_stack_boundary)
+    incoming_stack_boundary = ix86_user_incoming_stack_boundary;
+  /* In 32bit, use MIN_STACK_BOUNDARY for incoming stack boundary
+     if -mstackrealign is used, it isn't used for sibcall check and
+     estimated stack alignment is 128bit.  */
+  else if (!sibcall
+	   && ix86_force_align_arg_pointer
+	   && crtl->stack_alignment_estimated == 128)
+    incoming_stack_boundary = MIN_STACK_BOUNDARY;
+  else
+    incoming_stack_boundary = ix86_default_incoming_stack_boundary;
+
+  /* Incoming stack alignment can be changed on individual functions
+     via force_align_arg_pointer attribute.  We use the smallest
+     incoming stack boundary.  */
+  if (incoming_stack_boundary > MIN_STACK_BOUNDARY
+      && lookup_attribute ("force_align_arg_pointer",
+			   TYPE_ATTRIBUTES (TREE_TYPE (current_function_decl))))
+    incoming_stack_boundary = MIN_STACK_BOUNDARY;
+
+  /* The incoming stack frame has to be aligned at least at
+     parm_stack_boundary.  */
+  if (incoming_stack_boundary < crtl->parm_stack_boundary)
+    incoming_stack_boundary = crtl->parm_stack_boundary;
+
+  /* Stack at entrance of main is aligned by runtime.  We use the
+     smallest incoming stack boundary. */
+  if (incoming_stack_boundary > MAIN_STACK_BOUNDARY
+      && DECL_NAME (current_function_decl)
+      && MAIN_NAME_P (DECL_NAME (current_function_decl))
+      && DECL_FILE_SCOPE_P (current_function_decl))
+    incoming_stack_boundary = MAIN_STACK_BOUNDARY;
+
+  return incoming_stack_boundary;
+}
+
+/* Update incoming stack boundary and estimated stack alignment.  */
+
+static void
+ix86_update_stack_boundary (void)
+{
+  ix86_incoming_stack_boundary
+    = ix86_minimum_incoming_stack_boundary (false);
+
+  /* x86_64 vararg needs 16byte stack alignment for register save area.  */
+  if (TARGET_64BIT
+      && cfun->stdarg
+      && crtl->stack_alignment_estimated < 128)
+    crtl->stack_alignment_estimated = 128;
+
+  /* __tls_get_addr needs to be called with 16-byte aligned stack.  */
+  if (ix86_tls_descriptor_calls_expanded_in_cfun
+      && crtl->preferred_stack_boundary < 128)
+    crtl->preferred_stack_boundary = 128;
+}
+
+/* Handle the TARGET_GET_DRAP_RTX hook.  Return NULL if no DRAP is
+   needed or an rtx for DRAP otherwise.  */
+
+static rtx
+ix86_get_drap_rtx (void)
+{
+  /* We must use DRAP if there are outgoing arguments on stack or
+     the stack pointer register is clobbered by asm statment and
+     ACCUMULATE_OUTGOING_ARGS is false.  */
+  if (ix86_force_drap
+      || ((cfun->machine->outgoing_args_on_stack
+	   || crtl->sp_is_clobbered_by_asm)
+	  && !ACCUMULATE_OUTGOING_ARGS))
+    crtl->need_drap = true;
+
+  if (stack_realign_drap)
+    {
+      /* Assign DRAP to vDRAP and returns vDRAP */
+      unsigned int regno = find_drap_reg ();
+      rtx drap_vreg;
+      rtx arg_ptr;
+      rtx_insn *seq, *insn;
+
+      arg_ptr = gen_rtx_REG (Pmode, regno);
+      crtl->drap_reg = arg_ptr;
+
+      start_sequence ();
+      drap_vreg = copy_to_reg (arg_ptr);
+      seq = get_insns ();
+      end_sequence ();
+
+      insn = emit_insn_before (seq, NEXT_INSN (entry_of_function ()));
+      if (!optimize)
+	{
+	  add_reg_note (insn, REG_CFA_SET_VDRAP, drap_vreg);
+	  RTX_FRAME_RELATED_P (insn) = 1;
+	}
+      return drap_vreg;
+    }
+  else
+    return NULL;
+}
+
+/* Handle the TARGET_INTERNAL_ARG_POINTER hook.  */
+
+static rtx
+ix86_internal_arg_pointer (void)
+{
+  return virtual_incoming_args_rtx;
+}
+
+struct scratch_reg {
+  rtx reg;
+  bool saved;
+};
+
+/* Return a short-lived scratch register for use on function entry.
+   In 32-bit mode, it is valid only after the registers are saved
+   in the prologue.  This register must be released by means of
+   release_scratch_register_on_entry once it is dead.  */
+
+static void
+get_scratch_register_on_entry (struct scratch_reg *sr)
+{
+  int regno;
+
+  sr->saved = false;
+
+  if (TARGET_64BIT)
+    {
+      /* We always use R11 in 64-bit mode.  */
+      regno = R11_REG;
+    }
+  else
+    {
+      tree decl = current_function_decl, fntype = TREE_TYPE (decl);
+      bool fastcall_p
+	= lookup_attribute ("fastcall", TYPE_ATTRIBUTES (fntype)) != NULL_TREE;
+      bool thiscall_p
+	= lookup_attribute ("thiscall", TYPE_ATTRIBUTES (fntype)) != NULL_TREE;
+      bool static_chain_p = DECL_STATIC_CHAIN (decl);
+      int regparm = ix86_function_regparm (fntype, decl);
+      int drap_regno
+	= crtl->drap_reg ? REGNO (crtl->drap_reg) : INVALID_REGNUM;
+
+      /* 'fastcall' sets regparm to 2, uses ecx/edx for arguments and eax
+	  for the static chain register.  */
+      if ((regparm < 1 || (fastcall_p && !static_chain_p))
+	  && drap_regno != AX_REG)
+	regno = AX_REG;
+      /* 'thiscall' sets regparm to 1, uses ecx for arguments and edx
+	  for the static chain register.  */
+      else if (thiscall_p && !static_chain_p && drap_regno != AX_REG)
+        regno = AX_REG;
+      else if (regparm < 2 && !thiscall_p && drap_regno != DX_REG)
+	regno = DX_REG;
+      /* ecx is the static chain register.  */
+      else if (regparm < 3 && !fastcall_p && !thiscall_p
+	       && !static_chain_p
+	       && drap_regno != CX_REG)
+	regno = CX_REG;
+      else if (ix86_save_reg (BX_REG, true, false))
+	regno = BX_REG;
+      /* esi is the static chain register.  */
+      else if (!(regparm == 3 && static_chain_p)
+	       && ix86_save_reg (SI_REG, true, false))
+	regno = SI_REG;
+      else if (ix86_save_reg (DI_REG, true, false))
+	regno = DI_REG;
+      else
+	{
+	  regno = (drap_regno == AX_REG ? DX_REG : AX_REG);
+	  sr->saved = true;
+	}
+    }
+
+  sr->reg = gen_rtx_REG (Pmode, regno);
+  if (sr->saved)
+    {
+      rtx_insn *insn = emit_insn (gen_push (sr->reg));
+      RTX_FRAME_RELATED_P (insn) = 1;
+    }
+}
+
+/* Release a scratch register obtained from the preceding function.
+
+   If RELEASE_VIA_POP is true, we just pop the register off the stack
+   to release it.  This is what non-Linux systems use with -fstack-check.
+
+   Otherwise we use OFFSET to locate the saved register and the
+   allocated stack space becomes part of the local frame and is
+   deallocated by the epilogue.  */
+
+static void
+release_scratch_register_on_entry (struct scratch_reg *sr, HOST_WIDE_INT offset,
+				   bool release_via_pop)
+{
+  if (sr->saved)
+    {
+      if (release_via_pop)
+	{
+	  struct machine_function *m = cfun->machine;
+	  rtx x, insn = emit_insn (gen_pop (sr->reg));
+
+	  /* The RX FRAME_RELATED_P mechanism doesn't know about pop.  */
+	  RTX_FRAME_RELATED_P (insn) = 1;
+	  x = plus_constant (Pmode, stack_pointer_rtx, UNITS_PER_WORD);
+	  x = gen_rtx_SET (stack_pointer_rtx, x);
+	  add_reg_note (insn, REG_FRAME_RELATED_EXPR, x);
+	  m->fs.sp_offset -= UNITS_PER_WORD;
+	}
+      else
+	{
+	  rtx x = plus_constant (Pmode, stack_pointer_rtx, offset);
+	  x = gen_rtx_SET (sr->reg, gen_rtx_MEM (word_mode, x));
+	  emit_insn (x);
+	}
+    }
+}
+
+/* Emit code to adjust the stack pointer by SIZE bytes while probing it.
+
+   If INT_REGISTERS_SAVED is true, then integer registers have already been
+   pushed on the stack.
+
+   If PROTECTION AREA is true, then probe PROBE_INTERVAL plus a small dope
+   beyond SIZE bytes.
+
+   This assumes no knowledge of the current probing state, i.e. it is never
+   allowed to allocate more than PROBE_INTERVAL bytes of stack space without
+   a suitable probe.  */
+
+static void
+ix86_adjust_stack_and_probe (HOST_WIDE_INT size,
+			     const bool int_registers_saved,
+			     const bool protection_area)
+{
+  struct machine_function *m = cfun->machine;
+
+  /* If this function does not statically allocate stack space, then
+     no probes are needed.  */
+  if (!size)
+    {
+      /* However, the allocation of space via pushes for register
+	 saves could be viewed as allocating space, but without the
+	 need to probe.  */
+      if (m->frame.nregs || m->frame.nsseregs || frame_pointer_needed)
+        dump_stack_clash_frame_info (NO_PROBE_SMALL_FRAME, true);
+      else
+	dump_stack_clash_frame_info (NO_PROBE_NO_FRAME, false);
+      return;
+    }
+
+  /* If we are a noreturn function, then we have to consider the
+     possibility that we're called via a jump rather than a call.
+
+     Thus we don't have the implicit probe generated by saving the
+     return address into the stack at the call.  Thus, the stack
+     pointer could be anywhere in the guard page.  The safe thing
+     to do is emit a probe now.
+
+     The probe can be avoided if we have already emitted any callee
+     register saves into the stack or have a frame pointer (which will
+     have been saved as well).  Those saves will function as implicit
+     probes.
+
+     ?!? This should be revamped to work like aarch64 and s390 where
+     we track the offset from the most recent probe.  Normally that
+     offset would be zero.  For a noreturn function we would reset
+     it to PROBE_INTERVAL - (STACK_BOUNDARY / BITS_PER_UNIT).   Then
+     we just probe when we cross PROBE_INTERVAL.  */
+  if (TREE_THIS_VOLATILE (cfun->decl)
+      && !(m->frame.nregs || m->frame.nsseregs || frame_pointer_needed))
+    {
+      /* We can safely use any register here since we're just going to push
+	 its value and immediately pop it back.  But we do try and avoid
+	 argument passing registers so as not to introduce dependencies in
+	 the pipeline.  For 32 bit we use %esi and for 64 bit we use %rax.  */
+      rtx dummy_reg = gen_rtx_REG (word_mode, TARGET_64BIT ? AX_REG : SI_REG);
+      rtx_insn *insn_push = emit_insn (gen_push (dummy_reg));
+      rtx_insn *insn_pop = emit_insn (gen_pop (dummy_reg));
+      m->fs.sp_offset -= UNITS_PER_WORD;
+      if (m->fs.cfa_reg == stack_pointer_rtx)
+	{
+	  m->fs.cfa_offset -= UNITS_PER_WORD;
+	  rtx x = plus_constant (Pmode, stack_pointer_rtx, -UNITS_PER_WORD);
+	  x = gen_rtx_SET (stack_pointer_rtx, x);
+	  add_reg_note (insn_push, REG_CFA_ADJUST_CFA, x);
+	  RTX_FRAME_RELATED_P (insn_push) = 1;
+	  x = plus_constant (Pmode, stack_pointer_rtx, UNITS_PER_WORD);
+	  x = gen_rtx_SET (stack_pointer_rtx, x);
+	  add_reg_note (insn_pop, REG_CFA_ADJUST_CFA, x);
+	  RTX_FRAME_RELATED_P (insn_pop) = 1;
+	}
+      emit_insn (gen_blockage ());
+    }
+
+  const HOST_WIDE_INT probe_interval = get_probe_interval ();
+  const int dope = 4 * UNITS_PER_WORD;
+
+  /* If there is protection area, take it into account in the size.  */
+  if (protection_area)
+    size += probe_interval + dope;
+
+  /* If we allocate less than the size of the guard statically,
+     then no probing is necessary, but we do need to allocate
+     the stack.  */
+  else if (size < (1 << param_stack_clash_protection_guard_size))
+    {
+      pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,
+			         GEN_INT (-size), -1,
+			         m->fs.cfa_reg == stack_pointer_rtx);
+      dump_stack_clash_frame_info (NO_PROBE_SMALL_FRAME, true);
+      return;
+    }
+
+  /* We're allocating a large enough stack frame that we need to
+     emit probes.  Either emit them inline or in a loop depending
+     on the size.  */
+  if (size <= 4 * probe_interval)
+    {
+      HOST_WIDE_INT i;
+      for (i = probe_interval; i <= size; i += probe_interval)
+	{
+	  /* Allocate PROBE_INTERVAL bytes.  */
+	  rtx insn
+	    = pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,
+					 GEN_INT (-probe_interval), -1,
+					 m->fs.cfa_reg == stack_pointer_rtx);
+	  add_reg_note (insn, REG_STACK_CHECK, const0_rtx);
+
+	  /* And probe at *sp.  */
+	  emit_stack_probe (stack_pointer_rtx);
+	  emit_insn (gen_blockage ());
+	}
+
+      /* We need to allocate space for the residual, but we do not need
+	 to probe the residual...  */
+      HOST_WIDE_INT residual = (i - probe_interval - size);
+      if (residual)
+	{
+	  pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,
+				     GEN_INT (residual), -1,
+				     m->fs.cfa_reg == stack_pointer_rtx);
+
+	  /* ...except if there is a protection area to maintain.  */
+	  if (protection_area)
+	    emit_stack_probe (stack_pointer_rtx);
+	}
+
+      dump_stack_clash_frame_info (PROBE_INLINE, residual != 0);
+    }
+  else
+    {
+      /* We expect the GP registers to be saved when probes are used
+	 as the probing sequences might need a scratch register and
+	 the routine to allocate one assumes the integer registers
+	 have already been saved.  */
+      gcc_assert (int_registers_saved);
+
+      struct scratch_reg sr;
+      get_scratch_register_on_entry (&sr);
+
+      /* If we needed to save a register, then account for any space
+	 that was pushed (we are not going to pop the register when
+	 we do the restore).  */
+      if (sr.saved)
+	size -= UNITS_PER_WORD;
+
+      /* Step 1: round SIZE down to a multiple of the interval.  */
+      HOST_WIDE_INT rounded_size = size & -probe_interval;
+
+      /* Step 2: compute final value of the loop counter.  Use lea if
+	 possible.  */
+      rtx addr = plus_constant (Pmode, stack_pointer_rtx, -rounded_size);
+      rtx insn;
+      if (address_no_seg_operand (addr, Pmode))
+	insn = emit_insn (gen_rtx_SET (sr.reg, addr));
+      else
+	{
+	  emit_move_insn (sr.reg, GEN_INT (-rounded_size));
+	  insn = emit_insn (gen_rtx_SET (sr.reg,
+					 gen_rtx_PLUS (Pmode, sr.reg,
+						       stack_pointer_rtx)));
+	}
+      if (m->fs.cfa_reg == stack_pointer_rtx)
+	{
+	  add_reg_note (insn, REG_CFA_DEF_CFA,
+			plus_constant (Pmode, sr.reg,
+				       m->fs.cfa_offset + rounded_size));
+	  RTX_FRAME_RELATED_P (insn) = 1;
+	}
+
+      /* Step 3: the loop.  */
+      rtx size_rtx = GEN_INT (rounded_size);
+      insn = emit_insn (gen_adjust_stack_and_probe (Pmode, sr.reg, sr.reg,
+						    size_rtx));
+      if (m->fs.cfa_reg == stack_pointer_rtx)
+	{
+	  m->fs.cfa_offset += rounded_size;
+	  add_reg_note (insn, REG_CFA_DEF_CFA,
+			plus_constant (Pmode, stack_pointer_rtx,
+				       m->fs.cfa_offset));
+	  RTX_FRAME_RELATED_P (insn) = 1;
+	}
+      m->fs.sp_offset += rounded_size;
+      emit_insn (gen_blockage ());
+
+      /* Step 4: adjust SP if we cannot assert at compile-time that SIZE
+	 is equal to ROUNDED_SIZE.  */
+
+      if (size != rounded_size)
+	{
+	  pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,
+				     GEN_INT (rounded_size - size), -1,
+				     m->fs.cfa_reg == stack_pointer_rtx);
+
+	  if (protection_area)
+	    emit_stack_probe (stack_pointer_rtx);
+	}
+
+      dump_stack_clash_frame_info (PROBE_LOOP, size != rounded_size);
+
+      /* This does not deallocate the space reserved for the scratch
+	 register.  That will be deallocated in the epilogue.  */
+      release_scratch_register_on_entry (&sr, size, false);
+    }
+
+  /* Adjust back to account for the protection area.  */
+  if (protection_area)
+    pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,
+			       GEN_INT (probe_interval + dope), -1,
+			       m->fs.cfa_reg == stack_pointer_rtx);
+
+  /* Make sure nothing is scheduled before we are done.  */
+  emit_insn (gen_blockage ());
+}
+
+/* Adjust the stack pointer up to REG while probing it.  */
+
+const char *
+output_adjust_stack_and_probe (rtx reg)
+{
+  static int labelno = 0;
+  char loop_lab[32];
+  rtx xops[2];
+
+  ASM_GENERATE_INTERNAL_LABEL (loop_lab, "LPSRL", labelno++);
+
+  /* Loop.  */
+  ASM_OUTPUT_INTERNAL_LABEL (asm_out_file, loop_lab);
+
+  /* SP = SP + PROBE_INTERVAL.  */
+  xops[0] = stack_pointer_rtx;
+  xops[1] = GEN_INT (get_probe_interval ());
+  output_asm_insn ("sub%z0\t{%1, %0|%0, %1}", xops);
+
+  /* Probe at SP.  */
+  xops[1] = const0_rtx;
+  output_asm_insn ("or%z0\t{%1, (%0)|DWORD PTR [%0], %1}", xops);
+
+  /* Test if SP == LAST_ADDR.  */
+  xops[0] = stack_pointer_rtx;
+  xops[1] = reg;
+  output_asm_insn ("cmp%z0\t{%1, %0|%0, %1}", xops);
+
+  /* Branch.  */
+  fputs ("\tjne\t", asm_out_file);
+  assemble_name_raw (asm_out_file, loop_lab);
+  fputc ('\n', asm_out_file);
+
+  return "";
+}
+
+/* Emit code to probe a range of stack addresses from FIRST to FIRST+SIZE,
+   inclusive.  These are offsets from the current stack pointer.
+
+   INT_REGISTERS_SAVED is true if integer registers have already been
+   pushed on the stack.  */
+
+static void
+ix86_emit_probe_stack_range (HOST_WIDE_INT first, HOST_WIDE_INT size,
+			     const bool int_registers_saved)
+{
+  const HOST_WIDE_INT probe_interval = get_probe_interval ();
+
+  /* See if we have a constant small number of probes to generate.  If so,
+     that's the easy case.  The run-time loop is made up of 6 insns in the
+     generic case while the compile-time loop is made up of n insns for n #
+     of intervals.  */
+  if (size <= 6 * probe_interval)
+    {
+      HOST_WIDE_INT i;
+
+      /* Probe at FIRST + N * PROBE_INTERVAL for values of N from 1 until
+	 it exceeds SIZE.  If only one probe is needed, this will not
+	 generate any code.  Then probe at FIRST + SIZE.  */
+      for (i = probe_interval; i < size; i += probe_interval)
+	emit_stack_probe (plus_constant (Pmode, stack_pointer_rtx,
+					 -(first + i)));
+
+      emit_stack_probe (plus_constant (Pmode, stack_pointer_rtx,
+				       -(first + size)));
+    }
+
+  /* Otherwise, do the same as above, but in a loop.  Note that we must be
+     extra careful with variables wrapping around because we might be at
+     the very top (or the very bottom) of the address space and we have
+     to be able to handle this case properly; in particular, we use an
+     equality test for the loop condition.  */
+  else
+    {
+      /* We expect the GP registers to be saved when probes are used
+	 as the probing sequences might need a scratch register and
+	 the routine to allocate one assumes the integer registers
+	 have already been saved.  */
+      gcc_assert (int_registers_saved);
+
+      HOST_WIDE_INT rounded_size, last;
+      struct scratch_reg sr;
+
+      get_scratch_register_on_entry (&sr);
+
+
+      /* Step 1: round SIZE to the previous multiple of the interval.  */
+
+      rounded_size = ROUND_DOWN (size, probe_interval);
+
+
+      /* Step 2: compute initial and final value of the loop counter.  */
+
+      /* TEST_OFFSET = FIRST.  */
+      emit_move_insn (sr.reg, GEN_INT (-first));
+
+      /* LAST_OFFSET = FIRST + ROUNDED_SIZE.  */
+      last = first + rounded_size;
+
+
+      /* Step 3: the loop
+
+	 do
+	   {
+	     TEST_ADDR = TEST_ADDR + PROBE_INTERVAL
+	     probe at TEST_ADDR
+	   }
+	 while (TEST_ADDR != LAST_ADDR)
+
+         probes at FIRST + N * PROBE_INTERVAL for values of N from 1
+         until it is equal to ROUNDED_SIZE.  */
+
+      emit_insn
+	(gen_probe_stack_range (Pmode, sr.reg, sr.reg, GEN_INT (-last)));
+
+
+      /* Step 4: probe at FIRST + SIZE if we cannot assert at compile-time
+	 that SIZE is equal to ROUNDED_SIZE.  */
+
+      if (size != rounded_size)
+	emit_stack_probe (plus_constant (Pmode,
+					 gen_rtx_PLUS (Pmode,
+						       stack_pointer_rtx,
+						       sr.reg),
+					 rounded_size - size));
+
+      release_scratch_register_on_entry (&sr, size, true);
+    }
+
+  /* Make sure nothing is scheduled before we are done.  */
+  emit_insn (gen_blockage ());
+}
+
+/* Probe a range of stack addresses from REG to END, inclusive.  These are
+   offsets from the current stack pointer.  */
+
+const char *
+output_probe_stack_range (rtx reg, rtx end)
+{
+  static int labelno = 0;
+  char loop_lab[32];
+  rtx xops[3];
+
+  ASM_GENERATE_INTERNAL_LABEL (loop_lab, "LPSRL", labelno++);
+
+  /* Loop.  */
+  ASM_OUTPUT_INTERNAL_LABEL (asm_out_file, loop_lab);
+
+  /* TEST_ADDR = TEST_ADDR + PROBE_INTERVAL.  */
+  xops[0] = reg;
+  xops[1] = GEN_INT (get_probe_interval ());
+  output_asm_insn ("sub%z0\t{%1, %0|%0, %1}", xops);
+
+  /* Probe at TEST_ADDR.  */
+  xops[0] = stack_pointer_rtx;
+  xops[1] = reg;
+  xops[2] = const0_rtx;
+  output_asm_insn ("or%z0\t{%2, (%0,%1)|DWORD PTR [%0+%1], %2}", xops);
+
+  /* Test if TEST_ADDR == LAST_ADDR.  */
+  xops[0] = reg;
+  xops[1] = end;
+  output_asm_insn ("cmp%z0\t{%1, %0|%0, %1}", xops);
+
+  /* Branch.  */
+  fputs ("\tjne\t", asm_out_file);
+  assemble_name_raw (asm_out_file, loop_lab);
+  fputc ('\n', asm_out_file);
+
+  return "";
+}
+
+/* Set stack_frame_required to false if stack frame isn't required.
+   Update STACK_ALIGNMENT to the largest alignment, in bits, of stack
+   slot used if stack frame is required and CHECK_STACK_SLOT is true.  */
+
+static void
+ix86_find_max_used_stack_alignment (unsigned int &stack_alignment,
+				    bool check_stack_slot)
+{
+  HARD_REG_SET set_up_by_prologue, prologue_used;
+  basic_block bb;
+
+  CLEAR_HARD_REG_SET (prologue_used);
+  CLEAR_HARD_REG_SET (set_up_by_prologue);
+  add_to_hard_reg_set (&set_up_by_prologue, Pmode, STACK_POINTER_REGNUM);
+  add_to_hard_reg_set (&set_up_by_prologue, Pmode, ARG_POINTER_REGNUM);
+  add_to_hard_reg_set (&set_up_by_prologue, Pmode,
+		       HARD_FRAME_POINTER_REGNUM);
+
+  /* The preferred stack alignment is the minimum stack alignment.  */
+  if (stack_alignment > crtl->preferred_stack_boundary)
+    stack_alignment = crtl->preferred_stack_boundary;
+
+  bool require_stack_frame = false;
+
+  FOR_EACH_BB_FN (bb, cfun)
+    {
+      rtx_insn *insn;
+      FOR_BB_INSNS (bb, insn)
+	if (NONDEBUG_INSN_P (insn)
+	    && requires_stack_frame_p (insn, prologue_used,
+				       set_up_by_prologue))
+	  {
+	    require_stack_frame = true;
+
+	    if (check_stack_slot)
+	      {
+		/* Find the maximum stack alignment.  */
+		subrtx_iterator::array_type array;
+		FOR_EACH_SUBRTX (iter, array, PATTERN (insn), ALL)
+		  if (MEM_P (*iter)
+		      && (reg_mentioned_p (stack_pointer_rtx,
+					   *iter)
+			  || reg_mentioned_p (frame_pointer_rtx,
+					      *iter)))
+		    {
+		      unsigned int alignment = MEM_ALIGN (*iter);
+		      if (alignment > stack_alignment)
+			stack_alignment = alignment;
+		    }
+	      }
+	  }
+    }
+
+  cfun->machine->stack_frame_required = require_stack_frame;
+}
+
+/* Finalize stack_realign_needed and frame_pointer_needed flags, which
+   will guide prologue/epilogue to be generated in correct form.  */
+
+static void
+ix86_finalize_stack_frame_flags (void)
+{
+  /* Check if stack realign is really needed after reload, and
+     stores result in cfun */
+  unsigned int incoming_stack_boundary
+    = (crtl->parm_stack_boundary > ix86_incoming_stack_boundary
+       ? crtl->parm_stack_boundary : ix86_incoming_stack_boundary);
+  unsigned int stack_alignment
+    = (crtl->is_leaf && !ix86_current_function_calls_tls_descriptor
+       ? crtl->max_used_stack_slot_alignment
+       : crtl->stack_alignment_needed);
+  unsigned int stack_realign
+    = (incoming_stack_boundary < stack_alignment);
+  bool recompute_frame_layout_p = false;
+
+  if (crtl->stack_realign_finalized)
+    {
+      /* After stack_realign_needed is finalized, we can't no longer
+	 change it.  */
+      gcc_assert (crtl->stack_realign_needed == stack_realign);
+      return;
+    }
+
+  /* It is always safe to compute max_used_stack_alignment.  We
+     compute it only if 128-bit aligned load/store may be generated
+     on misaligned stack slot which will lead to segfault. */
+  bool check_stack_slot
+    = (stack_realign || crtl->max_used_stack_slot_alignment >= 128);
+  ix86_find_max_used_stack_alignment (stack_alignment,
+				      check_stack_slot);
+
+  /* If the only reason for frame_pointer_needed is that we conservatively
+     assumed stack realignment might be needed or -fno-omit-frame-pointer
+     is used, but in the end nothing that needed the stack alignment had
+     been spilled nor stack access, clear frame_pointer_needed and say we
+     don't need stack realignment.  */
+  if ((stack_realign || (!flag_omit_frame_pointer && optimize))
+      && frame_pointer_needed
+      && crtl->is_leaf
+      && crtl->sp_is_unchanging
+      && !ix86_current_function_calls_tls_descriptor
+      && !crtl->accesses_prior_frames
+      && !cfun->calls_alloca
+      && !crtl->calls_eh_return
+      /* See ira_setup_eliminable_regset for the rationale.  */
+      && !(STACK_CHECK_MOVING_SP
+	   && flag_stack_check
+	   && flag_exceptions
+	   && cfun->can_throw_non_call_exceptions)
+      && !ix86_frame_pointer_required ()
+      && ix86_get_frame_size () == 0
+      && ix86_nsaved_sseregs () == 0
+      && ix86_varargs_gpr_size + ix86_varargs_fpr_size == 0)
+    {
+      if (cfun->machine->stack_frame_required)
+	{
+	  /* Stack frame is required.  If stack alignment needed is less
+	     than incoming stack boundary, don't realign stack.  */
+	  stack_realign = incoming_stack_boundary < stack_alignment;
+	  if (!stack_realign)
+	    {
+	      crtl->max_used_stack_slot_alignment
+		= incoming_stack_boundary;
+	      crtl->stack_alignment_needed
+		= incoming_stack_boundary;
+	      /* Also update preferred_stack_boundary for leaf
+	         functions.  */
+	      crtl->preferred_stack_boundary
+		= incoming_stack_boundary;
+	    }
+	}
+      else
+	{
+	  /* If drap has been set, but it actually isn't live at the
+	     start of the function, there is no reason to set it up.  */
+	  if (crtl->drap_reg)
+	    {
+	      basic_block bb = ENTRY_BLOCK_PTR_FOR_FN (cfun)->next_bb;
+	      if (! REGNO_REG_SET_P (DF_LR_IN (bb),
+				     REGNO (crtl->drap_reg)))
+		{
+		  crtl->drap_reg = NULL_RTX;
+		  crtl->need_drap = false;
+		}
+	    }
+	  else
+	    cfun->machine->no_drap_save_restore = true;
+
+	  frame_pointer_needed = false;
+	  stack_realign = false;
+	  crtl->max_used_stack_slot_alignment = incoming_stack_boundary;
+	  crtl->stack_alignment_needed = incoming_stack_boundary;
+	  crtl->stack_alignment_estimated = incoming_stack_boundary;
+	  if (crtl->preferred_stack_boundary > incoming_stack_boundary)
+	    crtl->preferred_stack_boundary = incoming_stack_boundary;
+	  df_finish_pass (true);
+	  df_scan_alloc (NULL);
+	  df_scan_blocks ();
+	  df_compute_regs_ever_live (true);
+	  df_analyze ();
+
+	  if (flag_var_tracking)
+	    {
+	      /* Since frame pointer is no longer available, replace it with
+		 stack pointer - UNITS_PER_WORD in debug insns.  */
+	      df_ref ref, next;
+	      for (ref = DF_REG_USE_CHAIN (HARD_FRAME_POINTER_REGNUM);
+		   ref; ref = next)
+		{
+		  next = DF_REF_NEXT_REG (ref);
+		  if (!DF_REF_INSN_INFO (ref))
+		    continue;
+
+		  /* Make sure the next ref is for a different instruction,
+		     so that we're not affected by the rescan.  */
+		  rtx_insn *insn = DF_REF_INSN (ref);
+		  while (next && DF_REF_INSN (next) == insn)
+		    next = DF_REF_NEXT_REG (next);
+
+		  if (DEBUG_INSN_P (insn))
+		    {
+		      bool changed = false;
+		      for (; ref != next; ref = DF_REF_NEXT_REG (ref))
+			{
+			  rtx *loc = DF_REF_LOC (ref);
+			  if (*loc == hard_frame_pointer_rtx)
+			    {
+			      *loc = plus_constant (Pmode,
+						    stack_pointer_rtx,
+						    -UNITS_PER_WORD);
+			      changed = true;
+			    }
+			}
+		      if (changed)
+			df_insn_rescan (insn);
+		    }
+		}
+	    }
+
+	  recompute_frame_layout_p = true;
+	}
+    }
+  else if (crtl->max_used_stack_slot_alignment >= 128
+	   && cfun->machine->stack_frame_required)
+    {
+      /* We don't need to realign stack.  max_used_stack_alignment is
+	 used to decide how stack frame should be aligned.  This is
+	 independent of any psABIs nor 32-bit vs 64-bit.  */
+      cfun->machine->max_used_stack_alignment
+	= stack_alignment / BITS_PER_UNIT;
+    }
+
+  if (crtl->stack_realign_needed != stack_realign)
+    recompute_frame_layout_p = true;
+  crtl->stack_realign_needed = stack_realign;
+  crtl->stack_realign_finalized = true;
+  if (recompute_frame_layout_p)
+    ix86_compute_frame_layout ();
+}
+
+/* Delete SET_GOT right after entry block if it is allocated to reg.  */
+
+static void
+ix86_elim_entry_set_got (rtx reg)
+{
+  basic_block bb = ENTRY_BLOCK_PTR_FOR_FN (cfun)->next_bb;
+  rtx_insn *c_insn = BB_HEAD (bb);
+  if (!NONDEBUG_INSN_P (c_insn))
+    c_insn = next_nonnote_nondebug_insn (c_insn);
+  if (c_insn && NONJUMP_INSN_P (c_insn))
+    {
+      rtx pat = PATTERN (c_insn);
+      if (GET_CODE (pat) == PARALLEL)
+	{
+	  rtx vec = XVECEXP (pat, 0, 0);
+	  if (GET_CODE (vec) == SET
+	      && XINT (XEXP (vec, 1), 1) == UNSPEC_SET_GOT
+	      && REGNO (XEXP (vec, 0)) == REGNO (reg))
+	    delete_insn (c_insn);
+	}
+    }
+}
+
+static rtx
+gen_frame_set (rtx reg, rtx frame_reg, int offset, bool store)
+{
+  rtx addr, mem;
+
+  if (offset)
+    addr = plus_constant (Pmode, frame_reg, offset);
+  mem = gen_frame_mem (GET_MODE (reg), offset ? addr : frame_reg);
+  return gen_rtx_SET (store ? mem : reg, store ? reg : mem);
+}
+
+static inline rtx
+gen_frame_load (rtx reg, rtx frame_reg, int offset)
+{
+  return gen_frame_set (reg, frame_reg, offset, false);
+}
+
+static inline rtx
+gen_frame_store (rtx reg, rtx frame_reg, int offset)
+{
+  return gen_frame_set (reg, frame_reg, offset, true);
+}
+
+static void
+ix86_emit_outlined_ms2sysv_save (const struct ix86_frame &frame)
+{
+  struct machine_function *m = cfun->machine;
+  const unsigned ncregs = NUM_X86_64_MS_CLOBBERED_REGS
+			  + m->call_ms2sysv_extra_regs;
+  rtvec v = rtvec_alloc (ncregs + 1);
+  unsigned int align, i, vi = 0;
+  rtx_insn *insn;
+  rtx sym, addr;
+  rtx rax = gen_rtx_REG (word_mode, AX_REG);
+  const class xlogue_layout &xlogue = xlogue_layout::get_instance ();
+
+  /* AL should only be live with sysv_abi.  */
+  gcc_assert (!ix86_eax_live_at_start_p ());
+  gcc_assert (m->fs.sp_offset >= frame.sse_reg_save_offset);
+
+  /* Setup RAX as the stub's base pointer.  We use stack_realign_offset rather
+     we've actually realigned the stack or not.  */
+  align = GET_MODE_ALIGNMENT (V4SFmode);
+  addr = choose_baseaddr (frame.stack_realign_offset
+			  + xlogue.get_stub_ptr_offset (), &align, AX_REG);
+  gcc_assert (align >= GET_MODE_ALIGNMENT (V4SFmode));
+
+  emit_insn (gen_rtx_SET (rax, addr));
+
+  /* Get the stub symbol.  */
+  sym = xlogue.get_stub_rtx (frame_pointer_needed ? XLOGUE_STUB_SAVE_HFP
+						  : XLOGUE_STUB_SAVE);
+  RTVEC_ELT (v, vi++) = gen_rtx_USE (VOIDmode, sym);
+
+  for (i = 0; i < ncregs; ++i)
+    {
+      const xlogue_layout::reginfo &r = xlogue.get_reginfo (i);
+      rtx reg = gen_rtx_REG ((SSE_REGNO_P (r.regno) ? V4SFmode : word_mode),
+			     r.regno);
+      RTVEC_ELT (v, vi++) = gen_frame_store (reg, rax, -r.offset);
+    }
+
+  gcc_assert (vi == (unsigned)GET_NUM_ELEM (v));
+
+  insn = emit_insn (gen_rtx_PARALLEL (VOIDmode, v));
+  RTX_FRAME_RELATED_P (insn) = true;
+}
+
+/* Generate and return an insn body to AND X with Y.  */
+
+static rtx_insn *
+gen_and2_insn (rtx x, rtx y)
+{
+  enum insn_code icode = optab_handler (and_optab, GET_MODE (x));
+
+  gcc_assert (insn_operand_matches (icode, 0, x));
+  gcc_assert (insn_operand_matches (icode, 1, x));
+  gcc_assert (insn_operand_matches (icode, 2, y));
+
+  return GEN_FCN (icode) (x, x, y);
+}
+
+/* Expand the prologue into a bunch of separate insns.  */
+
+void
+ix86_expand_prologue (void)
+{
+  struct machine_function *m = cfun->machine;
+  rtx insn, t;
+  HOST_WIDE_INT allocate;
+  bool int_registers_saved;
+  bool sse_registers_saved;
+  bool save_stub_call_needed;
+  rtx static_chain = NULL_RTX;
+
+  if (ix86_function_naked (current_function_decl))
+    {
+      if (flag_stack_usage_info)
+	current_function_static_stack_size = 0;
+      return;
+    }
+
+  ix86_finalize_stack_frame_flags ();
+
+  /* DRAP should not coexist with stack_realign_fp */
+  gcc_assert (!(crtl->drap_reg && stack_realign_fp));
+
+  memset (&m->fs, 0, sizeof (m->fs));
+
+  /* Initialize CFA state for before the prologue.  */
+  m->fs.cfa_reg = stack_pointer_rtx;
+  m->fs.cfa_offset = INCOMING_FRAME_SP_OFFSET;
+
+  /* Track SP offset to the CFA.  We continue tracking this after we've
+     swapped the CFA register away from SP.  In the case of re-alignment
+     this is fudged; we're interested to offsets within the local frame.  */
+  m->fs.sp_offset = INCOMING_FRAME_SP_OFFSET;
+  m->fs.sp_valid = true;
+  m->fs.sp_realigned = false;
+
+  const struct ix86_frame &frame = cfun->machine->frame;
+
+  if (!TARGET_64BIT && ix86_function_ms_hook_prologue (current_function_decl))
+    {
+      /* We should have already generated an error for any use of
+         ms_hook on a nested function.  */
+      gcc_checking_assert (!ix86_static_chain_on_stack);
+
+      /* Check if profiling is active and we shall use profiling before
+         prologue variant. If so sorry.  */
+      if (crtl->profile && flag_fentry != 0)
+	sorry ("%<ms_hook_prologue%> attribute is not compatible "
+	       "with %<-mfentry%> for 32-bit");
+
+      /* In ix86_asm_output_function_label we emitted:
+	 8b ff     movl.s %edi,%edi
+	 55        push   %ebp
+	 8b ec     movl.s %esp,%ebp
+
+	 This matches the hookable function prologue in Win32 API
+	 functions in Microsoft Windows XP Service Pack 2 and newer.
+	 Wine uses this to enable Windows apps to hook the Win32 API
+	 functions provided by Wine.
+
+	 What that means is that we've already set up the frame pointer.  */
+
+      if (frame_pointer_needed
+	  && !(crtl->drap_reg && crtl->stack_realign_needed))
+	{
+	  rtx push, mov;
+
+	  /* We've decided to use the frame pointer already set up.
+	     Describe this to the unwinder by pretending that both
+	     push and mov insns happen right here.
+
+	     Putting the unwind info here at the end of the ms_hook
+	     is done so that we can make absolutely certain we get
+	     the required byte sequence at the start of the function,
+	     rather than relying on an assembler that can produce
+	     the exact encoding required.
+
+	     However it does mean (in the unpatched case) that we have
+	     a 1 insn window where the asynchronous unwind info is
+	     incorrect.  However, if we placed the unwind info at
+	     its correct location we would have incorrect unwind info
+	     in the patched case.  Which is probably all moot since
+	     I don't expect Wine generates dwarf2 unwind info for the
+	     system libraries that use this feature.  */
+
+	  insn = emit_insn (gen_blockage ());
+
+	  push = gen_push (hard_frame_pointer_rtx);
+	  mov = gen_rtx_SET (hard_frame_pointer_rtx,
+			     stack_pointer_rtx);
+	  RTX_FRAME_RELATED_P (push) = 1;
+	  RTX_FRAME_RELATED_P (mov) = 1;
+
+	  RTX_FRAME_RELATED_P (insn) = 1;
+	  add_reg_note (insn, REG_FRAME_RELATED_EXPR,
+			gen_rtx_PARALLEL (VOIDmode, gen_rtvec (2, push, mov)));
+
+	  /* Note that gen_push incremented m->fs.cfa_offset, even
+	     though we didn't emit the push insn here.  */
+	  m->fs.cfa_reg = hard_frame_pointer_rtx;
+	  m->fs.fp_offset = m->fs.cfa_offset;
+	  m->fs.fp_valid = true;
+	}
+      else
+	{
+	  /* The frame pointer is not needed so pop %ebp again.
+	     This leaves us with a pristine state.  */
+	  emit_insn (gen_pop (hard_frame_pointer_rtx));
+	}
+    }
+
+  /* The first insn of a function that accepts its static chain on the
+     stack is to push the register that would be filled in by a direct
+     call.  This insn will be skipped by the trampoline.  */
+  else if (ix86_static_chain_on_stack)
+    {
+      static_chain = ix86_static_chain (cfun->decl, false);
+      insn = emit_insn (gen_push (static_chain));
+      emit_insn (gen_blockage ());
+
+      /* We don't want to interpret this push insn as a register save,
+	 only as a stack adjustment.  The real copy of the register as
+	 a save will be done later, if needed.  */
+      t = plus_constant (Pmode, stack_pointer_rtx, -UNITS_PER_WORD);
+      t = gen_rtx_SET (stack_pointer_rtx, t);
+      add_reg_note (insn, REG_CFA_ADJUST_CFA, t);
+      RTX_FRAME_RELATED_P (insn) = 1;
+    }
+
+  /* Emit prologue code to adjust stack alignment and setup DRAP, in case
+     of DRAP is needed and stack realignment is really needed after reload */
+  if (stack_realign_drap)
+    {
+      int align_bytes = crtl->stack_alignment_needed / BITS_PER_UNIT;
+
+      /* Can't use DRAP in interrupt function.  */
+      if (cfun->machine->func_type != TYPE_NORMAL)
+	sorry ("Dynamic Realign Argument Pointer (DRAP) not supported "
+	       "in interrupt service routine.  This may be worked "
+	       "around by avoiding functions with aggregate return.");
+
+      /* Only need to push parameter pointer reg if it is caller saved.  */
+      if (!call_used_or_fixed_reg_p (REGNO (crtl->drap_reg)))
+	{
+	  /* Push arg pointer reg */
+	  insn = emit_insn (gen_push (crtl->drap_reg));
+	  RTX_FRAME_RELATED_P (insn) = 1;
+	}
+
+      /* Grab the argument pointer.  */
+      t = plus_constant (Pmode, stack_pointer_rtx, m->fs.sp_offset);
+      insn = emit_insn (gen_rtx_SET (crtl->drap_reg, t));
+      RTX_FRAME_RELATED_P (insn) = 1;
+      m->fs.cfa_reg = crtl->drap_reg;
+      m->fs.cfa_offset = 0;
+
+      /* Align the stack.  */
+      insn = emit_insn (gen_and2_insn (stack_pointer_rtx,
+				       GEN_INT (-align_bytes)));
+      RTX_FRAME_RELATED_P (insn) = 1;
+
+      /* Replicate the return address on the stack so that return
+	 address can be reached via (argp - 1) slot.  This is needed
+	 to implement macro RETURN_ADDR_RTX and intrinsic function
+	 expand_builtin_return_addr etc.  */
+      t = plus_constant (Pmode, crtl->drap_reg, -UNITS_PER_WORD);
+      t = gen_frame_mem (word_mode, t);
+      insn = emit_insn (gen_push (t));
+      RTX_FRAME_RELATED_P (insn) = 1;
+
+      /* For the purposes of frame and register save area addressing,
+	 we've started over with a new frame.  */
+      m->fs.sp_offset = INCOMING_FRAME_SP_OFFSET;
+      m->fs.realigned = true;
+
+      if (static_chain)
+	{
+	  /* Replicate static chain on the stack so that static chain
+	     can be reached via (argp - 2) slot.  This is needed for
+	     nested function with stack realignment.  */
+	  insn = emit_insn (gen_push (static_chain));
+	  RTX_FRAME_RELATED_P (insn) = 1;
+	}
+    }
+
+  int_registers_saved = (frame.nregs == 0);
+  sse_registers_saved = (frame.nsseregs == 0);
+  save_stub_call_needed = (m->call_ms2sysv);
+  gcc_assert (sse_registers_saved || !save_stub_call_needed);
+
+  if (frame_pointer_needed && !m->fs.fp_valid)
+    {
+      /* Note: AT&T enter does NOT have reversed args.  Enter is probably
+         slower on all targets.  Also sdb didn't like it.  */
+      insn = emit_insn (gen_push (hard_frame_pointer_rtx));
+      RTX_FRAME_RELATED_P (insn) = 1;
+
+      if (m->fs.sp_offset == frame.hard_frame_pointer_offset)
+	{
+	  insn = emit_move_insn (hard_frame_pointer_rtx, stack_pointer_rtx);
+	  RTX_FRAME_RELATED_P (insn) = 1;
+
+	  if (m->fs.cfa_reg == stack_pointer_rtx)
+	    m->fs.cfa_reg = hard_frame_pointer_rtx;
+	  m->fs.fp_offset = m->fs.sp_offset;
+	  m->fs.fp_valid = true;
+	}
+    }
+
+  if (!int_registers_saved)
+    {
+      /* If saving registers via PUSH, do so now.  */
+      if (!frame.save_regs_using_mov)
+	{
+	  ix86_emit_save_regs ();
+	  int_registers_saved = true;
+	  gcc_assert (m->fs.sp_offset == frame.reg_save_offset);
+	}
+
+      /* When using red zone we may start register saving before allocating
+	 the stack frame saving one cycle of the prologue.  However, avoid
+	 doing this if we have to probe the stack; at least on x86_64 the
+	 stack probe can turn into a call that clobbers a red zone location. */
+      else if (ix86_using_red_zone ()
+	       && (! TARGET_STACK_PROBE
+		   || frame.stack_pointer_offset < CHECK_STACK_LIMIT))
+	{
+	  ix86_emit_save_regs_using_mov (frame.reg_save_offset);
+	  cfun->machine->red_zone_used = true;
+	  int_registers_saved = true;
+	}
+    }
+
+  if (frame.red_zone_size != 0)
+    cfun->machine->red_zone_used = true;
+
+  if (stack_realign_fp)
+    {
+      int align_bytes = crtl->stack_alignment_needed / BITS_PER_UNIT;
+      gcc_assert (align_bytes > MIN_STACK_BOUNDARY / BITS_PER_UNIT);
+
+      /* Record last valid frame pointer offset.  */
+      m->fs.sp_realigned_fp_last = frame.reg_save_offset;
+
+      /* The computation of the size of the re-aligned stack frame means
+	 that we must allocate the size of the register save area before
+	 performing the actual alignment.  Otherwise we cannot guarantee
+	 that there's enough storage above the realignment point.  */
+      allocate = frame.reg_save_offset - m->fs.sp_offset
+		 + frame.stack_realign_allocate;
+      if (allocate)
+        pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,
+				   GEN_INT (-allocate), -1, false);
+
+      /* Align the stack.  */
+      emit_insn (gen_and2_insn (stack_pointer_rtx, GEN_INT (-align_bytes)));
+      m->fs.sp_offset = ROUND_UP (m->fs.sp_offset, align_bytes);
+      m->fs.sp_realigned_offset = m->fs.sp_offset
+					      - frame.stack_realign_allocate;
+      /* The stack pointer may no longer be equal to CFA - m->fs.sp_offset.
+	 Beyond this point, stack access should be done via choose_baseaddr or
+	 by using sp_valid_at and fp_valid_at to determine the correct base
+	 register.  Henceforth, any CFA offset should be thought of as logical
+	 and not physical.  */
+      gcc_assert (m->fs.sp_realigned_offset >= m->fs.sp_realigned_fp_last);
+      gcc_assert (m->fs.sp_realigned_offset == frame.stack_realign_offset);
+      m->fs.sp_realigned = true;
+
+      /* SEH unwind emit doesn't currently support REG_CFA_EXPRESSION, which
+	 is needed to describe where a register is saved using a realigned
+	 stack pointer, so we need to invalidate the stack pointer for that
+	 target.  */
+      if (TARGET_SEH)
+	m->fs.sp_valid = false;
+
+      /* If SP offset is non-immediate after allocation of the stack frame,
+	 then emit SSE saves or stub call prior to allocating the rest of the
+	 stack frame.  This is less efficient for the out-of-line stub because
+	 we can't combine allocations across the call barrier, but it's better
+	 than using a scratch register.  */
+      else if (!x86_64_immediate_operand (GEN_INT (frame.stack_pointer_offset
+						   - m->fs.sp_realigned_offset),
+					  Pmode))
+	{
+	  if (!sse_registers_saved)
+	    {
+	      ix86_emit_save_sse_regs_using_mov (frame.sse_reg_save_offset);
+	      sse_registers_saved = true;
+	    }
+	  else if (save_stub_call_needed)
+	    {
+	      ix86_emit_outlined_ms2sysv_save (frame);
+	      save_stub_call_needed = false;
+	    }
+	}
+    }
+
+  allocate = frame.stack_pointer_offset - m->fs.sp_offset;
+
+  if (flag_stack_usage_info)
+    {
+      /* We start to count from ARG_POINTER.  */
+      HOST_WIDE_INT stack_size = frame.stack_pointer_offset;
+
+      /* If it was realigned, take into account the fake frame.  */
+      if (stack_realign_drap)
+	{
+	  if (ix86_static_chain_on_stack)
+	    stack_size += UNITS_PER_WORD;
+
+	  if (!call_used_or_fixed_reg_p (REGNO (crtl->drap_reg)))
+	    stack_size += UNITS_PER_WORD;
+
+	  /* This over-estimates by 1 minimal-stack-alignment-unit but
+	     mitigates that by counting in the new return address slot.  */
+	  current_function_dynamic_stack_size
+	    += crtl->stack_alignment_needed / BITS_PER_UNIT;
+	}
+
+      current_function_static_stack_size = stack_size;
+    }
+
+  /* On SEH target with very large frame size, allocate an area to save
+     SSE registers (as the very large allocation won't be described).  */
+  if (TARGET_SEH
+      && frame.stack_pointer_offset > SEH_MAX_FRAME_SIZE
+      && !sse_registers_saved)
+    {
+      HOST_WIDE_INT sse_size
+	= frame.sse_reg_save_offset - frame.reg_save_offset;
+
+      gcc_assert (int_registers_saved);
+
+      /* No need to do stack checking as the area will be immediately
+	 written.  */
+      pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,
+			         GEN_INT (-sse_size), -1,
+				 m->fs.cfa_reg == stack_pointer_rtx);
+      allocate -= sse_size;
+      ix86_emit_save_sse_regs_using_mov (frame.sse_reg_save_offset);
+      sse_registers_saved = true;
+    }
+
+  /* If stack clash protection is requested, then probe the stack.  */
+  if (allocate >= 0 && flag_stack_clash_protection)
+    {
+      ix86_adjust_stack_and_probe (allocate, int_registers_saved, false);
+      allocate = 0;
+    }
+
+  /* The stack has already been decremented by the instruction calling us
+     so probe if the size is non-negative to preserve the protection area.  */
+  else if (allocate >= 0 && flag_stack_check == STATIC_BUILTIN_STACK_CHECK)
+    {
+      const HOST_WIDE_INT probe_interval = get_probe_interval ();
+
+      if (STACK_CHECK_MOVING_SP)
+	{
+	  if (crtl->is_leaf
+	      && !cfun->calls_alloca
+	      && allocate <= probe_interval)
+	    ;
+
+	  else
+	    {
+	      ix86_adjust_stack_and_probe (allocate, int_registers_saved, true);
+	      allocate = 0;
+	    }
+	}
+
+      else
+	{
+	  HOST_WIDE_INT size = allocate;
+
+	  if (TARGET_64BIT && size >= HOST_WIDE_INT_C (0x80000000))
+	    size = 0x80000000 - get_stack_check_protect () - 1;
+
+	  if (TARGET_STACK_PROBE)
+	    {
+	      if (crtl->is_leaf && !cfun->calls_alloca)
+		{
+		  if (size > probe_interval)
+		    ix86_emit_probe_stack_range (0, size, int_registers_saved);
+		}
+	      else
+		ix86_emit_probe_stack_range (0,
+					     size + get_stack_check_protect (),
+					     int_registers_saved);
+	    }
+	  else
+	    {
+	      if (crtl->is_leaf && !cfun->calls_alloca)
+		{
+		  if (size > probe_interval
+		      && size > get_stack_check_protect ())
+		    ix86_emit_probe_stack_range (get_stack_check_protect (),
+						 (size
+						  - get_stack_check_protect ()),
+						 int_registers_saved);
+		}
+	      else
+		ix86_emit_probe_stack_range (get_stack_check_protect (), size,
+					     int_registers_saved);
+	    }
+	}
+    }
+
+  if (allocate == 0)
+    ;
+  else if (!ix86_target_stack_probe ()
+	   || frame.stack_pointer_offset < CHECK_STACK_LIMIT)
+    {
+      pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,
+			         GEN_INT (-allocate), -1,
+			         m->fs.cfa_reg == stack_pointer_rtx);
+    }
+  else
+    {
+      rtx eax = gen_rtx_REG (Pmode, AX_REG);
+      rtx r10 = NULL;
+      const bool sp_is_cfa_reg = (m->fs.cfa_reg == stack_pointer_rtx);
+      bool eax_live = ix86_eax_live_at_start_p ();
+      bool r10_live = false;
+
+      if (TARGET_64BIT)
+        r10_live = (DECL_STATIC_CHAIN (current_function_decl) != 0);
+
+      if (eax_live)
+	{
+	  insn = emit_insn (gen_push (eax));
+	  allocate -= UNITS_PER_WORD;
+	  /* Note that SEH directives need to continue tracking the stack
+	     pointer even after the frame pointer has been set up.  */
+	  if (sp_is_cfa_reg || TARGET_SEH)
+	    {
+	      if (sp_is_cfa_reg)
+		m->fs.cfa_offset += UNITS_PER_WORD;
+	      RTX_FRAME_RELATED_P (insn) = 1;
+	      add_reg_note (insn, REG_FRAME_RELATED_EXPR,
+			    gen_rtx_SET (stack_pointer_rtx,
+					 plus_constant (Pmode,
+							stack_pointer_rtx,
+							-UNITS_PER_WORD)));
+	    }
+	}
+
+      if (r10_live)
+	{
+	  r10 = gen_rtx_REG (Pmode, R10_REG);
+	  insn = emit_insn (gen_push (r10));
+	  allocate -= UNITS_PER_WORD;
+	  if (sp_is_cfa_reg || TARGET_SEH)
+	    {
+	      if (sp_is_cfa_reg)
+		m->fs.cfa_offset += UNITS_PER_WORD;
+	      RTX_FRAME_RELATED_P (insn) = 1;
+	      add_reg_note (insn, REG_FRAME_RELATED_EXPR,
+			    gen_rtx_SET (stack_pointer_rtx,
+					 plus_constant (Pmode,
+							stack_pointer_rtx,
+							-UNITS_PER_WORD)));
+	    }
+	}
+
+      emit_move_insn (eax, GEN_INT (allocate));
+      emit_insn (gen_allocate_stack_worker_probe (Pmode, eax, eax));
+
+      /* Use the fact that AX still contains ALLOCATE.  */
+      insn = emit_insn (gen_pro_epilogue_adjust_stack_sub
+			(Pmode, stack_pointer_rtx, stack_pointer_rtx, eax));
+
+      if (sp_is_cfa_reg || TARGET_SEH)
+	{
+	  if (sp_is_cfa_reg)
+	    m->fs.cfa_offset += allocate;
+	  RTX_FRAME_RELATED_P (insn) = 1;
+	  add_reg_note (insn, REG_FRAME_RELATED_EXPR,
+			gen_rtx_SET (stack_pointer_rtx,
+				     plus_constant (Pmode, stack_pointer_rtx,
+						    -allocate)));
+	}
+      m->fs.sp_offset += allocate;
+
+      /* Use stack_pointer_rtx for relative addressing so that code works for
+	 realigned stack.  But this means that we need a blockage to prevent
+	 stores based on the frame pointer from being scheduled before.  */
+      if (r10_live && eax_live)
+        {
+	  t = gen_rtx_PLUS (Pmode, stack_pointer_rtx, eax);
+	  emit_move_insn (gen_rtx_REG (word_mode, R10_REG),
+			  gen_frame_mem (word_mode, t));
+	  t = plus_constant (Pmode, t, UNITS_PER_WORD);
+	  emit_move_insn (gen_rtx_REG (word_mode, AX_REG),
+			  gen_frame_mem (word_mode, t));
+	  emit_insn (gen_memory_blockage ());
+	}
+      else if (eax_live || r10_live)
+	{
+	  t = gen_rtx_PLUS (Pmode, stack_pointer_rtx, eax);
+	  emit_move_insn (gen_rtx_REG (word_mode,
+				       (eax_live ? AX_REG : R10_REG)),
+			  gen_frame_mem (word_mode, t));
+	  emit_insn (gen_memory_blockage ());
+	}
+    }
+  gcc_assert (m->fs.sp_offset == frame.stack_pointer_offset);
+
+  /* If we havn't already set up the frame pointer, do so now.  */
+  if (frame_pointer_needed && !m->fs.fp_valid)
+    {
+      insn = gen_add3_insn (hard_frame_pointer_rtx, stack_pointer_rtx,
+			    GEN_INT (frame.stack_pointer_offset
+				     - frame.hard_frame_pointer_offset));
+      insn = emit_insn (insn);
+      RTX_FRAME_RELATED_P (insn) = 1;
+      add_reg_note (insn, REG_CFA_ADJUST_CFA, NULL);
+
+      if (m->fs.cfa_reg == stack_pointer_rtx)
+	m->fs.cfa_reg = hard_frame_pointer_rtx;
+      m->fs.fp_offset = frame.hard_frame_pointer_offset;
+      m->fs.fp_valid = true;
+    }
+
+  if (!int_registers_saved)
+    ix86_emit_save_regs_using_mov (frame.reg_save_offset);
+  if (!sse_registers_saved)
+    ix86_emit_save_sse_regs_using_mov (frame.sse_reg_save_offset);
+  else if (save_stub_call_needed)
+    ix86_emit_outlined_ms2sysv_save (frame);
+
+  /* For the mcount profiling on 32 bit PIC mode we need to emit SET_GOT
+     in PROLOGUE.  */
+  if (!TARGET_64BIT && pic_offset_table_rtx && crtl->profile && !flag_fentry)
+    {
+      rtx pic = gen_rtx_REG (Pmode, REAL_PIC_OFFSET_TABLE_REGNUM);
+      insn = emit_insn (gen_set_got (pic));
+      RTX_FRAME_RELATED_P (insn) = 1;
+      add_reg_note (insn, REG_CFA_FLUSH_QUEUE, NULL_RTX);
+      emit_insn (gen_prologue_use (pic));
+      /* Deleting already emmitted SET_GOT if exist and allocated to
+	 REAL_PIC_OFFSET_TABLE_REGNUM.  */
+      ix86_elim_entry_set_got (pic);
+    }
+
+  if (crtl->drap_reg && !crtl->stack_realign_needed)
+    {
+      /* vDRAP is setup but after reload it turns out stack realign
+         isn't necessary, here we will emit prologue to setup DRAP
+         without stack realign adjustment */
+      t = choose_baseaddr (0, NULL);
+      emit_insn (gen_rtx_SET (crtl->drap_reg, t));
+    }
+
+  /* Prevent instructions from being scheduled into register save push
+     sequence when access to the redzone area is done through frame pointer.
+     The offset between the frame pointer and the stack pointer is calculated
+     relative to the value of the stack pointer at the end of the function
+     prologue, and moving instructions that access redzone area via frame
+     pointer inside push sequence violates this assumption.  */
+  if (frame_pointer_needed && frame.red_zone_size)
+    emit_insn (gen_memory_blockage ());
+
+  /* SEH requires that the prologue end within 256 bytes of the start of
+     the function.  Prevent instruction schedules that would extend that.
+     Further, prevent alloca modifications to the stack pointer from being
+     combined with prologue modifications.  */
+  if (TARGET_SEH)
+    emit_insn (gen_prologue_use (stack_pointer_rtx));
+}
+
+/* Emit code to restore REG using a POP insn.  */
+
+static void
+ix86_emit_restore_reg_using_pop (rtx reg)
+{
+  struct machine_function *m = cfun->machine;
+  rtx_insn *insn = emit_insn (gen_pop (reg));
+
+  ix86_add_cfa_restore_note (insn, reg, m->fs.sp_offset);
+  m->fs.sp_offset -= UNITS_PER_WORD;
+
+  if (m->fs.cfa_reg == crtl->drap_reg
+      && REGNO (reg) == REGNO (crtl->drap_reg))
+    {
+      /* Previously we'd represented the CFA as an expression
+	 like *(%ebp - 8).  We've just popped that value from
+	 the stack, which means we need to reset the CFA to
+	 the drap register.  This will remain until we restore
+	 the stack pointer.  */
+      add_reg_note (insn, REG_CFA_DEF_CFA, reg);
+      RTX_FRAME_RELATED_P (insn) = 1;
+
+      /* This means that the DRAP register is valid for addressing too.  */
+      m->fs.drap_valid = true;
+      return;
+    }
+
+  if (m->fs.cfa_reg == stack_pointer_rtx)
+    {
+      rtx x = plus_constant (Pmode, stack_pointer_rtx, UNITS_PER_WORD);
+      x = gen_rtx_SET (stack_pointer_rtx, x);
+      add_reg_note (insn, REG_CFA_ADJUST_CFA, x);
+      RTX_FRAME_RELATED_P (insn) = 1;
+
+      m->fs.cfa_offset -= UNITS_PER_WORD;
+    }
+
+  /* When the frame pointer is the CFA, and we pop it, we are
+     swapping back to the stack pointer as the CFA.  This happens
+     for stack frames that don't allocate other data, so we assume
+     the stack pointer is now pointing at the return address, i.e.
+     the function entry state, which makes the offset be 1 word.  */
+  if (reg == hard_frame_pointer_rtx)
+    {
+      m->fs.fp_valid = false;
+      if (m->fs.cfa_reg == hard_frame_pointer_rtx)
+	{
+	  m->fs.cfa_reg = stack_pointer_rtx;
+	  m->fs.cfa_offset -= UNITS_PER_WORD;
+
+	  add_reg_note (insn, REG_CFA_DEF_CFA,
+			plus_constant (Pmode, stack_pointer_rtx,
+				       m->fs.cfa_offset));
+	  RTX_FRAME_RELATED_P (insn) = 1;
+	}
+    }
+}
+
+/* Emit code to restore saved registers using POP insns.  */
+
+static void
+ix86_emit_restore_regs_using_pop (void)
+{
+  unsigned int regno;
+
+  for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)
+    if (GENERAL_REGNO_P (regno) && ix86_save_reg (regno, false, true))
+      ix86_emit_restore_reg_using_pop (gen_rtx_REG (word_mode, regno));
+}
+
+/* Emit code and notes for the LEAVE instruction.  If insn is non-null,
+   omits the emit and only attaches the notes.  */
+
+static void
+ix86_emit_leave (rtx_insn *insn)
+{
+  struct machine_function *m = cfun->machine;
+
+  if (!insn)
+    insn = emit_insn (gen_leave (word_mode));
+
+  ix86_add_queued_cfa_restore_notes (insn);
+
+  gcc_assert (m->fs.fp_valid);
+  m->fs.sp_valid = true;
+  m->fs.sp_realigned = false;
+  m->fs.sp_offset = m->fs.fp_offset - UNITS_PER_WORD;
+  m->fs.fp_valid = false;
+
+  if (m->fs.cfa_reg == hard_frame_pointer_rtx)
+    {
+      m->fs.cfa_reg = stack_pointer_rtx;
+      m->fs.cfa_offset = m->fs.sp_offset;
+
+      add_reg_note (insn, REG_CFA_DEF_CFA,
+		    plus_constant (Pmode, stack_pointer_rtx,
+				   m->fs.sp_offset));
+      RTX_FRAME_RELATED_P (insn) = 1;
+    }
+  ix86_add_cfa_restore_note (insn, hard_frame_pointer_rtx,
+			     m->fs.fp_offset);
+}
+
+/* Emit code to restore saved registers using MOV insns.
+   First register is restored from CFA - CFA_OFFSET.  */
+static void
+ix86_emit_restore_regs_using_mov (HOST_WIDE_INT cfa_offset,
+				  bool maybe_eh_return)
+{
+  struct machine_function *m = cfun->machine;
+  unsigned int regno;
+
+  for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)
+    if (GENERAL_REGNO_P (regno) && ix86_save_reg (regno, maybe_eh_return, true))
+      {
+	rtx reg = gen_rtx_REG (word_mode, regno);
+	rtx mem;
+	rtx_insn *insn;
+
+	mem = choose_baseaddr (cfa_offset, NULL);
+	mem = gen_frame_mem (word_mode, mem);
+	insn = emit_move_insn (reg, mem);
+
+        if (m->fs.cfa_reg == crtl->drap_reg && regno == REGNO (crtl->drap_reg))
+	  {
+	    /* Previously we'd represented the CFA as an expression
+	       like *(%ebp - 8).  We've just popped that value from
+	       the stack, which means we need to reset the CFA to
+	       the drap register.  This will remain until we restore
+	       the stack pointer.  */
+	    add_reg_note (insn, REG_CFA_DEF_CFA, reg);
+	    RTX_FRAME_RELATED_P (insn) = 1;
+
+	    /* This means that the DRAP register is valid for addressing.  */
+	    m->fs.drap_valid = true;
+	  }
+	else
+	  ix86_add_cfa_restore_note (NULL, reg, cfa_offset);
+
+	cfa_offset -= UNITS_PER_WORD;
+      }
+}
+
+/* Emit code to restore saved registers using MOV insns.
+   First register is restored from CFA - CFA_OFFSET.  */
+static void
+ix86_emit_restore_sse_regs_using_mov (HOST_WIDE_INT cfa_offset,
+				      bool maybe_eh_return)
+{
+  unsigned int regno;
+
+  for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)
+    if (SSE_REGNO_P (regno) && ix86_save_reg (regno, maybe_eh_return, true))
+      {
+	rtx reg = gen_rtx_REG (V4SFmode, regno);
+	rtx mem;
+	unsigned int align = GET_MODE_ALIGNMENT (V4SFmode);
+
+	mem = choose_baseaddr (cfa_offset, &align);
+	mem = gen_rtx_MEM (V4SFmode, mem);
+
+	/* The location aligment depends upon the base register.  */
+	align = MIN (GET_MODE_ALIGNMENT (V4SFmode), align);
+	gcc_assert (! (cfa_offset & (align / BITS_PER_UNIT - 1)));
+	set_mem_align (mem, align);
+	emit_insn (gen_rtx_SET (reg, mem));
+
+	ix86_add_cfa_restore_note (NULL, reg, cfa_offset);
+
+	cfa_offset -= GET_MODE_SIZE (V4SFmode);
+      }
+}
+
+static void
+ix86_emit_outlined_ms2sysv_restore (const struct ix86_frame &frame,
+				  bool use_call, int style)
+{
+  struct machine_function *m = cfun->machine;
+  const unsigned ncregs = NUM_X86_64_MS_CLOBBERED_REGS
+			  + m->call_ms2sysv_extra_regs;
+  rtvec v;
+  unsigned int elems_needed, align, i, vi = 0;
+  rtx_insn *insn;
+  rtx sym, tmp;
+  rtx rsi = gen_rtx_REG (word_mode, SI_REG);
+  rtx r10 = NULL_RTX;
+  const class xlogue_layout &xlogue = xlogue_layout::get_instance ();
+  HOST_WIDE_INT stub_ptr_offset = xlogue.get_stub_ptr_offset ();
+  HOST_WIDE_INT rsi_offset = frame.stack_realign_offset + stub_ptr_offset;
+  rtx rsi_frame_load = NULL_RTX;
+  HOST_WIDE_INT rsi_restore_offset = (HOST_WIDE_INT)-1;
+  enum xlogue_stub stub;
+
+  gcc_assert (!m->fs.fp_valid || frame_pointer_needed);
+
+  /* If using a realigned stack, we should never start with padding.  */
+  gcc_assert (!stack_realign_fp || !xlogue.get_stack_align_off_in ());
+
+  /* Setup RSI as the stub's base pointer.  */
+  align = GET_MODE_ALIGNMENT (V4SFmode);
+  tmp = choose_baseaddr (rsi_offset, &align, SI_REG);
+  gcc_assert (align >= GET_MODE_ALIGNMENT (V4SFmode));
+
+  emit_insn (gen_rtx_SET (rsi, tmp));
+
+  /* Get a symbol for the stub.  */
+  if (frame_pointer_needed)
+    stub = use_call ? XLOGUE_STUB_RESTORE_HFP
+		    : XLOGUE_STUB_RESTORE_HFP_TAIL;
+  else
+    stub = use_call ? XLOGUE_STUB_RESTORE
+		    : XLOGUE_STUB_RESTORE_TAIL;
+  sym = xlogue.get_stub_rtx (stub);
+
+  elems_needed = ncregs;
+  if (use_call)
+    elems_needed += 1;
+  else
+    elems_needed += frame_pointer_needed ? 5 : 3;
+  v = rtvec_alloc (elems_needed);
+
+  /* We call the epilogue stub when we need to pop incoming args or we are
+     doing a sibling call as the tail.  Otherwise, we will emit a jmp to the
+     epilogue stub and it is the tail-call.  */
+  if (use_call)
+      RTVEC_ELT (v, vi++) = gen_rtx_USE (VOIDmode, sym);
+  else
+    {
+      RTVEC_ELT (v, vi++) = ret_rtx;
+      RTVEC_ELT (v, vi++) = gen_rtx_USE (VOIDmode, sym);
+      if (frame_pointer_needed)
+	{
+	  rtx rbp = gen_rtx_REG (DImode, BP_REG);
+	  gcc_assert (m->fs.fp_valid);
+	  gcc_assert (m->fs.cfa_reg == hard_frame_pointer_rtx);
+
+	  tmp = plus_constant (DImode, rbp, 8);
+	  RTVEC_ELT (v, vi++) = gen_rtx_SET (stack_pointer_rtx, tmp);
+	  RTVEC_ELT (v, vi++) = gen_rtx_SET (rbp, gen_rtx_MEM (DImode, rbp));
+	  tmp = gen_rtx_MEM (BLKmode, gen_rtx_SCRATCH (VOIDmode));
+	  RTVEC_ELT (v, vi++) = gen_rtx_CLOBBER (VOIDmode, tmp);
+	}
+      else
+	{
+	  /* If no hard frame pointer, we set R10 to the SP restore value.  */
+	  gcc_assert (!m->fs.fp_valid);
+	  gcc_assert (m->fs.cfa_reg == stack_pointer_rtx);
+	  gcc_assert (m->fs.sp_valid);
+
+	  r10 = gen_rtx_REG (DImode, R10_REG);
+	  tmp = plus_constant (Pmode, rsi, stub_ptr_offset);
+	  emit_insn (gen_rtx_SET (r10, tmp));
+
+	  RTVEC_ELT (v, vi++) = gen_rtx_SET (stack_pointer_rtx, r10);
+	}
+    }
+
+  /* Generate frame load insns and restore notes.  */
+  for (i = 0; i < ncregs; ++i)
+    {
+      const xlogue_layout::reginfo &r = xlogue.get_reginfo (i);
+      machine_mode mode = SSE_REGNO_P (r.regno) ? V4SFmode : word_mode;
+      rtx reg, frame_load;
+
+      reg = gen_rtx_REG (mode, r.regno);
+      frame_load = gen_frame_load (reg, rsi, r.offset);
+
+      /* Save RSI frame load insn & note to add last.  */
+      if (r.regno == SI_REG)
+	{
+	  gcc_assert (!rsi_frame_load);
+	  rsi_frame_load = frame_load;
+	  rsi_restore_offset = r.offset;
+	}
+      else
+	{
+	  RTVEC_ELT (v, vi++) = frame_load;
+	  ix86_add_cfa_restore_note (NULL, reg, r.offset);
+	}
+    }
+
+  /* Add RSI frame load & restore note at the end.  */
+  gcc_assert (rsi_frame_load);
+  gcc_assert (rsi_restore_offset != (HOST_WIDE_INT)-1);
+  RTVEC_ELT (v, vi++) = rsi_frame_load;
+  ix86_add_cfa_restore_note (NULL, gen_rtx_REG (DImode, SI_REG),
+			     rsi_restore_offset);
+
+  /* Finally, for tail-call w/o a hard frame pointer, set SP to R10.  */
+  if (!use_call && !frame_pointer_needed)
+    {
+      gcc_assert (m->fs.sp_valid);
+      gcc_assert (!m->fs.sp_realigned);
+
+      /* At this point, R10 should point to frame.stack_realign_offset.  */
+      if (m->fs.cfa_reg == stack_pointer_rtx)
+	m->fs.cfa_offset += m->fs.sp_offset - frame.stack_realign_offset;
+      m->fs.sp_offset = frame.stack_realign_offset;
+    }
+
+  gcc_assert (vi == (unsigned int)GET_NUM_ELEM (v));
+  tmp = gen_rtx_PARALLEL (VOIDmode, v);
+  if (use_call)
+      insn = emit_insn (tmp);
+  else
+    {
+      insn = emit_jump_insn (tmp);
+      JUMP_LABEL (insn) = ret_rtx;
+
+      if (frame_pointer_needed)
+	ix86_emit_leave (insn);
+      else
+	{
+	  /* Need CFA adjust note.  */
+	  tmp = gen_rtx_SET (stack_pointer_rtx, r10);
+	  add_reg_note (insn, REG_CFA_ADJUST_CFA, tmp);
+	}
+    }
+
+  RTX_FRAME_RELATED_P (insn) = true;
+  ix86_add_queued_cfa_restore_notes (insn);
+
+  /* If we're not doing a tail-call, we need to adjust the stack.  */
+  if (use_call && m->fs.sp_valid)
+    {
+      HOST_WIDE_INT dealloc = m->fs.sp_offset - frame.stack_realign_offset;
+      pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,
+				GEN_INT (dealloc), style,
+				m->fs.cfa_reg == stack_pointer_rtx);
+    }
+}
+
+/* Restore function stack, frame, and registers.  */
+
+void
+ix86_expand_epilogue (int style)
+{
+  struct machine_function *m = cfun->machine;
+  struct machine_frame_state frame_state_save = m->fs;
+  bool restore_regs_via_mov;
+  bool using_drap;
+  bool restore_stub_is_tail = false;
+
+  if (ix86_function_naked (current_function_decl))
+    {
+      /* The program should not reach this point.  */
+      emit_insn (gen_ud2 ());
+      return;
+    }
+
+  ix86_finalize_stack_frame_flags ();
+  const struct ix86_frame &frame = cfun->machine->frame;
+
+  m->fs.sp_realigned = stack_realign_fp;
+  m->fs.sp_valid = stack_realign_fp
+		   || !frame_pointer_needed
+		   || crtl->sp_is_unchanging;
+  gcc_assert (!m->fs.sp_valid
+	      || m->fs.sp_offset == frame.stack_pointer_offset);
+
+  /* The FP must be valid if the frame pointer is present.  */
+  gcc_assert (frame_pointer_needed == m->fs.fp_valid);
+  gcc_assert (!m->fs.fp_valid
+	      || m->fs.fp_offset == frame.hard_frame_pointer_offset);
+
+  /* We must have *some* valid pointer to the stack frame.  */
+  gcc_assert (m->fs.sp_valid || m->fs.fp_valid);
+
+  /* The DRAP is never valid at this point.  */
+  gcc_assert (!m->fs.drap_valid);
+
+  /* See the comment about red zone and frame
+     pointer usage in ix86_expand_prologue.  */
+  if (frame_pointer_needed && frame.red_zone_size)
+    emit_insn (gen_memory_blockage ());
+
+  using_drap = crtl->drap_reg && crtl->stack_realign_needed;
+  gcc_assert (!using_drap || m->fs.cfa_reg == crtl->drap_reg);
+
+  /* Determine the CFA offset of the end of the red-zone.  */
+  m->fs.red_zone_offset = 0;
+  if (ix86_using_red_zone () && crtl->args.pops_args < 65536)
+    {
+      /* The red-zone begins below return address and error code in
+	 exception handler.  */
+      m->fs.red_zone_offset = RED_ZONE_SIZE + INCOMING_FRAME_SP_OFFSET;
+
+      /* When the register save area is in the aligned portion of
+         the stack, determine the maximum runtime displacement that
+	 matches up with the aligned frame.  */
+      if (stack_realign_drap)
+	m->fs.red_zone_offset -= (crtl->stack_alignment_needed / BITS_PER_UNIT
+				  + UNITS_PER_WORD);
+    }
+
+  HOST_WIDE_INT reg_save_offset = frame.reg_save_offset;
+
+  /* Special care must be taken for the normal return case of a function
+     using eh_return: the eax and edx registers are marked as saved, but
+     not restored along this path.  Adjust the save location to match.  */
+  if (crtl->calls_eh_return && style != 2)
+    reg_save_offset -= 2 * UNITS_PER_WORD;
+
+  /* EH_RETURN requires the use of moves to function properly.  */
+  if (crtl->calls_eh_return)
+    restore_regs_via_mov = true;
+  /* SEH requires the use of pops to identify the epilogue.  */
+  else if (TARGET_SEH)
+    restore_regs_via_mov = false;
+  /* If we're only restoring one register and sp cannot be used then
+     using a move instruction to restore the register since it's
+     less work than reloading sp and popping the register.  */
+  else if (!sp_valid_at (frame.hfp_save_offset) && frame.nregs <= 1)
+    restore_regs_via_mov = true;
+  else if (TARGET_EPILOGUE_USING_MOVE
+	   && cfun->machine->use_fast_prologue_epilogue
+	   && (frame.nregs > 1
+	       || m->fs.sp_offset != reg_save_offset))
+    restore_regs_via_mov = true;
+  else if (frame_pointer_needed
+	   && !frame.nregs
+	   && m->fs.sp_offset != reg_save_offset)
+    restore_regs_via_mov = true;
+  else if (frame_pointer_needed
+	   && TARGET_USE_LEAVE
+	   && cfun->machine->use_fast_prologue_epilogue
+	   && frame.nregs == 1)
+    restore_regs_via_mov = true;
+  else
+    restore_regs_via_mov = false;
+
+  if (restore_regs_via_mov || frame.nsseregs)
+    {
+      /* Ensure that the entire register save area is addressable via
+	 the stack pointer, if we will restore SSE regs via sp.  */
+      if (TARGET_64BIT
+	  && m->fs.sp_offset > 0x7fffffff
+	  && sp_valid_at (frame.stack_realign_offset + 1)
+	  && (frame.nsseregs + frame.nregs) != 0)
+	{
+	  pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,
+				     GEN_INT (m->fs.sp_offset
+					      - frame.sse_reg_save_offset),
+				     style,
+				     m->fs.cfa_reg == stack_pointer_rtx);
+	}
+    }
+
+  /* If there are any SSE registers to restore, then we have to do it
+     via moves, since there's obviously no pop for SSE regs.  */
+  if (frame.nsseregs)
+    ix86_emit_restore_sse_regs_using_mov (frame.sse_reg_save_offset,
+					  style == 2);
+
+  if (m->call_ms2sysv)
+    {
+      int pop_incoming_args = crtl->args.pops_args && crtl->args.size;
+
+      /* We cannot use a tail-call for the stub if:
+	 1. We have to pop incoming args,
+	 2. We have additional int regs to restore, or
+	 3. A sibling call will be the tail-call, or
+	 4. We are emitting an eh_return_internal epilogue.
+
+	 TODO: Item 4 has not yet tested!
+
+	 If any of the above are true, we will call the stub rather than
+	 jump to it.  */
+      restore_stub_is_tail = !(pop_incoming_args || frame.nregs || style != 1);
+      ix86_emit_outlined_ms2sysv_restore (frame, !restore_stub_is_tail, style);
+    }
+
+  /* If using out-of-line stub that is a tail-call, then...*/
+  if (m->call_ms2sysv && restore_stub_is_tail)
+    {
+      /* TODO: parinoid tests. (remove eventually)  */
+      gcc_assert (m->fs.sp_valid);
+      gcc_assert (!m->fs.sp_realigned);
+      gcc_assert (!m->fs.fp_valid);
+      gcc_assert (!m->fs.realigned);
+      gcc_assert (m->fs.sp_offset == UNITS_PER_WORD);
+      gcc_assert (!crtl->drap_reg);
+      gcc_assert (!frame.nregs);
+    }
+  else if (restore_regs_via_mov)
+    {
+      rtx t;
+
+      if (frame.nregs)
+	ix86_emit_restore_regs_using_mov (reg_save_offset, style == 2);
+
+      /* eh_return epilogues need %ecx added to the stack pointer.  */
+      if (style == 2)
+	{
+	  rtx sa = EH_RETURN_STACKADJ_RTX;
+	  rtx_insn *insn;
+
+	  /* %ecx can't be used for both DRAP register and eh_return.  */
+	  if (crtl->drap_reg)
+	    gcc_assert (REGNO (crtl->drap_reg) != CX_REG);
+
+	  /* regparm nested functions don't work with eh_return.  */
+	  gcc_assert (!ix86_static_chain_on_stack);
+
+	  if (frame_pointer_needed)
+	    {
+	      t = gen_rtx_PLUS (Pmode, hard_frame_pointer_rtx, sa);
+	      t = plus_constant (Pmode, t, m->fs.fp_offset - UNITS_PER_WORD);
+	      emit_insn (gen_rtx_SET (sa, t));
+
+	      /* NB: eh_return epilogues must restore the frame pointer
+		 in word_mode since the upper 32 bits of RBP register
+		 can have any values.  */
+	      t = gen_frame_mem (word_mode, hard_frame_pointer_rtx);
+	      rtx frame_reg = gen_rtx_REG (word_mode,
+					   HARD_FRAME_POINTER_REGNUM);
+	      insn = emit_move_insn (frame_reg, t);
+
+	      /* Note that we use SA as a temporary CFA, as the return
+		 address is at the proper place relative to it.  We
+		 pretend this happens at the FP restore insn because
+		 prior to this insn the FP would be stored at the wrong
+		 offset relative to SA, and after this insn we have no
+		 other reasonable register to use for the CFA.  We don't
+		 bother resetting the CFA to the SP for the duration of
+		 the return insn, unless the control flow instrumentation
+		 is done.  In this case the SP is used later and we have
+		 to reset CFA to SP.  */
+	      add_reg_note (insn, REG_CFA_DEF_CFA,
+			    plus_constant (Pmode, sa, UNITS_PER_WORD));
+	      ix86_add_queued_cfa_restore_notes (insn);
+	      add_reg_note (insn, REG_CFA_RESTORE, frame_reg);
+	      RTX_FRAME_RELATED_P (insn) = 1;
+
+	      m->fs.cfa_reg = sa;
+	      m->fs.cfa_offset = UNITS_PER_WORD;
+	      m->fs.fp_valid = false;
+
+	      pro_epilogue_adjust_stack (stack_pointer_rtx, sa,
+					 const0_rtx, style,
+					 flag_cf_protection);
+	    }
+	  else
+	    {
+	      t = gen_rtx_PLUS (Pmode, stack_pointer_rtx, sa);
+	      t = plus_constant (Pmode, t, m->fs.sp_offset - UNITS_PER_WORD);
+	      insn = emit_insn (gen_rtx_SET (stack_pointer_rtx, t));
+	      ix86_add_queued_cfa_restore_notes (insn);
+
+	      gcc_assert (m->fs.cfa_reg == stack_pointer_rtx);
+	      if (m->fs.cfa_offset != UNITS_PER_WORD)
+		{
+		  m->fs.cfa_offset = UNITS_PER_WORD;
+		  add_reg_note (insn, REG_CFA_DEF_CFA,
+				plus_constant (Pmode, stack_pointer_rtx,
+					       UNITS_PER_WORD));
+		  RTX_FRAME_RELATED_P (insn) = 1;
+		}
+	    }
+	  m->fs.sp_offset = UNITS_PER_WORD;
+	  m->fs.sp_valid = true;
+	  m->fs.sp_realigned = false;
+	}
+    }
+  else
+    {
+      /* SEH requires that the function end with (1) a stack adjustment
+	 if necessary, (2) a sequence of pops, and (3) a return or
+	 jump instruction.  Prevent insns from the function body from
+	 being scheduled into this sequence.  */
+      if (TARGET_SEH)
+	{
+	  /* Prevent a catch region from being adjacent to the standard
+	     epilogue sequence.  Unfortunately neither crtl->uses_eh_lsda
+	     nor several other flags that would be interesting to test are
+	     set up yet.  */
+	  if (flag_non_call_exceptions)
+	    emit_insn (gen_nops (const1_rtx));
+	  else
+	    emit_insn (gen_blockage ());
+	}
+
+      /* First step is to deallocate the stack frame so that we can
+	 pop the registers.  If the stack pointer was realigned, it needs
+	 to be restored now.  Also do it on SEH target for very large
+	 frame as the emitted instructions aren't allowed by the ABI
+	 in epilogues.  */
+      if (!m->fs.sp_valid || m->fs.sp_realigned
+ 	  || (TARGET_SEH
+	      && (m->fs.sp_offset - reg_save_offset
+		  >= SEH_MAX_FRAME_SIZE)))
+	{
+	  pro_epilogue_adjust_stack (stack_pointer_rtx, hard_frame_pointer_rtx,
+				     GEN_INT (m->fs.fp_offset
+					      - reg_save_offset),
+				     style, false);
+	}
+      else if (m->fs.sp_offset != reg_save_offset)
+	{
+	  pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,
+				     GEN_INT (m->fs.sp_offset
+					      - reg_save_offset),
+				     style,
+				     m->fs.cfa_reg == stack_pointer_rtx);
+	}
+
+      ix86_emit_restore_regs_using_pop ();
+    }
+
+  /* If we used a stack pointer and haven't already got rid of it,
+     then do so now.  */
+  if (m->fs.fp_valid)
+    {
+      /* If the stack pointer is valid and pointing at the frame
+	 pointer store address, then we only need a pop.  */
+      if (sp_valid_at (frame.hfp_save_offset)
+	  && m->fs.sp_offset == frame.hfp_save_offset)
+	ix86_emit_restore_reg_using_pop (hard_frame_pointer_rtx);
+      /* Leave results in shorter dependency chains on CPUs that are
+	 able to grok it fast.  */
+      else if (TARGET_USE_LEAVE
+	       || optimize_bb_for_size_p (EXIT_BLOCK_PTR_FOR_FN (cfun))
+	       || !cfun->machine->use_fast_prologue_epilogue)
+	ix86_emit_leave (NULL);
+      else
+        {
+	  pro_epilogue_adjust_stack (stack_pointer_rtx,
+				     hard_frame_pointer_rtx,
+				     const0_rtx, style, !using_drap);
+	  ix86_emit_restore_reg_using_pop (hard_frame_pointer_rtx);
+        }
+    }
+
+  if (using_drap)
+    {
+      int param_ptr_offset = UNITS_PER_WORD;
+      rtx_insn *insn;
+
+      gcc_assert (stack_realign_drap);
+
+      if (ix86_static_chain_on_stack)
+	param_ptr_offset += UNITS_PER_WORD;
+      if (!call_used_or_fixed_reg_p (REGNO (crtl->drap_reg)))
+	param_ptr_offset += UNITS_PER_WORD;
+
+      insn = emit_insn (gen_rtx_SET
+			(stack_pointer_rtx,
+			 plus_constant (Pmode, crtl->drap_reg,
+					-param_ptr_offset)));
+      m->fs.cfa_reg = stack_pointer_rtx;
+      m->fs.cfa_offset = param_ptr_offset;
+      m->fs.sp_offset = param_ptr_offset;
+      m->fs.realigned = false;
+
+      add_reg_note (insn, REG_CFA_DEF_CFA,
+		    plus_constant (Pmode, stack_pointer_rtx,
+				   param_ptr_offset));
+      RTX_FRAME_RELATED_P (insn) = 1;
+
+      if (!call_used_or_fixed_reg_p (REGNO (crtl->drap_reg)))
+	ix86_emit_restore_reg_using_pop (crtl->drap_reg);
+    }
+
+  /* At this point the stack pointer must be valid, and we must have
+     restored all of the registers.  We may not have deallocated the
+     entire stack frame.  We've delayed this until now because it may
+     be possible to merge the local stack deallocation with the
+     deallocation forced by ix86_static_chain_on_stack.   */
+  gcc_assert (m->fs.sp_valid);
+  gcc_assert (!m->fs.sp_realigned);
+  gcc_assert (!m->fs.fp_valid);
+  gcc_assert (!m->fs.realigned);
+  if (m->fs.sp_offset != UNITS_PER_WORD)
+    {
+      pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,
+				 GEN_INT (m->fs.sp_offset - UNITS_PER_WORD),
+				 style, true);
+    }
+  else
+    ix86_add_queued_cfa_restore_notes (get_last_insn ());
+
+  /* Sibcall epilogues don't want a return instruction.  */
+  if (style == 0)
+    {
+      m->fs = frame_state_save;
+      return;
+    }
+
+  if (cfun->machine->func_type != TYPE_NORMAL)
+    emit_jump_insn (gen_interrupt_return ());
+  else if (crtl->args.pops_args && crtl->args.size)
+    {
+      rtx popc = GEN_INT (crtl->args.pops_args);
+
+      /* i386 can only pop 64K bytes.  If asked to pop more, pop return
+	 address, do explicit add, and jump indirectly to the caller.  */
+
+      if (crtl->args.pops_args >= 65536)
+	{
+	  rtx ecx = gen_rtx_REG (SImode, CX_REG);
+	  rtx_insn *insn;
+
+	  /* There is no "pascal" calling convention in any 64bit ABI.  */
+	  gcc_assert (!TARGET_64BIT);
+
+	  insn = emit_insn (gen_pop (ecx));
+	  m->fs.cfa_offset -= UNITS_PER_WORD;
+	  m->fs.sp_offset -= UNITS_PER_WORD;
+
+	  rtx x = plus_constant (Pmode, stack_pointer_rtx, UNITS_PER_WORD);
+	  x = gen_rtx_SET (stack_pointer_rtx, x);
+	  add_reg_note (insn, REG_CFA_ADJUST_CFA, x);
+	  add_reg_note (insn, REG_CFA_REGISTER, gen_rtx_SET (ecx, pc_rtx));
+	  RTX_FRAME_RELATED_P (insn) = 1;
+
+	  pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,
+				     popc, -1, true);
+	  emit_jump_insn (gen_simple_return_indirect_internal (ecx));
+	}
+      else
+	emit_jump_insn (gen_simple_return_pop_internal (popc));
+    }
+  else if (!m->call_ms2sysv || !restore_stub_is_tail)
+    {
+      /* In case of return from EH a simple return cannot be used
+	 as a return address will be compared with a shadow stack
+	 return address.  Use indirect jump instead.  */
+      if (style == 2 && flag_cf_protection)
+	{
+	  /* Register used in indirect jump must be in word_mode.  But
+	     Pmode may not be the same as word_mode for x32.  */
+	  rtx ecx = gen_rtx_REG (word_mode, CX_REG);
+	  rtx_insn *insn;
+
+	  insn = emit_insn (gen_pop (ecx));
+	  m->fs.cfa_offset -= UNITS_PER_WORD;
+	  m->fs.sp_offset -= UNITS_PER_WORD;
+
+	  rtx x = plus_constant (Pmode, stack_pointer_rtx, UNITS_PER_WORD);
+	  x = gen_rtx_SET (stack_pointer_rtx, x);
+	  add_reg_note (insn, REG_CFA_ADJUST_CFA, x);
+	  add_reg_note (insn, REG_CFA_REGISTER, gen_rtx_SET (ecx, pc_rtx));
+	  RTX_FRAME_RELATED_P (insn) = 1;
+
+	  emit_jump_insn (gen_simple_return_indirect_internal (ecx));
+	}
+      else
+	emit_jump_insn (gen_simple_return_internal ());
+    }
+
+  /* Restore the state back to the state from the prologue,
+     so that it's correct for the next epilogue.  */
+  m->fs = frame_state_save;
+}
+
+/* Reset from the function's potential modifications.  */
+
+static void
+ix86_output_function_epilogue (FILE *file ATTRIBUTE_UNUSED)
+{
+  if (pic_offset_table_rtx
+      && !ix86_use_pseudo_pic_reg ())
+    SET_REGNO (pic_offset_table_rtx, REAL_PIC_OFFSET_TABLE_REGNUM);
+
+  if (TARGET_MACHO)
+    {
+      rtx_insn *insn = get_last_insn ();
+      rtx_insn *deleted_debug_label = NULL;
+
+      /* Mach-O doesn't support labels at the end of objects, so if
+         it looks like we might want one, take special action.
+        First, collect any sequence of deleted debug labels.  */
+      while (insn
+	     && NOTE_P (insn)
+	     && NOTE_KIND (insn) != NOTE_INSN_DELETED_LABEL)
+	{
+	  /* Don't insert a nop for NOTE_INSN_DELETED_DEBUG_LABEL
+	     notes only, instead set their CODE_LABEL_NUMBER to -1,
+	     otherwise there would be code generation differences
+	     in between -g and -g0.  */
+	  if (NOTE_P (insn) && NOTE_KIND (insn)
+	      == NOTE_INSN_DELETED_DEBUG_LABEL)
+	    deleted_debug_label = insn;
+	  insn = PREV_INSN (insn);
+	}
+
+      /* If we have:
+	 label:
+	    barrier
+	  then this needs to be detected, so skip past the barrier.  */
+
+      if (insn && BARRIER_P (insn))
+	insn = PREV_INSN (insn);
+
+      /* Up to now we've only seen notes or barriers.  */
+      if (insn)
+	{
+	  if (LABEL_P (insn)
+	      || (NOTE_P (insn)
+		  && NOTE_KIND (insn) == NOTE_INSN_DELETED_LABEL))
+	    /* Trailing label.  */
+	    fputs ("\tnop\n", file);
+	  else if (cfun && ! cfun->is_thunk)
+	    {
+	      /* See if we have a completely empty function body, skipping
+	         the special case of the picbase thunk emitted as asm.  */
+	      while (insn && ! INSN_P (insn))
+		insn = PREV_INSN (insn);
+	      /* If we don't find any insns, we've got an empty function body;
+		 I.e. completely empty - without a return or branch.  This is
+		 taken as the case where a function body has been removed
+		 because it contains an inline __builtin_unreachable().  GCC
+		 declares that reaching __builtin_unreachable() means UB so
+		 we're not obliged to do anything special; however, we want
+		 non-zero-sized function bodies.  To meet this, and help the
+		 user out, let's trap the case.  */
+	      if (insn == NULL)
+		fputs ("\tud2\n", file);
+	    }
+	}
+      else if (deleted_debug_label)
+	for (insn = deleted_debug_label; insn; insn = NEXT_INSN (insn))
+	  if (NOTE_KIND (insn) == NOTE_INSN_DELETED_DEBUG_LABEL)
+	    CODE_LABEL_NUMBER (insn) = -1;
+    }
+}
+
+/* Implement TARGET_ASM_PRINT_PATCHABLE_FUNCTION_ENTRY.  */
+
+void
+ix86_print_patchable_function_entry (FILE *file,
+				     unsigned HOST_WIDE_INT patch_area_size,
+				     bool record_p)
+{
+  if (cfun->machine->function_label_emitted)
+    {
+      /* NB: When ix86_print_patchable_function_entry is called after
+	 function table has been emitted, we have inserted or queued
+	 a pseudo UNSPECV_PATCHABLE_AREA instruction at the proper
+	 place.  There is nothing to do here.  */
+      return;
+    }
+
+  default_print_patchable_function_entry (file, patch_area_size,
+					  record_p);
+}
+
+/* Output patchable area.  NB: default_print_patchable_function_entry
+   isn't available in i386.md.  */
+
+void
+ix86_output_patchable_area (unsigned int patch_area_size,
+			    bool record_p)
+{
+  default_print_patchable_function_entry (asm_out_file,
+					  patch_area_size,
+					  record_p);
+}
+
+/* Return a scratch register to use in the split stack prologue.  The
+   split stack prologue is used for -fsplit-stack.  It is the first
+   instructions in the function, even before the regular prologue.
+   The scratch register can be any caller-saved register which is not
+   used for parameters or for the static chain.  */
+
+static unsigned int
+split_stack_prologue_scratch_regno (void)
+{
+  if (TARGET_64BIT)
+    return R11_REG;
+  else
+    {
+      bool is_fastcall, is_thiscall;
+      int regparm;
+
+      is_fastcall = (lookup_attribute ("fastcall",
+				       TYPE_ATTRIBUTES (TREE_TYPE (cfun->decl)))
+		     != NULL);
+      is_thiscall = (lookup_attribute ("thiscall",
+				       TYPE_ATTRIBUTES (TREE_TYPE (cfun->decl)))
+		     != NULL);
+      regparm = ix86_function_regparm (TREE_TYPE (cfun->decl), cfun->decl);
+
+      if (is_fastcall)
+	{
+	  if (DECL_STATIC_CHAIN (cfun->decl))
+	    {
+	      sorry ("%<-fsplit-stack%> does not support fastcall with "
+		     "nested function");
+	      return INVALID_REGNUM;
+	    }
+	  return AX_REG;
+	}
+      else if (is_thiscall)
+        {
+	  if (!DECL_STATIC_CHAIN (cfun->decl))
+	    return DX_REG;
+	  return AX_REG;
+	}
+      else if (regparm < 3)
+	{
+	  if (!DECL_STATIC_CHAIN (cfun->decl))
+	    return CX_REG;
+	  else
+	    {
+	      if (regparm >= 2)
+		{
+		  sorry ("%<-fsplit-stack%> does not support 2 register "
+			 "parameters for a nested function");
+		  return INVALID_REGNUM;
+		}
+	      return DX_REG;
+	    }
+	}
+      else
+	{
+	  /* FIXME: We could make this work by pushing a register
+	     around the addition and comparison.  */
+	  sorry ("%<-fsplit-stack%> does not support 3 register parameters");
+	  return INVALID_REGNUM;
+	}
+    }
+}
+
+/* A SYMBOL_REF for the function which allocates new stackspace for
+   -fsplit-stack.  */
+
+static GTY(()) rtx split_stack_fn;
+
+/* A SYMBOL_REF for the more stack function when using the large
+   model.  */
+
+static GTY(()) rtx split_stack_fn_large;
+
+/* Return location of the stack guard value in the TLS block.  */
+
+rtx
+ix86_split_stack_guard (void)
+{
+  int offset;
+  addr_space_t as = DEFAULT_TLS_SEG_REG;
+  rtx r;
+
+  gcc_assert (flag_split_stack);
+
+#ifdef TARGET_THREAD_SPLIT_STACK_OFFSET
+  offset = TARGET_THREAD_SPLIT_STACK_OFFSET;
+#else
+  gcc_unreachable ();
+#endif
+
+  r = GEN_INT (offset);
+  r = gen_const_mem (Pmode, r);
+  set_mem_addr_space (r, as);
+
+  return r;
+}
+
+/* Handle -fsplit-stack.  These are the first instructions in the
+   function, even before the regular prologue.  */
+
+void
+ix86_expand_split_stack_prologue (void)
+{
+  HOST_WIDE_INT allocate;
+  unsigned HOST_WIDE_INT args_size;
+  rtx_code_label *label;
+  rtx limit, current, allocate_rtx, call_fusage;
+  rtx_insn *call_insn;
+  rtx scratch_reg = NULL_RTX;
+  rtx_code_label *varargs_label = NULL;
+  rtx fn;
+
+  gcc_assert (flag_split_stack && reload_completed);
+
+  ix86_finalize_stack_frame_flags ();
+  struct ix86_frame &frame = cfun->machine->frame;
+  allocate = frame.stack_pointer_offset - INCOMING_FRAME_SP_OFFSET;
+
+  /* This is the label we will branch to if we have enough stack
+     space.  We expect the basic block reordering pass to reverse this
+     branch if optimizing, so that we branch in the unlikely case.  */
+  label = gen_label_rtx ();
+
+  /* We need to compare the stack pointer minus the frame size with
+     the stack boundary in the TCB.  The stack boundary always gives
+     us SPLIT_STACK_AVAILABLE bytes, so if we need less than that we
+     can compare directly.  Otherwise we need to do an addition.  */
+
+  limit = ix86_split_stack_guard ();
+
+  if (allocate < SPLIT_STACK_AVAILABLE)
+    current = stack_pointer_rtx;
+  else
+    {
+      unsigned int scratch_regno;
+      rtx offset;
+
+      /* We need a scratch register to hold the stack pointer minus
+	 the required frame size.  Since this is the very start of the
+	 function, the scratch register can be any caller-saved
+	 register which is not used for parameters.  */
+      offset = GEN_INT (- allocate);
+      scratch_regno = split_stack_prologue_scratch_regno ();
+      if (scratch_regno == INVALID_REGNUM)
+	return;
+      scratch_reg = gen_rtx_REG (Pmode, scratch_regno);
+      if (!TARGET_64BIT || x86_64_immediate_operand (offset, Pmode))
+	{
+	  /* We don't use gen_add in this case because it will
+	     want to split to lea, but when not optimizing the insn
+	     will not be split after this point.  */
+	  emit_insn (gen_rtx_SET (scratch_reg,
+				  gen_rtx_PLUS (Pmode, stack_pointer_rtx,
+						offset)));
+	}
+      else
+	{
+	  emit_move_insn (scratch_reg, offset);
+	  emit_insn (gen_add2_insn (scratch_reg, stack_pointer_rtx));
+	}
+      current = scratch_reg;
+    }
+
+  ix86_expand_branch (GEU, current, limit, label);
+  rtx_insn *jump_insn = get_last_insn ();
+  JUMP_LABEL (jump_insn) = label;
+
+  /* Mark the jump as very likely to be taken.  */
+  add_reg_br_prob_note (jump_insn, profile_probability::very_likely ());
+
+  if (split_stack_fn == NULL_RTX)
+    {
+      split_stack_fn = gen_rtx_SYMBOL_REF (Pmode, "__morestack");
+      SYMBOL_REF_FLAGS (split_stack_fn) |= SYMBOL_FLAG_LOCAL;
+    }
+  fn = split_stack_fn;
+
+  /* Get more stack space.  We pass in the desired stack space and the
+     size of the arguments to copy to the new stack.  In 32-bit mode
+     we push the parameters; __morestack will return on a new stack
+     anyhow.  In 64-bit mode we pass the parameters in r10 and
+     r11.  */
+  allocate_rtx = GEN_INT (allocate);
+  args_size = crtl->args.size >= 0 ? (HOST_WIDE_INT) crtl->args.size : 0;
+  call_fusage = NULL_RTX;
+  rtx pop = NULL_RTX;
+  if (TARGET_64BIT)
+    {
+      rtx reg10, reg11;
+
+      reg10 = gen_rtx_REG (Pmode, R10_REG);
+      reg11 = gen_rtx_REG (Pmode, R11_REG);
+
+      /* If this function uses a static chain, it will be in %r10.
+	 Preserve it across the call to __morestack.  */
+      if (DECL_STATIC_CHAIN (cfun->decl))
+	{
+	  rtx rax;
+
+	  rax = gen_rtx_REG (word_mode, AX_REG);
+	  emit_move_insn (rax, gen_rtx_REG (word_mode, R10_REG));
+	  use_reg (&call_fusage, rax);
+	}
+
+      if ((ix86_cmodel == CM_LARGE || ix86_cmodel == CM_LARGE_PIC)
+          && !TARGET_PECOFF)
+	{
+	  HOST_WIDE_INT argval;
+
+	  gcc_assert (Pmode == DImode);
+	  /* When using the large model we need to load the address
+	     into a register, and we've run out of registers.  So we
+	     switch to a different calling convention, and we call a
+	     different function: __morestack_large.  We pass the
+	     argument size in the upper 32 bits of r10 and pass the
+	     frame size in the lower 32 bits.  */
+	  gcc_assert ((allocate & HOST_WIDE_INT_C (0xffffffff)) == allocate);
+	  gcc_assert ((args_size & 0xffffffff) == args_size);
+
+	  if (split_stack_fn_large == NULL_RTX)
+	    {
+	      split_stack_fn_large
+		= gen_rtx_SYMBOL_REF (Pmode, "__morestack_large_model");
+	      SYMBOL_REF_FLAGS (split_stack_fn_large) |= SYMBOL_FLAG_LOCAL;
+	    }
+	  if (ix86_cmodel == CM_LARGE_PIC)
+	    {
+	      rtx_code_label *label;
+	      rtx x;
+
+	      label = gen_label_rtx ();
+	      emit_label (label);
+	      LABEL_PRESERVE_P (label) = 1;
+	      emit_insn (gen_set_rip_rex64 (reg10, label));
+	      emit_insn (gen_set_got_offset_rex64 (reg11, label));
+	      emit_insn (gen_add2_insn (reg10, reg11));
+	      x = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, split_stack_fn_large),
+				  UNSPEC_GOT);
+	      x = gen_rtx_CONST (Pmode, x);
+	      emit_move_insn (reg11, x);
+	      x = gen_rtx_PLUS (Pmode, reg10, reg11);
+	      x = gen_const_mem (Pmode, x);
+	      emit_move_insn (reg11, x);
+	    }
+	  else
+	    emit_move_insn (reg11, split_stack_fn_large);
+
+	  fn = reg11;
+
+	  argval = ((args_size << 16) << 16) + allocate;
+	  emit_move_insn (reg10, GEN_INT (argval));
+	}
+      else
+	{
+	  emit_move_insn (reg10, allocate_rtx);
+	  emit_move_insn (reg11, GEN_INT (args_size));
+	  use_reg (&call_fusage, reg11);
+	}
+
+      use_reg (&call_fusage, reg10);
+    }
+  else
+    {
+      rtx_insn *insn = emit_insn (gen_push (GEN_INT (args_size)));
+      add_reg_note (insn, REG_ARGS_SIZE, GEN_INT (UNITS_PER_WORD));
+      insn = emit_insn (gen_push (allocate_rtx));
+      add_reg_note (insn, REG_ARGS_SIZE, GEN_INT (2 * UNITS_PER_WORD));
+      pop = GEN_INT (2 * UNITS_PER_WORD);
+    }
+  call_insn = ix86_expand_call (NULL_RTX, gen_rtx_MEM (QImode, fn),
+				GEN_INT (UNITS_PER_WORD), constm1_rtx,
+				pop, false);
+  add_function_usage_to (call_insn, call_fusage);
+  if (!TARGET_64BIT)
+    add_reg_note (call_insn, REG_ARGS_SIZE, GEN_INT (0));
+  /* Indicate that this function can't jump to non-local gotos.  */
+  make_reg_eh_region_note_nothrow_nononlocal (call_insn);
+
+  /* In order to make call/return prediction work right, we now need
+     to execute a return instruction.  See
+     libgcc/config/i386/morestack.S for the details on how this works.
+
+     For flow purposes gcc must not see this as a return
+     instruction--we need control flow to continue at the subsequent
+     label.  Therefore, we use an unspec.  */
+  gcc_assert (crtl->args.pops_args < 65536);
+  rtx_insn *ret_insn
+    = emit_insn (gen_split_stack_return (GEN_INT (crtl->args.pops_args)));
+
+  if ((flag_cf_protection & CF_BRANCH))
+    {
+      /* Insert ENDBR since __morestack will jump back here via indirect
+	 call.  */
+      rtx cet_eb = gen_nop_endbr ();
+      emit_insn_after (cet_eb, ret_insn);
+    }
+
+  /* If we are in 64-bit mode and this function uses a static chain,
+     we saved %r10 in %rax before calling _morestack.  */
+  if (TARGET_64BIT && DECL_STATIC_CHAIN (cfun->decl))
+    emit_move_insn (gen_rtx_REG (word_mode, R10_REG),
+		    gen_rtx_REG (word_mode, AX_REG));
+
+  /* If this function calls va_start, we need to store a pointer to
+     the arguments on the old stack, because they may not have been
+     all copied to the new stack.  At this point the old stack can be
+     found at the frame pointer value used by __morestack, because
+     __morestack has set that up before calling back to us.  Here we
+     store that pointer in a scratch register, and in
+     ix86_expand_prologue we store the scratch register in a stack
+     slot.  */
+  if (cfun->machine->split_stack_varargs_pointer != NULL_RTX)
+    {
+      unsigned int scratch_regno;
+      rtx frame_reg;
+      int words;
+
+      scratch_regno = split_stack_prologue_scratch_regno ();
+      scratch_reg = gen_rtx_REG (Pmode, scratch_regno);
+      frame_reg = gen_rtx_REG (Pmode, BP_REG);
+
+      /* 64-bit:
+	 fp -> old fp value
+	       return address within this function
+	       return address of caller of this function
+	       stack arguments
+	 So we add three words to get to the stack arguments.
+
+	 32-bit:
+	 fp -> old fp value
+	       return address within this function
+               first argument to __morestack
+               second argument to __morestack
+               return address of caller of this function
+               stack arguments
+         So we add five words to get to the stack arguments.
+      */
+      words = TARGET_64BIT ? 3 : 5;
+      emit_insn (gen_rtx_SET (scratch_reg,
+			      plus_constant (Pmode, frame_reg,
+					     words * UNITS_PER_WORD)));
+
+      varargs_label = gen_label_rtx ();
+      emit_jump_insn (gen_jump (varargs_label));
+      JUMP_LABEL (get_last_insn ()) = varargs_label;
+
+      emit_barrier ();
+    }
+
+  emit_label (label);
+  LABEL_NUSES (label) = 1;
+
+  /* If this function calls va_start, we now have to set the scratch
+     register for the case where we do not call __morestack.  In this
+     case we need to set it based on the stack pointer.  */
+  if (cfun->machine->split_stack_varargs_pointer != NULL_RTX)
+    {
+      emit_insn (gen_rtx_SET (scratch_reg,
+			      plus_constant (Pmode, stack_pointer_rtx,
+					     UNITS_PER_WORD)));
+
+      emit_label (varargs_label);
+      LABEL_NUSES (varargs_label) = 1;
+    }
+}
+
+/* We may have to tell the dataflow pass that the split stack prologue
+   is initializing a scratch register.  */
+
+static void
+ix86_live_on_entry (bitmap regs)
+{
+  if (cfun->machine->split_stack_varargs_pointer != NULL_RTX)
+    {
+      gcc_assert (flag_split_stack);
+      bitmap_set_bit (regs, split_stack_prologue_scratch_regno ());
+    }
+}
+
+/* Extract the parts of an RTL expression that is a valid memory address
+   for an instruction.  Return 0 if the structure of the address is
+   grossly off.  Return -1 if the address contains ASHIFT, so it is not
+   strictly valid, but still used for computing length of lea instruction.  */
+
+int
+ix86_decompose_address (rtx addr, struct ix86_address *out)
+{
+  rtx base = NULL_RTX, index = NULL_RTX, disp = NULL_RTX;
+  rtx base_reg, index_reg;
+  HOST_WIDE_INT scale = 1;
+  rtx scale_rtx = NULL_RTX;
+  rtx tmp;
+  int retval = 1;
+  addr_space_t seg = ADDR_SPACE_GENERIC;
+
+  /* Allow zero-extended SImode addresses,
+     they will be emitted with addr32 prefix.  */
+  if (TARGET_64BIT && GET_MODE (addr) == DImode)
+    {
+      if (GET_CODE (addr) == ZERO_EXTEND
+	  && GET_MODE (XEXP (addr, 0)) == SImode)
+	{
+	  addr = XEXP (addr, 0);
+	  if (CONST_INT_P (addr))
+	    return 0;
+	}	      
+      else if (GET_CODE (addr) == AND
+	       && const_32bit_mask (XEXP (addr, 1), DImode))
+	{
+	  addr = lowpart_subreg (SImode, XEXP (addr, 0), DImode);
+	  if (addr == NULL_RTX)
+	    return 0;
+
+	  if (CONST_INT_P (addr))
+	    return 0;
+	}
+    }
+
+  /* Allow SImode subregs of DImode addresses,
+     they will be emitted with addr32 prefix.  */
+  if (TARGET_64BIT && GET_MODE (addr) == SImode)
+    {
+      if (SUBREG_P (addr)
+	  && GET_MODE (SUBREG_REG (addr)) == DImode)
+	{
+	  addr = SUBREG_REG (addr);
+	  if (CONST_INT_P (addr))
+	    return 0;
+	}
+    }
+
+  if (REG_P (addr))
+    base = addr;
+  else if (SUBREG_P (addr))
+    {
+      if (REG_P (SUBREG_REG (addr)))
+	base = addr;
+      else
+	return 0;
+    }
+  else if (GET_CODE (addr) == PLUS)
+    {
+      rtx addends[4], op;
+      int n = 0, i;
+
+      op = addr;
+      do
+	{
+	  if (n >= 4)
+	    return 0;
+	  addends[n++] = XEXP (op, 1);
+	  op = XEXP (op, 0);
+	}
+      while (GET_CODE (op) == PLUS);
+      if (n >= 4)
+	return 0;
+      addends[n] = op;
+
+      for (i = n; i >= 0; --i)
+	{
+	  op = addends[i];
+	  switch (GET_CODE (op))
+	    {
+	    case MULT:
+	      if (index)
+		return 0;
+	      index = XEXP (op, 0);
+	      scale_rtx = XEXP (op, 1);
+	      break;
+
+	    case ASHIFT:
+	      if (index)
+		return 0;
+	      index = XEXP (op, 0);
+	      tmp = XEXP (op, 1);
+	      if (!CONST_INT_P (tmp))
+		return 0;
+	      scale = INTVAL (tmp);
+	      if ((unsigned HOST_WIDE_INT) scale > 3)
+		return 0;
+	      scale = 1 << scale;
+	      break;
+
+	    case ZERO_EXTEND:
+	      op = XEXP (op, 0);
+	      if (GET_CODE (op) != UNSPEC)
+		return 0;
+	      /* FALLTHRU */
+
+	    case UNSPEC:
+	      if (XINT (op, 1) == UNSPEC_TP
+	          && TARGET_TLS_DIRECT_SEG_REFS
+	          && seg == ADDR_SPACE_GENERIC)
+		seg = DEFAULT_TLS_SEG_REG;
+	      else
+		return 0;
+	      break;
+
+	    case SUBREG:
+	      if (!REG_P (SUBREG_REG (op)))
+		return 0;
+	      /* FALLTHRU */
+
+	    case REG:
+	      if (!base)
+		base = op;
+	      else if (!index)
+		index = op;
+	      else
+		return 0;
+	      break;
+
+	    case CONST:
+	    case CONST_INT:
+	    case SYMBOL_REF:
+	    case LABEL_REF:
+	      if (disp)
+		return 0;
+	      disp = op;
+	      break;
+
+	    default:
+	      return 0;
+	    }
+	}
+    }
+  else if (GET_CODE (addr) == MULT)
+    {
+      index = XEXP (addr, 0);		/* index*scale */
+      scale_rtx = XEXP (addr, 1);
+    }
+  else if (GET_CODE (addr) == ASHIFT)
+    {
+      /* We're called for lea too, which implements ashift on occasion.  */
+      index = XEXP (addr, 0);
+      tmp = XEXP (addr, 1);
+      if (!CONST_INT_P (tmp))
+	return 0;
+      scale = INTVAL (tmp);
+      if ((unsigned HOST_WIDE_INT) scale > 3)
+	return 0;
+      scale = 1 << scale;
+      retval = -1;
+    }
+  else
+    disp = addr;			/* displacement */
+
+  if (index)
+    {
+      if (REG_P (index))
+	;
+      else if (SUBREG_P (index)
+	       && REG_P (SUBREG_REG (index)))
+	;
+      else
+	return 0;
+    }
+
+  /* Extract the integral value of scale.  */
+  if (scale_rtx)
+    {
+      if (!CONST_INT_P (scale_rtx))
+	return 0;
+      scale = INTVAL (scale_rtx);
+    }
+
+  base_reg = base && SUBREG_P (base) ? SUBREG_REG (base) : base;
+  index_reg = index && SUBREG_P (index) ? SUBREG_REG (index) : index;
+
+  /* Avoid useless 0 displacement.  */
+  if (disp == const0_rtx && (base || index))
+    disp = NULL_RTX;
+
+  /* Allow arg pointer and stack pointer as index if there is not scaling.  */
+  if (base_reg && index_reg && scale == 1
+      && (REGNO (index_reg) == ARG_POINTER_REGNUM
+	  || REGNO (index_reg) == FRAME_POINTER_REGNUM
+	  || REGNO (index_reg) == SP_REG))
+    {
+      std::swap (base, index);
+      std::swap (base_reg, index_reg);
+    }
+
+  /* Special case: %ebp cannot be encoded as a base without a displacement.
+     Similarly %r13.  */
+  if (!disp && base_reg
+      && (REGNO (base_reg) == ARG_POINTER_REGNUM
+	  || REGNO (base_reg) == FRAME_POINTER_REGNUM
+	  || REGNO (base_reg) == BP_REG
+	  || REGNO (base_reg) == R13_REG))
+    disp = const0_rtx;
+
+  /* Special case: on K6, [%esi] makes the instruction vector decoded.
+     Avoid this by transforming to [%esi+0].
+     Reload calls address legitimization without cfun defined, so we need
+     to test cfun for being non-NULL. */
+  if (TARGET_K6 && cfun && optimize_function_for_speed_p (cfun)
+      && base_reg && !index_reg && !disp
+      && REGNO (base_reg) == SI_REG)
+    disp = const0_rtx;
+
+  /* Special case: encode reg+reg instead of reg*2.  */
+  if (!base && index && scale == 2)
+    base = index, base_reg = index_reg, scale = 1;
+
+  /* Special case: scaling cannot be encoded without base or displacement.  */
+  if (!base && !disp && index && scale != 1)
+    disp = const0_rtx;
+
+  out->base = base;
+  out->index = index;
+  out->disp = disp;
+  out->scale = scale;
+  out->seg = seg;
+
+  return retval;
+}
+
+/* Return cost of the memory address x.
+   For i386, it is better to use a complex address than let gcc copy
+   the address into a reg and make a new pseudo.  But not if the address
+   requires to two regs - that would mean more pseudos with longer
+   lifetimes.  */
+static int
+ix86_address_cost (rtx x, machine_mode, addr_space_t, bool)
+{
+  struct ix86_address parts;
+  int cost = 1;
+  int ok = ix86_decompose_address (x, &parts);
+
+  gcc_assert (ok);
+
+  if (parts.base && SUBREG_P (parts.base))
+    parts.base = SUBREG_REG (parts.base);
+  if (parts.index && SUBREG_P (parts.index))
+    parts.index = SUBREG_REG (parts.index);
+
+  /* Attempt to minimize number of registers in the address by increasing
+     address cost for each used register.  We don't increase address cost
+     for "pic_offset_table_rtx".  When a memopt with "pic_offset_table_rtx"
+     is not invariant itself it most likely means that base or index is not
+     invariant.  Therefore only "pic_offset_table_rtx" could be hoisted out,
+     which is not profitable for x86.  */
+  if (parts.base
+      && (!REG_P (parts.base) || REGNO (parts.base) >= FIRST_PSEUDO_REGISTER)
+      && (current_pass->type == GIMPLE_PASS
+	  || !pic_offset_table_rtx
+	  || !REG_P (parts.base)
+	  || REGNO (pic_offset_table_rtx) != REGNO (parts.base)))
+    cost++;
+
+  if (parts.index
+      && (!REG_P (parts.index) || REGNO (parts.index) >= FIRST_PSEUDO_REGISTER)
+      && (current_pass->type == GIMPLE_PASS
+	  || !pic_offset_table_rtx
+	  || !REG_P (parts.index)
+	  || REGNO (pic_offset_table_rtx) != REGNO (parts.index)))
+    cost++;
+
+  /* AMD-K6 don't like addresses with ModR/M set to 00_xxx_100b,
+     since it's predecode logic can't detect the length of instructions
+     and it degenerates to vector decoded.  Increase cost of such
+     addresses here.  The penalty is minimally 2 cycles.  It may be worthwhile
+     to split such addresses or even refuse such addresses at all.
+
+     Following addressing modes are affected:
+      [base+scale*index]
+      [scale*index+disp]
+      [base+index]
+
+     The first and last case  may be avoidable by explicitly coding the zero in
+     memory address, but I don't have AMD-K6 machine handy to check this
+     theory.  */
+
+  if (TARGET_K6
+      && ((!parts.disp && parts.base && parts.index && parts.scale != 1)
+	  || (parts.disp && !parts.base && parts.index && parts.scale != 1)
+	  || (!parts.disp && parts.base && parts.index && parts.scale == 1)))
+    cost += 10;
+
+  return cost;
+}
+
+/* Allow {LABEL | SYMBOL}_REF - SYMBOL_REF-FOR-PICBASE for Mach-O as
+   this is used for to form addresses to local data when -fPIC is in
+   use.  */
+
+static bool
+darwin_local_data_pic (rtx disp)
+{
+  return (GET_CODE (disp) == UNSPEC
+	  && XINT (disp, 1) == UNSPEC_MACHOPIC_OFFSET);
+}
+
+/* True if the function symbol operand X should be loaded from GOT.
+
+   NB: In 32-bit mode, only non-PIC is allowed in inline assembly
+   statements, since a PIC register could not be available at the
+   call site.  */
+
+bool
+ix86_force_load_from_GOT_p (rtx x)
+{
+  return ((TARGET_64BIT || (!flag_pic && HAVE_AS_IX86_GOT32X))
+	  && !TARGET_PECOFF && !TARGET_MACHO
+	  && (!flag_pic || this_is_asm_operands)
+	  && ix86_cmodel != CM_LARGE
+	  && ix86_cmodel != CM_LARGE_PIC
+	  && GET_CODE (x) == SYMBOL_REF
+	  && SYMBOL_REF_FUNCTION_P (x)
+	  && (!flag_plt
+	      || (SYMBOL_REF_DECL (x)
+		  && lookup_attribute ("noplt",
+				       DECL_ATTRIBUTES (SYMBOL_REF_DECL (x)))))
+	  && !SYMBOL_REF_LOCAL_P (x));
+}
+
+/* Determine if a given RTX is a valid constant.  We already know this
+   satisfies CONSTANT_P.  */
+
+static bool
+ix86_legitimate_constant_p (machine_mode mode, rtx x)
+{
+  switch (GET_CODE (x))
+    {
+    case CONST:
+      x = XEXP (x, 0);
+
+      if (GET_CODE (x) == PLUS)
+	{
+	  if (!CONST_INT_P (XEXP (x, 1)))
+	    return false;
+	  x = XEXP (x, 0);
+	}
+
+      if (TARGET_MACHO && darwin_local_data_pic (x))
+	return true;
+
+      /* Only some unspecs are valid as "constants".  */
+      if (GET_CODE (x) == UNSPEC)
+	switch (XINT (x, 1))
+	  {
+	  case UNSPEC_GOT:
+	  case UNSPEC_GOTOFF:
+	  case UNSPEC_PLTOFF:
+	    return TARGET_64BIT;
+	  case UNSPEC_TPOFF:
+	  case UNSPEC_NTPOFF:
+	    x = XVECEXP (x, 0, 0);
+	    return (GET_CODE (x) == SYMBOL_REF
+		    && SYMBOL_REF_TLS_MODEL (x) == TLS_MODEL_LOCAL_EXEC);
+	  case UNSPEC_DTPOFF:
+	    x = XVECEXP (x, 0, 0);
+	    return (GET_CODE (x) == SYMBOL_REF
+		    && SYMBOL_REF_TLS_MODEL (x) == TLS_MODEL_LOCAL_DYNAMIC);
+	  default:
+	    return false;
+	  }
+
+      /* We must have drilled down to a symbol.  */
+      if (GET_CODE (x) == LABEL_REF)
+	return true;
+      if (GET_CODE (x) != SYMBOL_REF)
+	return false;
+      /* FALLTHRU */
+
+    case SYMBOL_REF:
+      /* TLS symbols are never valid.  */
+      if (SYMBOL_REF_TLS_MODEL (x))
+	return false;
+
+      /* DLLIMPORT symbols are never valid.  */
+      if (TARGET_DLLIMPORT_DECL_ATTRIBUTES
+	  && SYMBOL_REF_DLLIMPORT_P (x))
+	return false;
+
+#if TARGET_MACHO
+      /* mdynamic-no-pic */
+      if (MACHO_DYNAMIC_NO_PIC_P)
+	return machopic_symbol_defined_p (x);
+#endif
+
+      /* External function address should be loaded
+	 via the GOT slot to avoid PLT.  */
+      if (ix86_force_load_from_GOT_p (x))
+	return false;
+
+      break;
+
+    CASE_CONST_SCALAR_INT:
+      if (ix86_endbr_immediate_operand (x, VOIDmode))
+	return false;
+
+      switch (mode)
+	{
+	case E_TImode:
+	  if (TARGET_64BIT)
+	    return true;
+	  /* FALLTHRU */
+	case E_OImode:
+	case E_XImode:
+	  if (!standard_sse_constant_p (x, mode))
+	    return false;
+	default:
+	  break;
+	}
+      break;
+
+    case CONST_VECTOR:
+      if (!standard_sse_constant_p (x, mode))
+	return false;
+
+    default:
+      break;
+    }
+
+  /* Otherwise we handle everything else in the move patterns.  */
+  return true;
+}
+
+/* Determine if it's legal to put X into the constant pool.  This
+   is not possible for the address of thread-local symbols, which
+   is checked above.  */
+
+static bool
+ix86_cannot_force_const_mem (machine_mode mode, rtx x)
+{
+  /* We can put any immediate constant in memory.  */
+  switch (GET_CODE (x))
+    {
+    CASE_CONST_ANY:
+      return false;
+
+    default:
+      break;
+    }
+
+  return !ix86_legitimate_constant_p (mode, x);
+}
+
+/*  Nonzero if the symbol is marked as dllimport, or as stub-variable,
+    otherwise zero.  */
+
+static bool
+is_imported_p (rtx x)
+{
+  if (!TARGET_DLLIMPORT_DECL_ATTRIBUTES
+      || GET_CODE (x) != SYMBOL_REF)
+    return false;
+
+  return SYMBOL_REF_DLLIMPORT_P (x) || SYMBOL_REF_STUBVAR_P (x);
+}
+
+
+/* Nonzero if the constant value X is a legitimate general operand
+   when generating PIC code.  It is given that flag_pic is on and
+   that X satisfies CONSTANT_P.  */
+
+bool
+legitimate_pic_operand_p (rtx x)
+{
+  rtx inner;
+
+  switch (GET_CODE (x))
+    {
+    case CONST:
+      inner = XEXP (x, 0);
+      if (GET_CODE (inner) == PLUS
+	  && CONST_INT_P (XEXP (inner, 1)))
+	inner = XEXP (inner, 0);
+
+      /* Only some unspecs are valid as "constants".  */
+      if (GET_CODE (inner) == UNSPEC)
+	switch (XINT (inner, 1))
+	  {
+	  case UNSPEC_GOT:
+	  case UNSPEC_GOTOFF:
+	  case UNSPEC_PLTOFF:
+	    return TARGET_64BIT;
+	  case UNSPEC_TPOFF:
+	    x = XVECEXP (inner, 0, 0);
+	    return (GET_CODE (x) == SYMBOL_REF
+		    && SYMBOL_REF_TLS_MODEL (x) == TLS_MODEL_LOCAL_EXEC);
+	  case UNSPEC_MACHOPIC_OFFSET:
+	    return legitimate_pic_address_disp_p (x);
+	  default:
+	    return false;
+	  }
+      /* FALLTHRU */
+
+    case SYMBOL_REF:
+    case LABEL_REF:
+      return legitimate_pic_address_disp_p (x);
+
+    default:
+      return true;
+    }
+}
+
+/* Determine if a given CONST RTX is a valid memory displacement
+   in PIC mode.  */
+
+bool
+legitimate_pic_address_disp_p (rtx disp)
+{
+  bool saw_plus;
+
+  /* In 64bit mode we can allow direct addresses of symbols and labels
+     when they are not dynamic symbols.  */
+  if (TARGET_64BIT)
+    {
+      rtx op0 = disp, op1;
+
+      switch (GET_CODE (disp))
+	{
+	case LABEL_REF:
+	  return true;
+
+	case CONST:
+	  if (GET_CODE (XEXP (disp, 0)) != PLUS)
+	    break;
+	  op0 = XEXP (XEXP (disp, 0), 0);
+	  op1 = XEXP (XEXP (disp, 0), 1);
+	  if (!CONST_INT_P (op1))
+	    break;
+	  if (GET_CODE (op0) == UNSPEC
+	      && (XINT (op0, 1) == UNSPEC_DTPOFF
+		  || XINT (op0, 1) == UNSPEC_NTPOFF)
+	      && trunc_int_for_mode (INTVAL (op1), SImode) == INTVAL (op1))
+	    return true;
+	  if (INTVAL (op1) >= 16*1024*1024
+	      || INTVAL (op1) < -16*1024*1024)
+	    break;
+	  if (GET_CODE (op0) == LABEL_REF)
+	    return true;
+	  if (GET_CODE (op0) == CONST
+	      && GET_CODE (XEXP (op0, 0)) == UNSPEC
+	      && XINT (XEXP (op0, 0), 1) == UNSPEC_PCREL)
+	    return true;
+	  if (GET_CODE (op0) == UNSPEC
+	      && XINT (op0, 1) == UNSPEC_PCREL)
+	    return true;
+	  if (GET_CODE (op0) != SYMBOL_REF)
+	    break;
+	  /* FALLTHRU */
+
+	case SYMBOL_REF:
+	  /* TLS references should always be enclosed in UNSPEC.
+	     The dllimported symbol needs always to be resolved.  */
+	  if (SYMBOL_REF_TLS_MODEL (op0)
+	      || (TARGET_DLLIMPORT_DECL_ATTRIBUTES && SYMBOL_REF_DLLIMPORT_P (op0)))
+	    return false;
+
+	  if (TARGET_PECOFF)
+	    {
+	      if (is_imported_p (op0))
+		return true;
+
+	      if (SYMBOL_REF_FAR_ADDR_P (op0)
+		  || !SYMBOL_REF_LOCAL_P (op0))
+		break;
+
+	      /* Function-symbols need to be resolved only for
+	         large-model.
+	         For the small-model we don't need to resolve anything
+	         here.  */
+	      if ((ix86_cmodel != CM_LARGE_PIC
+	           && SYMBOL_REF_FUNCTION_P (op0))
+		  || ix86_cmodel == CM_SMALL_PIC)
+		return true;
+	      /* Non-external symbols don't need to be resolved for
+	         large, and medium-model.  */
+	      if ((ix86_cmodel == CM_LARGE_PIC
+		   || ix86_cmodel == CM_MEDIUM_PIC)
+		  && !SYMBOL_REF_EXTERNAL_P (op0))
+		return true;
+	    }
+	  else if (!SYMBOL_REF_FAR_ADDR_P (op0)
+		   && (SYMBOL_REF_LOCAL_P (op0)
+		       || (HAVE_LD_PIE_COPYRELOC
+			   && flag_pie
+			   && !SYMBOL_REF_WEAK (op0)
+			   && !SYMBOL_REF_FUNCTION_P (op0)))
+		   && ix86_cmodel != CM_LARGE_PIC)
+	    return true;
+	  break;
+
+	default:
+	  break;
+	}
+    }
+  if (GET_CODE (disp) != CONST)
+    return false;
+  disp = XEXP (disp, 0);
+
+  if (TARGET_64BIT)
+    {
+      /* We are unsafe to allow PLUS expressions.  This limit allowed distance
+         of GOT tables.  We should not need these anyway.  */
+      if (GET_CODE (disp) != UNSPEC
+	  || (XINT (disp, 1) != UNSPEC_GOTPCREL
+	      && XINT (disp, 1) != UNSPEC_GOTOFF
+	      && XINT (disp, 1) != UNSPEC_PCREL
+	      && XINT (disp, 1) != UNSPEC_PLTOFF))
+	return false;
+
+      if (GET_CODE (XVECEXP (disp, 0, 0)) != SYMBOL_REF
+	  && GET_CODE (XVECEXP (disp, 0, 0)) != LABEL_REF)
+	return false;
+      return true;
+    }
+
+  saw_plus = false;
+  if (GET_CODE (disp) == PLUS)
+    {
+      if (!CONST_INT_P (XEXP (disp, 1)))
+	return false;
+      disp = XEXP (disp, 0);
+      saw_plus = true;
+    }
+
+  if (TARGET_MACHO && darwin_local_data_pic (disp))
+    return true;
+
+  if (GET_CODE (disp) != UNSPEC)
+    return false;
+
+  switch (XINT (disp, 1))
+    {
+    case UNSPEC_GOT:
+      if (saw_plus)
+	return false;
+      /* We need to check for both symbols and labels because VxWorks loads
+	 text labels with @GOT rather than @GOTOFF.  See gotoff_operand for
+	 details.  */
+      return (GET_CODE (XVECEXP (disp, 0, 0)) == SYMBOL_REF
+	      || GET_CODE (XVECEXP (disp, 0, 0)) == LABEL_REF);
+    case UNSPEC_GOTOFF:
+      /* Refuse GOTOFF in 64bit mode since it is always 64bit when used.
+	 While ABI specify also 32bit relocation but we don't produce it in
+	 small PIC model at all.  */
+      if ((GET_CODE (XVECEXP (disp, 0, 0)) == SYMBOL_REF
+	   || GET_CODE (XVECEXP (disp, 0, 0)) == LABEL_REF)
+	  && !TARGET_64BIT)
+        return !TARGET_PECOFF && gotoff_operand (XVECEXP (disp, 0, 0), Pmode);
+      return false;
+    case UNSPEC_GOTTPOFF:
+    case UNSPEC_GOTNTPOFF:
+    case UNSPEC_INDNTPOFF:
+      if (saw_plus)
+	return false;
+      disp = XVECEXP (disp, 0, 0);
+      return (GET_CODE (disp) == SYMBOL_REF
+	      && SYMBOL_REF_TLS_MODEL (disp) == TLS_MODEL_INITIAL_EXEC);
+    case UNSPEC_NTPOFF:
+      disp = XVECEXP (disp, 0, 0);
+      return (GET_CODE (disp) == SYMBOL_REF
+	      && SYMBOL_REF_TLS_MODEL (disp) == TLS_MODEL_LOCAL_EXEC);
+    case UNSPEC_DTPOFF:
+      disp = XVECEXP (disp, 0, 0);
+      return (GET_CODE (disp) == SYMBOL_REF
+	      && SYMBOL_REF_TLS_MODEL (disp) == TLS_MODEL_LOCAL_DYNAMIC);
+    }
+
+  return false;
+}
+
+/* Determine if op is suitable RTX for an address register.
+   Return naked register if a register or a register subreg is
+   found, otherwise return NULL_RTX.  */
+
+static rtx
+ix86_validate_address_register (rtx op)
+{
+  machine_mode mode = GET_MODE (op);
+
+  /* Only SImode or DImode registers can form the address.  */
+  if (mode != SImode && mode != DImode)
+    return NULL_RTX;
+
+  if (REG_P (op))
+    return op;
+  else if (SUBREG_P (op))
+    {
+      rtx reg = SUBREG_REG (op);
+
+      if (!REG_P (reg))
+	return NULL_RTX;
+
+      mode = GET_MODE (reg);
+
+      /* Don't allow SUBREGs that span more than a word.  It can
+	 lead to spill failures when the register is one word out
+	 of a two word structure.  */
+      if (GET_MODE_SIZE (mode) > UNITS_PER_WORD)
+	return NULL_RTX;
+
+      /* Allow only SUBREGs of non-eliminable hard registers.  */
+      if (register_no_elim_operand (reg, mode))
+	return reg;
+    }
+
+  /* Op is not a register.  */
+  return NULL_RTX;
+}
+
+/* Recognizes RTL expressions that are valid memory addresses for an
+   instruction.  The MODE argument is the machine mode for the MEM
+   expression that wants to use this address.
+
+   It only recognizes address in canonical form.  LEGITIMIZE_ADDRESS should
+   convert common non-canonical forms to canonical form so that they will
+   be recognized.  */
+
+static bool
+ix86_legitimate_address_p (machine_mode, rtx addr, bool strict)
+{
+  struct ix86_address parts;
+  rtx base, index, disp;
+  HOST_WIDE_INT scale;
+  addr_space_t seg;
+
+  if (ix86_decompose_address (addr, &parts) <= 0)
+    /* Decomposition failed.  */
+    return false;
+
+  base = parts.base;
+  index = parts.index;
+  disp = parts.disp;
+  scale = parts.scale;
+  seg = parts.seg;
+
+  /* Validate base register.  */
+  if (base)
+    {
+      rtx reg = ix86_validate_address_register (base);
+
+      if (reg == NULL_RTX)
+	return false;
+
+      if ((strict && ! REG_OK_FOR_BASE_STRICT_P (reg))
+	  || (! strict && ! REG_OK_FOR_BASE_NONSTRICT_P (reg)))
+	/* Base is not valid.  */
+	return false;
+    }
+
+  /* Validate index register.  */
+  if (index)
+    {
+      rtx reg = ix86_validate_address_register (index);
+
+      if (reg == NULL_RTX)
+	return false;
+
+      if ((strict && ! REG_OK_FOR_INDEX_STRICT_P (reg))
+	  || (! strict && ! REG_OK_FOR_INDEX_NONSTRICT_P (reg)))
+	/* Index is not valid.  */
+	return false;
+    }
+
+  /* Index and base should have the same mode.  */
+  if (base && index
+      && GET_MODE (base) != GET_MODE (index))
+    return false;
+
+  /* Address override works only on the (%reg) part of %fs:(%reg).  */
+  if (seg != ADDR_SPACE_GENERIC
+      && ((base && GET_MODE (base) != word_mode)
+	  || (index && GET_MODE (index) != word_mode)))
+    return false;
+
+  /* Validate scale factor.  */
+  if (scale != 1)
+    {
+      if (!index)
+	/* Scale without index.  */
+	return false;
+
+      if (scale != 2 && scale != 4 && scale != 8)
+	/* Scale is not a valid multiplier.  */
+	return false;
+    }
+
+  /* Validate displacement.  */
+  if (disp)
+    {
+      if (ix86_endbr_immediate_operand (disp, VOIDmode))
+	return false;
+
+      if (GET_CODE (disp) == CONST
+	  && GET_CODE (XEXP (disp, 0)) == UNSPEC
+	  && XINT (XEXP (disp, 0), 1) != UNSPEC_MACHOPIC_OFFSET)
+	switch (XINT (XEXP (disp, 0), 1))
+	  {
+	  /* Refuse GOTOFF and GOT in 64bit mode since it is always 64bit
+	     when used.  While ABI specify also 32bit relocations, we
+	     don't produce them at all and use IP relative instead.
+	     Allow GOT in 32bit mode for both PIC and non-PIC if symbol
+	     should be loaded via GOT.  */
+	  case UNSPEC_GOT:
+	    if (!TARGET_64BIT
+		&& ix86_force_load_from_GOT_p (XVECEXP (XEXP (disp, 0), 0, 0)))
+	      goto is_legitimate_pic;
+	    /* FALLTHRU */
+	  case UNSPEC_GOTOFF:
+	    gcc_assert (flag_pic);
+	    if (!TARGET_64BIT)
+	      goto is_legitimate_pic;
+
+	    /* 64bit address unspec.  */
+	    return false;
+
+	  case UNSPEC_GOTPCREL:
+	    if (ix86_force_load_from_GOT_p (XVECEXP (XEXP (disp, 0), 0, 0)))
+	      goto is_legitimate_pic;
+	    /* FALLTHRU */
+	  case UNSPEC_PCREL:
+	    gcc_assert (flag_pic);
+	    goto is_legitimate_pic;
+
+	  case UNSPEC_GOTTPOFF:
+	  case UNSPEC_GOTNTPOFF:
+	  case UNSPEC_INDNTPOFF:
+	  case UNSPEC_NTPOFF:
+	  case UNSPEC_DTPOFF:
+	    break;
+
+	  default:
+	    /* Invalid address unspec.  */
+	    return false;
+	  }
+
+      else if (SYMBOLIC_CONST (disp)
+	       && (flag_pic
+		   || (TARGET_MACHO
+#if TARGET_MACHO
+		       && MACHOPIC_INDIRECT
+		       && !machopic_operand_p (disp)
+#endif
+	       )))
+	{
+
+	is_legitimate_pic:
+	  if (TARGET_64BIT && (index || base))
+	    {
+	      /* foo@dtpoff(%rX) is ok.  */
+	      if (GET_CODE (disp) != CONST
+		  || GET_CODE (XEXP (disp, 0)) != PLUS
+		  || GET_CODE (XEXP (XEXP (disp, 0), 0)) != UNSPEC
+		  || !CONST_INT_P (XEXP (XEXP (disp, 0), 1))
+		  || (XINT (XEXP (XEXP (disp, 0), 0), 1) != UNSPEC_DTPOFF
+		      && XINT (XEXP (XEXP (disp, 0), 0), 1) != UNSPEC_NTPOFF))
+		/* Non-constant pic memory reference.  */
+		return false;
+	    }
+	  else if ((!TARGET_MACHO || flag_pic)
+		    && ! legitimate_pic_address_disp_p (disp))
+	    /* Displacement is an invalid pic construct.  */
+	    return false;
+#if TARGET_MACHO
+	  else if (MACHO_DYNAMIC_NO_PIC_P
+		   && !ix86_legitimate_constant_p (Pmode, disp))
+	    /* displacment must be referenced via non_lazy_pointer */
+	    return false;
+#endif
+
+          /* This code used to verify that a symbolic pic displacement
+	     includes the pic_offset_table_rtx register.
+
+	     While this is good idea, unfortunately these constructs may
+	     be created by "adds using lea" optimization for incorrect
+	     code like:
+
+	     int a;
+	     int foo(int i)
+	       {
+	         return *(&a+i);
+	       }
+
+	     This code is nonsensical, but results in addressing
+	     GOT table with pic_offset_table_rtx base.  We can't
+	     just refuse it easily, since it gets matched by
+	     "addsi3" pattern, that later gets split to lea in the
+	     case output register differs from input.  While this
+	     can be handled by separate addsi pattern for this case
+	     that never results in lea, this seems to be easier and
+	     correct fix for crash to disable this test.  */
+	}
+      else if (GET_CODE (disp) != LABEL_REF
+	       && !CONST_INT_P (disp)
+	       && (GET_CODE (disp) != CONST
+		   || !ix86_legitimate_constant_p (Pmode, disp))
+	       && (GET_CODE (disp) != SYMBOL_REF
+		   || !ix86_legitimate_constant_p (Pmode, disp)))
+	/* Displacement is not constant.  */
+	return false;
+      else if (TARGET_64BIT
+	       && !x86_64_immediate_operand (disp, VOIDmode))
+	/* Displacement is out of range.  */
+	return false;
+      /* In x32 mode, constant addresses are sign extended to 64bit, so
+	 we have to prevent addresses from 0x80000000 to 0xffffffff.  */
+      else if (TARGET_X32 && !(index || base)
+	       && CONST_INT_P (disp)
+	       && val_signbit_known_set_p (SImode, INTVAL (disp)))
+	return false;
+    }
+
+  /* Everything looks valid.  */
+  return true;
+}
+
+/* Determine if a given RTX is a valid constant address.  */
+
+bool
+constant_address_p (rtx x)
+{
+  return CONSTANT_P (x) && ix86_legitimate_address_p (Pmode, x, 1);
+}
+
+/* Return a unique alias set for the GOT.  */
+
+alias_set_type
+ix86_GOT_alias_set (void)
+{
+  static alias_set_type set = -1;
+  if (set == -1)
+    set = new_alias_set ();
+  return set;
+}
+
+/* Return a legitimate reference for ORIG (an address) using the
+   register REG.  If REG is 0, a new pseudo is generated.
+
+   There are two types of references that must be handled:
+
+   1. Global data references must load the address from the GOT, via
+      the PIC reg.  An insn is emitted to do this load, and the reg is
+      returned.
+
+   2. Static data references, constant pool addresses, and code labels
+      compute the address as an offset from the GOT, whose base is in
+      the PIC reg.  Static data objects have SYMBOL_FLAG_LOCAL set to
+      differentiate them from global data objects.  The returned
+      address is the PIC reg + an unspec constant.
+
+   TARGET_LEGITIMATE_ADDRESS_P rejects symbolic references unless the PIC
+   reg also appears in the address.  */
+
+rtx
+legitimize_pic_address (rtx orig, rtx reg)
+{
+  rtx addr = orig;
+  rtx new_rtx = orig;
+
+#if TARGET_MACHO
+  if (TARGET_MACHO && !TARGET_64BIT)
+    {
+      if (reg == 0)
+	reg = gen_reg_rtx (Pmode);
+      /* Use the generic Mach-O PIC machinery.  */
+      return machopic_legitimize_pic_address (orig, GET_MODE (orig), reg);
+    }
+#endif
+
+  if (TARGET_64BIT && TARGET_DLLIMPORT_DECL_ATTRIBUTES)
+    {
+      rtx tmp = legitimize_pe_coff_symbol (addr, true);
+      if (tmp)
+        return tmp;
+    }
+
+  if (TARGET_64BIT && legitimate_pic_address_disp_p (addr))
+    new_rtx = addr;
+  else if ((!TARGET_64BIT
+	    || /* TARGET_64BIT && */ ix86_cmodel != CM_SMALL_PIC)
+	   && !TARGET_PECOFF
+	   && gotoff_operand (addr, Pmode))
+    {
+      /* This symbol may be referenced via a displacement
+	 from the PIC base address (@GOTOFF).  */
+      if (GET_CODE (addr) == CONST)
+	addr = XEXP (addr, 0);
+
+      if (GET_CODE (addr) == PLUS)
+	  {
+            new_rtx = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, XEXP (addr, 0)),
+				      UNSPEC_GOTOFF);
+	    new_rtx = gen_rtx_PLUS (Pmode, new_rtx, XEXP (addr, 1));
+	  }
+	else
+          new_rtx = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, addr), UNSPEC_GOTOFF);
+
+      new_rtx = gen_rtx_CONST (Pmode, new_rtx);
+
+      if (TARGET_64BIT)
+	new_rtx = copy_to_suggested_reg (new_rtx, reg, Pmode);
+
+      if (reg != 0)
+	{
+ 	  gcc_assert (REG_P (reg));
+	  new_rtx = expand_simple_binop (Pmode, PLUS, pic_offset_table_rtx,
+					 new_rtx, reg, 1, OPTAB_DIRECT);
+ 	}
+      else
+	new_rtx = gen_rtx_PLUS (Pmode, pic_offset_table_rtx, new_rtx);
+    }
+  else if ((GET_CODE (addr) == SYMBOL_REF && SYMBOL_REF_TLS_MODEL (addr) == 0)
+	   /* We can't use @GOTOFF for text labels
+	      on VxWorks, see gotoff_operand.  */
+	   || (TARGET_VXWORKS_RTP && GET_CODE (addr) == LABEL_REF))
+    {
+      rtx tmp = legitimize_pe_coff_symbol (addr, true);
+      if (tmp)
+        return tmp;
+
+      /* For x64 PE-COFF there is no GOT table,
+	 so we use address directly.  */
+      if (TARGET_64BIT && TARGET_PECOFF)
+	{
+	  new_rtx = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, addr), UNSPEC_PCREL);
+	  new_rtx = gen_rtx_CONST (Pmode, new_rtx);
+	}
+      else if (TARGET_64BIT && ix86_cmodel != CM_LARGE_PIC)
+	{
+	  new_rtx = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, addr),
+				    UNSPEC_GOTPCREL);
+	  new_rtx = gen_rtx_CONST (Pmode, new_rtx);
+	  new_rtx = gen_const_mem (Pmode, new_rtx);
+	  set_mem_alias_set (new_rtx, ix86_GOT_alias_set ());
+	}
+      else
+	{
+	  /* This symbol must be referenced via a load
+	     from the Global Offset Table (@GOT).  */
+	  new_rtx = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, addr), UNSPEC_GOT);
+	  new_rtx = gen_rtx_CONST (Pmode, new_rtx);
+	  if (TARGET_64BIT)
+	    new_rtx = force_reg (Pmode, new_rtx);
+	  new_rtx = gen_rtx_PLUS (Pmode, pic_offset_table_rtx, new_rtx);
+	  new_rtx = gen_const_mem (Pmode, new_rtx);
+	  set_mem_alias_set (new_rtx, ix86_GOT_alias_set ());
+	}
+
+      new_rtx = copy_to_suggested_reg (new_rtx, reg, Pmode);
+    }
+  else
+    {
+      if (CONST_INT_P (addr)
+	  && !x86_64_immediate_operand (addr, VOIDmode))
+	new_rtx = copy_to_suggested_reg (addr, reg, Pmode);
+      else if (GET_CODE (addr) == CONST)
+	{
+	  addr = XEXP (addr, 0);
+
+	  /* We must match stuff we generate before.  Assume the only
+	     unspecs that can get here are ours.  Not that we could do
+	     anything with them anyway....  */
+	  if (GET_CODE (addr) == UNSPEC
+	      || (GET_CODE (addr) == PLUS
+		  && GET_CODE (XEXP (addr, 0)) == UNSPEC))
+	    return orig;
+	  gcc_assert (GET_CODE (addr) == PLUS);
+	}
+
+      if (GET_CODE (addr) == PLUS)
+	{
+	  rtx op0 = XEXP (addr, 0), op1 = XEXP (addr, 1);
+
+	  /* Check first to see if this is a constant
+	     offset from a @GOTOFF symbol reference.  */
+	  if (!TARGET_PECOFF
+	      && gotoff_operand (op0, Pmode)
+	      && CONST_INT_P (op1))
+	    {
+	      if (!TARGET_64BIT)
+		{
+		  new_rtx = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, op0),
+					    UNSPEC_GOTOFF);
+		  new_rtx = gen_rtx_PLUS (Pmode, new_rtx, op1);
+		  new_rtx = gen_rtx_CONST (Pmode, new_rtx);
+
+		  if (reg != 0)
+		    {
+		      gcc_assert (REG_P (reg));
+		      new_rtx = expand_simple_binop (Pmode, PLUS,
+						     pic_offset_table_rtx,
+						     new_rtx, reg, 1,
+						     OPTAB_DIRECT);
+		    }
+		  else
+		    new_rtx
+		      = gen_rtx_PLUS (Pmode, pic_offset_table_rtx, new_rtx);
+		}
+	      else
+		{
+		  if (INTVAL (op1) < -16*1024*1024
+		      || INTVAL (op1) >= 16*1024*1024)
+		    {
+		      if (!x86_64_immediate_operand (op1, Pmode))
+			op1 = force_reg (Pmode, op1);
+
+		      new_rtx
+			= gen_rtx_PLUS (Pmode, force_reg (Pmode, op0), op1);
+		    }
+		}
+	    }
+	  else
+	    {
+	      rtx base = legitimize_pic_address (op0, reg);
+	      machine_mode mode = GET_MODE (base);
+	      new_rtx
+	        = legitimize_pic_address (op1, base == reg ? NULL_RTX : reg);
+
+	      if (CONST_INT_P (new_rtx))
+		{
+		  if (INTVAL (new_rtx) < -16*1024*1024
+		      || INTVAL (new_rtx) >= 16*1024*1024)
+		    {
+		      if (!x86_64_immediate_operand (new_rtx, mode))
+			new_rtx = force_reg (mode, new_rtx);
+
+		      new_rtx
+		        = gen_rtx_PLUS (mode, force_reg (mode, base), new_rtx);
+		    }
+		  else
+		    new_rtx = plus_constant (mode, base, INTVAL (new_rtx));
+		}
+	      else
+		{
+		  /* For %rip addressing, we have to use
+		     just disp32, not base nor index.  */
+		  if (TARGET_64BIT
+		      && (GET_CODE (base) == SYMBOL_REF
+			  || GET_CODE (base) == LABEL_REF))
+		    base = force_reg (mode, base);
+		  if (GET_CODE (new_rtx) == PLUS
+		      && CONSTANT_P (XEXP (new_rtx, 1)))
+		    {
+		      base = gen_rtx_PLUS (mode, base, XEXP (new_rtx, 0));
+		      new_rtx = XEXP (new_rtx, 1);
+		    }
+		  new_rtx = gen_rtx_PLUS (mode, base, new_rtx);
+		}
+	    }
+	}
+    }
+  return new_rtx;
+}
+
+/* Load the thread pointer.  If TO_REG is true, force it into a register.  */
+
+static rtx
+get_thread_pointer (machine_mode tp_mode, bool to_reg)
+{
+  rtx tp = gen_rtx_UNSPEC (ptr_mode, gen_rtvec (1, const0_rtx), UNSPEC_TP);
+
+  if (GET_MODE (tp) != tp_mode)
+    {
+      gcc_assert (GET_MODE (tp) == SImode);
+      gcc_assert (tp_mode == DImode);
+
+      tp = gen_rtx_ZERO_EXTEND (tp_mode, tp);
+    }
+
+  if (to_reg)
+    tp = copy_to_mode_reg (tp_mode, tp);
+
+  return tp;
+}
+
+/* Construct the SYMBOL_REF for the tls_get_addr function.  */
+
+static GTY(()) rtx ix86_tls_symbol;
+
+static rtx
+ix86_tls_get_addr (void)
+{
+  if (!ix86_tls_symbol)
+    {
+      const char *sym
+	= ((TARGET_ANY_GNU_TLS && !TARGET_64BIT)
+	   ? "___tls_get_addr" : "__tls_get_addr");
+
+      ix86_tls_symbol = gen_rtx_SYMBOL_REF (Pmode, sym);
+    }
+
+  if (ix86_cmodel == CM_LARGE_PIC && !TARGET_PECOFF)
+    {
+      rtx unspec = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, ix86_tls_symbol),
+				   UNSPEC_PLTOFF);
+      return gen_rtx_PLUS (Pmode, pic_offset_table_rtx,
+			   gen_rtx_CONST (Pmode, unspec));
+    }
+
+  return ix86_tls_symbol;
+}
+
+/* Construct the SYMBOL_REF for the _TLS_MODULE_BASE_ symbol.  */
+
+static GTY(()) rtx ix86_tls_module_base_symbol;
+
+rtx
+ix86_tls_module_base (void)
+{
+  if (!ix86_tls_module_base_symbol)
+    {
+      ix86_tls_module_base_symbol
+	= gen_rtx_SYMBOL_REF (ptr_mode, "_TLS_MODULE_BASE_");
+
+      SYMBOL_REF_FLAGS (ix86_tls_module_base_symbol)
+	|= TLS_MODEL_GLOBAL_DYNAMIC << SYMBOL_FLAG_TLS_SHIFT;
+    }
+
+  return ix86_tls_module_base_symbol;
+}
+
+/* A subroutine of ix86_legitimize_address and ix86_expand_move.  FOR_MOV is
+   false if we expect this to be used for a memory address and true if
+   we expect to load the address into a register.  */
+
+rtx
+legitimize_tls_address (rtx x, enum tls_model model, bool for_mov)
+{
+  rtx dest, base, off;
+  rtx pic = NULL_RTX, tp = NULL_RTX;
+  machine_mode tp_mode = Pmode;
+  int type;
+
+  /* Fall back to global dynamic model if tool chain cannot support local
+     dynamic.  */
+  if (TARGET_SUN_TLS && !TARGET_64BIT
+      && !HAVE_AS_IX86_TLSLDMPLT && !HAVE_AS_IX86_TLSLDM
+      && model == TLS_MODEL_LOCAL_DYNAMIC)
+    model = TLS_MODEL_GLOBAL_DYNAMIC;
+
+  switch (model)
+    {
+    case TLS_MODEL_GLOBAL_DYNAMIC:
+      if (!TARGET_64BIT)
+	{
+	  if (flag_pic && !TARGET_PECOFF)
+	    pic = pic_offset_table_rtx;
+	  else
+	    {
+	      pic = gen_reg_rtx (Pmode);
+	      emit_insn (gen_set_got (pic));
+	    }
+	}
+
+      if (TARGET_GNU2_TLS)
+	{
+	  dest = gen_reg_rtx (ptr_mode);
+	  if (TARGET_64BIT)
+	    emit_insn (gen_tls_dynamic_gnu2_64 (ptr_mode, dest, x));
+	  else
+	    emit_insn (gen_tls_dynamic_gnu2_32 (dest, x, pic));
+
+	  tp = get_thread_pointer (ptr_mode, true);
+	  dest = gen_rtx_PLUS (ptr_mode, tp, dest);
+	  if (GET_MODE (dest) != Pmode)
+	     dest = gen_rtx_ZERO_EXTEND (Pmode, dest);
+	  dest = force_reg (Pmode, dest);
+
+	  if (GET_MODE (x) != Pmode)
+	    x = gen_rtx_ZERO_EXTEND (Pmode, x);
+
+	  set_unique_reg_note (get_last_insn (), REG_EQUAL, x);
+	}
+      else
+	{
+	  rtx caddr = ix86_tls_get_addr ();
+
+	  dest = gen_reg_rtx (Pmode);
+	  if (TARGET_64BIT)
+	    {
+	      rtx rax = gen_rtx_REG (Pmode, AX_REG);
+	      rtx_insn *insns;
+
+	      start_sequence ();
+	      emit_call_insn
+		(gen_tls_global_dynamic_64 (Pmode, rax, x, caddr));
+	      insns = get_insns ();
+	      end_sequence ();
+
+	      if (GET_MODE (x) != Pmode)
+		x = gen_rtx_ZERO_EXTEND (Pmode, x);
+
+	      RTL_CONST_CALL_P (insns) = 1;
+	      emit_libcall_block (insns, dest, rax, x);
+	    }
+	  else
+	    emit_insn (gen_tls_global_dynamic_32 (dest, x, pic, caddr));
+	}
+      break;
+
+    case TLS_MODEL_LOCAL_DYNAMIC:
+      if (!TARGET_64BIT)
+	{
+	  if (flag_pic)
+	    pic = pic_offset_table_rtx;
+	  else
+	    {
+	      pic = gen_reg_rtx (Pmode);
+	      emit_insn (gen_set_got (pic));
+	    }
+	}
+
+      if (TARGET_GNU2_TLS)
+	{
+	  rtx tmp = ix86_tls_module_base ();
+
+	  base = gen_reg_rtx (ptr_mode);
+	  if (TARGET_64BIT)
+	    emit_insn (gen_tls_dynamic_gnu2_64 (ptr_mode, base, tmp));
+	  else
+	    emit_insn (gen_tls_dynamic_gnu2_32 (base, tmp, pic));
+
+	  tp = get_thread_pointer (ptr_mode, true);
+	  if (GET_MODE (base) != Pmode)
+	    base = gen_rtx_ZERO_EXTEND (Pmode, base);
+	  base = force_reg (Pmode, base);
+	}
+      else
+	{
+	  rtx caddr = ix86_tls_get_addr ();
+
+	  base = gen_reg_rtx (Pmode);
+	  if (TARGET_64BIT)
+	    {
+	      rtx rax = gen_rtx_REG (Pmode, AX_REG);
+	      rtx_insn *insns;
+	      rtx eqv;
+
+	      start_sequence ();
+	      emit_call_insn
+		(gen_tls_local_dynamic_base_64 (Pmode, rax, caddr));
+	      insns = get_insns ();
+	      end_sequence ();
+
+	      /* Attach a unique REG_EQUAL, to allow the RTL optimizers to
+		 share the LD_BASE result with other LD model accesses.  */
+	      eqv = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, const0_rtx),
+				    UNSPEC_TLS_LD_BASE);
+
+	      RTL_CONST_CALL_P (insns) = 1;
+	      emit_libcall_block (insns, base, rax, eqv);
+	    }
+	  else
+	    emit_insn (gen_tls_local_dynamic_base_32 (base, pic, caddr));
+	}
+
+      off = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, x), UNSPEC_DTPOFF);
+      off = gen_rtx_CONST (Pmode, off);
+
+      dest = force_reg (Pmode, gen_rtx_PLUS (Pmode, base, off));
+
+      if (TARGET_GNU2_TLS)
+	{
+	  if (GET_MODE (tp) != Pmode)
+	    {
+	      dest = lowpart_subreg (ptr_mode, dest, Pmode);
+	      dest = gen_rtx_PLUS (ptr_mode, tp, dest);
+	      dest = gen_rtx_ZERO_EXTEND (Pmode, dest);
+	    }
+	  else
+	    dest = gen_rtx_PLUS (Pmode, tp, dest);
+	  dest = force_reg (Pmode, dest);
+
+	  if (GET_MODE (x) != Pmode)
+	    x = gen_rtx_ZERO_EXTEND (Pmode, x);
+
+	  set_unique_reg_note (get_last_insn (), REG_EQUAL, x);
+	}
+      break;
+
+    case TLS_MODEL_INITIAL_EXEC:
+      if (TARGET_64BIT)
+	{
+	  if (TARGET_SUN_TLS && !TARGET_X32)
+	    {
+	      /* The Sun linker took the AMD64 TLS spec literally
+		 and can only handle %rax as destination of the
+		 initial executable code sequence.  */
+
+	      dest = gen_reg_rtx (DImode);
+	      emit_insn (gen_tls_initial_exec_64_sun (dest, x));
+	      return dest;
+	    }
+
+	  /* Generate DImode references to avoid %fs:(%reg32)
+	     problems and linker IE->LE relaxation bug.  */
+	  tp_mode = DImode;
+	  pic = NULL;
+	  type = UNSPEC_GOTNTPOFF;
+	}
+      else if (flag_pic)
+	{
+	  pic = pic_offset_table_rtx;
+	  type = TARGET_ANY_GNU_TLS ? UNSPEC_GOTNTPOFF : UNSPEC_GOTTPOFF;
+	}
+      else if (!TARGET_ANY_GNU_TLS)
+	{
+	  pic = gen_reg_rtx (Pmode);
+	  emit_insn (gen_set_got (pic));
+	  type = UNSPEC_GOTTPOFF;
+	}
+      else
+	{
+	  pic = NULL;
+	  type = UNSPEC_INDNTPOFF;
+	}
+
+      off = gen_rtx_UNSPEC (tp_mode, gen_rtvec (1, x), type);
+      off = gen_rtx_CONST (tp_mode, off);
+      if (pic)
+	off = gen_rtx_PLUS (tp_mode, pic, off);
+      off = gen_const_mem (tp_mode, off);
+      set_mem_alias_set (off, ix86_GOT_alias_set ());
+
+      if (TARGET_64BIT || TARGET_ANY_GNU_TLS)
+	{
+	  base = get_thread_pointer (tp_mode,
+				     for_mov || !TARGET_TLS_DIRECT_SEG_REFS);
+	  off = force_reg (tp_mode, off);
+	  dest = gen_rtx_PLUS (tp_mode, base, off);
+	  if (tp_mode != Pmode)
+	    dest = convert_to_mode (Pmode, dest, 1);
+	}
+      else
+	{
+	  base = get_thread_pointer (Pmode, true);
+	  dest = gen_reg_rtx (Pmode);
+	  emit_insn (gen_sub3_insn (dest, base, off));
+	}
+      break;
+
+    case TLS_MODEL_LOCAL_EXEC:
+      off = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, x),
+			    (TARGET_64BIT || TARGET_ANY_GNU_TLS)
+			    ? UNSPEC_NTPOFF : UNSPEC_TPOFF);
+      off = gen_rtx_CONST (Pmode, off);
+
+      if (TARGET_64BIT || TARGET_ANY_GNU_TLS)
+	{
+	  base = get_thread_pointer (Pmode,
+				     for_mov || !TARGET_TLS_DIRECT_SEG_REFS);
+	  return gen_rtx_PLUS (Pmode, base, off);
+	}
+      else
+	{
+	  base = get_thread_pointer (Pmode, true);
+	  dest = gen_reg_rtx (Pmode);
+	  emit_insn (gen_sub3_insn (dest, base, off));
+	}
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  return dest;
+}
+
+/* Return true if OP refers to a TLS address.  */
+bool
+ix86_tls_address_pattern_p (rtx op)
+{
+  subrtx_var_iterator::array_type array;
+  FOR_EACH_SUBRTX_VAR (iter, array, op, ALL)
+    {
+      rtx op = *iter;
+      if (MEM_P (op))
+	{
+	  rtx *x = &XEXP (op, 0);
+	  while (GET_CODE (*x) == PLUS)
+	    {
+	      int i;
+	      for (i = 0; i < 2; i++)
+		{
+		  rtx u = XEXP (*x, i);
+		  if (GET_CODE (u) == ZERO_EXTEND)
+		    u = XEXP (u, 0);
+		  if (GET_CODE (u) == UNSPEC
+		      && XINT (u, 1) == UNSPEC_TP)
+		    return true;
+		}
+	      x = &XEXP (*x, 0);
+	    }
+
+	  iter.skip_subrtxes ();
+	}
+    }
+
+  return false;
+}
+
+/* Rewrite *LOC so that it refers to a default TLS address space.  */
+void
+ix86_rewrite_tls_address_1 (rtx *loc)
+{
+  subrtx_ptr_iterator::array_type array;
+  FOR_EACH_SUBRTX_PTR (iter, array, loc, ALL)
+    {
+      rtx *loc = *iter;
+      if (MEM_P (*loc))
+	{
+	  rtx addr = XEXP (*loc, 0);
+	  rtx *x = &addr;
+	  while (GET_CODE (*x) == PLUS)
+	    {
+	      int i;
+	      for (i = 0; i < 2; i++)
+		{
+		  rtx u = XEXP (*x, i);
+		  if (GET_CODE (u) == ZERO_EXTEND)
+		    u = XEXP (u, 0);
+		  if (GET_CODE (u) == UNSPEC
+		      && XINT (u, 1) == UNSPEC_TP)
+		    {
+		      addr_space_t as = DEFAULT_TLS_SEG_REG;
+
+		      *x = XEXP (*x, 1 - i);
+
+		      *loc = replace_equiv_address_nv (*loc, addr, true);
+		      set_mem_addr_space (*loc, as);
+		      return;
+		    }
+		}
+	      x = &XEXP (*x, 0);
+	    }
+
+	  iter.skip_subrtxes ();
+	}
+    }
+}
+
+/* Rewrite instruction pattern involvning TLS address
+   so that it refers to a default TLS address space.  */
+rtx
+ix86_rewrite_tls_address (rtx pattern)
+{
+  pattern = copy_insn (pattern);
+  ix86_rewrite_tls_address_1 (&pattern);
+  return pattern;
+}
+
+/* Create or return the unique __imp_DECL dllimport symbol corresponding
+   to symbol DECL if BEIMPORT is true.  Otherwise create or return the
+   unique refptr-DECL symbol corresponding to symbol DECL.  */
+
+struct dllimport_hasher : ggc_cache_ptr_hash<tree_map>
+{
+  static inline hashval_t hash (tree_map *m) { return m->hash; }
+  static inline bool
+  equal (tree_map *a, tree_map *b)
+  {
+    return a->base.from == b->base.from;
+  }
+
+  static int
+  keep_cache_entry (tree_map *&m)
+  {
+    return ggc_marked_p (m->base.from);
+  }
+};
+
+static GTY((cache)) hash_table<dllimport_hasher> *dllimport_map;
+
+static tree
+get_dllimport_decl (tree decl, bool beimport)
+{
+  struct tree_map *h, in;
+  const char *name;
+  const char *prefix;
+  size_t namelen, prefixlen;
+  char *imp_name;
+  tree to;
+  rtx rtl;
+
+  if (!dllimport_map)
+    dllimport_map = hash_table<dllimport_hasher>::create_ggc (512);
+
+  in.hash = htab_hash_pointer (decl);
+  in.base.from = decl;
+  tree_map **loc = dllimport_map->find_slot_with_hash (&in, in.hash, INSERT);
+  h = *loc;
+  if (h)
+    return h->to;
+
+  *loc = h = ggc_alloc<tree_map> ();
+  h->hash = in.hash;
+  h->base.from = decl;
+  h->to = to = build_decl (DECL_SOURCE_LOCATION (decl),
+			   VAR_DECL, NULL, ptr_type_node);
+  DECL_ARTIFICIAL (to) = 1;
+  DECL_IGNORED_P (to) = 1;
+  DECL_EXTERNAL (to) = 1;
+  TREE_READONLY (to) = 1;
+
+  name = IDENTIFIER_POINTER (DECL_ASSEMBLER_NAME (decl));
+  name = targetm.strip_name_encoding (name);
+  if (beimport)
+    prefix = name[0] == FASTCALL_PREFIX || user_label_prefix[0] == 0
+      ? "*__imp_" : "*__imp__";
+  else
+    prefix = user_label_prefix[0] == 0 ? "*.refptr." : "*refptr.";
+  namelen = strlen (name);
+  prefixlen = strlen (prefix);
+  imp_name = (char *) alloca (namelen + prefixlen + 1);
+  memcpy (imp_name, prefix, prefixlen);
+  memcpy (imp_name + prefixlen, name, namelen + 1);
+
+  name = ggc_alloc_string (imp_name, namelen + prefixlen);
+  rtl = gen_rtx_SYMBOL_REF (Pmode, name);
+  SET_SYMBOL_REF_DECL (rtl, to);
+  SYMBOL_REF_FLAGS (rtl) = SYMBOL_FLAG_LOCAL | SYMBOL_FLAG_STUBVAR;
+  if (!beimport)
+    {
+      SYMBOL_REF_FLAGS (rtl) |= SYMBOL_FLAG_EXTERNAL;
+#ifdef SUB_TARGET_RECORD_STUB
+      SUB_TARGET_RECORD_STUB (name);
+#endif
+    }      
+
+  rtl = gen_const_mem (Pmode, rtl);
+  set_mem_alias_set (rtl, ix86_GOT_alias_set ());
+
+  SET_DECL_RTL (to, rtl);
+  SET_DECL_ASSEMBLER_NAME (to, get_identifier (name));
+
+  return to;
+}
+
+/* Expand SYMBOL into its corresponding far-address symbol.
+   WANT_REG is true if we require the result be a register.  */
+
+static rtx
+legitimize_pe_coff_extern_decl (rtx symbol, bool want_reg)
+{
+  tree imp_decl;
+  rtx x;
+
+  gcc_assert (SYMBOL_REF_DECL (symbol));
+  imp_decl = get_dllimport_decl (SYMBOL_REF_DECL (symbol), false);
+
+  x = DECL_RTL (imp_decl);
+  if (want_reg)
+    x = force_reg (Pmode, x);
+  return x;
+}
+
+/* Expand SYMBOL into its corresponding dllimport symbol.  WANT_REG is
+   true if we require the result be a register.  */
+
+static rtx
+legitimize_dllimport_symbol (rtx symbol, bool want_reg)
+{
+  tree imp_decl;
+  rtx x;
+
+  gcc_assert (SYMBOL_REF_DECL (symbol));
+  imp_decl = get_dllimport_decl (SYMBOL_REF_DECL (symbol), true);
+
+  x = DECL_RTL (imp_decl);
+  if (want_reg)
+    x = force_reg (Pmode, x);
+  return x;
+}
+
+/* Expand SYMBOL into its corresponding dllimport or refptr symbol.  WANT_REG 
+   is true if we require the result be a register.  */
+
+rtx
+legitimize_pe_coff_symbol (rtx addr, bool inreg)
+{
+  if (!TARGET_PECOFF)
+    return NULL_RTX;
+
+  if (TARGET_DLLIMPORT_DECL_ATTRIBUTES)
+    {
+      if (GET_CODE (addr) == SYMBOL_REF && SYMBOL_REF_DLLIMPORT_P (addr))
+	return legitimize_dllimport_symbol (addr, inreg);
+      if (GET_CODE (addr) == CONST
+	  && GET_CODE (XEXP (addr, 0)) == PLUS
+	  && GET_CODE (XEXP (XEXP (addr, 0), 0)) == SYMBOL_REF
+	  && SYMBOL_REF_DLLIMPORT_P (XEXP (XEXP (addr, 0), 0)))
+	{
+	  rtx t = legitimize_dllimport_symbol (XEXP (XEXP (addr, 0), 0), inreg);
+	  return gen_rtx_PLUS (Pmode, t, XEXP (XEXP (addr, 0), 1));
+	}
+    }
+
+  if (ix86_cmodel != CM_LARGE_PIC && ix86_cmodel != CM_MEDIUM_PIC)
+    return NULL_RTX;
+  if (GET_CODE (addr) == SYMBOL_REF
+      && !is_imported_p (addr)
+      && SYMBOL_REF_EXTERNAL_P (addr)
+      && SYMBOL_REF_DECL (addr))
+    return legitimize_pe_coff_extern_decl (addr, inreg);
+
+  if (GET_CODE (addr) == CONST
+      && GET_CODE (XEXP (addr, 0)) == PLUS
+      && GET_CODE (XEXP (XEXP (addr, 0), 0)) == SYMBOL_REF
+      && !is_imported_p (XEXP (XEXP (addr, 0), 0))
+      && SYMBOL_REF_EXTERNAL_P (XEXP (XEXP (addr, 0), 0))
+      && SYMBOL_REF_DECL (XEXP (XEXP (addr, 0), 0)))
+    {
+      rtx t = legitimize_pe_coff_extern_decl (XEXP (XEXP (addr, 0), 0), inreg);
+      return gen_rtx_PLUS (Pmode, t, XEXP (XEXP (addr, 0), 1));
+    }
+  return NULL_RTX;
+}
+
+/* Try machine-dependent ways of modifying an illegitimate address
+   to be legitimate.  If we find one, return the new, valid address.
+   This macro is used in only one place: `memory_address' in explow.c.
+
+   OLDX is the address as it was before break_out_memory_refs was called.
+   In some cases it is useful to look at this to decide what needs to be done.
+
+   It is always safe for this macro to do nothing.  It exists to recognize
+   opportunities to optimize the output.
+
+   For the 80386, we handle X+REG by loading X into a register R and
+   using R+REG.  R will go in a general reg and indexing will be used.
+   However, if REG is a broken-out memory address or multiplication,
+   nothing needs to be done because REG can certainly go in a general reg.
+
+   When -fpic is used, special handling is needed for symbolic references.
+   See comments by legitimize_pic_address in i386.c for details.  */
+
+static rtx
+ix86_legitimize_address (rtx x, rtx, machine_mode mode)
+{
+  bool changed = false;
+  unsigned log;
+
+  log = GET_CODE (x) == SYMBOL_REF ? SYMBOL_REF_TLS_MODEL (x) : 0;
+  if (log)
+    return legitimize_tls_address (x, (enum tls_model) log, false);
+  if (GET_CODE (x) == CONST
+      && GET_CODE (XEXP (x, 0)) == PLUS
+      && GET_CODE (XEXP (XEXP (x, 0), 0)) == SYMBOL_REF
+      && (log = SYMBOL_REF_TLS_MODEL (XEXP (XEXP (x, 0), 0))))
+    {
+      rtx t = legitimize_tls_address (XEXP (XEXP (x, 0), 0),
+				      (enum tls_model) log, false);
+      return gen_rtx_PLUS (Pmode, t, XEXP (XEXP (x, 0), 1));
+    }
+
+  if (TARGET_DLLIMPORT_DECL_ATTRIBUTES)
+    {
+      rtx tmp = legitimize_pe_coff_symbol (x, true);
+      if (tmp)
+        return tmp;
+    }
+
+  if (flag_pic && SYMBOLIC_CONST (x))
+    return legitimize_pic_address (x, 0);
+
+#if TARGET_MACHO
+  if (MACHO_DYNAMIC_NO_PIC_P && SYMBOLIC_CONST (x))
+    return machopic_indirect_data_reference (x, 0);
+#endif
+
+  /* Canonicalize shifts by 0, 1, 2, 3 into multiply */
+  if (GET_CODE (x) == ASHIFT
+      && CONST_INT_P (XEXP (x, 1))
+      && (unsigned HOST_WIDE_INT) INTVAL (XEXP (x, 1)) < 4)
+    {
+      changed = true;
+      log = INTVAL (XEXP (x, 1));
+      x = gen_rtx_MULT (Pmode, force_reg (Pmode, XEXP (x, 0)),
+			GEN_INT (1 << log));
+    }
+
+  if (GET_CODE (x) == PLUS)
+    {
+      /* Canonicalize shifts by 0, 1, 2, 3 into multiply.  */
+
+      if (GET_CODE (XEXP (x, 0)) == ASHIFT
+	  && CONST_INT_P (XEXP (XEXP (x, 0), 1))
+	  && (unsigned HOST_WIDE_INT) INTVAL (XEXP (XEXP (x, 0), 1)) < 4)
+	{
+	  changed = true;
+	  log = INTVAL (XEXP (XEXP (x, 0), 1));
+	  XEXP (x, 0) = gen_rtx_MULT (Pmode,
+				      force_reg (Pmode, XEXP (XEXP (x, 0), 0)),
+				      GEN_INT (1 << log));
+	}
+
+      if (GET_CODE (XEXP (x, 1)) == ASHIFT
+	  && CONST_INT_P (XEXP (XEXP (x, 1), 1))
+	  && (unsigned HOST_WIDE_INT) INTVAL (XEXP (XEXP (x, 1), 1)) < 4)
+	{
+	  changed = true;
+	  log = INTVAL (XEXP (XEXP (x, 1), 1));
+	  XEXP (x, 1) = gen_rtx_MULT (Pmode,
+				      force_reg (Pmode, XEXP (XEXP (x, 1), 0)),
+				      GEN_INT (1 << log));
+	}
+
+      /* Put multiply first if it isn't already.  */
+      if (GET_CODE (XEXP (x, 1)) == MULT)
+	{
+	  std::swap (XEXP (x, 0), XEXP (x, 1));
+	  changed = true;
+	}
+
+      /* Canonicalize (plus (mult (reg) (const)) (plus (reg) (const)))
+	 into (plus (plus (mult (reg) (const)) (reg)) (const)).  This can be
+	 created by virtual register instantiation, register elimination, and
+	 similar optimizations.  */
+      if (GET_CODE (XEXP (x, 0)) == MULT && GET_CODE (XEXP (x, 1)) == PLUS)
+	{
+	  changed = true;
+	  x = gen_rtx_PLUS (Pmode,
+			    gen_rtx_PLUS (Pmode, XEXP (x, 0),
+					  XEXP (XEXP (x, 1), 0)),
+			    XEXP (XEXP (x, 1), 1));
+	}
+
+      /* Canonicalize
+	 (plus (plus (mult (reg) (const)) (plus (reg) (const))) const)
+	 into (plus (plus (mult (reg) (const)) (reg)) (const)).  */
+      else if (GET_CODE (x) == PLUS && GET_CODE (XEXP (x, 0)) == PLUS
+	       && GET_CODE (XEXP (XEXP (x, 0), 0)) == MULT
+	       && GET_CODE (XEXP (XEXP (x, 0), 1)) == PLUS
+	       && CONSTANT_P (XEXP (x, 1)))
+	{
+	  rtx constant;
+	  rtx other = NULL_RTX;
+
+	  if (CONST_INT_P (XEXP (x, 1)))
+	    {
+	      constant = XEXP (x, 1);
+	      other = XEXP (XEXP (XEXP (x, 0), 1), 1);
+	    }
+	  else if (CONST_INT_P (XEXP (XEXP (XEXP (x, 0), 1), 1)))
+	    {
+	      constant = XEXP (XEXP (XEXP (x, 0), 1), 1);
+	      other = XEXP (x, 1);
+	    }
+	  else
+	    constant = 0;
+
+	  if (constant)
+	    {
+	      changed = true;
+	      x = gen_rtx_PLUS (Pmode,
+				gen_rtx_PLUS (Pmode, XEXP (XEXP (x, 0), 0),
+					      XEXP (XEXP (XEXP (x, 0), 1), 0)),
+				plus_constant (Pmode, other,
+					       INTVAL (constant)));
+	    }
+	}
+
+      if (changed && ix86_legitimate_address_p (mode, x, false))
+	return x;
+
+      if (GET_CODE (XEXP (x, 0)) == MULT)
+	{
+	  changed = true;
+	  XEXP (x, 0) = copy_addr_to_reg (XEXP (x, 0));
+	}
+
+      if (GET_CODE (XEXP (x, 1)) == MULT)
+	{
+	  changed = true;
+	  XEXP (x, 1) = copy_addr_to_reg (XEXP (x, 1));
+	}
+
+      if (changed
+	  && REG_P (XEXP (x, 1))
+	  && REG_P (XEXP (x, 0)))
+	return x;
+
+      if (flag_pic && SYMBOLIC_CONST (XEXP (x, 1)))
+	{
+	  changed = true;
+	  x = legitimize_pic_address (x, 0);
+	}
+
+      if (changed && ix86_legitimate_address_p (mode, x, false))
+	return x;
+
+      if (REG_P (XEXP (x, 0)))
+	{
+	  rtx temp = gen_reg_rtx (Pmode);
+	  rtx val  = force_operand (XEXP (x, 1), temp);
+	  if (val != temp)
+	    {
+	      val = convert_to_mode (Pmode, val, 1);
+	      emit_move_insn (temp, val);
+	    }
+
+	  XEXP (x, 1) = temp;
+	  return x;
+	}
+
+      else if (REG_P (XEXP (x, 1)))
+	{
+	  rtx temp = gen_reg_rtx (Pmode);
+	  rtx val  = force_operand (XEXP (x, 0), temp);
+	  if (val != temp)
+	    {
+	      val = convert_to_mode (Pmode, val, 1);
+	      emit_move_insn (temp, val);
+	    }
+
+	  XEXP (x, 0) = temp;
+	  return x;
+	}
+    }
+
+  return x;
+}
+
+/* Print an integer constant expression in assembler syntax.  Addition
+   and subtraction are the only arithmetic that may appear in these
+   expressions.  FILE is the stdio stream to write to, X is the rtx, and
+   CODE is the operand print code from the output string.  */
+
+static void
+output_pic_addr_const (FILE *file, rtx x, int code)
+{
+  char buf[256];
+
+  switch (GET_CODE (x))
+    {
+    case PC:
+      gcc_assert (flag_pic);
+      putc ('.', file);
+      break;
+
+    case SYMBOL_REF:
+      if (TARGET_64BIT || ! TARGET_MACHO_SYMBOL_STUBS)
+	output_addr_const (file, x);
+      else
+	{
+	  const char *name = XSTR (x, 0);
+
+	  /* Mark the decl as referenced so that cgraph will
+	     output the function.  */
+	  if (SYMBOL_REF_DECL (x))
+	    mark_decl_referenced (SYMBOL_REF_DECL (x));
+
+#if TARGET_MACHO
+	  if (MACHOPIC_INDIRECT
+	      && machopic_classify_symbol (x) == MACHOPIC_UNDEFINED_FUNCTION)
+	    name = machopic_indirection_name (x, /*stub_p=*/true);
+#endif
+	  assemble_name (file, name);
+	}
+      if (!TARGET_MACHO && !(TARGET_64BIT && TARGET_PECOFF)
+	  && code == 'P' && ! SYMBOL_REF_LOCAL_P (x))
+	fputs ("@PLT", file);
+      break;
+
+    case LABEL_REF:
+      x = XEXP (x, 0);
+      /* FALLTHRU */
+    case CODE_LABEL:
+      ASM_GENERATE_INTERNAL_LABEL (buf, "L", CODE_LABEL_NUMBER (x));
+      assemble_name (asm_out_file, buf);
+      break;
+
+    case CONST_INT:
+      fprintf (file, HOST_WIDE_INT_PRINT_DEC, INTVAL (x));
+      break;
+
+    case CONST:
+      /* This used to output parentheses around the expression,
+	 but that does not work on the 386 (either ATT or BSD assembler).  */
+      output_pic_addr_const (file, XEXP (x, 0), code);
+      break;
+
+    case CONST_DOUBLE:
+      /* We can't handle floating point constants;
+	 TARGET_PRINT_OPERAND must handle them.  */
+      output_operand_lossage ("floating constant misused");
+      break;
+
+    case PLUS:
+      /* Some assemblers need integer constants to appear first.  */
+      if (CONST_INT_P (XEXP (x, 0)))
+	{
+	  output_pic_addr_const (file, XEXP (x, 0), code);
+	  putc ('+', file);
+	  output_pic_addr_const (file, XEXP (x, 1), code);
+	}
+      else
+	{
+	  gcc_assert (CONST_INT_P (XEXP (x, 1)));
+	  output_pic_addr_const (file, XEXP (x, 1), code);
+	  putc ('+', file);
+	  output_pic_addr_const (file, XEXP (x, 0), code);
+	}
+      break;
+
+    case MINUS:
+      if (!TARGET_MACHO)
+	putc (ASSEMBLER_DIALECT == ASM_INTEL ? '(' : '[', file);
+      output_pic_addr_const (file, XEXP (x, 0), code);
+      putc ('-', file);
+      output_pic_addr_const (file, XEXP (x, 1), code);
+      if (!TARGET_MACHO)
+	putc (ASSEMBLER_DIALECT == ASM_INTEL ? ')' : ']', file);
+      break;
+
+    case UNSPEC:
+      gcc_assert (XVECLEN (x, 0) == 1);
+      output_pic_addr_const (file, XVECEXP (x, 0, 0), code);
+      switch (XINT (x, 1))
+	{
+	case UNSPEC_GOT:
+	  fputs ("@GOT", file);
+	  break;
+	case UNSPEC_GOTOFF:
+	  fputs ("@GOTOFF", file);
+	  break;
+	case UNSPEC_PLTOFF:
+	  fputs ("@PLTOFF", file);
+	  break;
+	case UNSPEC_PCREL:
+	  fputs (ASSEMBLER_DIALECT == ASM_ATT ?
+		 "(%rip)" : "[rip]", file);
+	  break;
+	case UNSPEC_GOTPCREL:
+	  fputs (ASSEMBLER_DIALECT == ASM_ATT ?
+		 "@GOTPCREL(%rip)" : "@GOTPCREL[rip]", file);
+	  break;
+	case UNSPEC_GOTTPOFF:
+	  /* FIXME: This might be @TPOFF in Sun ld too.  */
+	  fputs ("@gottpoff", file);
+	  break;
+	case UNSPEC_TPOFF:
+	  fputs ("@tpoff", file);
+	  break;
+	case UNSPEC_NTPOFF:
+	  if (TARGET_64BIT)
+	    fputs ("@tpoff", file);
+	  else
+	    fputs ("@ntpoff", file);
+	  break;
+	case UNSPEC_DTPOFF:
+	  fputs ("@dtpoff", file);
+	  break;
+	case UNSPEC_GOTNTPOFF:
+	  if (TARGET_64BIT)
+	    fputs (ASSEMBLER_DIALECT == ASM_ATT ?
+		   "@gottpoff(%rip)": "@gottpoff[rip]", file);
+	  else
+	    fputs ("@gotntpoff", file);
+	  break;
+	case UNSPEC_INDNTPOFF:
+	  fputs ("@indntpoff", file);
+	  break;
+#if TARGET_MACHO
+	case UNSPEC_MACHOPIC_OFFSET:
+	  putc ('-', file);
+	  machopic_output_function_base_name (file);
+	  break;
+#endif
+	default:
+	  output_operand_lossage ("invalid UNSPEC as operand");
+	  break;
+	}
+       break;
+
+    default:
+      output_operand_lossage ("invalid expression as operand");
+    }
+}
+
+/* This is called from dwarf2out.c via TARGET_ASM_OUTPUT_DWARF_DTPREL.
+   We need to emit DTP-relative relocations.  */
+
+static void ATTRIBUTE_UNUSED
+i386_output_dwarf_dtprel (FILE *file, int size, rtx x)
+{
+  fputs (ASM_LONG, file);
+  output_addr_const (file, x);
+  fputs ("@dtpoff", file);
+  switch (size)
+    {
+    case 4:
+      break;
+    case 8:
+      fputs (", 0", file);
+      break;
+    default:
+      gcc_unreachable ();
+   }
+}
+
+/* Return true if X is a representation of the PIC register.  This copes
+   with calls from ix86_find_base_term, where the register might have
+   been replaced by a cselib value.  */
+
+static bool
+ix86_pic_register_p (rtx x)
+{
+  if (GET_CODE (x) == VALUE && CSELIB_VAL_PTR (x))
+    return (pic_offset_table_rtx
+	    && rtx_equal_for_cselib_p (x, pic_offset_table_rtx));
+  else if (GET_CODE (x) == UNSPEC && XINT (x, 1) == UNSPEC_SET_GOT)
+    return true;
+  else if (!REG_P (x))
+    return false;
+  else if (pic_offset_table_rtx)
+    {
+      if (REGNO (x) == REGNO (pic_offset_table_rtx))
+	return true;
+      if (HARD_REGISTER_P (x)
+	  && !HARD_REGISTER_P (pic_offset_table_rtx)
+	  && ORIGINAL_REGNO (x) == REGNO (pic_offset_table_rtx))
+	return true;
+      return false;
+    }
+  else
+    return REGNO (x) == PIC_OFFSET_TABLE_REGNUM;
+}
+
+/* Helper function for ix86_delegitimize_address.
+   Attempt to delegitimize TLS local-exec accesses.  */
+
+static rtx
+ix86_delegitimize_tls_address (rtx orig_x)
+{
+  rtx x = orig_x, unspec;
+  struct ix86_address addr;
+
+  if (!TARGET_TLS_DIRECT_SEG_REFS)
+    return orig_x;
+  if (MEM_P (x))
+    x = XEXP (x, 0);
+  if (GET_CODE (x) != PLUS || GET_MODE (x) != Pmode)
+    return orig_x;
+  if (ix86_decompose_address (x, &addr) == 0
+      || addr.seg != DEFAULT_TLS_SEG_REG
+      || addr.disp == NULL_RTX
+      || GET_CODE (addr.disp) != CONST)
+    return orig_x;
+  unspec = XEXP (addr.disp, 0);
+  if (GET_CODE (unspec) == PLUS && CONST_INT_P (XEXP (unspec, 1)))
+    unspec = XEXP (unspec, 0);
+  if (GET_CODE (unspec) != UNSPEC || XINT (unspec, 1) != UNSPEC_NTPOFF)
+    return orig_x;
+  x = XVECEXP (unspec, 0, 0);
+  gcc_assert (GET_CODE (x) == SYMBOL_REF);
+  if (unspec != XEXP (addr.disp, 0))
+    x = gen_rtx_PLUS (Pmode, x, XEXP (XEXP (addr.disp, 0), 1));
+  if (addr.index)
+    {
+      rtx idx = addr.index;
+      if (addr.scale != 1)
+	idx = gen_rtx_MULT (Pmode, idx, GEN_INT (addr.scale));
+      x = gen_rtx_PLUS (Pmode, idx, x);
+    }
+  if (addr.base)
+    x = gen_rtx_PLUS (Pmode, addr.base, x);
+  if (MEM_P (orig_x))
+    x = replace_equiv_address_nv (orig_x, x);
+  return x;
+}
+
+/* In the name of slightly smaller debug output, and to cater to
+   general assembler lossage, recognize PIC+GOTOFF and turn it back
+   into a direct symbol reference.
+
+   On Darwin, this is necessary to avoid a crash, because Darwin
+   has a different PIC label for each routine but the DWARF debugging
+   information is not associated with any particular routine, so it's
+   necessary to remove references to the PIC label from RTL stored by
+   the DWARF output code.
+
+   This helper is used in the normal ix86_delegitimize_address
+   entrypoint (e.g. used in the target delegitimization hook) and
+   in ix86_find_base_term.  As compile time memory optimization, we
+   avoid allocating rtxes that will not change anything on the outcome
+   of the callers (find_base_value and find_base_term).  */
+
+static inline rtx
+ix86_delegitimize_address_1 (rtx x, bool base_term_p)
+{
+  rtx orig_x = delegitimize_mem_from_attrs (x);
+  /* addend is NULL or some rtx if x is something+GOTOFF where
+     something doesn't include the PIC register.  */
+  rtx addend = NULL_RTX;
+  /* reg_addend is NULL or a multiple of some register.  */
+  rtx reg_addend = NULL_RTX;
+  /* const_addend is NULL or a const_int.  */
+  rtx const_addend = NULL_RTX;
+  /* This is the result, or NULL.  */
+  rtx result = NULL_RTX;
+
+  x = orig_x;
+
+  if (MEM_P (x))
+    x = XEXP (x, 0);
+
+  if (TARGET_64BIT)
+    {
+      if (GET_CODE (x) == CONST
+          && GET_CODE (XEXP (x, 0)) == PLUS
+          && GET_MODE (XEXP (x, 0)) == Pmode
+          && CONST_INT_P (XEXP (XEXP (x, 0), 1))
+          && GET_CODE (XEXP (XEXP (x, 0), 0)) == UNSPEC
+          && XINT (XEXP (XEXP (x, 0), 0), 1) == UNSPEC_PCREL)
+        {
+	  /* find_base_{value,term} only care about MEMs with arg_pointer_rtx
+	     base.  A CONST can't be arg_pointer_rtx based.  */
+	  if (base_term_p && MEM_P (orig_x))
+	    return orig_x;
+	  rtx x2 = XVECEXP (XEXP (XEXP (x, 0), 0), 0, 0);
+	  x = gen_rtx_PLUS (Pmode, XEXP (XEXP (x, 0), 1), x2);
+	  if (MEM_P (orig_x))
+	    x = replace_equiv_address_nv (orig_x, x);
+	  return x;
+	}
+
+      if (GET_CODE (x) == CONST
+	  && GET_CODE (XEXP (x, 0)) == UNSPEC
+	  && (XINT (XEXP (x, 0), 1) == UNSPEC_GOTPCREL
+	      || XINT (XEXP (x, 0), 1) == UNSPEC_PCREL)
+	  && (MEM_P (orig_x) || XINT (XEXP (x, 0), 1) == UNSPEC_PCREL))
+	{
+	  x = XVECEXP (XEXP (x, 0), 0, 0);
+	  if (GET_MODE (orig_x) != GET_MODE (x) && MEM_P (orig_x))
+	    {
+	      x = lowpart_subreg (GET_MODE (orig_x), x, GET_MODE (x));
+	      if (x == NULL_RTX)
+		return orig_x;
+	    }
+	  return x;
+	}
+
+      if (ix86_cmodel != CM_MEDIUM_PIC && ix86_cmodel != CM_LARGE_PIC)
+	return ix86_delegitimize_tls_address (orig_x);
+
+      /* Fall thru into the code shared with -m32 for -mcmodel=large -fpic
+	 and -mcmodel=medium -fpic.  */
+    }
+
+  if (GET_CODE (x) != PLUS
+      || GET_CODE (XEXP (x, 1)) != CONST)
+    return ix86_delegitimize_tls_address (orig_x);
+
+  if (ix86_pic_register_p (XEXP (x, 0)))
+    /* %ebx + GOT/GOTOFF */
+    ;
+  else if (GET_CODE (XEXP (x, 0)) == PLUS)
+    {
+      /* %ebx + %reg * scale + GOT/GOTOFF */
+      reg_addend = XEXP (x, 0);
+      if (ix86_pic_register_p (XEXP (reg_addend, 0)))
+	reg_addend = XEXP (reg_addend, 1);
+      else if (ix86_pic_register_p (XEXP (reg_addend, 1)))
+	reg_addend = XEXP (reg_addend, 0);
+      else
+	{
+	  reg_addend = NULL_RTX;
+	  addend = XEXP (x, 0);
+	}
+    }
+  else
+    addend = XEXP (x, 0);
+
+  x = XEXP (XEXP (x, 1), 0);
+  if (GET_CODE (x) == PLUS
+      && CONST_INT_P (XEXP (x, 1)))
+    {
+      const_addend = XEXP (x, 1);
+      x = XEXP (x, 0);
+    }
+
+  if (GET_CODE (x) == UNSPEC
+      && ((XINT (x, 1) == UNSPEC_GOT && MEM_P (orig_x) && !addend)
+	  || (XINT (x, 1) == UNSPEC_GOTOFF && !MEM_P (orig_x))
+	  || (XINT (x, 1) == UNSPEC_PLTOFF && ix86_cmodel == CM_LARGE_PIC
+	      && !MEM_P (orig_x) && !addend)))
+    result = XVECEXP (x, 0, 0);
+
+  if (!TARGET_64BIT && TARGET_MACHO && darwin_local_data_pic (x)
+      && !MEM_P (orig_x))
+    result = XVECEXP (x, 0, 0);
+
+  if (! result)
+    return ix86_delegitimize_tls_address (orig_x);
+
+  /* For (PLUS something CONST_INT) both find_base_{value,term} just
+     recurse on the first operand.  */
+  if (const_addend && !base_term_p)
+    result = gen_rtx_CONST (Pmode, gen_rtx_PLUS (Pmode, result, const_addend));
+  if (reg_addend)
+    result = gen_rtx_PLUS (Pmode, reg_addend, result);
+  if (addend)
+    {
+      /* If the rest of original X doesn't involve the PIC register, add
+	 addend and subtract pic_offset_table_rtx.  This can happen e.g.
+	 for code like:
+	 leal (%ebx, %ecx, 4), %ecx
+	 ...
+	 movl foo@GOTOFF(%ecx), %edx
+	 in which case we return (%ecx - %ebx) + foo
+	 or (%ecx - _GLOBAL_OFFSET_TABLE_) + foo if pseudo_pic_reg
+	 and reload has completed.  Don't do the latter for debug,
+	 as _GLOBAL_OFFSET_TABLE_ can't be expressed in the assembly.  */
+      if (pic_offset_table_rtx
+	  && (!reload_completed || !ix86_use_pseudo_pic_reg ()))
+        result = gen_rtx_PLUS (Pmode, gen_rtx_MINUS (Pmode, copy_rtx (addend),
+						     pic_offset_table_rtx),
+			       result);
+      else if (base_term_p
+	       && pic_offset_table_rtx
+	       && !TARGET_MACHO
+	       && !TARGET_VXWORKS_RTP)
+	{
+	  rtx tmp = gen_rtx_SYMBOL_REF (Pmode, GOT_SYMBOL_NAME);
+	  tmp = gen_rtx_MINUS (Pmode, copy_rtx (addend), tmp);
+	  result = gen_rtx_PLUS (Pmode, tmp, result);
+	}
+      else
+	return orig_x;
+    }
+  if (GET_MODE (orig_x) != Pmode && MEM_P (orig_x))
+    {
+      result = lowpart_subreg (GET_MODE (orig_x), result, Pmode);
+      if (result == NULL_RTX)
+	return orig_x;
+    }
+  return result;
+}
+
+/* The normal instantiation of the above template.  */
+
+static rtx
+ix86_delegitimize_address (rtx x)
+{
+  return ix86_delegitimize_address_1 (x, false);
+}
+
+/* If X is a machine specific address (i.e. a symbol or label being
+   referenced as a displacement from the GOT implemented using an
+   UNSPEC), then return the base term.  Otherwise return X.  */
+
+rtx
+ix86_find_base_term (rtx x)
+{
+  rtx term;
+
+  if (TARGET_64BIT)
+    {
+      if (GET_CODE (x) != CONST)
+	return x;
+      term = XEXP (x, 0);
+      if (GET_CODE (term) == PLUS
+	  && CONST_INT_P (XEXP (term, 1)))
+	term = XEXP (term, 0);
+      if (GET_CODE (term) != UNSPEC
+	  || (XINT (term, 1) != UNSPEC_GOTPCREL
+	      && XINT (term, 1) != UNSPEC_PCREL))
+	return x;
+
+      return XVECEXP (term, 0, 0);
+    }
+
+  return ix86_delegitimize_address_1 (x, true);
+}
+
+/* Return true if X shouldn't be emitted into the debug info.
+   Disallow UNSPECs other than @gotoff - we can't emit _GLOBAL_OFFSET_TABLE_
+   symbol easily into the .debug_info section, so we need not to
+   delegitimize, but instead assemble as @gotoff.
+   Disallow _GLOBAL_OFFSET_TABLE_ SYMBOL_REF - the assembler magically
+   assembles that as _GLOBAL_OFFSET_TABLE_-. expression.  */
+
+static bool
+ix86_const_not_ok_for_debug_p (rtx x)
+{
+  if (GET_CODE (x) == UNSPEC && XINT (x, 1) != UNSPEC_GOTOFF)
+    return true;
+
+  if (SYMBOL_REF_P (x) && strcmp (XSTR (x, 0), GOT_SYMBOL_NAME) == 0)
+    return true;
+
+  return false;
+}
+
+static void
+put_condition_code (enum rtx_code code, machine_mode mode, bool reverse,
+		    bool fp, FILE *file)
+{
+  const char *suffix;
+
+  if (mode == CCFPmode)
+    {
+      code = ix86_fp_compare_code_to_integer (code);
+      mode = CCmode;
+    }
+  if (reverse)
+    code = reverse_condition (code);
+
+  switch (code)
+    {
+    case EQ:
+      gcc_assert (mode != CCGZmode);
+      switch (mode)
+	{
+	case E_CCAmode:
+	  suffix = "a";
+	  break;
+	case E_CCCmode:
+	  suffix = "c";
+	  break;
+	case E_CCOmode:
+	  suffix = "o";
+	  break;
+	case E_CCPmode:
+	  suffix = "p";
+	  break;
+	case E_CCSmode:
+	  suffix = "s";
+	  break;
+	default:
+	  suffix = "e";
+	  break;
+	}
+      break;
+    case NE:
+      gcc_assert (mode != CCGZmode);
+      switch (mode)
+	{
+	case E_CCAmode:
+	  suffix = "na";
+	  break;
+	case E_CCCmode:
+	  suffix = "nc";
+	  break;
+	case E_CCOmode:
+	  suffix = "no";
+	  break;
+	case E_CCPmode:
+	  suffix = "np";
+	  break;
+	case E_CCSmode:
+	  suffix = "ns";
+	  break;
+	default:
+	  suffix = "ne";
+	  break;
+	}
+      break;
+    case GT:
+      gcc_assert (mode == CCmode || mode == CCNOmode || mode == CCGCmode);
+      suffix = "g";
+      break;
+    case GTU:
+      /* ??? Use "nbe" instead of "a" for fcmov lossage on some assemblers.
+	 Those same assemblers have the same but opposite lossage on cmov.  */
+      if (mode == CCmode)
+	suffix = fp ? "nbe" : "a";
+      else
+	gcc_unreachable ();
+      break;
+    case LT:
+      switch (mode)
+	{
+	case E_CCNOmode:
+	case E_CCGOCmode:
+	  suffix = "s";
+	  break;
+
+	case E_CCmode:
+	case E_CCGCmode:
+	case E_CCGZmode:
+	  suffix = "l";
+	  break;
+
+	default:
+	  gcc_unreachable ();
+	}
+      break;
+    case LTU:
+      if (mode == CCmode || mode == CCGZmode)
+	suffix = "b";
+      else if (mode == CCCmode)
+	suffix = fp ? "b" : "c";
+      else
+	gcc_unreachable ();
+      break;
+    case GE:
+      switch (mode)
+	{
+	case E_CCNOmode:
+	case E_CCGOCmode:
+	  suffix = "ns";
+	  break;
+
+	case E_CCmode:
+	case E_CCGCmode:
+	case E_CCGZmode:
+	  suffix = "ge";
+	  break;
+
+	default:
+	  gcc_unreachable ();
+	}
+      break;
+    case GEU:
+      if (mode == CCmode || mode == CCGZmode)
+	suffix = "nb";
+      else if (mode == CCCmode)
+	suffix = fp ? "nb" : "nc";
+      else
+	gcc_unreachable ();
+      break;
+    case LE:
+      gcc_assert (mode == CCmode || mode == CCGCmode || mode == CCNOmode);
+      suffix = "le";
+      break;
+    case LEU:
+      if (mode == CCmode)
+	suffix = "be";
+      else
+	gcc_unreachable ();
+      break;
+    case UNORDERED:
+      suffix = fp ? "u" : "p";
+      break;
+    case ORDERED:
+      suffix = fp ? "nu" : "np";
+      break;
+    default:
+      gcc_unreachable ();
+    }
+  fputs (suffix, file);
+}
+
+/* Print the name of register X to FILE based on its machine mode and number.
+   If CODE is 'w', pretend the mode is HImode.
+   If CODE is 'b', pretend the mode is QImode.
+   If CODE is 'k', pretend the mode is SImode.
+   If CODE is 'q', pretend the mode is DImode.
+   If CODE is 'x', pretend the mode is V4SFmode.
+   If CODE is 't', pretend the mode is V8SFmode.
+   If CODE is 'g', pretend the mode is V16SFmode.
+   If CODE is 'h', pretend the reg is the 'high' byte register.
+   If CODE is 'y', print "st(0)" instead of "st", if the reg is stack op.
+   If CODE is 'd', duplicate the operand for AVX instruction.
+   If CODE is 'V', print naked full integer register name without %.
+ */
+
+void
+print_reg (rtx x, int code, FILE *file)
+{
+  const char *reg;
+  int msize;
+  unsigned int regno;
+  bool duplicated;
+
+  if (ASSEMBLER_DIALECT == ASM_ATT && code != 'V')
+    putc ('%', file);
+
+  if (x == pc_rtx)
+    {
+      gcc_assert (TARGET_64BIT);
+      fputs ("rip", file);
+      return;
+    }
+
+  if (code == 'y' && STACK_TOP_P (x))
+    {
+      fputs ("st(0)", file);
+      return;
+    }
+
+  if (code == 'w')
+    msize = 2;
+  else if (code == 'b')
+    msize = 1;
+  else if (code == 'k')
+    msize = 4;
+  else if (code == 'q')
+    msize = 8;
+  else if (code == 'h')
+    msize = 0;
+  else if (code == 'x')
+    msize = 16;
+  else if (code == 't')
+    msize = 32;
+  else if (code == 'g')
+    msize = 64;
+  else
+    msize = GET_MODE_SIZE (GET_MODE (x));
+
+  regno = REGNO (x);
+
+  if (regno == ARG_POINTER_REGNUM
+      || regno == FRAME_POINTER_REGNUM
+      || regno == FPSR_REG)
+    {
+      output_operand_lossage
+	("invalid use of register '%s'", reg_names[regno]);
+      return;
+    }
+  else if (regno == FLAGS_REG)
+    {
+      output_operand_lossage ("invalid use of asm flag output");
+      return;
+    }
+
+  if (code == 'V')
+    {
+      if (GENERAL_REGNO_P (regno))
+	msize = GET_MODE_SIZE (word_mode);
+      else
+	error ("%<V%> modifier on non-integer register");
+    }
+
+  duplicated = code == 'd' && TARGET_AVX;
+
+  switch (msize)
+    {
+    case 16:
+    case 12:
+    case 8:
+      if (GENERAL_REGNO_P (regno) && msize > GET_MODE_SIZE (word_mode))
+	warning (0, "unsupported size for integer register");
+      /* FALLTHRU */
+    case 4:
+      if (LEGACY_INT_REGNO_P (regno))
+	putc (msize > 4 && TARGET_64BIT ? 'r' : 'e', file);
+      /* FALLTHRU */
+    case 2:
+    normal:
+      reg = hi_reg_name[regno];
+      break;
+    case 1:
+      if (regno >= ARRAY_SIZE (qi_reg_name))
+	goto normal;
+      if (!ANY_QI_REGNO_P (regno))
+	error ("unsupported size for integer register");
+      reg = qi_reg_name[regno];
+      break;
+    case 0:
+      if (regno >= ARRAY_SIZE (qi_high_reg_name))
+	goto normal;
+      reg = qi_high_reg_name[regno];
+      break;
+    case 32:
+    case 64:
+      if (SSE_REGNO_P (regno))
+	{
+	  gcc_assert (!duplicated);
+	  putc (msize == 32 ? 'y' : 'z', file);
+	  reg = hi_reg_name[regno] + 1;
+	  break;
+	}
+      goto normal;
+    default:
+      gcc_unreachable ();
+    }
+
+  fputs (reg, file);
+
+  /* Irritatingly, AMD extended registers use
+     different naming convention: "r%d[bwd]"  */
+  if (REX_INT_REGNO_P (regno))
+    {
+      gcc_assert (TARGET_64BIT);
+      switch (msize)
+	{
+	  case 0:
+	    error ("extended registers have no high halves");
+	    break;
+	  case 1:
+	    putc ('b', file);
+	    break;
+	  case 2:
+	    putc ('w', file);
+	    break;
+	  case 4:
+	    putc ('d', file);
+	    break;
+	  case 8:
+	    /* no suffix */
+	    break;
+	  default:
+	    error ("unsupported operand size for extended register");
+	    break;
+	}
+      return;
+    }
+
+  if (duplicated)
+    {
+      if (ASSEMBLER_DIALECT == ASM_ATT)
+	fprintf (file, ", %%%s", reg);
+      else
+	fprintf (file, ", %s", reg);
+    }
+}
+
+/* Meaning of CODE:
+   L,W,B,Q,S,T -- print the opcode suffix for specified size of operand.
+   C -- print opcode suffix for set/cmov insn.
+   c -- like C, but print reversed condition
+   F,f -- likewise, but for floating-point.
+   O -- if HAVE_AS_IX86_CMOV_SUN_SYNTAX, expand to "w.", "l." or "q.",
+	otherwise nothing
+   R -- print embedded rounding and sae.
+   r -- print only sae.
+   z -- print the opcode suffix for the size of the current operand.
+   Z -- likewise, with special suffixes for x87 instructions.
+   * -- print a star (in certain assembler syntax)
+   A -- print an absolute memory reference.
+   E -- print address with DImode register names if TARGET_64BIT.
+   w -- print the operand as if it's a "word" (HImode) even if it isn't.
+   s -- print a shift double count, followed by the assemblers argument
+	delimiter.
+   b -- print the QImode name of the register for the indicated operand.
+	%b0 would print %al if operands[0] is reg 0.
+   w --  likewise, print the HImode name of the register.
+   k --  likewise, print the SImode name of the register.
+   q --  likewise, print the DImode name of the register.
+   x --  likewise, print the V4SFmode name of the register.
+   t --  likewise, print the V8SFmode name of the register.
+   g --  likewise, print the V16SFmode name of the register.
+   h -- print the QImode name for a "high" register, either ah, bh, ch or dh.
+   y -- print "st(0)" instead of "st" as a register.
+   d -- print duplicated register operand for AVX instruction.
+   D -- print condition for SSE cmp instruction.
+   P -- if PIC, print an @PLT suffix.  For -fno-plt, load function
+	address from GOT.
+   p -- print raw symbol name.
+   X -- don't print any sort of PIC '@' suffix for a symbol.
+   & -- print some in-use local-dynamic symbol name.
+   H -- print a memory address offset by 8; used for sse high-parts
+   Y -- print condition for XOP pcom* instruction.
+   V -- print naked full integer register name without %.
+   + -- print a branch hint as 'cs' or 'ds' prefix
+   ; -- print a semicolon (after prefixes due to bug in older gas).
+   ~ -- print "i" if TARGET_AVX2, "f" otherwise.
+   ^ -- print addr32 prefix if TARGET_64BIT and Pmode != word_mode
+   M -- print addr32 prefix for TARGET_X32 with VSIB address.
+   ! -- print NOTRACK prefix for jxx/call/ret instructions if required.
+   N -- print maskz if it's constant 0 operand.
+ */
+
+void
+ix86_print_operand (FILE *file, rtx x, int code)
+{
+  if (code)
+    {
+      switch (code)
+	{
+	case 'A':
+	  switch (ASSEMBLER_DIALECT)
+	    {
+	    case ASM_ATT:
+	      putc ('*', file);
+	      break;
+
+	    case ASM_INTEL:
+	      /* Intel syntax. For absolute addresses, registers should not
+		 be surrounded by braces.  */
+	      if (!REG_P (x))
+		{
+		  putc ('[', file);
+		  ix86_print_operand (file, x, 0);
+		  putc (']', file);
+		  return;
+		}
+	      break;
+
+	    default:
+	      gcc_unreachable ();
+	    }
+
+	  ix86_print_operand (file, x, 0);
+	  return;
+
+	case 'E':
+	  /* Wrap address in an UNSPEC to declare special handling.  */
+	  if (TARGET_64BIT)
+	    x = gen_rtx_UNSPEC (DImode, gen_rtvec (1, x), UNSPEC_LEA_ADDR);
+
+	  output_address (VOIDmode, x);
+	  return;
+
+	case 'L':
+	  if (ASSEMBLER_DIALECT == ASM_ATT)
+	    putc ('l', file);
+	  return;
+
+	case 'W':
+	  if (ASSEMBLER_DIALECT == ASM_ATT)
+	    putc ('w', file);
+	  return;
+
+	case 'B':
+	  if (ASSEMBLER_DIALECT == ASM_ATT)
+	    putc ('b', file);
+	  return;
+
+	case 'Q':
+	  if (ASSEMBLER_DIALECT == ASM_ATT)
+	    putc ('l', file);
+	  return;
+
+	case 'S':
+	  if (ASSEMBLER_DIALECT == ASM_ATT)
+	    putc ('s', file);
+	  return;
+
+	case 'T':
+	  if (ASSEMBLER_DIALECT == ASM_ATT)
+	    putc ('t', file);
+	  return;
+
+	case 'O':
+#ifdef HAVE_AS_IX86_CMOV_SUN_SYNTAX
+	  if (ASSEMBLER_DIALECT != ASM_ATT)
+	    return;
+
+	  switch (GET_MODE_SIZE (GET_MODE (x)))
+	    {
+	    case 2:
+	      putc ('w', file);
+	      break;
+  
+	    case 4:
+	      putc ('l', file);
+	      break;
+
+	    case 8:
+	      putc ('q', file);
+	      break;
+
+	    default:
+	      output_operand_lossage ("invalid operand size for operand "
+				      "code 'O'");
+	      return;
+	    }
+
+	  putc ('.', file);
+#endif
+	  return;
+
+	case 'z':
+	  if (GET_MODE_CLASS (GET_MODE (x)) == MODE_INT)
+	    {
+	      /* Opcodes don't get size suffixes if using Intel opcodes.  */
+	      if (ASSEMBLER_DIALECT == ASM_INTEL)
+		return;
+
+	      switch (GET_MODE_SIZE (GET_MODE (x)))
+		{
+		case 1:
+		  putc ('b', file);
+		  return;
+
+		case 2:
+		  putc ('w', file);
+		  return;
+
+		case 4:
+		  putc ('l', file);
+		  return;
+
+		case 8:
+		  putc ('q', file);
+		  return;
+
+		default:
+		  output_operand_lossage ("invalid operand size for operand "
+					  "code 'z'");
+		  return;
+		}
+	    }
+
+	  if (GET_MODE_CLASS (GET_MODE (x)) == MODE_FLOAT)
+	    warning (0, "non-integer operand used with operand code %<z%>");
+	  /* FALLTHRU */
+
+	case 'Z':
+	  /* 387 opcodes don't get size suffixes if using Intel opcodes.  */
+	  if (ASSEMBLER_DIALECT == ASM_INTEL)
+	    return;
+
+	  if (GET_MODE_CLASS (GET_MODE (x)) == MODE_INT)
+	    {
+	      switch (GET_MODE_SIZE (GET_MODE (x)))
+		{
+		case 2:
+#ifdef HAVE_AS_IX86_FILDS
+		  putc ('s', file);
+#endif
+		  return;
+
+		case 4:
+		  putc ('l', file);
+		  return;
+
+		case 8:
+#ifdef HAVE_AS_IX86_FILDQ
+		  putc ('q', file);
+#else
+		  fputs ("ll", file);
+#endif
+		  return;
+
+		default:
+		  break;
+		}
+	    }
+	  else if (GET_MODE_CLASS (GET_MODE (x)) == MODE_FLOAT)
+	    {
+	      /* 387 opcodes don't get size suffixes
+		 if the operands are registers.  */
+	      if (STACK_REG_P (x))
+		return;
+
+	      switch (GET_MODE_SIZE (GET_MODE (x)))
+		{
+		case 4:
+		  putc ('s', file);
+		  return;
+
+		case 8:
+		  putc ('l', file);
+		  return;
+
+		case 12:
+		case 16:
+		  putc ('t', file);
+		  return;
+
+		default:
+		  break;
+		}
+	    }
+	  else
+	    {
+	      output_operand_lossage ("invalid operand type used with "
+				      "operand code 'Z'");
+	      return;
+	    }
+
+	  output_operand_lossage ("invalid operand size for operand code 'Z'");
+	  return;
+
+	case 'd':
+	case 'b':
+	case 'w':
+	case 'k':
+	case 'q':
+	case 'h':
+	case 't':
+	case 'g':
+	case 'y':
+	case 'x':
+	case 'X':
+	case 'P':
+	case 'p':
+	case 'V':
+	  break;
+
+	case 's':
+	  if (CONST_INT_P (x) || ! SHIFT_DOUBLE_OMITS_COUNT)
+	    {
+	      ix86_print_operand (file, x, 0);
+	      fputs (", ", file);
+	    }
+	  return;
+
+	case 'Y':
+	  switch (GET_CODE (x))
+	    {
+	    case NE:
+	      fputs ("neq", file);
+	      break;
+	    case EQ:
+	      fputs ("eq", file);
+	      break;
+	    case GE:
+	    case GEU:
+	      fputs (INTEGRAL_MODE_P (GET_MODE (x)) ? "ge" : "unlt", file);
+	      break;
+	    case GT:
+	    case GTU:
+	      fputs (INTEGRAL_MODE_P (GET_MODE (x)) ? "gt" : "unle", file);
+	      break;
+	    case LE:
+	    case LEU:
+	      fputs ("le", file);
+	      break;
+	    case LT:
+	    case LTU:
+	      fputs ("lt", file);
+	      break;
+	    case UNORDERED:
+	      fputs ("unord", file);
+	      break;
+	    case ORDERED:
+	      fputs ("ord", file);
+	      break;
+	    case UNEQ:
+	      fputs ("ueq", file);
+	      break;
+	    case UNGE:
+	      fputs ("nlt", file);
+	      break;
+	    case UNGT:
+	      fputs ("nle", file);
+	      break;
+	    case UNLE:
+	      fputs ("ule", file);
+	      break;
+	    case UNLT:
+	      fputs ("ult", file);
+	      break;
+	    case LTGT:
+	      fputs ("une", file);
+	      break;
+	    default:
+	      output_operand_lossage ("operand is not a condition code, "
+				      "invalid operand code 'Y'");
+	      return;
+	    }
+	  return;
+
+	case 'D':
+	  /* Little bit of braindamage here.  The SSE compare instructions
+	     does use completely different names for the comparisons that the
+	     fp conditional moves.  */
+	  switch (GET_CODE (x))
+	    {
+	    case UNEQ:
+	      if (TARGET_AVX)
+		{
+		  fputs ("eq_us", file);
+		  break;
+		}
+	     /* FALLTHRU */
+	    case EQ:
+	      fputs ("eq", file);
+	      break;
+	    case UNLT:
+	      if (TARGET_AVX)
+		{
+		  fputs ("nge", file);
+		  break;
+		}
+	     /* FALLTHRU */
+	    case LT:
+	      fputs ("lt", file);
+	      break;
+	    case UNLE:
+	      if (TARGET_AVX)
+		{
+		  fputs ("ngt", file);
+		  break;
+		}
+	     /* FALLTHRU */
+	    case LE:
+	      fputs ("le", file);
+	      break;
+	    case UNORDERED:
+	      fputs ("unord", file);
+	      break;
+	    case LTGT:
+	      if (TARGET_AVX)
+		{
+		  fputs ("neq_oq", file);
+		  break;
+		}
+	     /* FALLTHRU */
+	    case NE:
+	      fputs ("neq", file);
+	      break;
+	    case GE:
+	      if (TARGET_AVX)
+		{
+		  fputs ("ge", file);
+		  break;
+		}
+	     /* FALLTHRU */
+	    case UNGE:
+	      fputs ("nlt", file);
+	      break;
+	    case GT:
+	      if (TARGET_AVX)
+		{
+		  fputs ("gt", file);
+		  break;
+		}
+	     /* FALLTHRU */
+	    case UNGT:
+	      fputs ("nle", file);
+	      break;
+	    case ORDERED:
+	      fputs ("ord", file);
+	      break;
+	    default:
+	      output_operand_lossage ("operand is not a condition code, "
+				      "invalid operand code 'D'");
+	      return;
+	    }
+	  return;
+
+	case 'F':
+	case 'f':
+#ifdef HAVE_AS_IX86_CMOV_SUN_SYNTAX
+	  if (ASSEMBLER_DIALECT == ASM_ATT)
+	    putc ('.', file);
+	  gcc_fallthrough ();
+#endif
+
+	case 'C':
+	case 'c':
+	  if (!COMPARISON_P (x))
+	    {
+	      output_operand_lossage ("operand is not a condition code, "
+				      "invalid operand code '%c'", code);
+	      return;
+	    }
+	  put_condition_code (GET_CODE (x), GET_MODE (XEXP (x, 0)),
+			      code == 'c' || code == 'f',
+			      code == 'F' || code == 'f',
+			      file);
+	  return;
+
+	case 'H':
+	  if (!offsettable_memref_p (x))
+	    {
+	      output_operand_lossage ("operand is not an offsettable memory "
+				      "reference, invalid operand code 'H'");
+	      return;
+	    }
+	  /* It doesn't actually matter what mode we use here, as we're
+	     only going to use this for printing.  */
+	  x = adjust_address_nv (x, DImode, 8);
+	  /* Output 'qword ptr' for intel assembler dialect.  */
+	  if (ASSEMBLER_DIALECT == ASM_INTEL)
+	    code = 'q';
+	  break;
+
+	case 'K':
+	  if (!CONST_INT_P (x))
+	    {
+	      output_operand_lossage ("operand is not an integer, invalid "
+				      "operand code 'K'");
+	      return;
+	    }
+
+	  if (INTVAL (x) & IX86_HLE_ACQUIRE)
+#ifdef HAVE_AS_IX86_HLE
+	    fputs ("xacquire ", file);
+#else
+	    fputs ("\n" ASM_BYTE "0xf2\n\t", file);
+#endif
+	  else if (INTVAL (x) & IX86_HLE_RELEASE)
+#ifdef HAVE_AS_IX86_HLE
+	    fputs ("xrelease ", file);
+#else
+	    fputs ("\n" ASM_BYTE "0xf3\n\t", file);
+#endif
+	  /* We do not want to print value of the operand.  */
+	  return;
+
+	case 'N':
+	  if (x == const0_rtx || x == CONST0_RTX (GET_MODE (x)))
+	    fputs ("{z}", file);
+	  return;
+
+	case 'r':
+	  if (!CONST_INT_P (x) || INTVAL (x) != ROUND_SAE)
+	    {
+	      output_operand_lossage ("operand is not a specific integer, "
+				      "invalid operand code 'r'");
+	      return;
+	    }
+
+	  if (ASSEMBLER_DIALECT == ASM_INTEL)
+	    fputs (", ", file);
+
+	  fputs ("{sae}", file);
+
+	  if (ASSEMBLER_DIALECT == ASM_ATT)
+	    fputs (", ", file);
+
+	  return;
+
+	case 'R':
+	  if (!CONST_INT_P (x))
+	    {
+	      output_operand_lossage ("operand is not an integer, invalid "
+				      "operand code 'R'");
+	      return;
+	    }
+
+	  if (ASSEMBLER_DIALECT == ASM_INTEL)
+	    fputs (", ", file);
+
+	  switch (INTVAL (x))
+	    {
+	    case ROUND_NEAREST_INT | ROUND_SAE:
+	      fputs ("{rn-sae}", file);
+	      break;
+	    case ROUND_NEG_INF | ROUND_SAE:
+	      fputs ("{rd-sae}", file);
+	      break;
+	    case ROUND_POS_INF | ROUND_SAE:
+	      fputs ("{ru-sae}", file);
+	      break;
+	    case ROUND_ZERO | ROUND_SAE:
+	      fputs ("{rz-sae}", file);
+	      break;
+	    default:
+	      output_operand_lossage ("operand is not a specific integer, "
+				      "invalid operand code 'R'");
+	    }
+
+	  if (ASSEMBLER_DIALECT == ASM_ATT)
+	    fputs (", ", file);
+
+	  return;
+
+	case '*':
+	  if (ASSEMBLER_DIALECT == ASM_ATT)
+	    putc ('*', file);
+	  return;
+
+	case '&':
+	  {
+	    const char *name = get_some_local_dynamic_name ();
+	    if (name == NULL)
+	      output_operand_lossage ("'%%&' used without any "
+				      "local dynamic TLS references");
+	    else
+	      assemble_name (file, name);
+	    return;
+	  }
+
+	case '+':
+	  {
+	    rtx x;
+
+	    if (!optimize
+	        || optimize_function_for_size_p (cfun)
+		|| !TARGET_BRANCH_PREDICTION_HINTS)
+	      return;
+
+	    x = find_reg_note (current_output_insn, REG_BR_PROB, 0);
+	    if (x)
+	      {
+		int pred_val = profile_probability::from_reg_br_prob_note
+				 (XINT (x, 0)).to_reg_br_prob_base ();
+
+		if (pred_val < REG_BR_PROB_BASE * 45 / 100
+		    || pred_val > REG_BR_PROB_BASE * 55 / 100)
+		  {
+		    bool taken = pred_val > REG_BR_PROB_BASE / 2;
+		    bool cputaken
+		      = final_forward_branch_p (current_output_insn) == 0;
+
+		    /* Emit hints only in the case default branch prediction
+		       heuristics would fail.  */
+		    if (taken != cputaken)
+		      {
+			/* We use 3e (DS) prefix for taken branches and
+			   2e (CS) prefix for not taken branches.  */
+			if (taken)
+			  fputs ("ds ; ", file);
+			else
+			  fputs ("cs ; ", file);
+		      }
+		  }
+	      }
+	    return;
+	  }
+
+	case ';':
+#ifndef HAVE_AS_IX86_REP_LOCK_PREFIX
+	  putc (';', file);
+#endif
+	  return;
+
+	case '~':
+	  putc (TARGET_AVX2 ? 'i' : 'f', file);
+	  return;
+
+	case 'M':
+	  if (TARGET_X32)
+	    {
+	      /* NB: 32-bit indices in VSIB address are sign-extended
+		 to 64 bits. In x32, if 32-bit address 0xf7fa3010 is
+		 sign-extended to 0xfffffffff7fa3010 which is invalid
+		 address.  Add addr32 prefix if there is no base
+		 register nor symbol.  */
+	      bool ok;
+	      struct ix86_address parts;
+	      ok = ix86_decompose_address (x, &parts);
+	      gcc_assert (ok && parts.index == NULL_RTX);
+	      if (parts.base == NULL_RTX
+		  && (parts.disp == NULL_RTX
+		      || !symbolic_operand (parts.disp,
+					    GET_MODE (parts.disp))))
+		fputs ("addr32 ", file);
+	    }
+	  return;
+
+	case '^':
+	  if (TARGET_64BIT && Pmode != word_mode)
+	    fputs ("addr32 ", file);
+	  return;
+
+	case '!':
+	  if (ix86_notrack_prefixed_insn_p (current_output_insn))
+	    fputs ("notrack ", file);
+	  return;
+
+	default:
+	  output_operand_lossage ("invalid operand code '%c'", code);
+	}
+    }
+
+  if (REG_P (x))
+    print_reg (x, code, file);
+
+  else if (MEM_P (x))
+    {
+      rtx addr = XEXP (x, 0);
+
+      /* No `byte ptr' prefix for call instructions ... */
+      if (ASSEMBLER_DIALECT == ASM_INTEL && code != 'X' && code != 'P')
+	{
+	  machine_mode mode = GET_MODE (x);
+	  const char *size;
+
+	  /* Check for explicit size override codes.  */
+	  if (code == 'b')
+	    size = "BYTE";
+	  else if (code == 'w')
+	    size = "WORD";
+	  else if (code == 'k')
+	    size = "DWORD";
+	  else if (code == 'q')
+	    size = "QWORD";
+	  else if (code == 'x')
+	    size = "XMMWORD";
+	  else if (code == 't')
+	    size = "YMMWORD";
+	  else if (code == 'g')
+	    size = "ZMMWORD";
+	  else if (mode == BLKmode)
+	    /* ... or BLKmode operands, when not overridden.  */
+	    size = NULL;
+	  else
+	    switch (GET_MODE_SIZE (mode))
+	      {
+	      case 1: size = "BYTE"; break;
+	      case 2: size = "WORD"; break;
+	      case 4: size = "DWORD"; break;
+	      case 8: size = "QWORD"; break;
+	      case 12: size = "TBYTE"; break;
+	      case 16:
+		if (mode == XFmode)
+		  size = "TBYTE";
+		else
+		  size = "XMMWORD";
+		break;
+	      case 32: size = "YMMWORD"; break;
+	      case 64: size = "ZMMWORD"; break;
+	      default:
+		gcc_unreachable ();
+	      }
+	  if (size)
+	    {
+	      fputs (size, file);
+	      fputs (" PTR ", file);
+	    }
+	}
+
+      if (this_is_asm_operands && ! address_operand (addr, VOIDmode))
+	output_operand_lossage ("invalid constraints for operand");
+      else
+	ix86_print_operand_address_as
+	  (file, addr, MEM_ADDR_SPACE (x), code == 'p' || code == 'P');
+    }
+
+  else if (CONST_DOUBLE_P (x) && GET_MODE (x) == SFmode)
+    {
+      long l;
+
+      REAL_VALUE_TO_TARGET_SINGLE (*CONST_DOUBLE_REAL_VALUE (x), l);
+
+      if (ASSEMBLER_DIALECT == ASM_ATT)
+	putc ('$', file);
+      /* Sign extend 32bit SFmode immediate to 8 bytes.  */
+      if (code == 'q')
+	fprintf (file, "0x%08" HOST_LONG_LONG_FORMAT "x",
+		 (unsigned long long) (int) l);
+      else
+	fprintf (file, "0x%08x", (unsigned int) l);
+    }
+
+  else if (CONST_DOUBLE_P (x) && GET_MODE (x) == DFmode)
+    {
+      long l[2];
+
+      REAL_VALUE_TO_TARGET_DOUBLE (*CONST_DOUBLE_REAL_VALUE (x), l);
+
+      if (ASSEMBLER_DIALECT == ASM_ATT)
+	putc ('$', file);
+      fprintf (file, "0x%lx%08lx", l[1] & 0xffffffff, l[0] & 0xffffffff);
+    }
+
+  /* These float cases don't actually occur as immediate operands.  */
+  else if (CONST_DOUBLE_P (x) && GET_MODE (x) == XFmode)
+    {
+      char dstr[30];
+
+      real_to_decimal (dstr, CONST_DOUBLE_REAL_VALUE (x), sizeof (dstr), 0, 1);
+      fputs (dstr, file);
+    }
+
+  /* Print bcst_mem_operand.  */
+  else if (GET_CODE (x) == VEC_DUPLICATE)
+    {
+      machine_mode vmode = GET_MODE (x);
+      /* Must be bcst_memory_operand.  */
+      gcc_assert (bcst_mem_operand (x, vmode));
+
+      rtx mem = XEXP (x,0);
+      ix86_print_operand (file, mem, 0);
+
+      switch (vmode)
+	{
+	case E_V2DImode:
+	case E_V2DFmode:
+	  fputs ("{1to2}", file);
+	  break;
+	case E_V4SImode:
+	case E_V4SFmode:
+	case E_V4DImode:
+	case E_V4DFmode:
+	  fputs ("{1to4}", file);
+	  break;
+	case E_V8SImode:
+	case E_V8SFmode:
+	case E_V8DFmode:
+	case E_V8DImode:
+	  fputs ("{1to8}", file);
+	  break;
+	case E_V16SFmode:
+	case E_V16SImode:
+	  fputs ("{1to16}", file);
+	  break;
+	default:
+	  gcc_unreachable ();
+	}
+    }
+
+  else
+    {
+      /* We have patterns that allow zero sets of memory, for instance.
+	 In 64-bit mode, we should probably support all 8-byte vectors,
+	 since we can in fact encode that into an immediate.  */
+      if (GET_CODE (x) == CONST_VECTOR)
+	{
+	  if (x != CONST0_RTX (GET_MODE (x)))
+	    output_operand_lossage ("invalid vector immediate");
+	  x = const0_rtx;
+	}
+
+      if (code == 'P')
+	{
+	  if (ix86_force_load_from_GOT_p (x))
+	    {
+	      /* For inline assembly statement, load function address
+		 from GOT with 'P' operand modifier to avoid PLT.  */
+	      x = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, x),
+				  (TARGET_64BIT
+				   ? UNSPEC_GOTPCREL
+				   : UNSPEC_GOT));
+	      x = gen_rtx_CONST (Pmode, x);
+	      x = gen_const_mem (Pmode, x);
+	      ix86_print_operand (file, x, 'A');
+	      return;
+	    }
+	}
+      else if (code != 'p')
+	{
+	  if (CONST_INT_P (x))
+	    {
+	      if (ASSEMBLER_DIALECT == ASM_ATT)
+		putc ('$', file);
+	    }
+	  else if (GET_CODE (x) == CONST || GET_CODE (x) == SYMBOL_REF
+		   || GET_CODE (x) == LABEL_REF)
+	    {
+	      if (ASSEMBLER_DIALECT == ASM_ATT)
+		putc ('$', file);
+	      else
+		fputs ("OFFSET FLAT:", file);
+	    }
+	}
+      if (CONST_INT_P (x))
+	fprintf (file, HOST_WIDE_INT_PRINT_DEC, INTVAL (x));
+      else if (flag_pic || MACHOPIC_INDIRECT)
+	output_pic_addr_const (file, x, code);
+      else
+	output_addr_const (file, x);
+    }
+}
+
+static bool
+ix86_print_operand_punct_valid_p (unsigned char code)
+{
+  return (code == '*' || code == '+' || code == '&' || code == ';'
+	  || code == '~' || code == '^' || code == '!');
+}
+
+/* Print a memory operand whose address is ADDR.  */
+
+static void
+ix86_print_operand_address_as (FILE *file, rtx addr,
+			       addr_space_t as, bool no_rip)
+{
+  struct ix86_address parts;
+  rtx base, index, disp;
+  int scale;
+  int ok;
+  bool vsib = false;
+  int code = 0;
+
+  if (GET_CODE (addr) == UNSPEC && XINT (addr, 1) == UNSPEC_VSIBADDR)
+    {
+      ok = ix86_decompose_address (XVECEXP (addr, 0, 0), &parts);
+      gcc_assert (parts.index == NULL_RTX);
+      parts.index = XVECEXP (addr, 0, 1);
+      parts.scale = INTVAL (XVECEXP (addr, 0, 2));
+      addr = XVECEXP (addr, 0, 0);
+      vsib = true;
+    }
+  else if (GET_CODE (addr) == UNSPEC && XINT (addr, 1) == UNSPEC_LEA_ADDR)
+    {
+      gcc_assert (TARGET_64BIT);
+      ok = ix86_decompose_address (XVECEXP (addr, 0, 0), &parts);
+      code = 'q';
+    }
+  else
+    ok = ix86_decompose_address (addr, &parts);
+
+  gcc_assert (ok);
+
+  base = parts.base;
+  index = parts.index;
+  disp = parts.disp;
+  scale = parts.scale;
+
+  if (ADDR_SPACE_GENERIC_P (as))
+    as = parts.seg;
+  else
+    gcc_assert (ADDR_SPACE_GENERIC_P (parts.seg));
+
+  if (!ADDR_SPACE_GENERIC_P (as))
+    {
+      if (ASSEMBLER_DIALECT == ASM_ATT)
+	putc ('%', file);
+
+      switch (as)
+	{
+	case ADDR_SPACE_SEG_FS:
+	  fputs ("fs:", file);
+	  break;
+	case ADDR_SPACE_SEG_GS:
+	  fputs ("gs:", file);
+	  break;
+	default:
+	  gcc_unreachable ();
+	}
+    }
+
+  /* Use one byte shorter RIP relative addressing for 64bit mode.  */
+  if (TARGET_64BIT && !base && !index && !no_rip)
+    {
+      rtx symbol = disp;
+
+      if (GET_CODE (disp) == CONST
+	  && GET_CODE (XEXP (disp, 0)) == PLUS
+	  && CONST_INT_P (XEXP (XEXP (disp, 0), 1)))
+	symbol = XEXP (XEXP (disp, 0), 0);
+
+      if (GET_CODE (symbol) == LABEL_REF
+	  || (GET_CODE (symbol) == SYMBOL_REF
+	      && SYMBOL_REF_TLS_MODEL (symbol) == 0))
+	base = pc_rtx;
+    }
+
+  if (!base && !index)
+    {
+      /* Displacement only requires special attention.  */
+      if (CONST_INT_P (disp))
+	{
+	  if (ASSEMBLER_DIALECT == ASM_INTEL && ADDR_SPACE_GENERIC_P (as))
+	    fputs ("ds:", file);
+	  fprintf (file, HOST_WIDE_INT_PRINT_DEC, INTVAL (disp));
+	}
+      /* Load the external function address via the GOT slot to avoid PLT.  */
+      else if (GET_CODE (disp) == CONST
+	       && GET_CODE (XEXP (disp, 0)) == UNSPEC
+	       && (XINT (XEXP (disp, 0), 1) == UNSPEC_GOTPCREL
+		   || XINT (XEXP (disp, 0), 1) == UNSPEC_GOT)
+	       && ix86_force_load_from_GOT_p (XVECEXP (XEXP (disp, 0), 0, 0)))
+	output_pic_addr_const (file, disp, 0);
+      else if (flag_pic)
+	output_pic_addr_const (file, disp, 0);
+      else
+	output_addr_const (file, disp);
+    }
+  else
+    {
+      /* Print SImode register names to force addr32 prefix.  */
+      if (SImode_address_operand (addr, VOIDmode))
+	{
+	  if (flag_checking)
+	    {
+	      gcc_assert (TARGET_64BIT);
+	      switch (GET_CODE (addr))
+		{
+		case SUBREG:
+		  gcc_assert (GET_MODE (addr) == SImode);
+		  gcc_assert (GET_MODE (SUBREG_REG (addr)) == DImode);
+		  break;
+		case ZERO_EXTEND:
+		case AND:
+		  gcc_assert (GET_MODE (addr) == DImode);
+		  break;
+		default:
+		  gcc_unreachable ();
+		}
+	    }
+	  gcc_assert (!code);
+	  code = 'k';
+	}
+      else if (code == 0
+	       && TARGET_X32
+	       && disp
+	       && CONST_INT_P (disp)
+	       && INTVAL (disp) < -16*1024*1024)
+	{
+	  /* X32 runs in 64-bit mode, where displacement, DISP, in
+	     address DISP(%r64), is encoded as 32-bit immediate sign-
+	     extended from 32-bit to 64-bit.  For -0x40000300(%r64),
+	     address is %r64 + 0xffffffffbffffd00.  When %r64 <
+	     0x40000300, like 0x37ffe064, address is 0xfffffffff7ffdd64,
+	     which is invalid for x32.  The correct address is %r64
+	     - 0x40000300 == 0xf7ffdd64.  To properly encode
+	     -0x40000300(%r64) for x32, we zero-extend negative
+	     displacement by forcing addr32 prefix which truncates
+	     0xfffffffff7ffdd64 to 0xf7ffdd64.  In theory, we should
+	     zero-extend all negative displacements, including -1(%rsp).
+	     However, for small negative displacements, sign-extension
+	     won't cause overflow.  We only zero-extend negative
+	     displacements if they < -16*1024*1024, which is also used
+	     to check legitimate address displacements for PIC.  */
+	  code = 'k';
+	}
+
+      /* Since the upper 32 bits of RSP are always zero for x32,
+	 we can encode %esp as %rsp to avoid 0x67 prefix if
+	 there is no index register.  */
+      if (TARGET_X32 && Pmode == SImode
+	  && !index && base && REG_P (base) && REGNO (base) == SP_REG)
+	code = 'q';
+
+      if (ASSEMBLER_DIALECT == ASM_ATT)
+	{
+	  if (disp)
+	    {
+	      if (flag_pic)
+		output_pic_addr_const (file, disp, 0);
+	      else if (GET_CODE (disp) == LABEL_REF)
+		output_asm_label (disp);
+	      else
+		output_addr_const (file, disp);
+	    }
+
+	  putc ('(', file);
+	  if (base)
+	    print_reg (base, code, file);
+	  if (index)
+	    {
+	      putc (',', file);
+	      print_reg (index, vsib ? 0 : code, file);
+	      if (scale != 1 || vsib)
+		fprintf (file, ",%d", scale);
+	    }
+	  putc (')', file);
+	}
+      else
+	{
+	  rtx offset = NULL_RTX;
+
+	  if (disp)
+	    {
+	      /* Pull out the offset of a symbol; print any symbol itself.  */
+	      if (GET_CODE (disp) == CONST
+		  && GET_CODE (XEXP (disp, 0)) == PLUS
+		  && CONST_INT_P (XEXP (XEXP (disp, 0), 1)))
+		{
+		  offset = XEXP (XEXP (disp, 0), 1);
+		  disp = gen_rtx_CONST (VOIDmode,
+					XEXP (XEXP (disp, 0), 0));
+		}
+
+	      if (flag_pic)
+		output_pic_addr_const (file, disp, 0);
+	      else if (GET_CODE (disp) == LABEL_REF)
+		output_asm_label (disp);
+	      else if (CONST_INT_P (disp))
+		offset = disp;
+	      else
+		output_addr_const (file, disp);
+	    }
+
+	  putc ('[', file);
+	  if (base)
+	    {
+	      print_reg (base, code, file);
+	      if (offset)
+		{
+		  if (INTVAL (offset) >= 0)
+		    putc ('+', file);
+		  fprintf (file, HOST_WIDE_INT_PRINT_DEC, INTVAL (offset));
+		}
+	    }
+	  else if (offset)
+	    fprintf (file, HOST_WIDE_INT_PRINT_DEC, INTVAL (offset));
+	  else
+	    putc ('0', file);
+
+	  if (index)
+	    {
+	      putc ('+', file);
+	      print_reg (index, vsib ? 0 : code, file);
+	      if (scale != 1 || vsib)
+		fprintf (file, "*%d", scale);
+	    }
+	  putc (']', file);
+	}
+    }
+}
+
+static void
+ix86_print_operand_address (FILE *file, machine_mode /*mode*/, rtx addr)
+{
+  ix86_print_operand_address_as (file, addr, ADDR_SPACE_GENERIC, false);
+}
+
+/* Implementation of TARGET_ASM_OUTPUT_ADDR_CONST_EXTRA.  */
+
+static bool
+i386_asm_output_addr_const_extra (FILE *file, rtx x)
+{
+  rtx op;
+
+  if (GET_CODE (x) != UNSPEC)
+    return false;
+
+  op = XVECEXP (x, 0, 0);
+  switch (XINT (x, 1))
+    {
+    case UNSPEC_GOTOFF:
+      output_addr_const (file, op);
+      fputs ("@gotoff", file);
+      break;
+    case UNSPEC_GOTTPOFF:
+      output_addr_const (file, op);
+      /* FIXME: This might be @TPOFF in Sun ld.  */
+      fputs ("@gottpoff", file);
+      break;
+    case UNSPEC_TPOFF:
+      output_addr_const (file, op);
+      fputs ("@tpoff", file);
+      break;
+    case UNSPEC_NTPOFF:
+      output_addr_const (file, op);
+      if (TARGET_64BIT)
+	fputs ("@tpoff", file);
+      else
+	fputs ("@ntpoff", file);
+      break;
+    case UNSPEC_DTPOFF:
+      output_addr_const (file, op);
+      fputs ("@dtpoff", file);
+      break;
+    case UNSPEC_GOTNTPOFF:
+      output_addr_const (file, op);
+      if (TARGET_64BIT)
+	fputs (ASSEMBLER_DIALECT == ASM_ATT ?
+	       "@gottpoff(%rip)" : "@gottpoff[rip]", file);
+      else
+	fputs ("@gotntpoff", file);
+      break;
+    case UNSPEC_INDNTPOFF:
+      output_addr_const (file, op);
+      fputs ("@indntpoff", file);
+      break;
+#if TARGET_MACHO
+    case UNSPEC_MACHOPIC_OFFSET:
+      output_addr_const (file, op);
+      putc ('-', file);
+      machopic_output_function_base_name (file);
+      break;
+#endif
+
+    default:
+      return false;
+    }
+
+  return true;
+}
+
+
+/* Output code to perform a 387 binary operation in INSN, one of PLUS,
+   MINUS, MULT or DIV.  OPERANDS are the insn operands, where operands[3]
+   is the expression of the binary operation.  The output may either be
+   emitted here, or returned to the caller, like all output_* functions.
+
+   There is no guarantee that the operands are the same mode, as they
+   might be within FLOAT or FLOAT_EXTEND expressions.  */
+
+#ifndef SYSV386_COMPAT
+/* Set to 1 for compatibility with brain-damaged assemblers.  No-one
+   wants to fix the assemblers because that causes incompatibility
+   with gcc.  No-one wants to fix gcc because that causes
+   incompatibility with assemblers...  You can use the option of
+   -DSYSV386_COMPAT=0 if you recompile both gcc and gas this way.  */
+#define SYSV386_COMPAT 1
+#endif
+
+const char *
+output_387_binary_op (rtx_insn *insn, rtx *operands)
+{
+  static char buf[40];
+  const char *p;
+  bool is_sse
+    = (SSE_REG_P (operands[0])
+       || SSE_REG_P (operands[1]) || SSE_REG_P (operands[2]));
+
+  if (is_sse)
+    p = "%v";
+  else if (GET_MODE_CLASS (GET_MODE (operands[1])) == MODE_INT
+	   || GET_MODE_CLASS (GET_MODE (operands[2])) == MODE_INT)
+    p = "fi";
+  else
+    p = "f";
+
+  strcpy (buf, p);
+
+  switch (GET_CODE (operands[3]))
+    {
+    case PLUS:
+      p = "add"; break;
+    case MINUS:
+      p = "sub"; break;
+    case MULT:
+      p = "mul"; break;
+    case DIV:
+      p = "div"; break;
+    default:
+      gcc_unreachable ();
+    }
+
+  strcat (buf, p);
+
+  if (is_sse)
+   {
+     p = (GET_MODE (operands[0]) == SFmode) ? "ss" : "sd";
+     strcat (buf, p);
+
+     if (TARGET_AVX)
+       p = "\t{%2, %1, %0|%0, %1, %2}";
+     else
+       p = "\t{%2, %0|%0, %2}";
+
+     strcat (buf, p);
+     return buf;
+   }
+
+  /* Even if we do not want to check the inputs, this documents input
+     constraints.  Which helps in understanding the following code.  */
+  if (flag_checking)
+    {
+      if (STACK_REG_P (operands[0])
+	  && ((REG_P (operands[1])
+	       && REGNO (operands[0]) == REGNO (operands[1])
+	       && (STACK_REG_P (operands[2]) || MEM_P (operands[2])))
+	      || (REG_P (operands[2])
+		  && REGNO (operands[0]) == REGNO (operands[2])
+		  && (STACK_REG_P (operands[1]) || MEM_P (operands[1]))))
+	  && (STACK_TOP_P (operands[1]) || STACK_TOP_P (operands[2])))
+	; /* ok */
+      else
+	gcc_unreachable ();
+    }
+
+  switch (GET_CODE (operands[3]))
+    {
+    case MULT:
+    case PLUS:
+      if (REG_P (operands[2]) && REGNO (operands[0]) == REGNO (operands[2]))
+	std::swap (operands[1], operands[2]);
+
+      /* know operands[0] == operands[1].  */
+
+      if (MEM_P (operands[2]))
+	{
+	  p = "%Z2\t%2";
+	  break;
+	}
+
+      if (find_regno_note (insn, REG_DEAD, REGNO (operands[2])))
+	{
+	  if (STACK_TOP_P (operands[0]))
+	    /* How is it that we are storing to a dead operand[2]?
+	       Well, presumably operands[1] is dead too.  We can't
+	       store the result to st(0) as st(0) gets popped on this
+	       instruction.  Instead store to operands[2] (which I
+	       think has to be st(1)).  st(1) will be popped later.
+	       gcc <= 2.8.1 didn't have this check and generated
+	       assembly code that the Unixware assembler rejected.  */
+	    p = "p\t{%0, %2|%2, %0}";	/* st(1) = st(0) op st(1); pop */
+	  else
+	    p = "p\t{%2, %0|%0, %2}";	/* st(r1) = st(r1) op st(0); pop */
+	  break;
+	}
+
+      if (STACK_TOP_P (operands[0]))
+	p = "\t{%y2, %0|%0, %y2}";	/* st(0) = st(0) op st(r2) */
+      else
+	p = "\t{%2, %0|%0, %2}";	/* st(r1) = st(r1) op st(0) */
+      break;
+
+    case MINUS:
+    case DIV:
+      if (MEM_P (operands[1]))
+	{
+	  p = "r%Z1\t%1";
+	  break;
+	}
+
+      if (MEM_P (operands[2]))
+	{
+	  p = "%Z2\t%2";
+	  break;
+	}
+
+      if (find_regno_note (insn, REG_DEAD, REGNO (operands[2])))
+	{
+#if SYSV386_COMPAT
+	  /* The SystemV/386 SVR3.2 assembler, and probably all AT&T
+	     derived assemblers, confusingly reverse the direction of
+	     the operation for fsub{r} and fdiv{r} when the
+	     destination register is not st(0).  The Intel assembler
+	     doesn't have this brain damage.  Read !SYSV386_COMPAT to
+	     figure out what the hardware really does.  */
+	  if (STACK_TOP_P (operands[0]))
+	    p = "{p\t%0, %2|rp\t%2, %0}";
+	  else
+	    p = "{rp\t%2, %0|p\t%0, %2}";
+#else
+	  if (STACK_TOP_P (operands[0]))
+	    /* As above for fmul/fadd, we can't store to st(0).  */
+	    p = "rp\t{%0, %2|%2, %0}";	/* st(1) = st(0) op st(1); pop */
+	  else
+	    p = "p\t{%2, %0|%0, %2}";	/* st(r1) = st(r1) op st(0); pop */
+#endif
+	  break;
+	}
+
+      if (find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
+	{
+#if SYSV386_COMPAT
+	  if (STACK_TOP_P (operands[0]))
+	    p = "{rp\t%0, %1|p\t%1, %0}";
+	  else
+	    p = "{p\t%1, %0|rp\t%0, %1}";
+#else
+	  if (STACK_TOP_P (operands[0]))
+	    p = "p\t{%0, %1|%1, %0}";	/* st(1) = st(1) op st(0); pop */
+	  else
+	    p = "rp\t{%1, %0|%0, %1}";	/* st(r2) = st(0) op st(r2); pop */
+#endif
+	  break;
+	}
+
+      if (STACK_TOP_P (operands[0]))
+	{
+	  if (STACK_TOP_P (operands[1]))
+	    p = "\t{%y2, %0|%0, %y2}";	/* st(0) = st(0) op st(r2) */
+	  else
+	    p = "r\t{%y1, %0|%0, %y1}";	/* st(0) = st(r1) op st(0) */
+	  break;
+	}
+      else if (STACK_TOP_P (operands[1]))
+	{
+#if SYSV386_COMPAT
+	  p = "{\t%1, %0|r\t%0, %1}";
+#else
+	  p = "r\t{%1, %0|%0, %1}";	/* st(r2) = st(0) op st(r2) */
+#endif
+	}
+      else
+	{
+#if SYSV386_COMPAT
+	  p = "{r\t%2, %0|\t%0, %2}";
+#else
+	  p = "\t{%2, %0|%0, %2}";	/* st(r1) = st(r1) op st(0) */
+#endif
+	}
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  strcat (buf, p);
+  return buf;
+}
+
+/* Return needed mode for entity in optimize_mode_switching pass.  */
+
+static int
+ix86_dirflag_mode_needed (rtx_insn *insn)
+{
+  if (CALL_P (insn))
+    {
+      if (cfun->machine->func_type == TYPE_NORMAL)
+	return X86_DIRFLAG_ANY;
+      else
+	/* No need to emit CLD in interrupt handler for TARGET_CLD.  */
+	return TARGET_CLD ? X86_DIRFLAG_ANY : X86_DIRFLAG_RESET;
+    }
+
+  if (recog_memoized (insn) < 0)
+    return X86_DIRFLAG_ANY;
+
+  if (get_attr_type (insn) == TYPE_STR)
+    {
+      /* Emit cld instruction if stringops are used in the function.  */
+      if (cfun->machine->func_type == TYPE_NORMAL)
+	return TARGET_CLD ? X86_DIRFLAG_RESET : X86_DIRFLAG_ANY;
+      else
+	return X86_DIRFLAG_RESET;
+    }
+
+  return X86_DIRFLAG_ANY;
+}
+
+/* Check if a 256bit or 512 bit AVX register is referenced inside of EXP.   */
+
+static bool
+ix86_check_avx_upper_register (const_rtx exp)
+{
+  return (SSE_REG_P (exp)
+	  && !EXT_REX_SSE_REG_P (exp)
+	  && GET_MODE_BITSIZE (GET_MODE (exp)) > 128);
+}
+
+/* Return needed mode for entity in optimize_mode_switching pass.  */
+
+static int
+ix86_avx_u128_mode_needed (rtx_insn *insn)
+{
+  if (CALL_P (insn))
+    {
+      rtx link;
+
+      /* Needed mode is set to AVX_U128_CLEAN if there are
+	 no 256bit or 512bit modes used in function arguments. */
+      for (link = CALL_INSN_FUNCTION_USAGE (insn);
+	   link;
+	   link = XEXP (link, 1))
+	{
+	  if (GET_CODE (XEXP (link, 0)) == USE)
+	    {
+	      rtx arg = XEXP (XEXP (link, 0), 0);
+
+	      if (ix86_check_avx_upper_register (arg))
+		return AVX_U128_DIRTY;
+	    }
+	}
+
+      /* If the function is known to preserve some SSE registers,
+	 RA and previous passes can legitimately rely on that for
+	 modes wider than 256 bits.  It's only safe to issue a
+	 vzeroupper if all SSE registers are clobbered.  */
+      const function_abi &abi = insn_callee_abi (insn);
+      if (!hard_reg_set_subset_p (reg_class_contents[SSE_REGS],
+				  abi.mode_clobbers (V4DImode)))
+	return AVX_U128_ANY;
+
+      return AVX_U128_CLEAN;
+    }
+
+  /* Require DIRTY mode if a 256bit or 512bit AVX register is referenced.
+     Hardware changes state only when a 256bit register is written to,
+     but we need to prevent the compiler from moving optimal insertion
+     point above eventual read from 256bit or 512 bit register.  */
+  subrtx_iterator::array_type array;
+  FOR_EACH_SUBRTX (iter, array, PATTERN (insn), NONCONST)
+    if (ix86_check_avx_upper_register (*iter))
+      return AVX_U128_DIRTY;
+
+  return AVX_U128_ANY;
+}
+
+/* Return mode that i387 must be switched into
+   prior to the execution of insn.  */
+
+static int
+ix86_i387_mode_needed (int entity, rtx_insn *insn)
+{
+  enum attr_i387_cw mode;
+
+  /* The mode UNINITIALIZED is used to store control word after a
+     function call or ASM pattern.  The mode ANY specify that function
+     has no requirements on the control word and make no changes in the
+     bits we are interested in.  */
+
+  if (CALL_P (insn)
+      || (NONJUMP_INSN_P (insn)
+	  && (asm_noperands (PATTERN (insn)) >= 0
+	      || GET_CODE (PATTERN (insn)) == ASM_INPUT)))
+    return I387_CW_UNINITIALIZED;
+
+  if (recog_memoized (insn) < 0)
+    return I387_CW_ANY;
+
+  mode = get_attr_i387_cw (insn);
+
+  switch (entity)
+    {
+    case I387_ROUNDEVEN:
+      if (mode == I387_CW_ROUNDEVEN)
+	return mode;
+      break;
+
+    case I387_TRUNC:
+      if (mode == I387_CW_TRUNC)
+	return mode;
+      break;
+
+    case I387_FLOOR:
+      if (mode == I387_CW_FLOOR)
+	return mode;
+      break;
+
+    case I387_CEIL:
+      if (mode == I387_CW_CEIL)
+	return mode;
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  return I387_CW_ANY;
+}
+
+/* Return mode that entity must be switched into
+   prior to the execution of insn.  */
+
+static int
+ix86_mode_needed (int entity, rtx_insn *insn)
+{
+  switch (entity)
+    {
+    case X86_DIRFLAG:
+      return ix86_dirflag_mode_needed (insn);
+    case AVX_U128:
+      return ix86_avx_u128_mode_needed (insn);
+    case I387_ROUNDEVEN:
+    case I387_TRUNC:
+    case I387_FLOOR:
+    case I387_CEIL:
+      return ix86_i387_mode_needed (entity, insn);
+    default:
+      gcc_unreachable ();
+    }
+  return 0;
+}
+
+/* Check if a 256bit or 512bit AVX register is referenced in stores.   */
+ 
+static void
+ix86_check_avx_upper_stores (rtx dest, const_rtx, void *data)
+ {
+   if (ix86_check_avx_upper_register (dest))
+    {
+      bool *used = (bool *) data;
+      *used = true;
+    }
+ } 
+
+/* Calculate mode of upper 128bit AVX registers after the insn.  */
+
+static int
+ix86_avx_u128_mode_after (int mode, rtx_insn *insn)
+{
+  rtx pat = PATTERN (insn);
+
+  if (vzeroupper_pattern (pat, VOIDmode)
+      || vzeroall_pattern (pat, VOIDmode))
+    return AVX_U128_CLEAN;
+
+  /* We know that state is clean after CALL insn if there are no
+     256bit or 512bit registers used in the function return register. */
+  if (CALL_P (insn))
+    {
+      bool avx_upper_reg_found = false;
+      note_stores (insn, ix86_check_avx_upper_stores, &avx_upper_reg_found);
+
+      return avx_upper_reg_found ? AVX_U128_DIRTY : AVX_U128_CLEAN;
+    }
+
+  /* Otherwise, return current mode.  Remember that if insn
+     references AVX 256bit or 512bit registers, the mode was already
+     changed to DIRTY from MODE_NEEDED.  */
+  return mode;
+}
+
+/* Return the mode that an insn results in.  */
+
+static int
+ix86_mode_after (int entity, int mode, rtx_insn *insn)
+{
+  switch (entity)
+    {
+    case X86_DIRFLAG:
+      return mode;
+    case AVX_U128:
+      return ix86_avx_u128_mode_after (mode, insn);
+    case I387_ROUNDEVEN:
+    case I387_TRUNC:
+    case I387_FLOOR:
+    case I387_CEIL:
+      return mode;
+    default:
+      gcc_unreachable ();
+    }
+}
+
+static int
+ix86_dirflag_mode_entry (void)
+{
+  /* For TARGET_CLD or in the interrupt handler we can't assume
+     direction flag state at function entry.  */
+  if (TARGET_CLD
+      || cfun->machine->func_type != TYPE_NORMAL)
+    return X86_DIRFLAG_ANY;
+
+  return X86_DIRFLAG_RESET;
+}
+
+static int
+ix86_avx_u128_mode_entry (void)
+{
+  tree arg;
+
+  /* Entry mode is set to AVX_U128_DIRTY if there are
+     256bit or 512bit modes used in function arguments.  */
+  for (arg = DECL_ARGUMENTS (current_function_decl); arg;
+       arg = TREE_CHAIN (arg))
+    {
+      rtx incoming = DECL_INCOMING_RTL (arg);
+
+      if (incoming && ix86_check_avx_upper_register (incoming))
+	return AVX_U128_DIRTY;
+    }
+
+  return AVX_U128_CLEAN;
+}
+
+/* Return a mode that ENTITY is assumed to be
+   switched to at function entry.  */
+
+static int
+ix86_mode_entry (int entity)
+{
+  switch (entity)
+    {
+    case X86_DIRFLAG:
+      return ix86_dirflag_mode_entry ();
+    case AVX_U128:
+      return ix86_avx_u128_mode_entry ();
+    case I387_ROUNDEVEN:
+    case I387_TRUNC:
+    case I387_FLOOR:
+    case I387_CEIL:
+      return I387_CW_ANY;
+    default:
+      gcc_unreachable ();
+    }
+}
+
+static int
+ix86_avx_u128_mode_exit (void)
+{
+  rtx reg = crtl->return_rtx;
+
+  /* Exit mode is set to AVX_U128_DIRTY if there are 256bit
+     or 512 bit modes used in the function return register. */
+  if (reg && ix86_check_avx_upper_register (reg))
+    return AVX_U128_DIRTY;
+
+  /* Exit mode is set to AVX_U128_DIRTY if there are 256bit or 512bit
+     modes used in function arguments, otherwise return AVX_U128_CLEAN.
+   */
+  return ix86_avx_u128_mode_entry ();
+}
+
+/* Return a mode that ENTITY is assumed to be
+   switched to at function exit.  */
+
+static int
+ix86_mode_exit (int entity)
+{
+  switch (entity)
+    {
+    case X86_DIRFLAG:
+      return X86_DIRFLAG_ANY;
+    case AVX_U128:
+      return ix86_avx_u128_mode_exit ();
+    case I387_ROUNDEVEN:
+    case I387_TRUNC:
+    case I387_FLOOR:
+    case I387_CEIL:
+      return I387_CW_ANY;
+    default:
+      gcc_unreachable ();
+    }
+}
+
+static int
+ix86_mode_priority (int, int n)
+{
+  return n;
+}
+
+/* Output code to initialize control word copies used by trunc?f?i and
+   rounding patterns.  CURRENT_MODE is set to current control word,
+   while NEW_MODE is set to new control word.  */
+
+static void
+emit_i387_cw_initialization (int mode)
+{
+  rtx stored_mode = assign_386_stack_local (HImode, SLOT_CW_STORED);
+  rtx new_mode;
+
+  enum ix86_stack_slot slot;
+
+  rtx reg = gen_reg_rtx (HImode);
+
+  emit_insn (gen_x86_fnstcw_1 (stored_mode));
+  emit_move_insn (reg, copy_rtx (stored_mode));
+
+  switch (mode)
+    {
+    case I387_CW_ROUNDEVEN:
+      /* round to nearest */
+      emit_insn (gen_andhi3 (reg, reg, GEN_INT (~0x0c00)));
+      slot = SLOT_CW_ROUNDEVEN;
+      break;
+
+    case I387_CW_TRUNC:
+      /* round toward zero (truncate) */
+      emit_insn (gen_iorhi3 (reg, reg, GEN_INT (0x0c00)));
+      slot = SLOT_CW_TRUNC;
+      break;
+
+    case I387_CW_FLOOR:
+      /* round down toward -oo */
+      emit_insn (gen_andhi3 (reg, reg, GEN_INT (~0x0c00)));
+      emit_insn (gen_iorhi3 (reg, reg, GEN_INT (0x0400)));
+      slot = SLOT_CW_FLOOR;
+      break;
+
+    case I387_CW_CEIL:
+      /* round up toward +oo */
+      emit_insn (gen_andhi3 (reg, reg, GEN_INT (~0x0c00)));
+      emit_insn (gen_iorhi3 (reg, reg, GEN_INT (0x0800)));
+      slot = SLOT_CW_CEIL;
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  gcc_assert (slot < MAX_386_STACK_LOCALS);
+
+  new_mode = assign_386_stack_local (HImode, slot);
+  emit_move_insn (new_mode, reg);
+}
+
+/* Generate one or more insns to set ENTITY to MODE.  */
+
+static void
+ix86_emit_mode_set (int entity, int mode, int prev_mode ATTRIBUTE_UNUSED,
+		    HARD_REG_SET regs_live ATTRIBUTE_UNUSED)
+{
+  switch (entity)
+    {
+    case X86_DIRFLAG:
+      if (mode == X86_DIRFLAG_RESET)
+	emit_insn (gen_cld ());
+      break;
+    case AVX_U128:
+      if (mode == AVX_U128_CLEAN)
+	emit_insn (gen_avx_vzeroupper ());
+      break;
+    case I387_ROUNDEVEN:
+    case I387_TRUNC:
+    case I387_FLOOR:
+    case I387_CEIL:
+      if (mode != I387_CW_ANY
+	  && mode != I387_CW_UNINITIALIZED)
+	emit_i387_cw_initialization (mode);
+      break;
+    default:
+      gcc_unreachable ();
+    }
+}
+
+/* Output code for INSN to convert a float to a signed int.  OPERANDS
+   are the insn operands.  The output may be [HSD]Imode and the input
+   operand may be [SDX]Fmode.  */
+
+const char *
+output_fix_trunc (rtx_insn *insn, rtx *operands, bool fisttp)
+{
+  bool stack_top_dies = find_regno_note (insn, REG_DEAD, FIRST_STACK_REG);
+  bool dimode_p = GET_MODE (operands[0]) == DImode;
+  int round_mode = get_attr_i387_cw (insn);
+
+  static char buf[40];
+  const char *p;
+
+  /* Jump through a hoop or two for DImode, since the hardware has no
+     non-popping instruction.  We used to do this a different way, but
+     that was somewhat fragile and broke with post-reload splitters.  */
+  if ((dimode_p || fisttp) && !stack_top_dies)
+    output_asm_insn ("fld\t%y1", operands);
+
+  gcc_assert (STACK_TOP_P (operands[1]));
+  gcc_assert (MEM_P (operands[0]));
+  gcc_assert (GET_MODE (operands[1]) != TFmode);
+
+  if (fisttp)
+    return "fisttp%Z0\t%0";
+
+  strcpy (buf, "fist");
+
+  if (round_mode != I387_CW_ANY)
+    output_asm_insn ("fldcw\t%3", operands);
+
+  p = "p%Z0\t%0";
+  strcat (buf, p + !(stack_top_dies || dimode_p));
+
+  output_asm_insn (buf, operands);
+
+  if (round_mode != I387_CW_ANY)
+    output_asm_insn ("fldcw\t%2", operands);
+
+  return "";
+}
+
+/* Output code for x87 ffreep insn.  The OPNO argument, which may only
+   have the values zero or one, indicates the ffreep insn's operand
+   from the OPERANDS array.  */
+
+static const char *
+output_387_ffreep (rtx *operands ATTRIBUTE_UNUSED, int opno)
+{
+  if (TARGET_USE_FFREEP)
+#ifdef HAVE_AS_IX86_FFREEP
+    return opno ? "ffreep\t%y1" : "ffreep\t%y0";
+#else
+    {
+      static char retval[32];
+      int regno = REGNO (operands[opno]);
+
+      gcc_assert (STACK_REGNO_P (regno));
+
+      regno -= FIRST_STACK_REG;
+
+      snprintf (retval, sizeof (retval), ASM_SHORT "0xc%ddf", regno);
+      return retval;
+    }
+#endif
+
+  return opno ? "fstp\t%y1" : "fstp\t%y0";
+}
+
+
+/* Output code for INSN to compare OPERANDS.  EFLAGS_P is 1 when fcomi
+   should be used.  UNORDERED_P is true when fucom should be used.  */
+
+const char *
+output_fp_compare (rtx_insn *insn, rtx *operands,
+		   bool eflags_p, bool unordered_p)
+{
+  rtx *xops = eflags_p ? &operands[0] : &operands[1];
+  bool stack_top_dies;
+
+  static char buf[40];
+  const char *p;
+
+  gcc_assert (STACK_TOP_P (xops[0]));
+
+  stack_top_dies = find_regno_note (insn, REG_DEAD, FIRST_STACK_REG);
+
+  if (eflags_p)
+    {
+      p = unordered_p ? "fucomi" : "fcomi";
+      strcpy (buf, p);
+
+      p = "p\t{%y1, %0|%0, %y1}";
+      strcat (buf, p + !stack_top_dies);
+
+      return buf;
+    }
+
+  if (STACK_REG_P (xops[1])
+      && stack_top_dies
+      && find_regno_note (insn, REG_DEAD, FIRST_STACK_REG + 1))
+    {
+      gcc_assert (REGNO (xops[1]) == FIRST_STACK_REG + 1);
+
+      /* If both the top of the 387 stack die, and the other operand
+	 is also a stack register that dies, then this must be a
+	 `fcompp' float compare.  */
+      p = unordered_p ? "fucompp" : "fcompp";
+      strcpy (buf, p);
+    }
+  else if (const0_operand (xops[1], VOIDmode))
+    {
+      gcc_assert (!unordered_p);
+      strcpy (buf, "ftst");
+    }
+  else
+    {
+      if (GET_MODE_CLASS (GET_MODE (xops[1])) == MODE_INT)
+	{
+	  gcc_assert (!unordered_p);
+	  p = "ficom";
+	}
+      else
+	p = unordered_p ? "fucom" : "fcom";
+
+      strcpy (buf, p);
+
+      p = "p%Z2\t%y2";
+      strcat (buf, p + !stack_top_dies);
+    }
+
+  output_asm_insn (buf, operands);
+  return "fnstsw\t%0";
+}
+
+void
+ix86_output_addr_vec_elt (FILE *file, int value)
+{
+  const char *directive = ASM_LONG;
+
+#ifdef ASM_QUAD
+  if (TARGET_LP64)
+    directive = ASM_QUAD;
+#else
+  gcc_assert (!TARGET_64BIT);
+#endif
+
+  fprintf (file, "%s%s%d\n", directive, LPREFIX, value);
+}
+
+void
+ix86_output_addr_diff_elt (FILE *file, int value, int rel)
+{
+  const char *directive = ASM_LONG;
+
+#ifdef ASM_QUAD
+  if (TARGET_64BIT && CASE_VECTOR_MODE == DImode)
+    directive = ASM_QUAD;
+#else
+  gcc_assert (!TARGET_64BIT);
+#endif
+  /* We can't use @GOTOFF for text labels on VxWorks; see gotoff_operand.  */
+  if (TARGET_64BIT || TARGET_VXWORKS_RTP)
+    fprintf (file, "%s%s%d-%s%d\n",
+	     directive, LPREFIX, value, LPREFIX, rel);
+#if TARGET_MACHO
+  else if (TARGET_MACHO)
+    {
+      fprintf (file, ASM_LONG "%s%d-", LPREFIX, value);
+      machopic_output_function_base_name (file);
+      putc ('\n', file);
+    }
+#endif
+  else if (HAVE_AS_GOTOFF_IN_DATA)
+    fprintf (file, ASM_LONG "%s%d@GOTOFF\n", LPREFIX, value);
+  else
+    asm_fprintf (file, ASM_LONG "%U%s+[.-%s%d]\n",
+		 GOT_SYMBOL_NAME, LPREFIX, value);
+}
+
+#define LEA_MAX_STALL (3)
+#define LEA_SEARCH_THRESHOLD (LEA_MAX_STALL << 1)
+
+/* Increase given DISTANCE in half-cycles according to
+   dependencies between PREV and NEXT instructions.
+   Add 1 half-cycle if there is no dependency and
+   go to next cycle if there is some dependecy.  */
+
+static unsigned int
+increase_distance (rtx_insn *prev, rtx_insn *next, unsigned int distance)
+{
+  df_ref def, use;
+
+  if (!prev || !next)
+    return distance + (distance & 1) + 2;
+
+  if (!DF_INSN_USES (next) || !DF_INSN_DEFS (prev))
+    return distance + 1;
+
+  FOR_EACH_INSN_USE (use, next)
+    FOR_EACH_INSN_DEF (def, prev)
+      if (!DF_REF_IS_ARTIFICIAL (def)
+	  && DF_REF_REGNO (use) == DF_REF_REGNO (def))
+	return distance + (distance & 1) + 2;
+
+  return distance + 1;
+}
+
+/* Function checks if instruction INSN defines register number
+   REGNO1 or REGNO2.  */
+
+bool
+insn_defines_reg (unsigned int regno1, unsigned int regno2,
+		  rtx_insn *insn)
+{
+  df_ref def;
+
+  FOR_EACH_INSN_DEF (def, insn)
+    if (DF_REF_REG_DEF_P (def)
+	&& !DF_REF_IS_ARTIFICIAL (def)
+	&& (regno1 == DF_REF_REGNO (def)
+	    || regno2 == DF_REF_REGNO (def)))
+      return true;
+
+  return false;
+}
+
+/* Function checks if instruction INSN uses register number
+   REGNO as a part of address expression.  */
+
+static bool
+insn_uses_reg_mem (unsigned int regno, rtx insn)
+{
+  df_ref use;
+
+  FOR_EACH_INSN_USE (use, insn)
+    if (DF_REF_REG_MEM_P (use) && regno == DF_REF_REGNO (use))
+      return true;
+
+  return false;
+}
+
+/* Search backward for non-agu definition of register number REGNO1
+   or register number REGNO2 in basic block starting from instruction
+   START up to head of basic block or instruction INSN.
+
+   Function puts true value into *FOUND var if definition was found
+   and false otherwise.
+
+   Distance in half-cycles between START and found instruction or head
+   of BB is added to DISTANCE and returned.  */
+
+static int
+distance_non_agu_define_in_bb (unsigned int regno1, unsigned int regno2,
+			       rtx_insn *insn, int distance,
+			       rtx_insn *start, bool *found)
+{
+  basic_block bb = start ? BLOCK_FOR_INSN (start) : NULL;
+  rtx_insn *prev = start;
+  rtx_insn *next = NULL;
+
+  *found = false;
+
+  while (prev
+	 && prev != insn
+	 && distance < LEA_SEARCH_THRESHOLD)
+    {
+      if (NONDEBUG_INSN_P (prev) && NONJUMP_INSN_P (prev))
+	{
+	  distance = increase_distance (prev, next, distance);
+	  if (insn_defines_reg (regno1, regno2, prev))
+	    {
+	      if (recog_memoized (prev) < 0
+		  || get_attr_type (prev) != TYPE_LEA)
+		{
+		  *found = true;
+		  return distance;
+		}
+	    }
+
+	  next = prev;
+	}
+      if (prev == BB_HEAD (bb))
+	break;
+
+      prev = PREV_INSN (prev);
+    }
+
+  return distance;
+}
+
+/* Search backward for non-agu definition of register number REGNO1
+   or register number REGNO2 in INSN's basic block until
+   1. Pass LEA_SEARCH_THRESHOLD instructions, or
+   2. Reach neighbor BBs boundary, or
+   3. Reach agu definition.
+   Returns the distance between the non-agu definition point and INSN.
+   If no definition point, returns -1.  */
+
+static int
+distance_non_agu_define (unsigned int regno1, unsigned int regno2,
+			 rtx_insn *insn)
+{
+  basic_block bb = BLOCK_FOR_INSN (insn);
+  int distance = 0;
+  bool found = false;
+
+  if (insn != BB_HEAD (bb))
+    distance = distance_non_agu_define_in_bb (regno1, regno2, insn,
+					      distance, PREV_INSN (insn),
+					      &found);
+
+  if (!found && distance < LEA_SEARCH_THRESHOLD)
+    {
+      edge e;
+      edge_iterator ei;
+      bool simple_loop = false;
+
+      FOR_EACH_EDGE (e, ei, bb->preds)
+	if (e->src == bb)
+	  {
+	    simple_loop = true;
+	    break;
+	  }
+
+      if (simple_loop)
+	distance = distance_non_agu_define_in_bb (regno1, regno2,
+						  insn, distance,
+						  BB_END (bb), &found);
+      else
+	{
+	  int shortest_dist = -1;
+	  bool found_in_bb = false;
+
+	  FOR_EACH_EDGE (e, ei, bb->preds)
+	    {
+	      int bb_dist
+		= distance_non_agu_define_in_bb (regno1, regno2,
+						 insn, distance,
+						 BB_END (e->src),
+						 &found_in_bb);
+	      if (found_in_bb)
+		{
+		  if (shortest_dist < 0)
+		    shortest_dist = bb_dist;
+		  else if (bb_dist > 0)
+		    shortest_dist = MIN (bb_dist, shortest_dist);
+
+		  found = true;
+		}
+	    }
+
+	  distance = shortest_dist;
+	}
+    }
+
+  if (!found)
+    return -1;
+
+  return distance >> 1;
+}
+
+/* Return the distance in half-cycles between INSN and the next
+   insn that uses register number REGNO in memory address added
+   to DISTANCE.  Return -1 if REGNO0 is set.
+
+   Put true value into *FOUND if register usage was found and
+   false otherwise.
+   Put true value into *REDEFINED if register redefinition was
+   found and false otherwise.  */
+
+static int
+distance_agu_use_in_bb (unsigned int regno,
+			rtx_insn *insn, int distance, rtx_insn *start,
+			bool *found, bool *redefined)
+{
+  basic_block bb = NULL;
+  rtx_insn *next = start;
+  rtx_insn *prev = NULL;
+
+  *found = false;
+  *redefined = false;
+
+  if (start != NULL_RTX)
+    {
+      bb = BLOCK_FOR_INSN (start);
+      if (start != BB_HEAD (bb))
+	/* If insn and start belong to the same bb, set prev to insn,
+	   so the call to increase_distance will increase the distance
+	   between insns by 1.  */
+	prev = insn;
+    }
+
+  while (next
+	 && next != insn
+	 && distance < LEA_SEARCH_THRESHOLD)
+    {
+      if (NONDEBUG_INSN_P (next) && NONJUMP_INSN_P (next))
+	{
+	  distance = increase_distance(prev, next, distance);
+	  if (insn_uses_reg_mem (regno, next))
+	    {
+	      /* Return DISTANCE if OP0 is used in memory
+		 address in NEXT.  */
+	      *found = true;
+	      return distance;
+	    }
+
+	  if (insn_defines_reg (regno, INVALID_REGNUM, next))
+	    {
+	      /* Return -1 if OP0 is set in NEXT.  */
+	      *redefined = true;
+	      return -1;
+	    }
+
+	  prev = next;
+	}
+
+      if (next == BB_END (bb))
+	break;
+
+      next = NEXT_INSN (next);
+    }
+
+  return distance;
+}
+
+/* Return the distance between INSN and the next insn that uses
+   register number REGNO0 in memory address.  Return -1 if no such
+   a use is found within LEA_SEARCH_THRESHOLD or REGNO0 is set.  */
+
+static int
+distance_agu_use (unsigned int regno0, rtx_insn *insn)
+{
+  basic_block bb = BLOCK_FOR_INSN (insn);
+  int distance = 0;
+  bool found = false;
+  bool redefined = false;
+
+  if (insn != BB_END (bb))
+    distance = distance_agu_use_in_bb (regno0, insn, distance,
+				       NEXT_INSN (insn),
+				       &found, &redefined);
+
+  if (!found && !redefined && distance < LEA_SEARCH_THRESHOLD)
+    {
+      edge e;
+      edge_iterator ei;
+      bool simple_loop = false;
+
+      FOR_EACH_EDGE (e, ei, bb->succs)
+        if (e->dest == bb)
+	  {
+	    simple_loop = true;
+	    break;
+	  }
+
+      if (simple_loop)
+	distance = distance_agu_use_in_bb (regno0, insn,
+					   distance, BB_HEAD (bb),
+					   &found, &redefined);
+      else
+	{
+	  int shortest_dist = -1;
+	  bool found_in_bb = false;
+	  bool redefined_in_bb = false;
+
+	  FOR_EACH_EDGE (e, ei, bb->succs)
+	    {
+	      int bb_dist
+		= distance_agu_use_in_bb (regno0, insn,
+					  distance, BB_HEAD (e->dest),
+					  &found_in_bb, &redefined_in_bb);
+	      if (found_in_bb)
+		{
+		  if (shortest_dist < 0)
+		    shortest_dist = bb_dist;
+		  else if (bb_dist > 0)
+		    shortest_dist = MIN (bb_dist, shortest_dist);
+
+		  found = true;
+		}
+	    }
+
+	  distance = shortest_dist;
+	}
+    }
+
+  if (!found || redefined)
+    return -1;
+
+  return distance >> 1;
+}
+
+/* Define this macro to tune LEA priority vs ADD, it take effect when
+   there is a dilemma of choosing LEA or ADD
+   Negative value: ADD is more preferred than LEA
+   Zero: Neutral
+   Positive value: LEA is more preferred than ADD.  */
+#define IX86_LEA_PRIORITY 0
+
+/* Return true if usage of lea INSN has performance advantage
+   over a sequence of instructions.  Instructions sequence has
+   SPLIT_COST cycles higher latency than lea latency.  */
+
+static bool
+ix86_lea_outperforms (rtx_insn *insn, unsigned int regno0, unsigned int regno1,
+		      unsigned int regno2, int split_cost, bool has_scale)
+{
+  int dist_define, dist_use;
+
+  /* For Atom processors newer than Bonnell, if using a 2-source or
+     3-source LEA for non-destructive destination purposes, or due to
+     wanting ability to use SCALE, the use of LEA is justified.  */
+  if (!TARGET_BONNELL)
+    {
+      if (has_scale)
+	return true;
+      if (split_cost < 1)
+	return false;
+      if (regno0 == regno1 || regno0 == regno2)
+	return false;
+      return true;
+    }
+
+  /* Remember recog_data content.  */
+  struct recog_data_d recog_data_save = recog_data;
+
+  dist_define = distance_non_agu_define (regno1, regno2, insn);
+  dist_use = distance_agu_use (regno0, insn);
+
+  /* distance_non_agu_define can call get_attr_type which can call
+     recog_memoized, restore recog_data back to previous content.  */
+  recog_data = recog_data_save;
+
+  if (dist_define < 0 || dist_define >= LEA_MAX_STALL)
+    {
+      /* If there is no non AGU operand definition, no AGU
+	 operand usage and split cost is 0 then both lea
+	 and non lea variants have same priority.  Currently
+	 we prefer lea for 64 bit code and non lea on 32 bit
+	 code.  */
+      if (dist_use < 0 && split_cost == 0)
+	return TARGET_64BIT || IX86_LEA_PRIORITY;
+      else
+	return true;
+    }
+
+  /* With longer definitions distance lea is more preferable.
+     Here we change it to take into account splitting cost and
+     lea priority.  */
+  dist_define += split_cost + IX86_LEA_PRIORITY;
+
+  /* If there is no use in memory addess then we just check
+     that split cost exceeds AGU stall.  */
+  if (dist_use < 0)
+    return dist_define > LEA_MAX_STALL;
+
+  /* If this insn has both backward non-agu dependence and forward
+     agu dependence, the one with short distance takes effect.  */
+  return dist_define >= dist_use;
+}
+
+/* Return true if we need to split op0 = op1 + op2 into a sequence of
+   move and add to avoid AGU stalls.  */
+
+bool
+ix86_avoid_lea_for_add (rtx_insn *insn, rtx operands[])
+{
+  unsigned int regno0, regno1, regno2;
+
+  /* Check if we need to optimize.  */
+  if (!TARGET_OPT_AGU || optimize_function_for_size_p (cfun))
+    return false;
+
+  regno0 = true_regnum (operands[0]);
+  regno1 = true_regnum (operands[1]);
+  regno2 = true_regnum (operands[2]);
+
+  /* We need to split only adds with non destructive
+     destination operand.  */
+  if (regno0 == regno1 || regno0 == regno2)
+    return false;
+  else
+    return !ix86_lea_outperforms (insn, regno0, regno1, regno2, 1, false);
+}
+
+/* Return true if we should emit lea instruction instead of mov
+   instruction.  */
+
+bool
+ix86_use_lea_for_mov (rtx_insn *insn, rtx operands[])
+{
+  unsigned int regno0, regno1;
+
+  /* Check if we need to optimize.  */
+  if (!TARGET_OPT_AGU || optimize_function_for_size_p (cfun))
+    return false;
+
+  /* Use lea for reg to reg moves only.  */
+  if (!REG_P (operands[0]) || !REG_P (operands[1]))
+    return false;
+
+  regno0 = true_regnum (operands[0]);
+  regno1 = true_regnum (operands[1]);
+
+  return ix86_lea_outperforms (insn, regno0, regno1, INVALID_REGNUM, 0, false);
+}
+
+/* Return true if we need to split lea into a sequence of
+   instructions to avoid AGU stalls during peephole2. */
+
+bool
+ix86_avoid_lea_for_addr (rtx_insn *insn, rtx operands[])
+{
+  unsigned int regno0, regno1, regno2;
+  int split_cost;
+  struct ix86_address parts;
+  int ok;
+
+  /* The "at least two components" test below might not catch simple
+     move or zero extension insns if parts.base is non-NULL and parts.disp
+     is const0_rtx as the only components in the address, e.g. if the
+     register is %rbp or %r13.  As this test is much cheaper and moves or
+     zero extensions are the common case, do this check first.  */
+  if (REG_P (operands[1])
+      || (SImode_address_operand (operands[1], VOIDmode)
+	  && REG_P (XEXP (operands[1], 0))))
+    return false;
+
+  ok = ix86_decompose_address (operands[1], &parts);
+  gcc_assert (ok);
+
+  /* There should be at least two components in the address.  */
+  if ((parts.base != NULL_RTX) + (parts.index != NULL_RTX)
+      + (parts.disp != NULL_RTX) + (parts.scale > 1) < 2)
+    return false;
+
+  /* We should not split into add if non legitimate pic
+     operand is used as displacement. */
+  if (parts.disp && flag_pic && !LEGITIMATE_PIC_OPERAND_P (parts.disp))
+    return false;
+
+  regno0 = true_regnum (operands[0]) ;
+  regno1 = INVALID_REGNUM;
+  regno2 = INVALID_REGNUM;
+
+  if (parts.base)
+    regno1 = true_regnum (parts.base);
+  if (parts.index)
+    regno2 = true_regnum (parts.index);
+
+  /* Use add for a = a + b and a = b + a since it is faster and shorter
+     than lea for most processors.  For the processors like BONNELL, if
+     the destination register of LEA holds an actual address which will
+     be used soon, LEA is better and otherwise ADD is better.  */
+  if (!TARGET_BONNELL
+      && parts.scale == 1
+      && (!parts.disp || parts.disp == const0_rtx)
+      && (regno0 == regno1 || regno0 == regno2))
+    return true;
+
+  /* Check we need to optimize.  */
+  if (!TARGET_AVOID_LEA_FOR_ADDR || optimize_function_for_size_p (cfun))
+    return false;
+
+  split_cost = 0;
+
+  /* Compute how many cycles we will add to execution time
+     if split lea into a sequence of instructions.  */
+  if (parts.base || parts.index)
+    {
+      /* Have to use mov instruction if non desctructive
+	 destination form is used.  */
+      if (regno1 != regno0 && regno2 != regno0)
+	split_cost += 1;
+
+      /* Have to add index to base if both exist.  */
+      if (parts.base && parts.index)
+	split_cost += 1;
+
+      /* Have to use shift and adds if scale is 2 or greater.  */
+      if (parts.scale > 1)
+	{
+	  if (regno0 != regno1)
+	    split_cost += 1;
+	  else if (regno2 == regno0)
+	    split_cost += 4;
+	  else
+	    split_cost += parts.scale;
+	}
+
+      /* Have to use add instruction with immediate if
+	 disp is non zero.  */
+      if (parts.disp && parts.disp != const0_rtx)
+	split_cost += 1;
+
+      /* Subtract the price of lea.  */
+      split_cost -= 1;
+    }
+
+  return !ix86_lea_outperforms (insn, regno0, regno1, regno2, split_cost,
+				parts.scale > 1);
+}
+
+/* Return true if it is ok to optimize an ADD operation to LEA
+   operation to avoid flag register consumation.  For most processors,
+   ADD is faster than LEA.  For the processors like BONNELL, if the
+   destination register of LEA holds an actual address which will be
+   used soon, LEA is better and otherwise ADD is better.  */
+
+bool
+ix86_lea_for_add_ok (rtx_insn *insn, rtx operands[])
+{
+  unsigned int regno0 = true_regnum (operands[0]);
+  unsigned int regno1 = true_regnum (operands[1]);
+  unsigned int regno2 = true_regnum (operands[2]);
+
+  /* If a = b + c, (a!=b && a!=c), must use lea form. */
+  if (regno0 != regno1 && regno0 != regno2)
+    return true;
+
+  if (!TARGET_OPT_AGU || optimize_function_for_size_p (cfun))
+    return false;
+
+  return ix86_lea_outperforms (insn, regno0, regno1, regno2, 0, false);
+}
+
+/* Return true if destination reg of SET_BODY is shift count of
+   USE_BODY.  */
+
+static bool
+ix86_dep_by_shift_count_body (const_rtx set_body, const_rtx use_body)
+{
+  rtx set_dest;
+  rtx shift_rtx;
+  int i;
+
+  /* Retrieve destination of SET_BODY.  */
+  switch (GET_CODE (set_body))
+    {
+    case SET:
+      set_dest = SET_DEST (set_body);
+      if (!set_dest || !REG_P (set_dest))
+	return false;
+      break;
+    case PARALLEL:
+      for (i = XVECLEN (set_body, 0) - 1; i >= 0; i--)
+	if (ix86_dep_by_shift_count_body (XVECEXP (set_body, 0, i),
+					  use_body))
+	  return true;
+      /* FALLTHROUGH */
+    default:
+      return false;
+    }
+
+  /* Retrieve shift count of USE_BODY.  */
+  switch (GET_CODE (use_body))
+    {
+    case SET:
+      shift_rtx = XEXP (use_body, 1);
+      break;
+    case PARALLEL:
+      for (i = XVECLEN (use_body, 0) - 1; i >= 0; i--)
+	if (ix86_dep_by_shift_count_body (set_body,
+					  XVECEXP (use_body, 0, i)))
+	  return true;
+      /* FALLTHROUGH */
+    default:
+      return false;
+    }
+
+  if (shift_rtx
+      && (GET_CODE (shift_rtx) == ASHIFT
+	  || GET_CODE (shift_rtx) == LSHIFTRT
+	  || GET_CODE (shift_rtx) == ASHIFTRT
+	  || GET_CODE (shift_rtx) == ROTATE
+	  || GET_CODE (shift_rtx) == ROTATERT))
+    {
+      rtx shift_count = XEXP (shift_rtx, 1);
+
+      /* Return true if shift count is dest of SET_BODY.  */
+      if (REG_P (shift_count))
+	{
+	  /* Add check since it can be invoked before register
+	     allocation in pre-reload schedule.  */
+	  if (reload_completed
+	      && true_regnum (set_dest) == true_regnum (shift_count))
+	    return true;
+	  else if (REGNO(set_dest) == REGNO(shift_count))
+	    return true;
+	}
+    }
+
+  return false;
+}
+
+/* Return true if destination reg of SET_INSN is shift count of
+   USE_INSN.  */
+
+bool
+ix86_dep_by_shift_count (const_rtx set_insn, const_rtx use_insn)
+{
+  return ix86_dep_by_shift_count_body (PATTERN (set_insn),
+				       PATTERN (use_insn));
+}
+
+/* Return TRUE or FALSE depending on whether the unary operator meets the
+   appropriate constraints.  */
+
+bool
+ix86_unary_operator_ok (enum rtx_code,
+			machine_mode,
+			rtx operands[2])
+{
+  /* If one of operands is memory, source and destination must match.  */
+  if ((MEM_P (operands[0])
+       || MEM_P (operands[1]))
+      && ! rtx_equal_p (operands[0], operands[1]))
+    return false;
+  return true;
+}
+
+/* Return TRUE if the operands to a vec_interleave_{high,low}v2df
+   are ok, keeping in mind the possible movddup alternative.  */
+
+bool
+ix86_vec_interleave_v2df_operator_ok (rtx operands[3], bool high)
+{
+  if (MEM_P (operands[0]))
+    return rtx_equal_p (operands[0], operands[1 + high]);
+  if (MEM_P (operands[1]) && MEM_P (operands[2]))
+    return TARGET_SSE3 && rtx_equal_p (operands[1], operands[2]);
+  return true;
+}
+
+/* A subroutine of ix86_build_signbit_mask.  If VECT is true,
+   then replicate the value for all elements of the vector
+   register.  */
+
+rtx
+ix86_build_const_vector (machine_mode mode, bool vect, rtx value)
+{
+  int i, n_elt;
+  rtvec v;
+  machine_mode scalar_mode;
+
+  switch (mode)
+    {
+    case E_V64QImode:
+    case E_V32QImode:
+    case E_V16QImode:
+    case E_V32HImode:
+    case E_V16HImode:
+    case E_V8HImode:
+    case E_V16SImode:
+    case E_V8SImode:
+    case E_V4SImode:
+    case E_V8DImode:
+    case E_V4DImode:
+    case E_V2DImode:
+      gcc_assert (vect);
+      /* FALLTHRU */
+    case E_V16SFmode:
+    case E_V8SFmode:
+    case E_V4SFmode:
+    case E_V2SFmode:
+    case E_V8DFmode:
+    case E_V4DFmode:
+    case E_V2DFmode:
+      n_elt = GET_MODE_NUNITS (mode);
+      v = rtvec_alloc (n_elt);
+      scalar_mode = GET_MODE_INNER (mode);
+
+      RTVEC_ELT (v, 0) = value;
+
+      for (i = 1; i < n_elt; ++i)
+	RTVEC_ELT (v, i) = vect ? value : CONST0_RTX (scalar_mode);
+
+      return gen_rtx_CONST_VECTOR (mode, v);
+
+    default:
+      gcc_unreachable ();
+    }
+}
+
+/* A subroutine of ix86_expand_fp_absneg_operator, copysign expanders
+   and ix86_expand_int_vcond.  Create a mask for the sign bit in MODE
+   for an SSE register.  If VECT is true, then replicate the mask for
+   all elements of the vector register.  If INVERT is true, then create
+   a mask excluding the sign bit.  */
+
+rtx
+ix86_build_signbit_mask (machine_mode mode, bool vect, bool invert)
+{
+  machine_mode vec_mode, imode;
+  wide_int w;
+  rtx mask, v;
+
+  switch (mode)
+    {
+    case E_V16SImode:
+    case E_V16SFmode:
+    case E_V8SImode:
+    case E_V4SImode:
+    case E_V8SFmode:
+    case E_V4SFmode:
+    case E_V2SFmode:
+      vec_mode = mode;
+      imode = SImode;
+      break;
+
+    case E_V8DImode:
+    case E_V4DImode:
+    case E_V2DImode:
+    case E_V8DFmode:
+    case E_V4DFmode:
+    case E_V2DFmode:
+      vec_mode = mode;
+      imode = DImode;
+      break;
+
+    case E_TImode:
+    case E_TFmode:
+      vec_mode = VOIDmode;
+      imode = TImode;
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  machine_mode inner_mode = GET_MODE_INNER (mode);
+  w = wi::set_bit_in_zero (GET_MODE_BITSIZE (inner_mode) - 1,
+			   GET_MODE_BITSIZE (inner_mode));
+  if (invert)
+    w = wi::bit_not (w);
+
+  /* Force this value into the low part of a fp vector constant.  */
+  mask = immed_wide_int_const (w, imode);
+  mask = gen_lowpart (inner_mode, mask);
+
+  if (vec_mode == VOIDmode)
+    return force_reg (inner_mode, mask);
+
+  v = ix86_build_const_vector (vec_mode, vect, mask);
+  return force_reg (vec_mode, v);
+}
+
+/* Return TRUE or FALSE depending on whether the first SET in INSN
+   has source and destination with matching CC modes, and that the
+   CC mode is at least as constrained as REQ_MODE.  */
+
+bool
+ix86_match_ccmode (rtx insn, machine_mode req_mode)
+{
+  rtx set;
+  machine_mode set_mode;
+
+  set = PATTERN (insn);
+  if (GET_CODE (set) == PARALLEL)
+    set = XVECEXP (set, 0, 0);
+  gcc_assert (GET_CODE (set) == SET);
+  gcc_assert (GET_CODE (SET_SRC (set)) == COMPARE);
+
+  set_mode = GET_MODE (SET_DEST (set));
+  switch (set_mode)
+    {
+    case E_CCNOmode:
+      if (req_mode != CCNOmode
+	  && (req_mode != CCmode
+	      || XEXP (SET_SRC (set), 1) != const0_rtx))
+	return false;
+      break;
+    case E_CCmode:
+      if (req_mode == CCGCmode)
+	return false;
+      /* FALLTHRU */
+    case E_CCGCmode:
+      if (req_mode == CCGOCmode || req_mode == CCNOmode)
+	return false;
+      /* FALLTHRU */
+    case E_CCGOCmode:
+      if (req_mode == CCZmode)
+	return false;
+      /* FALLTHRU */
+    case E_CCZmode:
+      break;
+
+    case E_CCGZmode:
+
+    case E_CCAmode:
+    case E_CCCmode:
+    case E_CCOmode:
+    case E_CCPmode:
+    case E_CCSmode:
+      if (set_mode != req_mode)
+	return false;
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  return GET_MODE (SET_SRC (set)) == set_mode;
+}
+
+machine_mode
+ix86_cc_mode (enum rtx_code code, rtx op0, rtx op1)
+{
+  machine_mode mode = GET_MODE (op0);
+
+  if (SCALAR_FLOAT_MODE_P (mode))
+    {
+      gcc_assert (!DECIMAL_FLOAT_MODE_P (mode));
+      return CCFPmode;
+    }
+
+  switch (code)
+    {
+      /* Only zero flag is needed.  */
+    case EQ:			/* ZF=0 */
+    case NE:			/* ZF!=0 */
+      return CCZmode;
+      /* Codes needing carry flag.  */
+    case GEU:			/* CF=0 */
+    case LTU:			/* CF=1 */
+      rtx geu;
+      /* Detect overflow checks.  They need just the carry flag.  */
+      if (GET_CODE (op0) == PLUS
+	  && (rtx_equal_p (op1, XEXP (op0, 0))
+	      || rtx_equal_p (op1, XEXP (op0, 1))))
+	return CCCmode;
+      /* Similarly for *setcc_qi_addqi3_cconly_overflow_1_* patterns.
+	 Match LTU of op0
+	 (neg:QI (geu:QI (reg:CC_CCC FLAGS_REG) (const_int 0)))
+	 and op1
+	 (ltu:QI (reg:CC_CCC FLAGS_REG) (const_int 0))
+	 where CC_CCC is either CC or CCC.  */
+      else if (code == LTU
+	       && GET_CODE (op0) == NEG
+	       && GET_CODE (geu = XEXP (op0, 0)) == GEU
+	       && REG_P (XEXP (geu, 0))
+	       && (GET_MODE (XEXP (geu, 0)) == CCCmode
+		   || GET_MODE (XEXP (geu, 0)) == CCmode)
+	       && REGNO (XEXP (geu, 0)) == FLAGS_REG
+	       && XEXP (geu, 1) == const0_rtx
+	       && GET_CODE (op1) == LTU
+	       && REG_P (XEXP (op1, 0))
+	       && GET_MODE (XEXP (op1, 0)) == GET_MODE (XEXP (geu, 0))
+	       && REGNO (XEXP (op1, 0)) == FLAGS_REG
+	       && XEXP (op1, 1) == const0_rtx)
+	return CCCmode;
+      else
+	return CCmode;
+    case GTU:			/* CF=0 & ZF=0 */
+    case LEU:			/* CF=1 | ZF=1 */
+      return CCmode;
+      /* Codes possibly doable only with sign flag when
+         comparing against zero.  */
+    case GE:			/* SF=OF   or   SF=0 */
+    case LT:			/* SF<>OF  or   SF=1 */
+      if (op1 == const0_rtx)
+	return CCGOCmode;
+      else
+	/* For other cases Carry flag is not required.  */
+	return CCGCmode;
+      /* Codes doable only with sign flag when comparing
+         against zero, but we miss jump instruction for it
+         so we need to use relational tests against overflow
+         that thus needs to be zero.  */
+    case GT:			/* ZF=0 & SF=OF */
+    case LE:			/* ZF=1 | SF<>OF */
+      if (op1 == const0_rtx)
+	return CCNOmode;
+      else
+	return CCGCmode;
+      /* strcmp pattern do (use flags) and combine may ask us for proper
+	 mode.  */
+    case USE:
+      return CCmode;
+    default:
+      gcc_unreachable ();
+    }
+}
+
+/* Return the fixed registers used for condition codes.  */
+
+static bool
+ix86_fixed_condition_code_regs (unsigned int *p1, unsigned int *p2)
+{
+  *p1 = FLAGS_REG;
+  *p2 = INVALID_REGNUM;
+  return true;
+}
+
+/* If two condition code modes are compatible, return a condition code
+   mode which is compatible with both.  Otherwise, return
+   VOIDmode.  */
+
+static machine_mode
+ix86_cc_modes_compatible (machine_mode m1, machine_mode m2)
+{
+  if (m1 == m2)
+    return m1;
+
+  if (GET_MODE_CLASS (m1) != MODE_CC || GET_MODE_CLASS (m2) != MODE_CC)
+    return VOIDmode;
+
+  if ((m1 == CCGCmode && m2 == CCGOCmode)
+      || (m1 == CCGOCmode && m2 == CCGCmode))
+    return CCGCmode;
+
+  if ((m1 == CCNOmode && m2 == CCGOCmode)
+      || (m1 == CCGOCmode && m2 == CCNOmode))
+    return CCNOmode;
+
+  if (m1 == CCZmode
+      && (m2 == CCGCmode || m2 == CCGOCmode || m2 == CCNOmode))
+    return m2;
+  else if (m2 == CCZmode
+	   && (m1 == CCGCmode || m1 == CCGOCmode || m1 == CCNOmode))
+    return m1;
+
+  switch (m1)
+    {
+    default:
+      gcc_unreachable ();
+
+    case E_CCmode:
+    case E_CCGCmode:
+    case E_CCGOCmode:
+    case E_CCNOmode:
+    case E_CCAmode:
+    case E_CCCmode:
+    case E_CCOmode:
+    case E_CCPmode:
+    case E_CCSmode:
+    case E_CCZmode:
+      switch (m2)
+	{
+	default:
+	  return VOIDmode;
+
+	case E_CCmode:
+	case E_CCGCmode:
+	case E_CCGOCmode:
+	case E_CCNOmode:
+	case E_CCAmode:
+	case E_CCCmode:
+	case E_CCOmode:
+	case E_CCPmode:
+	case E_CCSmode:
+	case E_CCZmode:
+	  return CCmode;
+	}
+
+    case E_CCFPmode:
+      /* These are only compatible with themselves, which we already
+	 checked above.  */
+      return VOIDmode;
+    }
+}
+
+/* Return strategy to use for floating-point.  We assume that fcomi is always
+   preferrable where available, since that is also true when looking at size
+   (2 bytes, vs. 3 for fnstsw+sahf and at least 5 for fnstsw+test).  */
+
+enum ix86_fpcmp_strategy
+ix86_fp_comparison_strategy (enum rtx_code)
+{
+  /* Do fcomi/sahf based test when profitable.  */
+
+  if (TARGET_CMOVE)
+    return IX86_FPCMP_COMI;
+
+  if (TARGET_SAHF && (TARGET_USE_SAHF || optimize_insn_for_size_p ()))
+    return IX86_FPCMP_SAHF;
+
+  return IX86_FPCMP_ARITH;
+}
+
+/* Convert comparison codes we use to represent FP comparison to integer
+   code that will result in proper branch.  Return UNKNOWN if no such code
+   is available.  */
+
+enum rtx_code
+ix86_fp_compare_code_to_integer (enum rtx_code code)
+{
+  switch (code)
+    {
+    case GT:
+      return GTU;
+    case GE:
+      return GEU;
+    case ORDERED:
+    case UNORDERED:
+      return code;
+    case UNEQ:
+      return EQ;
+    case UNLT:
+      return LTU;
+    case UNLE:
+      return LEU;
+    case LTGT:
+      return NE;
+    default:
+      return UNKNOWN;
+    }
+}
+
+/* Zero extend possibly SImode EXP to Pmode register.  */
+rtx
+ix86_zero_extend_to_Pmode (rtx exp)
+{
+  return force_reg (Pmode, convert_to_mode (Pmode, exp, 1));
+}
+
+/* Return true if the function being called was marked with attribute
+   "noplt" or using -fno-plt and we are compiling for non-PIC.  We need
+   to handle the non-PIC case in the backend because there is no easy
+   interface for the front-end to force non-PLT calls to use the GOT.
+   This is currently used only with 64-bit or 32-bit GOT32X ELF targets
+   to call the function marked "noplt" indirectly.  */
+
+static bool
+ix86_nopic_noplt_attribute_p (rtx call_op)
+{
+  if (flag_pic || ix86_cmodel == CM_LARGE
+      || !(TARGET_64BIT || HAVE_AS_IX86_GOT32X)
+      || TARGET_MACHO || TARGET_SEH || TARGET_PECOFF
+      || SYMBOL_REF_LOCAL_P (call_op))
+    return false;
+
+  tree symbol_decl = SYMBOL_REF_DECL (call_op);
+
+  if (!flag_plt
+      || (symbol_decl != NULL_TREE
+          && lookup_attribute ("noplt", DECL_ATTRIBUTES (symbol_decl))))
+    return true;
+
+  return false;
+}
+
+/* Helper to output the jmp/call.  */
+static void
+ix86_output_jmp_thunk_or_indirect (const char *thunk_name, const int regno)
+{
+  if (thunk_name != NULL)
+    {
+      fprintf (asm_out_file, "\tjmp\t");
+      assemble_name (asm_out_file, thunk_name);
+      putc ('\n', asm_out_file);
+    }
+  else
+    output_indirect_thunk (regno);
+}
+
+/* Output indirect branch via a call and return thunk.  CALL_OP is a
+   register which contains the branch target.  XASM is the assembly
+   template for CALL_OP.  Branch is a tail call if SIBCALL_P is true.
+   A normal call is converted to:
+
+	call __x86_indirect_thunk_reg
+
+   and a tail call is converted to:
+
+	jmp __x86_indirect_thunk_reg
+ */
+
+static void
+ix86_output_indirect_branch_via_reg (rtx call_op, bool sibcall_p)
+{
+  char thunk_name_buf[32];
+  char *thunk_name;
+  enum indirect_thunk_prefix need_prefix
+    = indirect_thunk_need_prefix (current_output_insn);
+  int regno = REGNO (call_op);
+
+  if (cfun->machine->indirect_branch_type
+      != indirect_branch_thunk_inline)
+    {
+      if (cfun->machine->indirect_branch_type == indirect_branch_thunk)
+	{
+	  int i = regno;
+	  if (i >= FIRST_REX_INT_REG)
+	    i -= (FIRST_REX_INT_REG - LAST_INT_REG - 1);
+	  indirect_thunks_used |= 1 << i;
+	}
+      indirect_thunk_name (thunk_name_buf, regno, need_prefix, false);
+      thunk_name = thunk_name_buf;
+    }
+  else
+    thunk_name = NULL;
+
+  if (sibcall_p)
+     ix86_output_jmp_thunk_or_indirect (thunk_name, regno);
+  else
+    {
+      if (thunk_name != NULL)
+	{
+	  fprintf (asm_out_file, "\tcall\t");
+	  assemble_name (asm_out_file, thunk_name);
+	  putc ('\n', asm_out_file);
+	  return;
+	}
+
+      char indirectlabel1[32];
+      char indirectlabel2[32];
+
+      ASM_GENERATE_INTERNAL_LABEL (indirectlabel1,
+				   INDIRECT_LABEL,
+				   indirectlabelno++);
+      ASM_GENERATE_INTERNAL_LABEL (indirectlabel2,
+				   INDIRECT_LABEL,
+				   indirectlabelno++);
+
+      /* Jump.  */
+      fputs ("\tjmp\t", asm_out_file);
+      assemble_name_raw (asm_out_file, indirectlabel2);
+      fputc ('\n', asm_out_file);
+
+      ASM_OUTPUT_INTERNAL_LABEL (asm_out_file, indirectlabel1);
+
+     ix86_output_jmp_thunk_or_indirect (thunk_name, regno);
+
+      ASM_OUTPUT_INTERNAL_LABEL (asm_out_file, indirectlabel2);
+
+      /* Call.  */
+      fputs ("\tcall\t", asm_out_file);
+      assemble_name_raw (asm_out_file, indirectlabel1);
+      fputc ('\n', asm_out_file);
+    }
+}
+
+/* Output indirect branch via a call and return thunk.  CALL_OP is
+   the branch target.  XASM is the assembly template for CALL_OP.
+   Branch is a tail call if SIBCALL_P is true.  A normal call is
+   converted to:
+
+	jmp L2
+   L1:
+	push CALL_OP
+	jmp __x86_indirect_thunk
+   L2:
+	call L1
+
+   and a tail call is converted to:
+
+	push CALL_OP
+	jmp __x86_indirect_thunk
+ */
+
+static void
+ix86_output_indirect_branch_via_push (rtx call_op, const char *xasm,
+				      bool sibcall_p)
+{
+  char thunk_name_buf[32];
+  char *thunk_name;
+  char push_buf[64];
+  enum indirect_thunk_prefix need_prefix
+    = indirect_thunk_need_prefix (current_output_insn);
+  int regno = -1;
+
+  if (cfun->machine->indirect_branch_type
+      != indirect_branch_thunk_inline)
+    {
+      if (cfun->machine->indirect_branch_type == indirect_branch_thunk)
+	indirect_thunk_needed = true;
+      indirect_thunk_name (thunk_name_buf, regno, need_prefix, false);
+      thunk_name = thunk_name_buf;
+    }
+  else
+    thunk_name = NULL;
+
+  snprintf (push_buf, sizeof (push_buf), "push{%c}\t%s",
+	    TARGET_64BIT ? 'q' : 'l', xasm);
+
+  if (sibcall_p)
+    {
+      output_asm_insn (push_buf, &call_op);
+      ix86_output_jmp_thunk_or_indirect (thunk_name, regno);
+    }
+  else
+    {
+      char indirectlabel1[32];
+      char indirectlabel2[32];
+
+      ASM_GENERATE_INTERNAL_LABEL (indirectlabel1,
+				   INDIRECT_LABEL,
+				   indirectlabelno++);
+      ASM_GENERATE_INTERNAL_LABEL (indirectlabel2,
+				   INDIRECT_LABEL,
+				   indirectlabelno++);
+
+      /* Jump.  */
+      fputs ("\tjmp\t", asm_out_file);
+      assemble_name_raw (asm_out_file, indirectlabel2);
+      fputc ('\n', asm_out_file);
+
+      ASM_OUTPUT_INTERNAL_LABEL (asm_out_file, indirectlabel1);
+
+      /* An external function may be called via GOT, instead of PLT.  */
+      if (MEM_P (call_op))
+	{
+	  struct ix86_address parts;
+	  rtx addr = XEXP (call_op, 0);
+	  if (ix86_decompose_address (addr, &parts)
+	      && parts.base == stack_pointer_rtx)
+	    {
+	      /* Since call will adjust stack by -UNITS_PER_WORD,
+		 we must convert "disp(stack, index, scale)" to
+		 "disp+UNITS_PER_WORD(stack, index, scale)".  */
+	      if (parts.index)
+		{
+		  addr = gen_rtx_MULT (Pmode, parts.index,
+				       GEN_INT (parts.scale));
+		  addr = gen_rtx_PLUS (Pmode, stack_pointer_rtx,
+				       addr);
+		}
+	      else
+		addr = stack_pointer_rtx;
+
+	      rtx disp;
+	      if (parts.disp != NULL_RTX)
+		disp = plus_constant (Pmode, parts.disp,
+				      UNITS_PER_WORD);
+	      else
+		disp = GEN_INT (UNITS_PER_WORD);
+
+	      addr = gen_rtx_PLUS (Pmode, addr, disp);
+	      call_op = gen_rtx_MEM (GET_MODE (call_op), addr);
+	    }
+	}
+
+      output_asm_insn (push_buf, &call_op);
+
+      ix86_output_jmp_thunk_or_indirect (thunk_name, regno);
+
+      ASM_OUTPUT_INTERNAL_LABEL (asm_out_file, indirectlabel2);
+
+      /* Call.  */
+      fputs ("\tcall\t", asm_out_file);
+      assemble_name_raw (asm_out_file, indirectlabel1);
+      fputc ('\n', asm_out_file);
+    }
+}
+
+/* Output indirect branch via a call and return thunk.  CALL_OP is
+   the branch target.  XASM is the assembly template for CALL_OP.
+   Branch is a tail call if SIBCALL_P is true.   */
+
+static void
+ix86_output_indirect_branch (rtx call_op, const char *xasm,
+			     bool sibcall_p)
+{
+  if (REG_P (call_op))
+    ix86_output_indirect_branch_via_reg (call_op, sibcall_p);
+  else
+    ix86_output_indirect_branch_via_push (call_op, xasm, sibcall_p);
+}
+
+/* Output indirect jump.  CALL_OP is the jump target.  */
+
+const char *
+ix86_output_indirect_jmp (rtx call_op)
+{
+  if (cfun->machine->indirect_branch_type != indirect_branch_keep)
+    {
+      /* We can't have red-zone since "call" in the indirect thunk
+         pushes the return address onto stack, destroying red-zone.  */
+      if (ix86_red_zone_used)
+	gcc_unreachable ();
+
+      ix86_output_indirect_branch (call_op, "%0", true);
+      return "";
+    }
+  else
+    return "%!jmp\t%A0";
+}
+
+/* Output return instrumentation for current function if needed.  */
+
+static void
+output_return_instrumentation (void)
+{
+  if (ix86_instrument_return != instrument_return_none
+      && flag_fentry
+      && !DECL_NO_INSTRUMENT_FUNCTION_ENTRY_EXIT (cfun->decl))
+    {
+      if (ix86_flag_record_return)
+	fprintf (asm_out_file, "1:\n");
+      switch (ix86_instrument_return)
+	{
+	case instrument_return_call:
+	  fprintf (asm_out_file, "\tcall\t__return__\n");
+	  break;
+	case instrument_return_nop5:
+	  /* 5 byte nop: nopl 0(%[re]ax,%[re]ax,1)  */
+	  fprintf (asm_out_file, ASM_BYTE "0x0f, 0x1f, 0x44, 0x00, 0x00\n");
+	  break;
+	case instrument_return_none:
+	  break;
+	}
+
+      if (ix86_flag_record_return)
+	{
+	  fprintf (asm_out_file, "\t.section __return_loc, \"a\",@progbits\n");
+	  fprintf (asm_out_file, "\t.%s 1b\n", TARGET_64BIT ? "quad" : "long");
+	  fprintf (asm_out_file, "\t.previous\n");
+	}
+    }
+}
+
+/* Output function return.  CALL_OP is the jump target.  Add a REP
+   prefix to RET if LONG_P is true and function return is kept.  */
+
+const char *
+ix86_output_function_return (bool long_p)
+{
+  output_return_instrumentation ();
+
+  if (cfun->machine->function_return_type != indirect_branch_keep)
+    {
+      char thunk_name[32];
+      enum indirect_thunk_prefix need_prefix
+	= indirect_thunk_need_prefix (current_output_insn);
+
+      if (cfun->machine->function_return_type
+	  != indirect_branch_thunk_inline)
+	{
+	  bool need_thunk = (cfun->machine->function_return_type
+			     == indirect_branch_thunk);
+	  indirect_thunk_name (thunk_name, INVALID_REGNUM, need_prefix,
+			       true);
+	  indirect_return_needed |= need_thunk;
+	  fprintf (asm_out_file, "\tjmp\t");
+	  assemble_name (asm_out_file, thunk_name);
+	  putc ('\n', asm_out_file);
+	}
+      else
+	output_indirect_thunk (INVALID_REGNUM);
+
+      return "";
+    }
+
+  if (!long_p)
+    return "%!ret";
+
+  return "rep%; ret";
+}
+
+/* Output indirect function return.  RET_OP is the function return
+   target.  */
+
+const char *
+ix86_output_indirect_function_return (rtx ret_op)
+{
+  if (cfun->machine->function_return_type != indirect_branch_keep)
+    {
+      char thunk_name[32];
+      enum indirect_thunk_prefix need_prefix
+	= indirect_thunk_need_prefix (current_output_insn);
+      unsigned int regno = REGNO (ret_op);
+      gcc_assert (regno == CX_REG);
+
+      if (cfun->machine->function_return_type
+	  != indirect_branch_thunk_inline)
+	{
+	  bool need_thunk = (cfun->machine->function_return_type
+			     == indirect_branch_thunk);
+	  indirect_thunk_name (thunk_name, regno, need_prefix, true);
+
+	  if (need_thunk)
+	    {
+	      indirect_return_via_cx = true;
+	      indirect_thunks_used |= 1 << CX_REG;
+	    }
+	  fprintf (asm_out_file, "\tjmp\t");
+	  assemble_name (asm_out_file, thunk_name);
+	  putc ('\n', asm_out_file);
+	}
+      else
+	output_indirect_thunk (regno);
+
+      return "";
+    }
+  else
+    return "%!jmp\t%A0";
+}
+
+/* Output the assembly for a call instruction.  */
+
+const char *
+ix86_output_call_insn (rtx_insn *insn, rtx call_op)
+{
+  bool direct_p = constant_call_address_operand (call_op, VOIDmode);
+  bool output_indirect_p
+    = (!TARGET_SEH
+       && cfun->machine->indirect_branch_type != indirect_branch_keep);
+  bool seh_nop_p = false;
+  const char *xasm;
+
+  if (SIBLING_CALL_P (insn))
+    {
+      output_return_instrumentation ();
+      if (direct_p)
+	{
+	  if (ix86_nopic_noplt_attribute_p (call_op))
+	    {
+	      direct_p = false;
+	      if (TARGET_64BIT)
+		{
+		  if (output_indirect_p)
+		    xasm = "{%p0@GOTPCREL(%%rip)|[QWORD PTR %p0@GOTPCREL[rip]]}";
+		  else
+		    xasm = "%!jmp\t{*%p0@GOTPCREL(%%rip)|[QWORD PTR %p0@GOTPCREL[rip]]}";
+		}
+	      else
+		{
+		  if (output_indirect_p)
+		    xasm = "{%p0@GOT|[DWORD PTR %p0@GOT]}";
+		  else
+		    xasm = "%!jmp\t{*%p0@GOT|[DWORD PTR %p0@GOT]}";
+		}
+	    }
+	  else
+	    xasm = "%!jmp\t%P0";
+	}
+      /* SEH epilogue detection requires the indirect branch case
+	 to include REX.W.  */
+      else if (TARGET_SEH)
+	xasm = "%!rex.W jmp\t%A0";
+      else
+	{
+	  if (output_indirect_p)
+	    xasm = "%0";
+	  else
+	    xasm = "%!jmp\t%A0";
+	}
+
+      if (output_indirect_p && !direct_p)
+	ix86_output_indirect_branch (call_op, xasm, true);
+      else
+	output_asm_insn (xasm, &call_op);
+      return "";
+    }
+
+  /* SEH unwinding can require an extra nop to be emitted in several
+     circumstances.  Determine if we have one of those.  */
+  if (TARGET_SEH)
+    {
+      rtx_insn *i;
+
+      for (i = NEXT_INSN (insn); i ; i = NEXT_INSN (i))
+	{
+	  /* Prevent a catch region from being adjacent to a jump that would
+	     be interpreted as an epilogue sequence by the unwinder.  */
+	  if (JUMP_P(i) && CROSSING_JUMP_P (i))
+	    {
+	      seh_nop_p = true;
+	      break;
+	    }
+	    
+	  /* If we get to another real insn, we don't need the nop.  */
+	  if (INSN_P (i))
+	    break;
+
+	  /* If we get to the epilogue note, prevent a catch region from
+	     being adjacent to the standard epilogue sequence.  If non-
+	     call-exceptions, we'll have done this during epilogue emission. */
+	  if (NOTE_P (i) && NOTE_KIND (i) == NOTE_INSN_EPILOGUE_BEG
+	      && !flag_non_call_exceptions
+	      && !can_throw_internal (insn))
+	    {
+	      seh_nop_p = true;
+	      break;
+	    }
+	}
+
+      /* If we didn't find a real insn following the call, prevent the
+	 unwinder from looking into the next function.  */
+      if (i == NULL)
+	seh_nop_p = true;
+    }
+
+  if (direct_p)
+    {
+      if (ix86_nopic_noplt_attribute_p (call_op))
+	{
+	  direct_p = false;
+	  if (TARGET_64BIT)
+	    {
+	      if (output_indirect_p)
+		xasm = "{%p0@GOTPCREL(%%rip)|[QWORD PTR %p0@GOTPCREL[rip]]}";
+	      else
+		xasm = "%!call\t{*%p0@GOTPCREL(%%rip)|[QWORD PTR %p0@GOTPCREL[rip]]}";
+	    }
+	  else
+	    {
+	      if (output_indirect_p)
+		xasm = "{%p0@GOT|[DWORD PTR %p0@GOT]}";
+	      else
+		xasm = "%!call\t{*%p0@GOT|[DWORD PTR %p0@GOT]}";
+	    }
+	}
+      else
+	xasm = "%!call\t%P0";
+    }
+  else
+    {
+      if (output_indirect_p)
+	xasm = "%0";
+      else
+	xasm = "%!call\t%A0";
+    }
+
+  if (output_indirect_p && !direct_p)
+    ix86_output_indirect_branch (call_op, xasm, false);
+  else
+    output_asm_insn (xasm, &call_op);
+
+  if (seh_nop_p)
+    return "nop";
+
+  return "";
+}
+
+/* Return a MEM corresponding to a stack slot with mode MODE.
+   Allocate a new slot if necessary.
+
+   The RTL for a function can have several slots available: N is
+   which slot to use.  */
+
+rtx
+assign_386_stack_local (machine_mode mode, enum ix86_stack_slot n)
+{
+  struct stack_local_entry *s;
+
+  gcc_assert (n < MAX_386_STACK_LOCALS);
+
+  for (s = ix86_stack_locals; s; s = s->next)
+    if (s->mode == mode && s->n == n)
+      return validize_mem (copy_rtx (s->rtl));
+
+  s = ggc_alloc<stack_local_entry> ();
+  s->n = n;
+  s->mode = mode;
+  s->rtl = assign_stack_local (mode, GET_MODE_SIZE (mode), 0);
+
+  s->next = ix86_stack_locals;
+  ix86_stack_locals = s;
+  return validize_mem (copy_rtx (s->rtl));
+}
+
+static void
+ix86_instantiate_decls (void)
+{
+  struct stack_local_entry *s;
+
+  for (s = ix86_stack_locals; s; s = s->next)
+    if (s->rtl != NULL_RTX)
+      instantiate_decl_rtl (s->rtl);
+}
+
+/* Check whether x86 address PARTS is a pc-relative address.  */
+
+bool
+ix86_rip_relative_addr_p (struct ix86_address *parts)
+{
+  rtx base, index, disp;
+
+  base = parts->base;
+  index = parts->index;
+  disp = parts->disp;
+
+  if (disp && !base && !index)
+    {
+      if (TARGET_64BIT)
+	{
+	  rtx symbol = disp;
+
+	  if (GET_CODE (disp) == CONST)
+	    symbol = XEXP (disp, 0);
+	  if (GET_CODE (symbol) == PLUS
+	      && CONST_INT_P (XEXP (symbol, 1)))
+	    symbol = XEXP (symbol, 0);
+
+	  if (GET_CODE (symbol) == LABEL_REF
+	      || (GET_CODE (symbol) == SYMBOL_REF
+		  && SYMBOL_REF_TLS_MODEL (symbol) == 0)
+	      || (GET_CODE (symbol) == UNSPEC
+		  && (XINT (symbol, 1) == UNSPEC_GOTPCREL
+		      || XINT (symbol, 1) == UNSPEC_PCREL
+		      || XINT (symbol, 1) == UNSPEC_GOTNTPOFF)))
+	    return true;
+	}
+    }
+  return false;
+}
+
+/* Calculate the length of the memory address in the instruction encoding.
+   Includes addr32 prefix, does not include the one-byte modrm, opcode,
+   or other prefixes.  We never generate addr32 prefix for LEA insn.  */
+
+int
+memory_address_length (rtx addr, bool lea)
+{
+  struct ix86_address parts;
+  rtx base, index, disp;
+  int len;
+  int ok;
+
+  if (GET_CODE (addr) == PRE_DEC
+      || GET_CODE (addr) == POST_INC
+      || GET_CODE (addr) == PRE_MODIFY
+      || GET_CODE (addr) == POST_MODIFY)
+    return 0;
+
+  ok = ix86_decompose_address (addr, &parts);
+  gcc_assert (ok);
+
+  len = (parts.seg == ADDR_SPACE_GENERIC) ? 0 : 1;
+
+  /*  If this is not LEA instruction, add the length of addr32 prefix.  */
+  if (TARGET_64BIT && !lea
+      && (SImode_address_operand (addr, VOIDmode)
+	  || (parts.base && GET_MODE (parts.base) == SImode)
+	  || (parts.index && GET_MODE (parts.index) == SImode)))
+    len++;
+
+  base = parts.base;
+  index = parts.index;
+  disp = parts.disp;
+
+  if (base && SUBREG_P (base))
+    base = SUBREG_REG (base);
+  if (index && SUBREG_P (index))
+    index = SUBREG_REG (index);
+
+  gcc_assert (base == NULL_RTX || REG_P (base));
+  gcc_assert (index == NULL_RTX || REG_P (index));
+
+  /* Rule of thumb:
+       - esp as the base always wants an index,
+       - ebp as the base always wants a displacement,
+       - r12 as the base always wants an index,
+       - r13 as the base always wants a displacement.  */
+
+  /* Register Indirect.  */
+  if (base && !index && !disp)
+    {
+      /* esp (for its index) and ebp (for its displacement) need
+	 the two-byte modrm form.  Similarly for r12 and r13 in 64-bit
+	 code.  */
+      if (base == arg_pointer_rtx
+	  || base == frame_pointer_rtx
+	  || REGNO (base) == SP_REG
+	  || REGNO (base) == BP_REG
+	  || REGNO (base) == R12_REG
+	  || REGNO (base) == R13_REG)
+	len++;
+    }
+
+  /* Direct Addressing.  In 64-bit mode mod 00 r/m 5
+     is not disp32, but disp32(%rip), so for disp32
+     SIB byte is needed, unless print_operand_address
+     optimizes it into disp32(%rip) or (%rip) is implied
+     by UNSPEC.  */
+  else if (disp && !base && !index)
+    {
+      len += 4;
+      if (!ix86_rip_relative_addr_p (&parts))
+	len++;
+    }
+  else
+    {
+      /* Find the length of the displacement constant.  */
+      if (disp)
+	{
+	  if (base && satisfies_constraint_K (disp))
+	    len += 1;
+	  else
+	    len += 4;
+	}
+      /* ebp always wants a displacement.  Similarly r13.  */
+      else if (base && (REGNO (base) == BP_REG || REGNO (base) == R13_REG))
+	len++;
+
+      /* An index requires the two-byte modrm form....  */
+      if (index
+	  /* ...like esp (or r12), which always wants an index.  */
+	  || base == arg_pointer_rtx
+	  || base == frame_pointer_rtx
+	  || (base && (REGNO (base) == SP_REG || REGNO (base) == R12_REG)))
+	len++;
+    }
+
+  return len;
+}
+
+/* Compute default value for "length_immediate" attribute.  When SHORTFORM
+   is set, expect that insn have 8bit immediate alternative.  */
+int
+ix86_attr_length_immediate_default (rtx_insn *insn, bool shortform)
+{
+  int len = 0;
+  int i;
+  extract_insn_cached (insn);
+  for (i = recog_data.n_operands - 1; i >= 0; --i)
+    if (CONSTANT_P (recog_data.operand[i]))
+      {
+        enum attr_mode mode = get_attr_mode (insn);
+
+	gcc_assert (!len);
+	if (shortform && CONST_INT_P (recog_data.operand[i]))
+	  {
+	    HOST_WIDE_INT ival = INTVAL (recog_data.operand[i]);
+	    switch (mode)
+	      {
+	      case MODE_QI:
+		len = 1;
+		continue;
+	      case MODE_HI:
+		ival = trunc_int_for_mode (ival, HImode);
+		break;
+	      case MODE_SI:
+		ival = trunc_int_for_mode (ival, SImode);
+		break;
+	      default:
+		break;
+	      }
+	    if (IN_RANGE (ival, -128, 127))
+	      {
+		len = 1;
+		continue;
+	      }
+	  }
+	switch (mode)
+	  {
+	  case MODE_QI:
+	    len = 1;
+	    break;
+	  case MODE_HI:
+	    len = 2;
+	    break;
+	  case MODE_SI:
+	    len = 4;
+	    break;
+	  /* Immediates for DImode instructions are encoded
+	     as 32bit sign extended values.  */
+	  case MODE_DI:
+	    len = 4;
+	    break;
+	  default:
+	    fatal_insn ("unknown insn mode", insn);
+	}
+      }
+  return len;
+}
+
+/* Compute default value for "length_address" attribute.  */
+int
+ix86_attr_length_address_default (rtx_insn *insn)
+{
+  int i;
+
+  if (get_attr_type (insn) == TYPE_LEA)
+    {
+      rtx set = PATTERN (insn), addr;
+
+      if (GET_CODE (set) == PARALLEL)
+	set = XVECEXP (set, 0, 0);
+
+      gcc_assert (GET_CODE (set) == SET);
+
+      addr = SET_SRC (set);
+
+      return memory_address_length (addr, true);
+    }
+
+  extract_insn_cached (insn);
+  for (i = recog_data.n_operands - 1; i >= 0; --i)
+    {
+      rtx op = recog_data.operand[i];
+      if (MEM_P (op))
+	{
+	  constrain_operands_cached (insn, reload_completed);
+	  if (which_alternative != -1)
+	    {
+	      const char *constraints = recog_data.constraints[i];
+	      int alt = which_alternative;
+
+	      while (*constraints == '=' || *constraints == '+')
+		constraints++;
+	      while (alt-- > 0)
+	        while (*constraints++ != ',')
+		  ;
+	      /* Skip ignored operands.  */
+	      if (*constraints == 'X')
+		continue;
+	    }
+
+	  int len = memory_address_length (XEXP (op, 0), false);
+
+	  /* Account for segment prefix for non-default addr spaces.  */
+	  if (!ADDR_SPACE_GENERIC_P (MEM_ADDR_SPACE (op)))
+	    len++;
+
+	  return len;
+	}
+    }
+  return 0;
+}
+
+/* Compute default value for "length_vex" attribute. It includes
+   2 or 3 byte VEX prefix and 1 opcode byte.  */
+
+int
+ix86_attr_length_vex_default (rtx_insn *insn, bool has_0f_opcode,
+			      bool has_vex_w)
+{
+  int i;
+
+  /* Only 0f opcode can use 2 byte VEX prefix and  VEX W bit uses 3
+     byte VEX prefix.  */
+  if (!has_0f_opcode || has_vex_w)
+    return 3 + 1;
+
+ /* We can always use 2 byte VEX prefix in 32bit.  */
+  if (!TARGET_64BIT)
+    return 2 + 1;
+
+  extract_insn_cached (insn);
+
+  for (i = recog_data.n_operands - 1; i >= 0; --i)
+    if (REG_P (recog_data.operand[i]))
+      {
+	/* REX.W bit uses 3 byte VEX prefix.  */
+	if (GET_MODE (recog_data.operand[i]) == DImode
+	    && GENERAL_REG_P (recog_data.operand[i]))
+	  return 3 + 1;
+      }
+    else
+      {
+	/* REX.X or REX.B bits use 3 byte VEX prefix.  */
+	if (MEM_P (recog_data.operand[i])
+	    && x86_extended_reg_mentioned_p (recog_data.operand[i]))
+	  return 3 + 1;
+      }
+
+  return 2 + 1;
+}
+
+
+static bool
+ix86_class_likely_spilled_p (reg_class_t);
+
+/* Returns true if lhs of insn is HW function argument register and set up
+   is_spilled to true if it is likely spilled HW register.  */
+static bool
+insn_is_function_arg (rtx insn, bool* is_spilled)
+{
+  rtx dst;
+
+  if (!NONDEBUG_INSN_P (insn))
+    return false;
+  /* Call instructions are not movable, ignore it.  */
+  if (CALL_P (insn))
+    return false;
+  insn = PATTERN (insn);
+  if (GET_CODE (insn) == PARALLEL)
+    insn = XVECEXP (insn, 0, 0);
+  if (GET_CODE (insn) != SET)
+    return false;
+  dst = SET_DEST (insn);
+  if (REG_P (dst) && HARD_REGISTER_P (dst)
+      && ix86_function_arg_regno_p (REGNO (dst)))
+    {
+      /* Is it likely spilled HW register?  */
+      if (!TEST_HARD_REG_BIT (fixed_reg_set, REGNO (dst))
+	  && ix86_class_likely_spilled_p (REGNO_REG_CLASS (REGNO (dst))))
+	*is_spilled = true;
+      return true;
+    }
+  return false;
+}
+
+/* Add output dependencies for chain of function adjacent arguments if only
+   there is a move to likely spilled HW register.  Return first argument
+   if at least one dependence was added or NULL otherwise.  */
+static rtx_insn *
+add_parameter_dependencies (rtx_insn *call, rtx_insn *head)
+{
+  rtx_insn *insn;
+  rtx_insn *last = call;
+  rtx_insn *first_arg = NULL;
+  bool is_spilled = false;
+
+  head = PREV_INSN (head);
+
+  /* Find nearest to call argument passing instruction.  */
+  while (true)
+    {
+      last = PREV_INSN (last);
+      if (last == head)
+	return NULL;
+      if (!NONDEBUG_INSN_P (last))
+	continue;
+      if (insn_is_function_arg (last, &is_spilled))
+	break;
+      return NULL;
+    }
+
+  first_arg = last;
+  while (true)
+    {
+      insn = PREV_INSN (last);
+      if (!INSN_P (insn))
+	break;
+      if (insn == head)
+	break;
+      if (!NONDEBUG_INSN_P (insn))
+	{
+	  last = insn;
+	  continue;
+	}
+      if (insn_is_function_arg (insn, &is_spilled))
+	{
+	  /* Add output depdendence between two function arguments if chain
+	     of output arguments contains likely spilled HW registers.  */
+	  if (is_spilled)
+	    add_dependence (first_arg, insn, REG_DEP_OUTPUT);
+	  first_arg = last = insn;
+	}
+      else
+	break;
+    }
+  if (!is_spilled)
+    return NULL;
+  return first_arg;
+}
+
+/* Add output or anti dependency from insn to first_arg to restrict its code
+   motion.  */
+static void
+avoid_func_arg_motion (rtx_insn *first_arg, rtx_insn *insn)
+{
+  rtx set;
+  rtx tmp;
+
+  set = single_set (insn);
+  if (!set)
+    return;
+  tmp = SET_DEST (set);
+  if (REG_P (tmp))
+    {
+      /* Add output dependency to the first function argument.  */
+      add_dependence (first_arg, insn, REG_DEP_OUTPUT);
+      return;
+    }
+  /* Add anti dependency.  */
+  add_dependence (first_arg, insn, REG_DEP_ANTI);
+}
+
+/* Avoid cross block motion of function argument through adding dependency
+   from the first non-jump instruction in bb.  */
+static void
+add_dependee_for_func_arg (rtx_insn *arg, basic_block bb)
+{
+  rtx_insn *insn = BB_END (bb);
+
+  while (insn)
+    {
+      if (NONDEBUG_INSN_P (insn) && NONJUMP_INSN_P (insn))
+	{
+	  rtx set = single_set (insn);
+	  if (set)
+	    {
+	      avoid_func_arg_motion (arg, insn);
+	      return;
+	    }
+	}
+      if (insn == BB_HEAD (bb))
+	return;
+      insn = PREV_INSN (insn);
+    }
+}
+
+/* Hook for pre-reload schedule - avoid motion of function arguments
+   passed in likely spilled HW registers.  */
+static void
+ix86_dependencies_evaluation_hook (rtx_insn *head, rtx_insn *tail)
+{
+  rtx_insn *insn;
+  rtx_insn *first_arg = NULL;
+  if (reload_completed)
+    return;
+  while (head != tail && DEBUG_INSN_P (head))
+    head = NEXT_INSN (head);
+  for (insn = tail; insn != head; insn = PREV_INSN (insn))
+    if (INSN_P (insn) && CALL_P (insn))
+      {
+	first_arg = add_parameter_dependencies (insn, head);
+	if (first_arg)
+	  {
+	    /* Add dependee for first argument to predecessors if only
+	       region contains more than one block.  */
+	    basic_block bb =  BLOCK_FOR_INSN (insn);
+	    int rgn = CONTAINING_RGN (bb->index);
+	    int nr_blks = RGN_NR_BLOCKS (rgn);
+	    /* Skip trivial regions and region head blocks that can have
+	       predecessors outside of region.  */
+	    if (nr_blks > 1 && BLOCK_TO_BB (bb->index) != 0)
+	      {
+		edge e;
+		edge_iterator ei;
+
+		/* Regions are SCCs with the exception of selective
+		   scheduling with pipelining of outer blocks enabled.
+		   So also check that immediate predecessors of a non-head
+		   block are in the same region.  */
+		FOR_EACH_EDGE (e, ei, bb->preds)
+		  {
+		    /* Avoid creating of loop-carried dependencies through
+		       using topological ordering in the region.  */
+		    if (rgn == CONTAINING_RGN (e->src->index)
+			&& BLOCK_TO_BB (bb->index) > BLOCK_TO_BB (e->src->index))
+		      add_dependee_for_func_arg (first_arg, e->src); 
+		  }
+	      }
+	    insn = first_arg;
+	    if (insn == head)
+	      break;
+	  }
+      }
+    else if (first_arg)
+      avoid_func_arg_motion (first_arg, insn);
+}
+
+/* Hook for pre-reload schedule - set priority of moves from likely spilled
+   HW registers to maximum, to schedule them at soon as possible. These are
+   moves from function argument registers at the top of the function entry
+   and moves from function return value registers after call.  */
+static int
+ix86_adjust_priority (rtx_insn *insn, int priority)
+{
+  rtx set;
+
+  if (reload_completed)
+    return priority;
+
+  if (!NONDEBUG_INSN_P (insn))
+    return priority;
+
+  set = single_set (insn);
+  if (set)
+    {
+      rtx tmp = SET_SRC (set);
+      if (REG_P (tmp)
+          && HARD_REGISTER_P (tmp)
+          && !TEST_HARD_REG_BIT (fixed_reg_set, REGNO (tmp))
+          && ix86_class_likely_spilled_p (REGNO_REG_CLASS (REGNO (tmp))))
+	return current_sched_info->sched_max_insns_priority;
+    }
+
+  return priority;
+}
+
+/* Prepare for scheduling pass.  */
+static void
+ix86_sched_init_global (FILE *, int, int)
+{
+  /* Install scheduling hooks for current CPU.  Some of these hooks are used
+     in time-critical parts of the scheduler, so we only set them up when
+     they are actually used.  */
+  switch (ix86_tune)
+    {
+    case PROCESSOR_CORE2:
+    case PROCESSOR_NEHALEM:
+    case PROCESSOR_SANDYBRIDGE:
+    case PROCESSOR_HASWELL:
+    case PROCESSOR_GENERIC:
+      /* Do not perform multipass scheduling for pre-reload schedule
+         to save compile time.  */
+      if (reload_completed)
+	{
+	  ix86_core2i7_init_hooks ();
+	  break;
+	}
+      /* Fall through.  */
+    default:
+      targetm.sched.dfa_post_advance_cycle = NULL;
+      targetm.sched.first_cycle_multipass_init = NULL;
+      targetm.sched.first_cycle_multipass_begin = NULL;
+      targetm.sched.first_cycle_multipass_issue = NULL;
+      targetm.sched.first_cycle_multipass_backtrack = NULL;
+      targetm.sched.first_cycle_multipass_end = NULL;
+      targetm.sched.first_cycle_multipass_fini = NULL;
+      break;
+    }
+}
+
+
+/* Implement TARGET_STATIC_RTX_ALIGNMENT.  */
+
+static HOST_WIDE_INT
+ix86_static_rtx_alignment (machine_mode mode)
+{
+  if (mode == DFmode)
+    return 64;
+  if (ALIGN_MODE_128 (mode))
+    return MAX (128, GET_MODE_ALIGNMENT (mode));
+  return GET_MODE_ALIGNMENT (mode);
+}
+
+/* Implement TARGET_CONSTANT_ALIGNMENT.  */
+
+static HOST_WIDE_INT
+ix86_constant_alignment (const_tree exp, HOST_WIDE_INT align)
+{
+  if (TREE_CODE (exp) == REAL_CST || TREE_CODE (exp) == VECTOR_CST
+      || TREE_CODE (exp) == INTEGER_CST)
+    {
+      machine_mode mode = TYPE_MODE (TREE_TYPE (exp));
+      HOST_WIDE_INT mode_align = ix86_static_rtx_alignment (mode);
+      return MAX (mode_align, align);
+    }
+  else if (!optimize_size && TREE_CODE (exp) == STRING_CST
+	   && TREE_STRING_LENGTH (exp) >= 31 && align < BITS_PER_WORD)
+    return BITS_PER_WORD;
+
+  return align;
+}
+
+/* Implement TARGET_EMPTY_RECORD_P.  */
+
+static bool
+ix86_is_empty_record (const_tree type)
+{
+  if (!TARGET_64BIT)
+    return false;
+  return default_is_empty_record (type);
+}
+
+/* Implement TARGET_WARN_PARAMETER_PASSING_ABI.  */
+
+static void
+ix86_warn_parameter_passing_abi (cumulative_args_t cum_v, tree type)
+{
+  CUMULATIVE_ARGS *cum = get_cumulative_args (cum_v);
+
+  if (!cum->warn_empty)
+    return;
+
+  if (!TYPE_EMPTY_P (type))
+    return;
+
+  /* Don't warn if the function isn't visible outside of the TU.  */
+  if (cum->decl && !TREE_PUBLIC (cum->decl))
+    return;
+
+  const_tree ctx = get_ultimate_context (cum->decl);
+  if (ctx != NULL_TREE
+      && !TRANSLATION_UNIT_WARN_EMPTY_P (ctx))
+    return;
+
+  /* If the actual size of the type is zero, then there is no change
+     in how objects of this size are passed.  */
+  if (int_size_in_bytes (type) == 0)
+    return;
+
+  warning (OPT_Wabi, "empty class %qT parameter passing ABI "
+	   "changes in %<-fabi-version=12%> (GCC 8)", type);
+
+  /* Only warn once.  */
+  cum->warn_empty = false;
+}
+
+/* This hook returns name of multilib ABI.  */
+
+static const char *
+ix86_get_multilib_abi_name (void)
+{
+  if (!(TARGET_64BIT_P (ix86_isa_flags)))
+    return "i386";
+  else if (TARGET_X32_P (ix86_isa_flags))
+    return "x32";
+  else
+    return "x86_64";
+}
+
+/* Compute the alignment for a variable for Intel MCU psABI.  TYPE is
+   the data type, and ALIGN is the alignment that the object would
+   ordinarily have.  */
+
+static int
+iamcu_alignment (tree type, int align)
+{
+  machine_mode mode;
+
+  if (align < 32 || TYPE_USER_ALIGN (type))
+    return align;
+
+  /* Intel MCU psABI specifies scalar types > 4 bytes aligned to 4
+     bytes.  */
+  type = strip_array_types (type);
+  if (TYPE_ATOMIC (type))
+    return align;
+
+  mode = TYPE_MODE (type);
+  switch (GET_MODE_CLASS (mode))
+    {
+    case MODE_INT:
+    case MODE_COMPLEX_INT:
+    case MODE_COMPLEX_FLOAT:
+    case MODE_FLOAT:
+    case MODE_DECIMAL_FLOAT:
+      return 32;
+    default:
+      return align;
+    }
+}
+
+/* Compute the alignment for a static variable.
+   TYPE is the data type, and ALIGN is the alignment that
+   the object would ordinarily have.  The value of this function is used
+   instead of that alignment to align the object.  */
+
+int
+ix86_data_alignment (tree type, unsigned int align, bool opt)
+{
+  /* GCC 4.8 and earlier used to incorrectly assume this alignment even
+     for symbols from other compilation units or symbols that don't need
+     to bind locally.  In order to preserve some ABI compatibility with
+     those compilers, ensure we don't decrease alignment from what we
+     used to assume.  */
+
+  unsigned int max_align_compat = MIN (256, MAX_OFILE_ALIGNMENT);
+
+  /* A data structure, equal or greater than the size of a cache line
+     (64 bytes in the Pentium 4 and other recent Intel processors, including
+     processors based on Intel Core microarchitecture) should be aligned
+     so that its base address is a multiple of a cache line size.  */
+
+  unsigned int max_align
+    = MIN ((unsigned) ix86_tune_cost->prefetch_block * 8, MAX_OFILE_ALIGNMENT);
+
+  if (max_align < BITS_PER_WORD)
+    max_align = BITS_PER_WORD;
+
+  switch (ix86_align_data_type)
+    {
+    case ix86_align_data_type_abi: opt = false; break;
+    case ix86_align_data_type_compat: max_align = BITS_PER_WORD; break;
+    case ix86_align_data_type_cacheline: break;
+    }
+
+  if (TARGET_IAMCU)
+    align = iamcu_alignment (type, align);
+
+  if (opt
+      && AGGREGATE_TYPE_P (type)
+      && TYPE_SIZE (type)
+      && TREE_CODE (TYPE_SIZE (type)) == INTEGER_CST)
+    {
+      if (wi::geu_p (wi::to_wide (TYPE_SIZE (type)), max_align_compat)
+	  && align < max_align_compat)
+	align = max_align_compat;
+      if (wi::geu_p (wi::to_wide (TYPE_SIZE (type)), max_align)
+	  && align < max_align)
+	align = max_align;
+    }
+
+  /* x86-64 ABI requires arrays greater than 16 bytes to be aligned
+     to 16byte boundary.  */
+  if (TARGET_64BIT)
+    {
+      if ((opt ? AGGREGATE_TYPE_P (type) : TREE_CODE (type) == ARRAY_TYPE)
+	  && TYPE_SIZE (type)
+	  && TREE_CODE (TYPE_SIZE (type)) == INTEGER_CST
+	  && wi::geu_p (wi::to_wide (TYPE_SIZE (type)), 128)
+	  && align < 128)
+	return 128;
+    }
+
+  if (!opt)
+    return align;
+
+  if (TREE_CODE (type) == ARRAY_TYPE)
+    {
+      if (TYPE_MODE (TREE_TYPE (type)) == DFmode && align < 64)
+	return 64;
+      if (ALIGN_MODE_128 (TYPE_MODE (TREE_TYPE (type))) && align < 128)
+	return 128;
+    }
+  else if (TREE_CODE (type) == COMPLEX_TYPE)
+    {
+
+      if (TYPE_MODE (type) == DCmode && align < 64)
+	return 64;
+      if ((TYPE_MODE (type) == XCmode
+	   || TYPE_MODE (type) == TCmode) && align < 128)
+	return 128;
+    }
+  else if ((TREE_CODE (type) == RECORD_TYPE
+	    || TREE_CODE (type) == UNION_TYPE
+	    || TREE_CODE (type) == QUAL_UNION_TYPE)
+	   && TYPE_FIELDS (type))
+    {
+      if (DECL_MODE (TYPE_FIELDS (type)) == DFmode && align < 64)
+	return 64;
+      if (ALIGN_MODE_128 (DECL_MODE (TYPE_FIELDS (type))) && align < 128)
+	return 128;
+    }
+  else if (TREE_CODE (type) == REAL_TYPE || TREE_CODE (type) == VECTOR_TYPE
+	   || TREE_CODE (type) == INTEGER_TYPE)
+    {
+      if (TYPE_MODE (type) == DFmode && align < 64)
+	return 64;
+      if (ALIGN_MODE_128 (TYPE_MODE (type)) && align < 128)
+	return 128;
+    }
+
+  return align;
+}
+
+/* Implememnt TARGET_LOWER_LOCAL_DECL_ALIGNMENT.  */
+static void
+ix86_lower_local_decl_alignment (tree decl)
+{
+  unsigned int new_align = ix86_local_alignment (decl, VOIDmode,
+						 DECL_ALIGN (decl), true);
+  if (new_align < DECL_ALIGN (decl))
+    SET_DECL_ALIGN (decl, new_align);
+}
+
+/* Compute the alignment for a local variable or a stack slot.  EXP is
+   the data type or decl itself, MODE is the widest mode available and
+   ALIGN is the alignment that the object would ordinarily have.  The
+   value of this macro is used instead of that alignment to align the
+   object.  */
+
+unsigned int
+ix86_local_alignment (tree exp, machine_mode mode,
+		      unsigned int align, bool may_lower)
+{
+  tree type, decl;
+
+  if (exp && DECL_P (exp))
+    {
+      type = TREE_TYPE (exp);
+      decl = exp;
+    }
+  else
+    {
+      type = exp;
+      decl = NULL;
+    }
+
+  /* Don't do dynamic stack realignment for long long objects with
+     -mpreferred-stack-boundary=2.  */
+  if (may_lower
+      && !TARGET_64BIT
+      && align == 64
+      && ix86_preferred_stack_boundary < 64
+      && (mode == DImode || (type && TYPE_MODE (type) == DImode))
+      && (!type || (!TYPE_USER_ALIGN (type)
+		    && !TYPE_ATOMIC (strip_array_types (type))))
+      && (!decl || !DECL_USER_ALIGN (decl)))
+    align = 32;
+
+  /* If TYPE is NULL, we are allocating a stack slot for caller-save
+     register in MODE.  We will return the largest alignment of XF
+     and DF.  */
+  if (!type)
+    {
+      if (mode == XFmode && align < GET_MODE_ALIGNMENT (DFmode))
+	align = GET_MODE_ALIGNMENT (DFmode);
+      return align;
+    }
+
+  /* Don't increase alignment for Intel MCU psABI.  */
+  if (TARGET_IAMCU)
+    return align;
+
+  /* x86-64 ABI requires arrays greater than 16 bytes to be aligned
+     to 16byte boundary.  Exact wording is:
+
+     An array uses the same alignment as its elements, except that a local or
+     global array variable of length at least 16 bytes or
+     a C99 variable-length array variable always has alignment of at least 16 bytes.
+
+     This was added to allow use of aligned SSE instructions at arrays.  This
+     rule is meant for static storage (where compiler cannot do the analysis
+     by itself).  We follow it for automatic variables only when convenient.
+     We fully control everything in the function compiled and functions from
+     other unit cannot rely on the alignment.
+
+     Exclude va_list type.  It is the common case of local array where
+     we cannot benefit from the alignment.  
+
+     TODO: Probably one should optimize for size only when var is not escaping.  */
+  if (TARGET_64BIT && optimize_function_for_speed_p (cfun)
+      && TARGET_SSE)
+    {
+      if (AGGREGATE_TYPE_P (type)
+	  && (va_list_type_node == NULL_TREE
+	      || (TYPE_MAIN_VARIANT (type)
+		  != TYPE_MAIN_VARIANT (va_list_type_node)))
+	  && TYPE_SIZE (type)
+	  && TREE_CODE (TYPE_SIZE (type)) == INTEGER_CST
+	  && wi::geu_p (wi::to_wide (TYPE_SIZE (type)), 128)
+	  && align < 128)
+	return 128;
+    }
+  if (TREE_CODE (type) == ARRAY_TYPE)
+    {
+      if (TYPE_MODE (TREE_TYPE (type)) == DFmode && align < 64)
+	return 64;
+      if (ALIGN_MODE_128 (TYPE_MODE (TREE_TYPE (type))) && align < 128)
+	return 128;
+    }
+  else if (TREE_CODE (type) == COMPLEX_TYPE)
+    {
+      if (TYPE_MODE (type) == DCmode && align < 64)
+	return 64;
+      if ((TYPE_MODE (type) == XCmode
+	   || TYPE_MODE (type) == TCmode) && align < 128)
+	return 128;
+    }
+  else if ((TREE_CODE (type) == RECORD_TYPE
+	    || TREE_CODE (type) == UNION_TYPE
+	    || TREE_CODE (type) == QUAL_UNION_TYPE)
+	   && TYPE_FIELDS (type))
+    {
+      if (DECL_MODE (TYPE_FIELDS (type)) == DFmode && align < 64)
+	return 64;
+      if (ALIGN_MODE_128 (DECL_MODE (TYPE_FIELDS (type))) && align < 128)
+	return 128;
+    }
+  else if (TREE_CODE (type) == REAL_TYPE || TREE_CODE (type) == VECTOR_TYPE
+	   || TREE_CODE (type) == INTEGER_TYPE)
+    {
+
+      if (TYPE_MODE (type) == DFmode && align < 64)
+	return 64;
+      if (ALIGN_MODE_128 (TYPE_MODE (type)) && align < 128)
+	return 128;
+    }
+  return align;
+}
+
+/* Compute the minimum required alignment for dynamic stack realignment
+   purposes for a local variable, parameter or a stack slot.  EXP is
+   the data type or decl itself, MODE is its mode and ALIGN is the
+   alignment that the object would ordinarily have.  */
+
+unsigned int
+ix86_minimum_alignment (tree exp, machine_mode mode,
+			unsigned int align)
+{
+  tree type, decl;
+
+  if (exp && DECL_P (exp))
+    {
+      type = TREE_TYPE (exp);
+      decl = exp;
+    }
+  else
+    {
+      type = exp;
+      decl = NULL;
+    }
+
+  if (TARGET_64BIT || align != 64 || ix86_preferred_stack_boundary >= 64)
+    return align;
+
+  /* Don't do dynamic stack realignment for long long objects with
+     -mpreferred-stack-boundary=2.  */
+  if ((mode == DImode || (type && TYPE_MODE (type) == DImode))
+      && (!type || (!TYPE_USER_ALIGN (type)
+		    && !TYPE_ATOMIC (strip_array_types (type))))
+      && (!decl || !DECL_USER_ALIGN (decl)))
+    {
+      gcc_checking_assert (!TARGET_STV);
+      return 32;
+    }
+
+  return align;
+}
+
+/* Find a location for the static chain incoming to a nested function.
+   This is a register, unless all free registers are used by arguments.  */
+
+static rtx
+ix86_static_chain (const_tree fndecl_or_type, bool incoming_p)
+{
+  unsigned regno;
+
+  if (TARGET_64BIT)
+    {
+      /* We always use R10 in 64-bit mode.  */
+      regno = R10_REG;
+    }
+  else
+    {
+      const_tree fntype, fndecl;
+      unsigned int ccvt;
+
+      /* By default in 32-bit mode we use ECX to pass the static chain.  */
+      regno = CX_REG;
+
+      if (TREE_CODE (fndecl_or_type) == FUNCTION_DECL)
+	{
+          fntype = TREE_TYPE (fndecl_or_type);
+	  fndecl = fndecl_or_type;
+	}
+      else
+	{
+	  fntype = fndecl_or_type;
+	  fndecl = NULL;
+	}
+
+      ccvt = ix86_get_callcvt (fntype);
+      if ((ccvt & IX86_CALLCVT_FASTCALL) != 0)
+	{
+	  /* Fastcall functions use ecx/edx for arguments, which leaves
+	     us with EAX for the static chain.
+	     Thiscall functions use ecx for arguments, which also
+	     leaves us with EAX for the static chain.  */
+	  regno = AX_REG;
+	}
+      else if ((ccvt & IX86_CALLCVT_THISCALL) != 0)
+	{
+	  /* Thiscall functions use ecx for arguments, which leaves
+	     us with EAX and EDX for the static chain.
+	     We are using for abi-compatibility EAX.  */
+	  regno = AX_REG;
+	}
+      else if (ix86_function_regparm (fntype, fndecl) == 3)
+	{
+	  /* For regparm 3, we have no free call-clobbered registers in
+	     which to store the static chain.  In order to implement this,
+	     we have the trampoline push the static chain to the stack.
+	     However, we can't push a value below the return address when
+	     we call the nested function directly, so we have to use an
+	     alternate entry point.  For this we use ESI, and have the
+	     alternate entry point push ESI, so that things appear the
+	     same once we're executing the nested function.  */
+	  if (incoming_p)
+	    {
+	      if (fndecl == current_function_decl
+		  && !ix86_static_chain_on_stack)
+		{
+		  gcc_assert (!reload_completed);
+		  ix86_static_chain_on_stack = true;
+		}
+	      return gen_frame_mem (SImode,
+				    plus_constant (Pmode,
+						   arg_pointer_rtx, -8));
+	    }
+	  regno = SI_REG;
+	}
+    }
+
+  return gen_rtx_REG (Pmode, regno);
+}
+
+/* Emit RTL insns to initialize the variable parts of a trampoline.
+   FNDECL is the decl of the target address; M_TRAMP is a MEM for
+   the trampoline, and CHAIN_VALUE is an RTX for the static chain
+   to be passed to the target function.  */
+
+static void
+ix86_trampoline_init (rtx m_tramp, tree fndecl, rtx chain_value)
+{
+  rtx mem, fnaddr;
+  int opcode;
+  int offset = 0;
+  bool need_endbr = (flag_cf_protection & CF_BRANCH);
+
+  fnaddr = XEXP (DECL_RTL (fndecl), 0);
+
+  if (TARGET_64BIT)
+    {
+      int size;
+
+      if (need_endbr)
+	{
+	  /* Insert ENDBR64.  */
+	  mem = adjust_address (m_tramp, SImode, offset);
+	  emit_move_insn (mem, gen_int_mode (0xfa1e0ff3, SImode));
+	  offset += 4;
+	}
+
+      /* Load the function address to r11.  Try to load address using
+	 the shorter movl instead of movabs.  We may want to support
+	 movq for kernel mode, but kernel does not use trampolines at
+	 the moment.  FNADDR is a 32bit address and may not be in
+	 DImode when ptr_mode == SImode.  Always use movl in this
+	 case.  */
+      if (ptr_mode == SImode
+	  || x86_64_zext_immediate_operand (fnaddr, VOIDmode))
+	{
+	  fnaddr = copy_addr_to_reg (fnaddr);
+
+	  mem = adjust_address (m_tramp, HImode, offset);
+	  emit_move_insn (mem, gen_int_mode (0xbb41, HImode));
+
+	  mem = adjust_address (m_tramp, SImode, offset + 2);
+	  emit_move_insn (mem, gen_lowpart (SImode, fnaddr));
+	  offset += 6;
+	}
+      else
+	{
+	  mem = adjust_address (m_tramp, HImode, offset);
+	  emit_move_insn (mem, gen_int_mode (0xbb49, HImode));
+
+	  mem = adjust_address (m_tramp, DImode, offset + 2);
+	  emit_move_insn (mem, fnaddr);
+	  offset += 10;
+	}
+
+      /* Load static chain using movabs to r10.  Use the shorter movl
+         instead of movabs when ptr_mode == SImode.  */
+      if (ptr_mode == SImode)
+	{
+	  opcode = 0xba41;
+	  size = 6;
+	}
+      else
+	{
+	  opcode = 0xba49;
+	  size = 10;
+	}
+
+      mem = adjust_address (m_tramp, HImode, offset);
+      emit_move_insn (mem, gen_int_mode (opcode, HImode));
+
+      mem = adjust_address (m_tramp, ptr_mode, offset + 2);
+      emit_move_insn (mem, chain_value);
+      offset += size;
+
+      /* Jump to r11; the last (unused) byte is a nop, only there to
+	 pad the write out to a single 32-bit store.  */
+      mem = adjust_address (m_tramp, SImode, offset);
+      emit_move_insn (mem, gen_int_mode (0x90e3ff49, SImode));
+      offset += 4;
+    }
+  else
+    {
+      rtx disp, chain;
+
+      /* Depending on the static chain location, either load a register
+	 with a constant, or push the constant to the stack.  All of the
+	 instructions are the same size.  */
+      chain = ix86_static_chain (fndecl, true);
+      if (REG_P (chain))
+	{
+	  switch (REGNO (chain))
+	    {
+	    case AX_REG:
+	      opcode = 0xb8; break;
+	    case CX_REG:
+	      opcode = 0xb9; break;
+	    default:
+	      gcc_unreachable ();
+	    }
+	}
+      else
+	opcode = 0x68;
+
+      if (need_endbr)
+	{
+	  /* Insert ENDBR32.  */
+	  mem = adjust_address (m_tramp, SImode, offset);
+	  emit_move_insn (mem, gen_int_mode (0xfb1e0ff3, SImode));
+	  offset += 4;
+	}
+
+      mem = adjust_address (m_tramp, QImode, offset);
+      emit_move_insn (mem, gen_int_mode (opcode, QImode));
+
+      mem = adjust_address (m_tramp, SImode, offset + 1);
+      emit_move_insn (mem, chain_value);
+      offset += 5;
+
+      mem = adjust_address (m_tramp, QImode, offset);
+      emit_move_insn (mem, gen_int_mode (0xe9, QImode));
+
+      mem = adjust_address (m_tramp, SImode, offset + 1);
+
+      /* Compute offset from the end of the jmp to the target function.
+	 In the case in which the trampoline stores the static chain on
+	 the stack, we need to skip the first insn which pushes the
+	 (call-saved) register static chain; this push is 1 byte.  */
+      offset += 5;
+      int skip = MEM_P (chain) ? 1 : 0;
+      /* Skip ENDBR32 at the entry of the target function.  */
+      if (need_endbr
+	  && !cgraph_node::get (fndecl)->only_called_directly_p ())
+	skip += 4;
+      disp = expand_binop (SImode, sub_optab, fnaddr,
+			   plus_constant (Pmode, XEXP (m_tramp, 0),
+					  offset - skip),
+			   NULL_RTX, 1, OPTAB_DIRECT);
+      emit_move_insn (mem, disp);
+    }
+
+  gcc_assert (offset <= TRAMPOLINE_SIZE);
+
+#ifdef HAVE_ENABLE_EXECUTE_STACK
+#ifdef CHECK_EXECUTE_STACK_ENABLED
+  if (CHECK_EXECUTE_STACK_ENABLED)
+#endif
+  emit_library_call (gen_rtx_SYMBOL_REF (Pmode, "__enable_execute_stack"),
+		     LCT_NORMAL, VOIDmode, XEXP (m_tramp, 0), Pmode);
+#endif
+}
+
+static bool
+ix86_allocate_stack_slots_for_args (void)
+{
+  /* Naked functions should not allocate stack slots for arguments.  */
+  return !ix86_function_naked (current_function_decl);
+}
+
+static bool
+ix86_warn_func_return (tree decl)
+{
+  /* Naked functions are implemented entirely in assembly, including the
+     return sequence, so suppress warnings about this.  */
+  return !ix86_function_naked (decl);
+}
+
+/* Return the shift count of a vector by scalar shift builtin second argument
+   ARG1.  */
+static tree
+ix86_vector_shift_count (tree arg1)
+{
+  if (tree_fits_uhwi_p (arg1))
+    return arg1;
+  else if (TREE_CODE (arg1) == VECTOR_CST && CHAR_BIT == 8)
+    {
+      /* The count argument is weird, passed in as various 128-bit
+	 (or 64-bit) vectors, the low 64 bits from it are the count.  */
+      unsigned char buf[16];
+      int len = native_encode_expr (arg1, buf, 16);
+      if (len == 0)
+	return NULL_TREE;
+      tree t = native_interpret_expr (uint64_type_node, buf, len);
+      if (t && tree_fits_uhwi_p (t))
+	return t;
+    }
+  return NULL_TREE;
+}
+
+static tree
+ix86_fold_builtin (tree fndecl, int n_args,
+		   tree *args, bool ignore ATTRIBUTE_UNUSED)
+{
+  if (DECL_BUILT_IN_CLASS (fndecl) == BUILT_IN_MD)
+    {
+      enum ix86_builtins fn_code
+	= (enum ix86_builtins) DECL_MD_FUNCTION_CODE (fndecl);
+      enum rtx_code rcode;
+      bool is_vshift;
+      unsigned HOST_WIDE_INT mask;
+
+      switch (fn_code)
+	{
+	case IX86_BUILTIN_CPU_IS:
+	case IX86_BUILTIN_CPU_SUPPORTS:
+	  gcc_assert (n_args == 1);
+	  return fold_builtin_cpu (fndecl, args);
+
+	case IX86_BUILTIN_NANQ:
+	case IX86_BUILTIN_NANSQ:
+	  {
+	    tree type = TREE_TYPE (TREE_TYPE (fndecl));
+	    const char *str = c_getstr (*args);
+	    int quiet = fn_code == IX86_BUILTIN_NANQ;
+	    REAL_VALUE_TYPE real;
+
+	    if (str && real_nan (&real, str, quiet, TYPE_MODE (type)))
+	      return build_real (type, real);
+	    return NULL_TREE;
+	  }
+
+	case IX86_BUILTIN_INFQ:
+	case IX86_BUILTIN_HUGE_VALQ:
+	  {
+	    tree type = TREE_TYPE (TREE_TYPE (fndecl));
+	    REAL_VALUE_TYPE inf;
+	    real_inf (&inf);
+	    return build_real (type, inf);
+	  }
+
+	case IX86_BUILTIN_TZCNT16:
+	case IX86_BUILTIN_CTZS:
+	case IX86_BUILTIN_TZCNT32:
+	case IX86_BUILTIN_TZCNT64:
+	  gcc_assert (n_args == 1);
+	  if (TREE_CODE (args[0]) == INTEGER_CST)
+	    {
+	      tree type = TREE_TYPE (TREE_TYPE (fndecl));
+	      tree arg = args[0];
+	      if (fn_code == IX86_BUILTIN_TZCNT16
+		  || fn_code == IX86_BUILTIN_CTZS)
+		arg = fold_convert (short_unsigned_type_node, arg);
+	      if (integer_zerop (arg))
+		return build_int_cst (type, TYPE_PRECISION (TREE_TYPE (arg)));
+	      else
+		return fold_const_call (CFN_CTZ, type, arg);
+	    }
+	  break;
+
+	case IX86_BUILTIN_LZCNT16:
+	case IX86_BUILTIN_CLZS:
+	case IX86_BUILTIN_LZCNT32:
+	case IX86_BUILTIN_LZCNT64:
+	  gcc_assert (n_args == 1);
+	  if (TREE_CODE (args[0]) == INTEGER_CST)
+	    {
+	      tree type = TREE_TYPE (TREE_TYPE (fndecl));
+	      tree arg = args[0];
+	      if (fn_code == IX86_BUILTIN_LZCNT16
+		  || fn_code == IX86_BUILTIN_CLZS)
+		arg = fold_convert (short_unsigned_type_node, arg);
+	      if (integer_zerop (arg))
+		return build_int_cst (type, TYPE_PRECISION (TREE_TYPE (arg)));
+	      else
+		return fold_const_call (CFN_CLZ, type, arg);
+	    }
+	  break;
+
+	case IX86_BUILTIN_BEXTR32:
+	case IX86_BUILTIN_BEXTR64:
+	case IX86_BUILTIN_BEXTRI32:
+	case IX86_BUILTIN_BEXTRI64:
+	  gcc_assert (n_args == 2);
+	  if (tree_fits_uhwi_p (args[1]))
+	    {
+	      unsigned HOST_WIDE_INT res = 0;
+	      unsigned int prec = TYPE_PRECISION (TREE_TYPE (args[0]));
+	      unsigned int start = tree_to_uhwi (args[1]);
+	      unsigned int len = (start & 0xff00) >> 8;
+	      start &= 0xff;
+	      if (start >= prec || len == 0)
+		res = 0;
+	      else if (!tree_fits_uhwi_p (args[0]))
+		break;
+	      else
+		res = tree_to_uhwi (args[0]) >> start;
+	      if (len > prec)
+		len = prec;
+	      if (len < HOST_BITS_PER_WIDE_INT)
+		res &= (HOST_WIDE_INT_1U << len) - 1;
+	      return build_int_cstu (TREE_TYPE (TREE_TYPE (fndecl)), res);
+	    }
+	  break;
+
+	case IX86_BUILTIN_BZHI32:
+	case IX86_BUILTIN_BZHI64:
+	  gcc_assert (n_args == 2);
+	  if (tree_fits_uhwi_p (args[1]))
+	    {
+	      unsigned int idx = tree_to_uhwi (args[1]) & 0xff;
+	      if (idx >= TYPE_PRECISION (TREE_TYPE (args[0])))
+		return args[0];
+	      if (idx == 0)
+		return build_int_cst (TREE_TYPE (TREE_TYPE (fndecl)), 0);
+	      if (!tree_fits_uhwi_p (args[0]))
+		break;
+	      unsigned HOST_WIDE_INT res = tree_to_uhwi (args[0]);
+	      res &= ~(HOST_WIDE_INT_M1U << idx);
+	      return build_int_cstu (TREE_TYPE (TREE_TYPE (fndecl)), res);
+	    }
+	  break;
+
+	case IX86_BUILTIN_PDEP32:
+	case IX86_BUILTIN_PDEP64:
+	  gcc_assert (n_args == 2);
+	  if (tree_fits_uhwi_p (args[0]) && tree_fits_uhwi_p (args[1]))
+	    {
+	      unsigned HOST_WIDE_INT src = tree_to_uhwi (args[0]);
+	      unsigned HOST_WIDE_INT mask = tree_to_uhwi (args[1]);
+	      unsigned HOST_WIDE_INT res = 0;
+	      unsigned HOST_WIDE_INT m, k = 1;
+	      for (m = 1; m; m <<= 1)
+		if ((mask & m) != 0)
+		  {
+		    if ((src & k) != 0)
+		      res |= m;
+		    k <<= 1;
+		  }
+	      return build_int_cstu (TREE_TYPE (TREE_TYPE (fndecl)), res);
+	    }
+	  break;
+
+	case IX86_BUILTIN_PEXT32:
+	case IX86_BUILTIN_PEXT64:
+	  gcc_assert (n_args == 2);
+	  if (tree_fits_uhwi_p (args[0]) && tree_fits_uhwi_p (args[1]))
+	    {
+	      unsigned HOST_WIDE_INT src = tree_to_uhwi (args[0]);
+	      unsigned HOST_WIDE_INT mask = tree_to_uhwi (args[1]);
+	      unsigned HOST_WIDE_INT res = 0;
+	      unsigned HOST_WIDE_INT m, k = 1;
+	      for (m = 1; m; m <<= 1)
+		if ((mask & m) != 0)
+		  {
+		    if ((src & m) != 0)
+		      res |= k;
+		    k <<= 1;
+		  }
+	      return build_int_cstu (TREE_TYPE (TREE_TYPE (fndecl)), res);
+	    }
+	  break;
+
+	case IX86_BUILTIN_MOVMSKPS:
+	case IX86_BUILTIN_PMOVMSKB:
+	case IX86_BUILTIN_MOVMSKPD:
+	case IX86_BUILTIN_PMOVMSKB128:
+	case IX86_BUILTIN_MOVMSKPD256:
+	case IX86_BUILTIN_MOVMSKPS256:
+	case IX86_BUILTIN_PMOVMSKB256:
+	  gcc_assert (n_args == 1);
+	  if (TREE_CODE (args[0]) == VECTOR_CST)
+	    {
+	      HOST_WIDE_INT res = 0;
+	      for (unsigned i = 0; i < VECTOR_CST_NELTS (args[0]); ++i)
+		{
+		  tree e = VECTOR_CST_ELT (args[0], i);
+		  if (TREE_CODE (e) == INTEGER_CST && !TREE_OVERFLOW (e))
+		    {
+		      if (wi::neg_p (wi::to_wide (e)))
+			res |= HOST_WIDE_INT_1 << i;
+		    }
+		  else if (TREE_CODE (e) == REAL_CST && !TREE_OVERFLOW (e))
+		    {
+		      if (TREE_REAL_CST (e).sign)
+			res |= HOST_WIDE_INT_1 << i;
+		    }
+		  else
+		    return NULL_TREE;
+		}
+	      return build_int_cst (TREE_TYPE (TREE_TYPE (fndecl)), res);
+	    }
+	  break;
+
+	case IX86_BUILTIN_PSLLD:
+	case IX86_BUILTIN_PSLLD128:
+	case IX86_BUILTIN_PSLLD128_MASK:
+	case IX86_BUILTIN_PSLLD256:
+	case IX86_BUILTIN_PSLLD256_MASK:
+	case IX86_BUILTIN_PSLLD512:
+	case IX86_BUILTIN_PSLLDI:
+	case IX86_BUILTIN_PSLLDI128:
+	case IX86_BUILTIN_PSLLDI128_MASK:
+	case IX86_BUILTIN_PSLLDI256:
+	case IX86_BUILTIN_PSLLDI256_MASK:
+	case IX86_BUILTIN_PSLLDI512:
+	case IX86_BUILTIN_PSLLQ:
+	case IX86_BUILTIN_PSLLQ128:
+	case IX86_BUILTIN_PSLLQ128_MASK:
+	case IX86_BUILTIN_PSLLQ256:
+	case IX86_BUILTIN_PSLLQ256_MASK:
+	case IX86_BUILTIN_PSLLQ512:
+	case IX86_BUILTIN_PSLLQI:
+	case IX86_BUILTIN_PSLLQI128:
+	case IX86_BUILTIN_PSLLQI128_MASK:
+	case IX86_BUILTIN_PSLLQI256:
+	case IX86_BUILTIN_PSLLQI256_MASK:
+	case IX86_BUILTIN_PSLLQI512:
+	case IX86_BUILTIN_PSLLW:
+	case IX86_BUILTIN_PSLLW128:
+	case IX86_BUILTIN_PSLLW128_MASK:
+	case IX86_BUILTIN_PSLLW256:
+	case IX86_BUILTIN_PSLLW256_MASK:
+	case IX86_BUILTIN_PSLLW512_MASK:
+	case IX86_BUILTIN_PSLLWI:
+	case IX86_BUILTIN_PSLLWI128:
+	case IX86_BUILTIN_PSLLWI128_MASK:
+	case IX86_BUILTIN_PSLLWI256:
+	case IX86_BUILTIN_PSLLWI256_MASK:
+	case IX86_BUILTIN_PSLLWI512_MASK:
+	  rcode = ASHIFT;
+	  is_vshift = false;
+	  goto do_shift;
+	case IX86_BUILTIN_PSRAD:
+	case IX86_BUILTIN_PSRAD128:
+	case IX86_BUILTIN_PSRAD128_MASK:
+	case IX86_BUILTIN_PSRAD256:
+	case IX86_BUILTIN_PSRAD256_MASK:
+	case IX86_BUILTIN_PSRAD512:
+	case IX86_BUILTIN_PSRADI:
+	case IX86_BUILTIN_PSRADI128:
+	case IX86_BUILTIN_PSRADI128_MASK:
+	case IX86_BUILTIN_PSRADI256:
+	case IX86_BUILTIN_PSRADI256_MASK:
+	case IX86_BUILTIN_PSRADI512:
+	case IX86_BUILTIN_PSRAQ128_MASK:
+	case IX86_BUILTIN_PSRAQ256_MASK:
+	case IX86_BUILTIN_PSRAQ512:
+	case IX86_BUILTIN_PSRAQI128_MASK:
+	case IX86_BUILTIN_PSRAQI256_MASK:
+	case IX86_BUILTIN_PSRAQI512:
+	case IX86_BUILTIN_PSRAW:
+	case IX86_BUILTIN_PSRAW128:
+	case IX86_BUILTIN_PSRAW128_MASK:
+	case IX86_BUILTIN_PSRAW256:
+	case IX86_BUILTIN_PSRAW256_MASK:
+	case IX86_BUILTIN_PSRAW512:
+	case IX86_BUILTIN_PSRAWI:
+	case IX86_BUILTIN_PSRAWI128:
+	case IX86_BUILTIN_PSRAWI128_MASK:
+	case IX86_BUILTIN_PSRAWI256:
+	case IX86_BUILTIN_PSRAWI256_MASK:
+	case IX86_BUILTIN_PSRAWI512:
+	  rcode = ASHIFTRT;
+	  is_vshift = false;
+	  goto do_shift;
+	case IX86_BUILTIN_PSRLD:
+	case IX86_BUILTIN_PSRLD128:
+	case IX86_BUILTIN_PSRLD128_MASK:
+	case IX86_BUILTIN_PSRLD256:
+	case IX86_BUILTIN_PSRLD256_MASK:
+	case IX86_BUILTIN_PSRLD512:
+	case IX86_BUILTIN_PSRLDI:
+	case IX86_BUILTIN_PSRLDI128:
+	case IX86_BUILTIN_PSRLDI128_MASK:
+	case IX86_BUILTIN_PSRLDI256:
+	case IX86_BUILTIN_PSRLDI256_MASK:
+	case IX86_BUILTIN_PSRLDI512:
+	case IX86_BUILTIN_PSRLQ:
+	case IX86_BUILTIN_PSRLQ128:
+	case IX86_BUILTIN_PSRLQ128_MASK:
+	case IX86_BUILTIN_PSRLQ256:
+	case IX86_BUILTIN_PSRLQ256_MASK:
+	case IX86_BUILTIN_PSRLQ512:
+	case IX86_BUILTIN_PSRLQI:
+	case IX86_BUILTIN_PSRLQI128:
+	case IX86_BUILTIN_PSRLQI128_MASK:
+	case IX86_BUILTIN_PSRLQI256:
+	case IX86_BUILTIN_PSRLQI256_MASK:
+	case IX86_BUILTIN_PSRLQI512:
+	case IX86_BUILTIN_PSRLW:
+	case IX86_BUILTIN_PSRLW128:
+	case IX86_BUILTIN_PSRLW128_MASK:
+	case IX86_BUILTIN_PSRLW256:
+	case IX86_BUILTIN_PSRLW256_MASK:
+	case IX86_BUILTIN_PSRLW512:
+	case IX86_BUILTIN_PSRLWI:
+	case IX86_BUILTIN_PSRLWI128:
+	case IX86_BUILTIN_PSRLWI128_MASK:
+	case IX86_BUILTIN_PSRLWI256:
+	case IX86_BUILTIN_PSRLWI256_MASK:
+	case IX86_BUILTIN_PSRLWI512:
+	  rcode = LSHIFTRT;
+	  is_vshift = false;
+	  goto do_shift;
+	case IX86_BUILTIN_PSLLVV16HI:
+	case IX86_BUILTIN_PSLLVV16SI:
+	case IX86_BUILTIN_PSLLVV2DI:
+	case IX86_BUILTIN_PSLLVV2DI_MASK:
+	case IX86_BUILTIN_PSLLVV32HI:
+	case IX86_BUILTIN_PSLLVV4DI:
+	case IX86_BUILTIN_PSLLVV4DI_MASK:
+	case IX86_BUILTIN_PSLLVV4SI:
+	case IX86_BUILTIN_PSLLVV4SI_MASK:
+	case IX86_BUILTIN_PSLLVV8DI:
+	case IX86_BUILTIN_PSLLVV8HI:
+	case IX86_BUILTIN_PSLLVV8SI:
+	case IX86_BUILTIN_PSLLVV8SI_MASK:
+	  rcode = ASHIFT;
+	  is_vshift = true;
+	  goto do_shift;
+	case IX86_BUILTIN_PSRAVQ128:
+	case IX86_BUILTIN_PSRAVQ256:
+	case IX86_BUILTIN_PSRAVV16HI:
+	case IX86_BUILTIN_PSRAVV16SI:
+	case IX86_BUILTIN_PSRAVV32HI:
+	case IX86_BUILTIN_PSRAVV4SI:
+	case IX86_BUILTIN_PSRAVV4SI_MASK:
+	case IX86_BUILTIN_PSRAVV8DI:
+	case IX86_BUILTIN_PSRAVV8HI:
+	case IX86_BUILTIN_PSRAVV8SI:
+	case IX86_BUILTIN_PSRAVV8SI_MASK:
+	  rcode = ASHIFTRT;
+	  is_vshift = true;
+	  goto do_shift;
+	case IX86_BUILTIN_PSRLVV16HI:
+	case IX86_BUILTIN_PSRLVV16SI:
+	case IX86_BUILTIN_PSRLVV2DI:
+	case IX86_BUILTIN_PSRLVV2DI_MASK:
+	case IX86_BUILTIN_PSRLVV32HI:
+	case IX86_BUILTIN_PSRLVV4DI:
+	case IX86_BUILTIN_PSRLVV4DI_MASK:
+	case IX86_BUILTIN_PSRLVV4SI:
+	case IX86_BUILTIN_PSRLVV4SI_MASK:
+	case IX86_BUILTIN_PSRLVV8DI:
+	case IX86_BUILTIN_PSRLVV8HI:
+	case IX86_BUILTIN_PSRLVV8SI:
+	case IX86_BUILTIN_PSRLVV8SI_MASK:
+	  rcode = LSHIFTRT;
+	  is_vshift = true;
+	  goto do_shift;
+
+	do_shift:
+	  gcc_assert (n_args >= 2);
+	  if (TREE_CODE (args[0]) != VECTOR_CST)
+	    break;
+	  mask = HOST_WIDE_INT_M1U;
+	  if (n_args > 2)
+	    {
+	      /* This is masked shift.  */
+	      if (!tree_fits_uhwi_p (args[n_args - 1])
+		  || TREE_SIDE_EFFECTS (args[n_args - 2]))
+		break;
+	      mask = tree_to_uhwi (args[n_args - 1]);
+	      unsigned elems = TYPE_VECTOR_SUBPARTS (TREE_TYPE (args[0]));
+	      mask |= HOST_WIDE_INT_M1U << elems;
+	      if (mask != HOST_WIDE_INT_M1U
+		  && TREE_CODE (args[n_args - 2]) != VECTOR_CST)
+		break;
+	      if (mask == (HOST_WIDE_INT_M1U << elems))
+		return args[n_args - 2];
+	    }
+	  if (is_vshift && TREE_CODE (args[1]) != VECTOR_CST)
+	    break;
+	  if (tree tem = (is_vshift ? integer_one_node
+			  : ix86_vector_shift_count (args[1])))
+	    {
+	      unsigned HOST_WIDE_INT count = tree_to_uhwi (tem);
+	      unsigned HOST_WIDE_INT prec
+		= TYPE_PRECISION (TREE_TYPE (TREE_TYPE (args[0])));
+	      if (count == 0 && mask == HOST_WIDE_INT_M1U)
+		return args[0];
+	      if (count >= prec)
+		{
+		  if (rcode == ASHIFTRT)
+		    count = prec - 1;
+		  else if (mask == HOST_WIDE_INT_M1U)
+		    return build_zero_cst (TREE_TYPE (args[0]));
+		}
+	      tree countt = NULL_TREE;
+	      if (!is_vshift)
+		{
+		  if (count >= prec)
+		    countt = integer_zero_node;
+		  else
+		    countt = build_int_cst (integer_type_node, count);
+		}
+	      tree_vector_builder builder;
+	      if (mask != HOST_WIDE_INT_M1U || is_vshift)
+		builder.new_vector (TREE_TYPE (args[0]),
+				    TYPE_VECTOR_SUBPARTS (TREE_TYPE (args[0])),
+				    1);
+	      else
+		builder.new_unary_operation (TREE_TYPE (args[0]), args[0],
+					     false);
+	      unsigned int cnt = builder.encoded_nelts ();
+	      for (unsigned int i = 0; i < cnt; ++i)
+		{
+		  tree elt = VECTOR_CST_ELT (args[0], i);
+		  if (TREE_CODE (elt) != INTEGER_CST || TREE_OVERFLOW (elt))
+		    return NULL_TREE;
+		  tree type = TREE_TYPE (elt);
+		  if (rcode == LSHIFTRT)
+		    elt = fold_convert (unsigned_type_for (type), elt);
+		  if (is_vshift)
+		    {
+		      countt = VECTOR_CST_ELT (args[1], i);
+		      if (TREE_CODE (countt) != INTEGER_CST
+			  || TREE_OVERFLOW (countt))
+			return NULL_TREE;
+		      if (wi::neg_p (wi::to_wide (countt))
+			  || wi::to_widest (countt) >= prec)
+			{
+			  if (rcode == ASHIFTRT)
+			    countt = build_int_cst (TREE_TYPE (countt),
+						    prec - 1);
+			  else
+			    {
+			      elt = build_zero_cst (TREE_TYPE (elt));
+			      countt = build_zero_cst (TREE_TYPE (countt));
+			    }
+			}
+		    }
+		  else if (count >= prec)
+		    elt = build_zero_cst (TREE_TYPE (elt));
+		  elt = const_binop (rcode == ASHIFT
+				     ? LSHIFT_EXPR : RSHIFT_EXPR,
+				     TREE_TYPE (elt), elt, countt);
+		  if (!elt || TREE_CODE (elt) != INTEGER_CST)
+		    return NULL_TREE;
+		  if (rcode == LSHIFTRT)
+		    elt = fold_convert (type, elt);
+		  if ((mask & (HOST_WIDE_INT_1U << i)) == 0)
+		    {
+		      elt = VECTOR_CST_ELT (args[n_args - 2], i);
+		      if (TREE_CODE (elt) != INTEGER_CST
+			  || TREE_OVERFLOW (elt))
+			return NULL_TREE;
+		    }
+		  builder.quick_push (elt);
+		}
+	      return builder.build ();
+	    }
+	  break;
+
+	default:
+	  break;
+	}
+    }
+
+#ifdef SUBTARGET_FOLD_BUILTIN
+  return SUBTARGET_FOLD_BUILTIN (fndecl, n_args, args, ignore);
+#endif
+
+  return NULL_TREE;
+}
+
+/* Fold a MD builtin (use ix86_fold_builtin for folding into
+   constant) in GIMPLE.  */
+
+bool
+ix86_gimple_fold_builtin (gimple_stmt_iterator *gsi)
+{
+  gimple *stmt = gsi_stmt (*gsi);
+  tree fndecl = gimple_call_fndecl (stmt);
+  gcc_checking_assert (fndecl && fndecl_built_in_p (fndecl, BUILT_IN_MD));
+  int n_args = gimple_call_num_args (stmt);
+  enum ix86_builtins fn_code
+    = (enum ix86_builtins) DECL_MD_FUNCTION_CODE (fndecl);
+  tree decl = NULL_TREE;
+  tree arg0, arg1, arg2;
+  enum rtx_code rcode;
+  unsigned HOST_WIDE_INT count;
+  bool is_vshift;
+
+  switch (fn_code)
+    {
+    case IX86_BUILTIN_TZCNT32:
+      decl = builtin_decl_implicit (BUILT_IN_CTZ);
+      goto fold_tzcnt_lzcnt;
+
+    case IX86_BUILTIN_TZCNT64:
+      decl = builtin_decl_implicit (BUILT_IN_CTZLL);
+      goto fold_tzcnt_lzcnt;
+
+    case IX86_BUILTIN_LZCNT32:
+      decl = builtin_decl_implicit (BUILT_IN_CLZ);
+      goto fold_tzcnt_lzcnt;
+
+    case IX86_BUILTIN_LZCNT64:
+      decl = builtin_decl_implicit (BUILT_IN_CLZLL);
+      goto fold_tzcnt_lzcnt;
+
+    fold_tzcnt_lzcnt:
+      gcc_assert (n_args == 1);
+      arg0 = gimple_call_arg (stmt, 0);
+      if (TREE_CODE (arg0) == SSA_NAME && decl && gimple_call_lhs (stmt))
+	{
+	  int prec = TYPE_PRECISION (TREE_TYPE (arg0));
+	  /* If arg0 is provably non-zero, optimize into generic
+	     __builtin_c[tl]z{,ll} function the middle-end handles
+	     better.  */
+	  if (!expr_not_equal_to (arg0, wi::zero (prec)))
+	    return false;
+
+	  location_t loc = gimple_location (stmt);
+	  gimple *g = gimple_build_call (decl, 1, arg0);
+	  gimple_set_location (g, loc);
+	  tree lhs = make_ssa_name (integer_type_node);
+	  gimple_call_set_lhs (g, lhs);
+	  gsi_insert_before (gsi, g, GSI_SAME_STMT);
+	  g = gimple_build_assign (gimple_call_lhs (stmt), NOP_EXPR, lhs);
+	  gimple_set_location (g, loc);
+	  gsi_replace (gsi, g, false);
+	  return true;
+	}
+      break;
+
+    case IX86_BUILTIN_BZHI32:
+    case IX86_BUILTIN_BZHI64:
+      gcc_assert (n_args == 2);
+      arg1 = gimple_call_arg (stmt, 1);
+      if (tree_fits_uhwi_p (arg1) && gimple_call_lhs (stmt))
+	{
+	  unsigned int idx = tree_to_uhwi (arg1) & 0xff;
+	  arg0 = gimple_call_arg (stmt, 0);
+	  if (idx < TYPE_PRECISION (TREE_TYPE (arg0)))
+	    break;
+	  location_t loc = gimple_location (stmt);
+	  gimple *g = gimple_build_assign (gimple_call_lhs (stmt), arg0);
+	  gimple_set_location (g, loc);
+	  gsi_replace (gsi, g, false);
+	  return true;
+	}
+      break;
+
+    case IX86_BUILTIN_PDEP32:
+    case IX86_BUILTIN_PDEP64:
+    case IX86_BUILTIN_PEXT32:
+    case IX86_BUILTIN_PEXT64:
+      gcc_assert (n_args == 2);
+      arg1 = gimple_call_arg (stmt, 1);
+      if (integer_all_onesp (arg1) && gimple_call_lhs (stmt))
+	{
+	  location_t loc = gimple_location (stmt);
+	  arg0 = gimple_call_arg (stmt, 0);
+	  gimple *g = gimple_build_assign (gimple_call_lhs (stmt), arg0);
+	  gimple_set_location (g, loc);
+	  gsi_replace (gsi, g, false);
+	  return true;
+	}
+      break;
+
+    case IX86_BUILTIN_PSLLD:
+    case IX86_BUILTIN_PSLLD128:
+    case IX86_BUILTIN_PSLLD128_MASK:
+    case IX86_BUILTIN_PSLLD256:
+    case IX86_BUILTIN_PSLLD256_MASK:
+    case IX86_BUILTIN_PSLLD512:
+    case IX86_BUILTIN_PSLLDI:
+    case IX86_BUILTIN_PSLLDI128:
+    case IX86_BUILTIN_PSLLDI128_MASK:
+    case IX86_BUILTIN_PSLLDI256:
+    case IX86_BUILTIN_PSLLDI256_MASK:
+    case IX86_BUILTIN_PSLLDI512:
+    case IX86_BUILTIN_PSLLQ:
+    case IX86_BUILTIN_PSLLQ128:
+    case IX86_BUILTIN_PSLLQ128_MASK:
+    case IX86_BUILTIN_PSLLQ256:
+    case IX86_BUILTIN_PSLLQ256_MASK:
+    case IX86_BUILTIN_PSLLQ512:
+    case IX86_BUILTIN_PSLLQI:
+    case IX86_BUILTIN_PSLLQI128:
+    case IX86_BUILTIN_PSLLQI128_MASK:
+    case IX86_BUILTIN_PSLLQI256:
+    case IX86_BUILTIN_PSLLQI256_MASK:
+    case IX86_BUILTIN_PSLLQI512:
+    case IX86_BUILTIN_PSLLW:
+    case IX86_BUILTIN_PSLLW128:
+    case IX86_BUILTIN_PSLLW128_MASK:
+    case IX86_BUILTIN_PSLLW256:
+    case IX86_BUILTIN_PSLLW256_MASK:
+    case IX86_BUILTIN_PSLLW512_MASK:
+    case IX86_BUILTIN_PSLLWI:
+    case IX86_BUILTIN_PSLLWI128:
+    case IX86_BUILTIN_PSLLWI128_MASK:
+    case IX86_BUILTIN_PSLLWI256:
+    case IX86_BUILTIN_PSLLWI256_MASK:
+    case IX86_BUILTIN_PSLLWI512_MASK:
+      rcode = ASHIFT;
+      is_vshift = false;
+      goto do_shift;
+    case IX86_BUILTIN_PSRAD:
+    case IX86_BUILTIN_PSRAD128:
+    case IX86_BUILTIN_PSRAD128_MASK:
+    case IX86_BUILTIN_PSRAD256:
+    case IX86_BUILTIN_PSRAD256_MASK:
+    case IX86_BUILTIN_PSRAD512:
+    case IX86_BUILTIN_PSRADI:
+    case IX86_BUILTIN_PSRADI128:
+    case IX86_BUILTIN_PSRADI128_MASK:
+    case IX86_BUILTIN_PSRADI256:
+    case IX86_BUILTIN_PSRADI256_MASK:
+    case IX86_BUILTIN_PSRADI512:
+    case IX86_BUILTIN_PSRAQ128_MASK:
+    case IX86_BUILTIN_PSRAQ256_MASK:
+    case IX86_BUILTIN_PSRAQ512:
+    case IX86_BUILTIN_PSRAQI128_MASK:
+    case IX86_BUILTIN_PSRAQI256_MASK:
+    case IX86_BUILTIN_PSRAQI512:
+    case IX86_BUILTIN_PSRAW:
+    case IX86_BUILTIN_PSRAW128:
+    case IX86_BUILTIN_PSRAW128_MASK:
+    case IX86_BUILTIN_PSRAW256:
+    case IX86_BUILTIN_PSRAW256_MASK:
+    case IX86_BUILTIN_PSRAW512:
+    case IX86_BUILTIN_PSRAWI:
+    case IX86_BUILTIN_PSRAWI128:
+    case IX86_BUILTIN_PSRAWI128_MASK:
+    case IX86_BUILTIN_PSRAWI256:
+    case IX86_BUILTIN_PSRAWI256_MASK:
+    case IX86_BUILTIN_PSRAWI512:
+      rcode = ASHIFTRT;
+      is_vshift = false;
+      goto do_shift;
+    case IX86_BUILTIN_PSRLD:
+    case IX86_BUILTIN_PSRLD128:
+    case IX86_BUILTIN_PSRLD128_MASK:
+    case IX86_BUILTIN_PSRLD256:
+    case IX86_BUILTIN_PSRLD256_MASK:
+    case IX86_BUILTIN_PSRLD512:
+    case IX86_BUILTIN_PSRLDI:
+    case IX86_BUILTIN_PSRLDI128:
+    case IX86_BUILTIN_PSRLDI128_MASK:
+    case IX86_BUILTIN_PSRLDI256:
+    case IX86_BUILTIN_PSRLDI256_MASK:
+    case IX86_BUILTIN_PSRLDI512:
+    case IX86_BUILTIN_PSRLQ:
+    case IX86_BUILTIN_PSRLQ128:
+    case IX86_BUILTIN_PSRLQ128_MASK:
+    case IX86_BUILTIN_PSRLQ256:
+    case IX86_BUILTIN_PSRLQ256_MASK:
+    case IX86_BUILTIN_PSRLQ512:
+    case IX86_BUILTIN_PSRLQI:
+    case IX86_BUILTIN_PSRLQI128:
+    case IX86_BUILTIN_PSRLQI128_MASK:
+    case IX86_BUILTIN_PSRLQI256:
+    case IX86_BUILTIN_PSRLQI256_MASK:
+    case IX86_BUILTIN_PSRLQI512:
+    case IX86_BUILTIN_PSRLW:
+    case IX86_BUILTIN_PSRLW128:
+    case IX86_BUILTIN_PSRLW128_MASK:
+    case IX86_BUILTIN_PSRLW256:
+    case IX86_BUILTIN_PSRLW256_MASK:
+    case IX86_BUILTIN_PSRLW512:
+    case IX86_BUILTIN_PSRLWI:
+    case IX86_BUILTIN_PSRLWI128:
+    case IX86_BUILTIN_PSRLWI128_MASK:
+    case IX86_BUILTIN_PSRLWI256:
+    case IX86_BUILTIN_PSRLWI256_MASK:
+    case IX86_BUILTIN_PSRLWI512:
+      rcode = LSHIFTRT;
+      is_vshift = false;
+      goto do_shift;
+    case IX86_BUILTIN_PSLLVV16HI:
+    case IX86_BUILTIN_PSLLVV16SI:
+    case IX86_BUILTIN_PSLLVV2DI:
+    case IX86_BUILTIN_PSLLVV2DI_MASK:
+    case IX86_BUILTIN_PSLLVV32HI:
+    case IX86_BUILTIN_PSLLVV4DI:
+    case IX86_BUILTIN_PSLLVV4DI_MASK:
+    case IX86_BUILTIN_PSLLVV4SI:
+    case IX86_BUILTIN_PSLLVV4SI_MASK:
+    case IX86_BUILTIN_PSLLVV8DI:
+    case IX86_BUILTIN_PSLLVV8HI:
+    case IX86_BUILTIN_PSLLVV8SI:
+    case IX86_BUILTIN_PSLLVV8SI_MASK:
+      rcode = ASHIFT;
+      is_vshift = true;
+      goto do_shift;
+    case IX86_BUILTIN_PSRAVQ128:
+    case IX86_BUILTIN_PSRAVQ256:
+    case IX86_BUILTIN_PSRAVV16HI:
+    case IX86_BUILTIN_PSRAVV16SI:
+    case IX86_BUILTIN_PSRAVV32HI:
+    case IX86_BUILTIN_PSRAVV4SI:
+    case IX86_BUILTIN_PSRAVV4SI_MASK:
+    case IX86_BUILTIN_PSRAVV8DI:
+    case IX86_BUILTIN_PSRAVV8HI:
+    case IX86_BUILTIN_PSRAVV8SI:
+    case IX86_BUILTIN_PSRAVV8SI_MASK:
+      rcode = ASHIFTRT;
+      is_vshift = true;
+      goto do_shift;
+    case IX86_BUILTIN_PSRLVV16HI:
+    case IX86_BUILTIN_PSRLVV16SI:
+    case IX86_BUILTIN_PSRLVV2DI:
+    case IX86_BUILTIN_PSRLVV2DI_MASK:
+    case IX86_BUILTIN_PSRLVV32HI:
+    case IX86_BUILTIN_PSRLVV4DI:
+    case IX86_BUILTIN_PSRLVV4DI_MASK:
+    case IX86_BUILTIN_PSRLVV4SI:
+    case IX86_BUILTIN_PSRLVV4SI_MASK:
+    case IX86_BUILTIN_PSRLVV8DI:
+    case IX86_BUILTIN_PSRLVV8HI:
+    case IX86_BUILTIN_PSRLVV8SI:
+    case IX86_BUILTIN_PSRLVV8SI_MASK:
+      rcode = LSHIFTRT;
+      is_vshift = true;
+      goto do_shift;
+
+    do_shift:
+      gcc_assert (n_args >= 2);
+      arg0 = gimple_call_arg (stmt, 0);
+      arg1 = gimple_call_arg (stmt, 1);
+      if (n_args > 2)
+	{
+	  /* This is masked shift.  Only optimize if the mask is all ones.  */
+	  tree argl = gimple_call_arg (stmt, n_args - 1);
+	  if (!tree_fits_uhwi_p (argl))
+	    break;
+	  unsigned HOST_WIDE_INT mask = tree_to_uhwi (argl);
+	  unsigned elems = TYPE_VECTOR_SUBPARTS (TREE_TYPE (arg0));
+	  if ((mask | (HOST_WIDE_INT_M1U << elems)) != HOST_WIDE_INT_M1U)
+	    break;
+	}
+      if (is_vshift)
+	{
+	  if (TREE_CODE (arg1) != VECTOR_CST)
+	    break;
+	  count = TYPE_PRECISION (TREE_TYPE (TREE_TYPE (arg0)));
+	  if (integer_zerop (arg1))
+	    count = 0;
+	  else if (rcode == ASHIFTRT)
+	    break;
+	  else
+	    for (unsigned int i = 0; i < VECTOR_CST_NELTS (arg1); ++i)
+	      {
+		tree elt = VECTOR_CST_ELT (arg1, i);
+		if (!wi::neg_p (wi::to_wide (elt))
+		    && wi::to_widest (elt) < count)
+		  return false;
+	      }
+	}
+      else
+	{
+	  arg1 = ix86_vector_shift_count (arg1);
+	  if (!arg1)
+	    break;
+	  count = tree_to_uhwi (arg1);
+	}
+      if (count == 0)
+	{
+	  /* Just return the first argument for shift by 0.  */
+	  location_t loc = gimple_location (stmt);
+	  gimple *g = gimple_build_assign (gimple_call_lhs (stmt), arg0);
+	  gimple_set_location (g, loc);
+	  gsi_replace (gsi, g, false);
+	  return true;
+	}
+      if (rcode != ASHIFTRT
+	  && count >= TYPE_PRECISION (TREE_TYPE (TREE_TYPE (arg0))))
+	{
+	  /* For shift counts equal or greater than precision, except for
+	     arithmetic right shift the result is zero.  */
+	  location_t loc = gimple_location (stmt);
+	  gimple *g = gimple_build_assign (gimple_call_lhs (stmt),
+					   build_zero_cst (TREE_TYPE (arg0)));
+	  gimple_set_location (g, loc);
+	  gsi_replace (gsi, g, false);
+	  return true;
+	}
+      break;
+
+    case IX86_BUILTIN_SHUFPD:
+      arg2 = gimple_call_arg (stmt, 2);
+      if (TREE_CODE (arg2) == INTEGER_CST)
+	{
+	  location_t loc = gimple_location (stmt);
+	  unsigned HOST_WIDE_INT imask = TREE_INT_CST_LOW (arg2);
+	  arg0 = gimple_call_arg (stmt, 0);
+	  arg1 = gimple_call_arg (stmt, 1);
+	  tree itype = long_long_integer_type_node;
+	  tree vtype = build_vector_type (itype, 2); /* V2DI */
+	  tree_vector_builder elts (vtype, 2, 1);
+	  /* Ignore bits other than the lowest 2.  */
+	  elts.quick_push (build_int_cst (itype, imask & 1));
+	  imask >>= 1;
+	  elts.quick_push (build_int_cst (itype, 2 + (imask & 1)));
+	  tree omask = elts.build ();
+	  gimple *g = gimple_build_assign (gimple_call_lhs (stmt),
+					   VEC_PERM_EXPR,
+					   arg0, arg1, omask);
+	  gimple_set_location (g, loc);
+	  gsi_replace (gsi, g, false);
+	  return true;
+	}
+      // Do not error yet, the constant could be propagated later?
+      break;
+
+    default:
+      break;
+    }
+
+  return false;
+}
+
+/* Handler for an SVML-style interface to
+   a library with vectorized intrinsics.  */
+
+tree
+ix86_veclibabi_svml (combined_fn fn, tree type_out, tree type_in)
+{
+  char name[20];
+  tree fntype, new_fndecl, args;
+  unsigned arity;
+  const char *bname;
+  machine_mode el_mode, in_mode;
+  int n, in_n;
+
+  /* The SVML is suitable for unsafe math only.  */
+  if (!flag_unsafe_math_optimizations)
+    return NULL_TREE;
+
+  el_mode = TYPE_MODE (TREE_TYPE (type_out));
+  n = TYPE_VECTOR_SUBPARTS (type_out);
+  in_mode = TYPE_MODE (TREE_TYPE (type_in));
+  in_n = TYPE_VECTOR_SUBPARTS (type_in);
+  if (el_mode != in_mode
+      || n != in_n)
+    return NULL_TREE;
+
+  switch (fn)
+    {
+    CASE_CFN_EXP:
+    CASE_CFN_LOG:
+    CASE_CFN_LOG10:
+    CASE_CFN_POW:
+    CASE_CFN_TANH:
+    CASE_CFN_TAN:
+    CASE_CFN_ATAN:
+    CASE_CFN_ATAN2:
+    CASE_CFN_ATANH:
+    CASE_CFN_CBRT:
+    CASE_CFN_SINH:
+    CASE_CFN_SIN:
+    CASE_CFN_ASINH:
+    CASE_CFN_ASIN:
+    CASE_CFN_COSH:
+    CASE_CFN_COS:
+    CASE_CFN_ACOSH:
+    CASE_CFN_ACOS:
+      if ((el_mode != DFmode || n != 2)
+	  && (el_mode != SFmode || n != 4))
+	return NULL_TREE;
+      break;
+
+    default:
+      return NULL_TREE;
+    }
+
+  tree fndecl = mathfn_built_in (TREE_TYPE (type_in), fn);
+  bname = IDENTIFIER_POINTER (DECL_NAME (fndecl));
+
+  if (DECL_FUNCTION_CODE (fndecl) == BUILT_IN_LOGF)
+    strcpy (name, "vmlsLn4");
+  else if (DECL_FUNCTION_CODE (fndecl) == BUILT_IN_LOG)
+    strcpy (name, "vmldLn2");
+  else if (n == 4)
+    {
+      sprintf (name, "vmls%s", bname+10);
+      name[strlen (name)-1] = '4';
+    }
+  else
+    sprintf (name, "vmld%s2", bname+10);
+
+  /* Convert to uppercase. */
+  name[4] &= ~0x20;
+
+  arity = 0;
+  for (args = DECL_ARGUMENTS (fndecl); args; args = TREE_CHAIN (args))
+    arity++;
+
+  if (arity == 1)
+    fntype = build_function_type_list (type_out, type_in, NULL);
+  else
+    fntype = build_function_type_list (type_out, type_in, type_in, NULL);
+
+  /* Build a function declaration for the vectorized function.  */
+  new_fndecl = build_decl (BUILTINS_LOCATION,
+			   FUNCTION_DECL, get_identifier (name), fntype);
+  TREE_PUBLIC (new_fndecl) = 1;
+  DECL_EXTERNAL (new_fndecl) = 1;
+  DECL_IS_NOVOPS (new_fndecl) = 1;
+  TREE_READONLY (new_fndecl) = 1;
+
+  return new_fndecl;
+}
+
+/* Handler for an ACML-style interface to
+   a library with vectorized intrinsics.  */
+
+tree
+ix86_veclibabi_acml (combined_fn fn, tree type_out, tree type_in)
+{
+  char name[20] = "__vr.._";
+  tree fntype, new_fndecl, args;
+  unsigned arity;
+  const char *bname;
+  machine_mode el_mode, in_mode;
+  int n, in_n;
+
+  /* The ACML is 64bits only and suitable for unsafe math only as
+     it does not correctly support parts of IEEE with the required
+     precision such as denormals.  */
+  if (!TARGET_64BIT
+      || !flag_unsafe_math_optimizations)
+    return NULL_TREE;
+
+  el_mode = TYPE_MODE (TREE_TYPE (type_out));
+  n = TYPE_VECTOR_SUBPARTS (type_out);
+  in_mode = TYPE_MODE (TREE_TYPE (type_in));
+  in_n = TYPE_VECTOR_SUBPARTS (type_in);
+  if (el_mode != in_mode
+      || n != in_n)
+    return NULL_TREE;
+
+  switch (fn)
+    {
+    CASE_CFN_SIN:
+    CASE_CFN_COS:
+    CASE_CFN_EXP:
+    CASE_CFN_LOG:
+    CASE_CFN_LOG2:
+    CASE_CFN_LOG10:
+      if (el_mode == DFmode && n == 2)
+	{
+	  name[4] = 'd';
+	  name[5] = '2';
+	}
+      else if (el_mode == SFmode && n == 4)
+	{
+	  name[4] = 's';
+	  name[5] = '4';
+	}
+      else
+	return NULL_TREE;
+      break;
+
+    default:
+      return NULL_TREE;
+    }
+
+  tree fndecl = mathfn_built_in (TREE_TYPE (type_in), fn);
+  bname = IDENTIFIER_POINTER (DECL_NAME (fndecl));
+  sprintf (name + 7, "%s", bname+10);
+
+  arity = 0;
+  for (args = DECL_ARGUMENTS (fndecl); args; args = TREE_CHAIN (args))
+    arity++;
+
+  if (arity == 1)
+    fntype = build_function_type_list (type_out, type_in, NULL);
+  else
+    fntype = build_function_type_list (type_out, type_in, type_in, NULL);
+
+  /* Build a function declaration for the vectorized function.  */
+  new_fndecl = build_decl (BUILTINS_LOCATION,
+			   FUNCTION_DECL, get_identifier (name), fntype);
+  TREE_PUBLIC (new_fndecl) = 1;
+  DECL_EXTERNAL (new_fndecl) = 1;
+  DECL_IS_NOVOPS (new_fndecl) = 1;
+  TREE_READONLY (new_fndecl) = 1;
+
+  return new_fndecl;
+}
+
+/* Returns a decl of a function that implements scatter store with
+   register type VECTYPE and index type INDEX_TYPE and SCALE.
+   Return NULL_TREE if it is not available.  */
+
+static tree
+ix86_vectorize_builtin_scatter (const_tree vectype,
+				const_tree index_type, int scale)
+{
+  bool si;
+  enum ix86_builtins code;
+
+  if (!TARGET_AVX512F)
+    return NULL_TREE;
+
+  if ((TREE_CODE (index_type) != INTEGER_TYPE
+       && !POINTER_TYPE_P (index_type))
+      || (TYPE_MODE (index_type) != SImode
+	  && TYPE_MODE (index_type) != DImode))
+    return NULL_TREE;
+
+  if (TYPE_PRECISION (index_type) > POINTER_SIZE)
+    return NULL_TREE;
+
+  /* v*scatter* insn sign extends index to pointer mode.  */
+  if (TYPE_PRECISION (index_type) < POINTER_SIZE
+      && TYPE_UNSIGNED (index_type))
+    return NULL_TREE;
+
+  /* Scale can be 1, 2, 4 or 8.  */
+  if (scale <= 0
+      || scale > 8
+      || (scale & (scale - 1)) != 0)
+    return NULL_TREE;
+
+  si = TYPE_MODE (index_type) == SImode;
+  switch (TYPE_MODE (vectype))
+    {
+    case E_V8DFmode:
+      code = si ? IX86_BUILTIN_SCATTERALTSIV8DF : IX86_BUILTIN_SCATTERDIV8DF;
+      break;
+    case E_V8DImode:
+      code = si ? IX86_BUILTIN_SCATTERALTSIV8DI : IX86_BUILTIN_SCATTERDIV8DI;
+      break;
+    case E_V16SFmode:
+      code = si ? IX86_BUILTIN_SCATTERSIV16SF : IX86_BUILTIN_SCATTERALTDIV16SF;
+      break;
+    case E_V16SImode:
+      code = si ? IX86_BUILTIN_SCATTERSIV16SI : IX86_BUILTIN_SCATTERALTDIV16SI;
+      break;
+    case E_V4DFmode:
+      if (TARGET_AVX512VL)
+	code = si ? IX86_BUILTIN_SCATTERALTSIV4DF : IX86_BUILTIN_SCATTERDIV4DF;
+      else
+	return NULL_TREE;
+      break;
+    case E_V4DImode:
+      if (TARGET_AVX512VL)
+	code = si ? IX86_BUILTIN_SCATTERALTSIV4DI : IX86_BUILTIN_SCATTERDIV4DI;
+      else
+	return NULL_TREE;
+      break;
+    case E_V8SFmode:
+      if (TARGET_AVX512VL)
+	code = si ? IX86_BUILTIN_SCATTERSIV8SF : IX86_BUILTIN_SCATTERALTDIV8SF;
+      else
+	return NULL_TREE;
+      break;
+    case E_V8SImode:
+      if (TARGET_AVX512VL)
+	code = si ? IX86_BUILTIN_SCATTERSIV8SI : IX86_BUILTIN_SCATTERALTDIV8SI;
+      else
+	return NULL_TREE;
+      break;
+    case E_V2DFmode:
+      if (TARGET_AVX512VL)
+	code = si ? IX86_BUILTIN_SCATTERALTSIV2DF : IX86_BUILTIN_SCATTERDIV2DF;
+      else
+	return NULL_TREE;
+      break;
+    case E_V2DImode:
+      if (TARGET_AVX512VL)
+	code = si ? IX86_BUILTIN_SCATTERALTSIV2DI : IX86_BUILTIN_SCATTERDIV2DI;
+      else
+	return NULL_TREE;
+      break;
+    case E_V4SFmode:
+      if (TARGET_AVX512VL)
+	code = si ? IX86_BUILTIN_SCATTERSIV4SF : IX86_BUILTIN_SCATTERALTDIV4SF;
+      else
+	return NULL_TREE;
+      break;
+    case E_V4SImode:
+      if (TARGET_AVX512VL)
+	code = si ? IX86_BUILTIN_SCATTERSIV4SI : IX86_BUILTIN_SCATTERALTDIV4SI;
+      else
+	return NULL_TREE;
+      break;
+    default:
+      return NULL_TREE;
+    }
+
+  return get_ix86_builtin (code);
+}
+
+/* Return true if it is safe to use the rsqrt optabs to optimize
+   1.0/sqrt.  */
+
+static bool
+use_rsqrt_p ()
+{
+  return (TARGET_SSE && TARGET_SSE_MATH
+	  && flag_finite_math_only
+	  && !flag_trapping_math
+	  && flag_unsafe_math_optimizations);
+}
+
+/* Helper for avx_vpermilps256_operand et al.  This is also used by
+   the expansion functions to turn the parallel back into a mask.
+   The return value is 0 for no match and the imm8+1 for a match.  */
+
+int
+avx_vpermilp_parallel (rtx par, machine_mode mode)
+{
+  unsigned i, nelt = GET_MODE_NUNITS (mode);
+  unsigned mask = 0;
+  unsigned char ipar[16] = {};  /* Silence -Wuninitialized warning.  */
+
+  if (XVECLEN (par, 0) != (int) nelt)
+    return 0;
+
+  /* Validate that all of the elements are constants, and not totally
+     out of range.  Copy the data into an integral array to make the
+     subsequent checks easier.  */
+  for (i = 0; i < nelt; ++i)
+    {
+      rtx er = XVECEXP (par, 0, i);
+      unsigned HOST_WIDE_INT ei;
+
+      if (!CONST_INT_P (er))
+	return 0;
+      ei = INTVAL (er);
+      if (ei >= nelt)
+	return 0;
+      ipar[i] = ei;
+    }
+
+  switch (mode)
+    {
+    case E_V8DFmode:
+      /* In the 512-bit DFmode case, we can only move elements within
+         a 128-bit lane.  First fill the second part of the mask,
+	 then fallthru.  */
+      for (i = 4; i < 6; ++i)
+	{
+	  if (ipar[i] < 4 || ipar[i] >= 6)
+	    return 0;
+	  mask |= (ipar[i] - 4) << i;
+	}
+      for (i = 6; i < 8; ++i)
+	{
+	  if (ipar[i] < 6)
+	    return 0;
+	  mask |= (ipar[i] - 6) << i;
+	}
+      /* FALLTHRU */
+
+    case E_V4DFmode:
+      /* In the 256-bit DFmode case, we can only move elements within
+         a 128-bit lane.  */
+      for (i = 0; i < 2; ++i)
+	{
+	  if (ipar[i] >= 2)
+	    return 0;
+	  mask |= ipar[i] << i;
+	}
+      for (i = 2; i < 4; ++i)
+	{
+	  if (ipar[i] < 2)
+	    return 0;
+	  mask |= (ipar[i] - 2) << i;
+	}
+      break;
+
+    case E_V16SFmode:
+      /* In 512 bit SFmode case, permutation in the upper 256 bits
+	 must mirror the permutation in the lower 256-bits.  */
+      for (i = 0; i < 8; ++i)
+	if (ipar[i] + 8 != ipar[i + 8])
+	  return 0;
+      /* FALLTHRU */
+
+    case E_V8SFmode:
+      /* In 256 bit SFmode case, we have full freedom of
+         movement within the low 128-bit lane, but the high 128-bit
+         lane must mirror the exact same pattern.  */
+      for (i = 0; i < 4; ++i)
+	if (ipar[i] + 4 != ipar[i + 4])
+	  return 0;
+      nelt = 4;
+      /* FALLTHRU */
+
+    case E_V2DFmode:
+    case E_V4SFmode:
+      /* In the 128-bit case, we've full freedom in the placement of
+	 the elements from the source operand.  */
+      for (i = 0; i < nelt; ++i)
+	mask |= ipar[i] << (i * (nelt / 2));
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  /* Make sure success has a non-zero value by adding one.  */
+  return mask + 1;
+}
+
+/* Helper for avx_vperm2f128_v4df_operand et al.  This is also used by
+   the expansion functions to turn the parallel back into a mask.
+   The return value is 0 for no match and the imm8+1 for a match.  */
+
+int
+avx_vperm2f128_parallel (rtx par, machine_mode mode)
+{
+  unsigned i, nelt = GET_MODE_NUNITS (mode), nelt2 = nelt / 2;
+  unsigned mask = 0;
+  unsigned char ipar[8] = {};  /* Silence -Wuninitialized warning.  */
+
+  if (XVECLEN (par, 0) != (int) nelt)
+    return 0;
+
+  /* Validate that all of the elements are constants, and not totally
+     out of range.  Copy the data into an integral array to make the
+     subsequent checks easier.  */
+  for (i = 0; i < nelt; ++i)
+    {
+      rtx er = XVECEXP (par, 0, i);
+      unsigned HOST_WIDE_INT ei;
+
+      if (!CONST_INT_P (er))
+	return 0;
+      ei = INTVAL (er);
+      if (ei >= 2 * nelt)
+	return 0;
+      ipar[i] = ei;
+    }
+
+  /* Validate that the halves of the permute are halves.  */
+  for (i = 0; i < nelt2 - 1; ++i)
+    if (ipar[i] + 1 != ipar[i + 1])
+      return 0;
+  for (i = nelt2; i < nelt - 1; ++i)
+    if (ipar[i] + 1 != ipar[i + 1])
+      return 0;
+
+  /* Reconstruct the mask.  */
+  for (i = 0; i < 2; ++i)
+    {
+      unsigned e = ipar[i * nelt2];
+      if (e % nelt2)
+	return 0;
+      e /= nelt2;
+      mask |= e << (i * 4);
+    }
+
+  /* Make sure success has a non-zero value by adding one.  */
+  return mask + 1;
+}
+
+/* Return a register priority for hard reg REGNO.  */
+static int
+ix86_register_priority (int hard_regno)
+{
+  /* ebp and r13 as the base always wants a displacement, r12 as the
+     base always wants an index.  So discourage their usage in an
+     address.  */
+  if (hard_regno == R12_REG || hard_regno == R13_REG)
+    return 0;
+  if (hard_regno == BP_REG)
+    return 1;
+  /* New x86-64 int registers result in bigger code size.  Discourage
+     them.  */
+  if (IN_RANGE (hard_regno, FIRST_REX_INT_REG, LAST_REX_INT_REG))
+    return 2;
+  /* New x86-64 SSE registers result in bigger code size.  Discourage
+     them.  */
+  if (IN_RANGE (hard_regno, FIRST_REX_SSE_REG, LAST_REX_SSE_REG))
+    return 2;
+  if (IN_RANGE (hard_regno, FIRST_EXT_REX_SSE_REG, LAST_EXT_REX_SSE_REG))
+    return 1;
+  /* Usage of AX register results in smaller code.  Prefer it.  */
+  if (hard_regno == AX_REG)
+    return 4;
+  return 3;
+}
+
+/* Implement TARGET_PREFERRED_RELOAD_CLASS.
+
+   Put float CONST_DOUBLE in the constant pool instead of fp regs.
+   QImode must go into class Q_REGS.
+   Narrow ALL_REGS to GENERAL_REGS.  This supports allowing movsf and
+   movdf to do mem-to-mem moves through integer regs.  */
+
+static reg_class_t
+ix86_preferred_reload_class (rtx x, reg_class_t regclass)
+{
+  machine_mode mode = GET_MODE (x);
+
+  /* We're only allowed to return a subclass of CLASS.  Many of the
+     following checks fail for NO_REGS, so eliminate that early.  */
+  if (regclass == NO_REGS)
+    return NO_REGS;
+
+  /* All classes can load zeros.  */
+  if (x == CONST0_RTX (mode))
+    return regclass;
+
+  /* Force constants into memory if we are loading a (nonzero) constant into
+     an MMX, SSE or MASK register.  This is because there are no MMX/SSE/MASK
+     instructions to load from a constant.  */
+  if (CONSTANT_P (x)
+      && (MAYBE_MMX_CLASS_P (regclass)
+	  || MAYBE_SSE_CLASS_P (regclass)
+	  || MAYBE_MASK_CLASS_P (regclass)))
+    return NO_REGS;
+
+  /* Floating-point constants need more complex checks.  */
+  if (CONST_DOUBLE_P (x))
+    {
+      /* General regs can load everything.  */
+      if (INTEGER_CLASS_P (regclass))
+        return regclass;
+
+      /* Floats can load 0 and 1 plus some others.  Note that we eliminated
+	 zero above.  We only want to wind up preferring 80387 registers if
+	 we plan on doing computation with them.  */
+      if (IS_STACK_MODE (mode)
+	  && standard_80387_constant_p (x) > 0)
+	{
+	  /* Limit class to FP regs.  */
+	  if (FLOAT_CLASS_P (regclass))
+	    return FLOAT_REGS;
+	}
+
+      return NO_REGS;
+    }
+
+  /* Prefer SSE regs only, if we can use them for math.  */
+  if (SSE_FLOAT_MODE_P (mode) && TARGET_SSE_MATH)
+    return SSE_CLASS_P (regclass) ? regclass : NO_REGS;
+
+  /* Generally when we see PLUS here, it's the function invariant
+     (plus soft-fp const_int).  Which can only be computed into general
+     regs.  */
+  if (GET_CODE (x) == PLUS)
+    return INTEGER_CLASS_P (regclass) ? regclass : NO_REGS;
+
+  /* QImode constants are easy to load, but non-constant QImode data
+     must go into Q_REGS or ALL_MASK_REGS.  */
+  if (GET_MODE (x) == QImode && !CONSTANT_P (x))
+    {
+      if (Q_CLASS_P (regclass))
+	return regclass;
+      else if (reg_class_subset_p (Q_REGS, regclass))
+	return Q_REGS;
+      else if (MASK_CLASS_P (regclass))
+	return regclass;
+      else
+	return NO_REGS;
+    }
+
+  return regclass;
+}
+
+/* Discourage putting floating-point values in SSE registers unless
+   SSE math is being used, and likewise for the 387 registers.  */
+static reg_class_t
+ix86_preferred_output_reload_class (rtx x, reg_class_t regclass)
+{
+  /* Restrict the output reload class to the register bank that we are doing
+     math on.  If we would like not to return a subset of CLASS, reject this
+     alternative: if reload cannot do this, it will still use its choice.  */
+  machine_mode mode = GET_MODE (x);
+  if (SSE_FLOAT_MODE_P (mode) && TARGET_SSE_MATH)
+    return MAYBE_SSE_CLASS_P (regclass) ? ALL_SSE_REGS : NO_REGS;
+
+  if (IS_STACK_MODE (mode))
+    return FLOAT_CLASS_P (regclass) ? regclass : NO_REGS;
+
+  return regclass;
+}
+
+static reg_class_t
+ix86_secondary_reload (bool in_p, rtx x, reg_class_t rclass,
+		       machine_mode mode, secondary_reload_info *sri)
+{
+  /* Double-word spills from general registers to non-offsettable memory
+     references (zero-extended addresses) require special handling.  */
+  if (TARGET_64BIT
+      && MEM_P (x)
+      && GET_MODE_SIZE (mode) > UNITS_PER_WORD
+      && INTEGER_CLASS_P (rclass)
+      && !offsettable_memref_p (x))
+    {
+      sri->icode = (in_p
+		    ? CODE_FOR_reload_noff_load
+		    : CODE_FOR_reload_noff_store);
+      /* Add the cost of moving address to a temporary.  */
+      sri->extra_cost = 1;
+
+      return NO_REGS;
+    }
+
+  /* QImode spills from non-QI registers require
+     intermediate register on 32bit targets.  */
+  if (mode == QImode
+      && ((!TARGET_64BIT && !in_p
+	   && INTEGER_CLASS_P (rclass)
+	   && MAYBE_NON_Q_CLASS_P (rclass))
+	  || (!TARGET_AVX512DQ
+	      && MAYBE_MASK_CLASS_P (rclass))))
+    {
+      int regno = true_regnum (x);
+
+      /* Return Q_REGS if the operand is in memory.  */
+      if (regno == -1)
+	return Q_REGS;
+
+      return NO_REGS;
+    }
+
+  /* This condition handles corner case where an expression involving
+     pointers gets vectorized.  We're trying to use the address of a
+     stack slot as a vector initializer.
+
+     (set (reg:V2DI 74 [ vect_cst_.2 ])
+          (vec_duplicate:V2DI (reg/f:DI 20 frame)))
+
+     Eventually frame gets turned into sp+offset like this:
+
+     (set (reg:V2DI 21 xmm0 [orig:74 vect_cst_.2 ] [74])
+          (vec_duplicate:V2DI (plus:DI (reg/f:DI 7 sp)
+	                               (const_int 392 [0x188]))))
+
+     That later gets turned into:
+
+     (set (reg:V2DI 21 xmm0 [orig:74 vect_cst_.2 ] [74])
+          (vec_duplicate:V2DI (plus:DI (reg/f:DI 7 sp)
+	    (mem/u/c/i:DI (symbol_ref/u:DI ("*.LC0") [flags 0x2]) [0 S8 A64]))))
+
+     We'll have the following reload recorded:
+
+     Reload 0: reload_in (DI) =
+           (plus:DI (reg/f:DI 7 sp)
+            (mem/u/c/i:DI (symbol_ref/u:DI ("*.LC0") [flags 0x2]) [0 S8 A64]))
+     reload_out (V2DI) = (reg:V2DI 21 xmm0 [orig:74 vect_cst_.2 ] [74])
+     SSE_REGS, RELOAD_OTHER (opnum = 0), can't combine
+     reload_in_reg: (plus:DI (reg/f:DI 7 sp) (const_int 392 [0x188]))
+     reload_out_reg: (reg:V2DI 21 xmm0 [orig:74 vect_cst_.2 ] [74])
+     reload_reg_rtx: (reg:V2DI 22 xmm1)
+
+     Which isn't going to work since SSE instructions can't handle scalar
+     additions.  Returning GENERAL_REGS forces the addition into integer
+     register and reload can handle subsequent reloads without problems.  */
+
+  if (in_p && GET_CODE (x) == PLUS
+      && SSE_CLASS_P (rclass)
+      && SCALAR_INT_MODE_P (mode))
+    return GENERAL_REGS;
+
+  return NO_REGS;
+}
+
+/* Implement TARGET_CLASS_LIKELY_SPILLED_P.  */
+
+static bool
+ix86_class_likely_spilled_p (reg_class_t rclass)
+{
+  switch (rclass)
+    {
+      case AREG:
+      case DREG:
+      case CREG:
+      case BREG:
+      case AD_REGS:
+      case SIREG:
+      case DIREG:
+      case SSE_FIRST_REG:
+      case FP_TOP_REG:
+      case FP_SECOND_REG:
+	return true;
+
+      default:
+	break;
+    }
+
+  return false;
+}
+
+/* Return true if a set of DST by the expression SRC should be allowed.
+   This prevents complex sets of likely_spilled hard regs before reload.  */
+
+bool
+ix86_hardreg_mov_ok (rtx dst, rtx src)
+{
+  /* Avoid complex sets of likely_spilled hard registers before reload.  */
+  if (REG_P (dst) && HARD_REGISTER_P (dst)
+      && !REG_P (src) && !MEM_P (src)
+      && !x86_64_immediate_operand (src, GET_MODE (dst))
+      && ix86_class_likely_spilled_p (REGNO_REG_CLASS (REGNO (dst)))
+      && !reload_completed)
+    return false;
+  return true;
+}
+
+/* If we are copying between registers from different register sets
+   (e.g. FP and integer), we may need a memory location.
+
+   The function can't work reliably when one of the CLASSES is a class
+   containing registers from multiple sets.  We avoid this by never combining
+   different sets in a single alternative in the machine description.
+   Ensure that this constraint holds to avoid unexpected surprises.
+
+   When STRICT is false, we are being called from REGISTER_MOVE_COST,
+   so do not enforce these sanity checks.
+
+   To optimize register_move_cost performance, define inline variant.  */
+
+static inline bool
+inline_secondary_memory_needed (machine_mode mode, reg_class_t class1,
+				reg_class_t class2, int strict)
+{
+  if (lra_in_progress && (class1 == NO_REGS || class2 == NO_REGS))
+    return false;
+
+  if (MAYBE_FLOAT_CLASS_P (class1) != FLOAT_CLASS_P (class1)
+      || MAYBE_FLOAT_CLASS_P (class2) != FLOAT_CLASS_P (class2)
+      || MAYBE_SSE_CLASS_P (class1) != SSE_CLASS_P (class1)
+      || MAYBE_SSE_CLASS_P (class2) != SSE_CLASS_P (class2)
+      || MAYBE_MMX_CLASS_P (class1) != MMX_CLASS_P (class1)
+      || MAYBE_MMX_CLASS_P (class2) != MMX_CLASS_P (class2)
+      || MAYBE_MASK_CLASS_P (class1) != MASK_CLASS_P (class1)
+      || MAYBE_MASK_CLASS_P (class2) != MASK_CLASS_P (class2))
+    {
+      gcc_assert (!strict || lra_in_progress);
+      return true;
+    }
+
+  if (FLOAT_CLASS_P (class1) != FLOAT_CLASS_P (class2))
+    return true;
+
+  /* ??? This is a lie.  We do have moves between mmx/general, and for
+     mmx/sse2.  But by saying we need secondary memory we discourage the
+     register allocator from using the mmx registers unless needed.  */
+  if (MMX_CLASS_P (class1) != MMX_CLASS_P (class2))
+    return true;
+
+  /* Between mask and general, we have moves no larger than word size.  */
+  if (MASK_CLASS_P (class1) != MASK_CLASS_P (class2))
+    {
+      if (!(INTEGER_CLASS_P (class1) || INTEGER_CLASS_P (class2))
+	  || GET_MODE_SIZE (mode) > UNITS_PER_WORD)
+	return true;
+    }
+
+  if (SSE_CLASS_P (class1) != SSE_CLASS_P (class2))
+    {
+      /* SSE1 doesn't have any direct moves from other classes.  */
+      if (!TARGET_SSE2)
+	return true;
+
+      /* Between SSE and general, we have moves no larger than word size.  */
+      if (!(INTEGER_CLASS_P (class1) || INTEGER_CLASS_P (class2))
+	  || GET_MODE_SIZE (mode) < GET_MODE_SIZE (SImode)
+	  || GET_MODE_SIZE (mode) > UNITS_PER_WORD)
+	return true;
+
+      /* If the target says that inter-unit moves are more expensive
+	 than moving through memory, then don't generate them.  */
+      if ((SSE_CLASS_P (class1) && !TARGET_INTER_UNIT_MOVES_FROM_VEC)
+	  || (SSE_CLASS_P (class2) && !TARGET_INTER_UNIT_MOVES_TO_VEC))
+	return true;
+    }
+
+  return false;
+}
+
+/* Implement TARGET_SECONDARY_MEMORY_NEEDED.  */
+
+static bool
+ix86_secondary_memory_needed (machine_mode mode, reg_class_t class1,
+			      reg_class_t class2)
+{
+  return inline_secondary_memory_needed (mode, class1, class2, true);
+}
+
+/* Implement TARGET_SECONDARY_MEMORY_NEEDED_MODE.
+
+   get_secondary_mem widens integral modes to BITS_PER_WORD.
+   There is no need to emit full 64 bit move on 64 bit targets
+   for integral modes that can be moved using 32 bit move.  */
+
+static machine_mode
+ix86_secondary_memory_needed_mode (machine_mode mode)
+{
+  if (GET_MODE_BITSIZE (mode) < 32 && INTEGRAL_MODE_P (mode))
+    return mode_for_size (32, GET_MODE_CLASS (mode), 0).require ();
+  return mode;
+}
+
+/* Implement the TARGET_CLASS_MAX_NREGS hook.
+
+   On the 80386, this is the size of MODE in words,
+   except in the FP regs, where a single reg is always enough.  */
+
+static unsigned char
+ix86_class_max_nregs (reg_class_t rclass, machine_mode mode)
+{
+  if (MAYBE_INTEGER_CLASS_P (rclass))
+    {
+      if (mode == XFmode)
+	return (TARGET_64BIT ? 2 : 3);
+      else if (mode == XCmode)
+	return (TARGET_64BIT ? 4 : 6);
+      else
+	return CEIL (GET_MODE_SIZE (mode), UNITS_PER_WORD);
+    }
+  else
+    {
+      if (COMPLEX_MODE_P (mode))
+	return 2;
+      else
+	return 1;
+    }
+}
+
+/* Implement TARGET_CAN_CHANGE_MODE_CLASS.  */
+
+static bool
+ix86_can_change_mode_class (machine_mode from, machine_mode to,
+			    reg_class_t regclass)
+{
+  if (from == to)
+    return true;
+
+  /* x87 registers can't do subreg at all, as all values are reformatted
+     to extended precision.  */
+  if (MAYBE_FLOAT_CLASS_P (regclass))
+    return false;
+
+  if (MAYBE_SSE_CLASS_P (regclass) || MAYBE_MMX_CLASS_P (regclass))
+    {
+      /* Vector registers do not support QI or HImode loads.  If we don't
+	 disallow a change to these modes, reload will assume it's ok to
+	 drop the subreg from (subreg:SI (reg:HI 100) 0).  This affects
+	 the vec_dupv4hi pattern.  */
+      if (GET_MODE_SIZE (from) < 4)
+	return false;
+    }
+
+  return true;
+}
+
+/* Return index of MODE in the sse load/store tables.  */
+
+static inline int
+sse_store_index (machine_mode mode)
+{
+      switch (GET_MODE_SIZE (mode))
+	{
+	  case 4:
+	    return 0;
+	  case 8:
+	    return 1;
+	  case 16:
+	    return 2;
+	  case 32:
+	    return 3;
+	  case 64:
+	    return 4;
+	  default:
+	    return -1;
+	}
+}
+
+/* Return the cost of moving data of mode M between a
+   register and memory.  A value of 2 is the default; this cost is
+   relative to those in `REGISTER_MOVE_COST'.
+
+   This function is used extensively by register_move_cost that is used to
+   build tables at startup.  Make it inline in this case.
+   When IN is 2, return maximum of in and out move cost.
+
+   If moving between registers and memory is more expensive than
+   between two registers, you should define this macro to express the
+   relative cost.
+
+   Model also increased moving costs of QImode registers in non
+   Q_REGS classes.
+ */
+static inline int
+inline_memory_move_cost (machine_mode mode, enum reg_class regclass, int in)
+{
+  int cost;
+  if (FLOAT_CLASS_P (regclass))
+    {
+      int index;
+      switch (mode)
+	{
+	  case E_SFmode:
+	    index = 0;
+	    break;
+	  case E_DFmode:
+	    index = 1;
+	    break;
+	  case E_XFmode:
+	    index = 2;
+	    break;
+	  default:
+	    return 100;
+	}
+      if (in == 2)
+        return MAX (ix86_cost->hard_register.fp_load [index],
+		    ix86_cost->hard_register.fp_store [index]);
+      return in ? ix86_cost->hard_register.fp_load [index]
+		: ix86_cost->hard_register.fp_store [index];
+    }
+  if (SSE_CLASS_P (regclass))
+    {
+      int index = sse_store_index (mode);
+      if (index == -1)
+	return 100;
+      if (in == 2)
+        return MAX (ix86_cost->hard_register.sse_load [index],
+		    ix86_cost->hard_register.sse_store [index]);
+      return in ? ix86_cost->hard_register.sse_load [index]
+		: ix86_cost->hard_register.sse_store [index];
+    }
+  if (MASK_CLASS_P (regclass))
+    {
+      int index;
+      switch (GET_MODE_SIZE (mode))
+	{
+	case 1:
+	  index = 0;
+	  break;
+	case 2:
+	  index = 1;
+	  break;
+	/* DImode loads and stores assumed to cost the same as SImode.  */
+	default:
+	  index = 2;
+	  break;
+	}
+
+      if (in == 2)
+	return MAX (ix86_cost->hard_register.mask_load[index],
+		    ix86_cost->hard_register.mask_store[index]);
+      return in ? ix86_cost->hard_register.mask_load[2]
+		: ix86_cost->hard_register.mask_store[2];
+    }
+  if (MMX_CLASS_P (regclass))
+    {
+      int index;
+      switch (GET_MODE_SIZE (mode))
+	{
+	  case 4:
+	    index = 0;
+	    break;
+	  case 8:
+	    index = 1;
+	    break;
+	  default:
+	    return 100;
+	}
+      if (in == 2)
+        return MAX (ix86_cost->hard_register.mmx_load [index],
+		    ix86_cost->hard_register.mmx_store [index]);
+      return in ? ix86_cost->hard_register.mmx_load [index]
+		: ix86_cost->hard_register.mmx_store [index];
+    }
+  switch (GET_MODE_SIZE (mode))
+    {
+      case 1:
+	if (Q_CLASS_P (regclass) || TARGET_64BIT)
+	  {
+	    if (!in)
+	      return ix86_cost->hard_register.int_store[0];
+	    if (TARGET_PARTIAL_REG_DEPENDENCY
+	        && optimize_function_for_speed_p (cfun))
+	      cost = ix86_cost->hard_register.movzbl_load;
+	    else
+	      cost = ix86_cost->hard_register.int_load[0];
+	    if (in == 2)
+	      return MAX (cost, ix86_cost->hard_register.int_store[0]);
+	    return cost;
+	  }
+	else
+	  {
+	   if (in == 2)
+	     return MAX (ix86_cost->hard_register.movzbl_load,
+			 ix86_cost->hard_register.int_store[0] + 4);
+	   if (in)
+	     return ix86_cost->hard_register.movzbl_load;
+	   else
+	     return ix86_cost->hard_register.int_store[0] + 4;
+	  }
+	break;
+      case 2:
+	if (in == 2)
+	  return MAX (ix86_cost->hard_register.int_load[1],
+		      ix86_cost->hard_register.int_store[1]);
+	return in ? ix86_cost->hard_register.int_load[1]
+		  : ix86_cost->hard_register.int_store[1];
+      default:
+	if (in == 2)
+	  cost = MAX (ix86_cost->hard_register.int_load[2],
+		      ix86_cost->hard_register.int_store[2]);
+	else if (in)
+	  cost = ix86_cost->hard_register.int_load[2];
+	else
+	  cost = ix86_cost->hard_register.int_store[2];
+	/* Multiply with the number of GPR moves needed.  */
+	return cost * CEIL ((int) GET_MODE_SIZE (mode), UNITS_PER_WORD);
+    }
+}
+
+static int
+ix86_memory_move_cost (machine_mode mode, reg_class_t regclass, bool in)
+{
+  return inline_memory_move_cost (mode, (enum reg_class) regclass, in ? 1 : 0);
+}
+
+
+/* Return the cost of moving data from a register in class CLASS1 to
+   one in class CLASS2.
+
+   It is not required that the cost always equal 2 when FROM is the same as TO;
+   on some machines it is expensive to move between registers if they are not
+   general registers.  */
+
+static int
+ix86_register_move_cost (machine_mode mode, reg_class_t class1_i,
+			 reg_class_t class2_i)
+{
+  enum reg_class class1 = (enum reg_class) class1_i;
+  enum reg_class class2 = (enum reg_class) class2_i;
+
+  /* In case we require secondary memory, compute cost of the store followed
+     by load.  In order to avoid bad register allocation choices, we need
+     for this to be *at least* as high as the symmetric MEMORY_MOVE_COST.  */
+
+  if (inline_secondary_memory_needed (mode, class1, class2, false))
+    {
+      int cost = 1;
+
+      cost += inline_memory_move_cost (mode, class1, 2);
+      cost += inline_memory_move_cost (mode, class2, 2);
+
+      /* In case of copying from general_purpose_register we may emit multiple
+         stores followed by single load causing memory size mismatch stall.
+         Count this as arbitrarily high cost of 20.  */
+      if (GET_MODE_BITSIZE (mode) > BITS_PER_WORD
+	  && TARGET_MEMORY_MISMATCH_STALL
+	  && targetm.class_max_nregs (class1, mode)
+	     > targetm.class_max_nregs (class2, mode))
+	cost += 20;
+
+      /* In the case of FP/MMX moves, the registers actually overlap, and we
+	 have to switch modes in order to treat them differently.  */
+      if ((MMX_CLASS_P (class1) && MAYBE_FLOAT_CLASS_P (class2))
+          || (MMX_CLASS_P (class2) && MAYBE_FLOAT_CLASS_P (class1)))
+	cost += 20;
+
+      return cost;
+    }
+
+  /* Moves between MMX and non-MMX units require secondary memory.  */
+  if (MMX_CLASS_P (class1) != MMX_CLASS_P (class2))
+    gcc_unreachable ();
+
+  if (SSE_CLASS_P (class1) != SSE_CLASS_P (class2))
+    return (SSE_CLASS_P (class1)
+	    ? ix86_cost->hard_register.sse_to_integer
+	    : ix86_cost->hard_register.integer_to_sse);
+
+  /* Moves between mask register and GPR.  */
+  if (MASK_CLASS_P (class1) != MASK_CLASS_P (class2))
+    {
+      return (MASK_CLASS_P (class1)
+	      ? ix86_cost->hard_register.mask_to_integer
+	      : ix86_cost->hard_register.integer_to_mask);
+    }
+  /* Moving between mask registers.  */
+  if (MASK_CLASS_P (class1) && MASK_CLASS_P (class2))
+    return ix86_cost->hard_register.mask_move;
+
+  if (MAYBE_FLOAT_CLASS_P (class1))
+    return ix86_cost->hard_register.fp_move;
+  if (MAYBE_SSE_CLASS_P (class1))
+    {
+      if (GET_MODE_BITSIZE (mode) <= 128)
+	return ix86_cost->hard_register.xmm_move;
+      if (GET_MODE_BITSIZE (mode) <= 256)
+	return ix86_cost->hard_register.ymm_move;
+      return ix86_cost->hard_register.zmm_move;
+    }
+  if (MAYBE_MMX_CLASS_P (class1))
+    return ix86_cost->hard_register.mmx_move;
+  return 2;
+}
+
+/* Implement TARGET_HARD_REGNO_NREGS.  This is ordinarily the length in
+   words of a value of mode MODE but can be less for certain modes in
+   special long registers.
+
+   Actually there are no two word move instructions for consecutive
+   registers.  And only registers 0-3 may have mov byte instructions
+   applied to them.  */
+
+static unsigned int
+ix86_hard_regno_nregs (unsigned int regno, machine_mode mode)
+{
+  if (GENERAL_REGNO_P (regno))
+    {
+      if (mode == XFmode)
+	return TARGET_64BIT ? 2 : 3;
+      if (mode == XCmode)
+	return TARGET_64BIT ? 4 : 6;
+      return CEIL (GET_MODE_SIZE (mode), UNITS_PER_WORD);
+    }
+  if (COMPLEX_MODE_P (mode))
+    return 2;
+  /* Register pair for mask registers.  */
+  if (mode == P2QImode || mode == P2HImode)
+    return 2;
+  if (mode == V64SFmode || mode == V64SImode)
+    return 4;
+  return 1;
+}
+
+/* Implement REGMODE_NATURAL_SIZE(MODE).  */
+unsigned int
+ix86_regmode_natural_size (machine_mode mode)
+{
+  if (mode == P2HImode || mode == P2QImode)
+    return GET_MODE_SIZE (mode) / 2;
+  return UNITS_PER_WORD;
+}
+
+/* Implement TARGET_HARD_REGNO_MODE_OK.  */
+
+static bool
+ix86_hard_regno_mode_ok (unsigned int regno, machine_mode mode)
+{
+  /* Flags and only flags can only hold CCmode values.  */
+  if (CC_REGNO_P (regno))
+    return GET_MODE_CLASS (mode) == MODE_CC;
+  if (GET_MODE_CLASS (mode) == MODE_CC
+      || GET_MODE_CLASS (mode) == MODE_RANDOM)
+    return false;
+  if (STACK_REGNO_P (regno))
+    return VALID_FP_MODE_P (mode);
+  if (MASK_REGNO_P (regno))
+    {
+      /* Register pair only starts at even register number.  */
+      if ((mode == P2QImode || mode == P2HImode))
+	return MASK_PAIR_REGNO_P(regno);
+
+      return ((TARGET_AVX512F && VALID_MASK_REG_MODE (mode))
+	      || (TARGET_AVX512BW
+		  && VALID_MASK_AVX512BW_MODE (mode)));
+    }
+
+  if (GET_MODE_CLASS (mode) == MODE_PARTIAL_INT)
+    return false;
+
+  if (SSE_REGNO_P (regno))
+    {
+      /* We implement the move patterns for all vector modes into and
+	 out of SSE registers, even when no operation instructions
+	 are available.  */
+
+      /* For AVX-512 we allow, regardless of regno:
+	  - XI mode
+	  - any of 512-bit wide vector mode
+	  - any scalar mode.  */
+      if (TARGET_AVX512F
+	  && (mode == XImode
+	      || VALID_AVX512F_REG_MODE (mode)
+	      || VALID_AVX512F_SCALAR_MODE (mode)))
+	return true;
+
+      /* For AVX-5124FMAPS or AVX-5124VNNIW
+	 allow V64SF and V64SI modes for special regnos.  */
+      if ((TARGET_AVX5124FMAPS || TARGET_AVX5124VNNIW)
+	  && (mode == V64SFmode || mode == V64SImode)
+	  && MOD4_SSE_REGNO_P (regno))
+	return true;
+
+      /* TODO check for QI/HI scalars.  */
+      /* AVX512VL allows sse regs16+ for 128/256 bit modes.  */
+      if (TARGET_AVX512VL
+	  && (mode == OImode
+	      || mode == TImode
+	      || VALID_AVX256_REG_MODE (mode)
+	      || VALID_AVX512VL_128_REG_MODE (mode)))
+	return true;
+
+      /* xmm16-xmm31 are only available for AVX-512.  */
+      if (EXT_REX_SSE_REGNO_P (regno))
+	return false;
+
+      /* OImode and AVX modes are available only when AVX is enabled.  */
+      return ((TARGET_AVX
+	       && VALID_AVX256_REG_OR_OI_MODE (mode))
+	      || VALID_SSE_REG_MODE (mode)
+	      || VALID_SSE2_REG_MODE (mode)
+	      || VALID_MMX_REG_MODE (mode)
+	      || VALID_MMX_REG_MODE_3DNOW (mode));
+    }
+  if (MMX_REGNO_P (regno))
+    {
+      /* We implement the move patterns for 3DNOW modes even in MMX mode,
+	 so if the register is available at all, then we can move data of
+	 the given mode into or out of it.  */
+      return (VALID_MMX_REG_MODE (mode)
+	      || VALID_MMX_REG_MODE_3DNOW (mode));
+    }
+
+  if (mode == QImode)
+    {
+      /* Take care for QImode values - they can be in non-QI regs,
+	 but then they do cause partial register stalls.  */
+      if (ANY_QI_REGNO_P (regno))
+	return true;
+      if (!TARGET_PARTIAL_REG_STALL)
+	return true;
+      /* LRA checks if the hard register is OK for the given mode.
+	 QImode values can live in non-QI regs, so we allow all
+	 registers here.  */
+      if (lra_in_progress)
+       return true;
+      return !can_create_pseudo_p ();
+    }
+  /* We handle both integer and floats in the general purpose registers.  */
+  else if (VALID_INT_MODE_P (mode))
+    return true;
+  else if (VALID_FP_MODE_P (mode))
+    return true;
+  else if (VALID_DFP_MODE_P (mode))
+    return true;
+  /* Lots of MMX code casts 8 byte vector modes to DImode.  If we then go
+     on to use that value in smaller contexts, this can easily force a
+     pseudo to be allocated to GENERAL_REGS.  Since this is no worse than
+     supporting DImode, allow it.  */
+  else if (VALID_MMX_REG_MODE_3DNOW (mode) || VALID_MMX_REG_MODE (mode))
+    return true;
+
+  return false;
+}
+
+/* Implement TARGET_HARD_REGNO_CALL_PART_CLOBBERED.  The only ABI that
+   saves SSE registers across calls is Win64 (thus no need to check the
+   current ABI here), and with AVX enabled Win64 only guarantees that
+   the low 16 bytes are saved.  */
+
+static bool
+ix86_hard_regno_call_part_clobbered (unsigned int, unsigned int regno,
+				     machine_mode mode)
+{
+  return SSE_REGNO_P (regno) && GET_MODE_SIZE (mode) > 16;
+}
+
+/* A subroutine of ix86_modes_tieable_p.  Return true if MODE is a
+   tieable integer mode.  */
+
+static bool
+ix86_tieable_integer_mode_p (machine_mode mode)
+{
+  switch (mode)
+    {
+    case E_HImode:
+    case E_SImode:
+      return true;
+
+    case E_QImode:
+      return TARGET_64BIT || !TARGET_PARTIAL_REG_STALL;
+
+    case E_DImode:
+      return TARGET_64BIT;
+
+    default:
+      return false;
+    }
+}
+
+/* Implement TARGET_MODES_TIEABLE_P.
+
+   Return true if MODE1 is accessible in a register that can hold MODE2
+   without copying.  That is, all register classes that can hold MODE2
+   can also hold MODE1.  */
+
+static bool
+ix86_modes_tieable_p (machine_mode mode1, machine_mode mode2)
+{
+  if (mode1 == mode2)
+    return true;
+
+  if (ix86_tieable_integer_mode_p (mode1)
+      && ix86_tieable_integer_mode_p (mode2))
+    return true;
+
+  /* MODE2 being XFmode implies fp stack or general regs, which means we
+     can tie any smaller floating point modes to it.  Note that we do not
+     tie this with TFmode.  */
+  if (mode2 == XFmode)
+    return mode1 == SFmode || mode1 == DFmode;
+
+  /* MODE2 being DFmode implies fp stack, general or sse regs, which means
+     that we can tie it with SFmode.  */
+  if (mode2 == DFmode)
+    return mode1 == SFmode;
+
+  /* If MODE2 is only appropriate for an SSE register, then tie with
+     any other mode acceptable to SSE registers.  */
+  if (GET_MODE_SIZE (mode2) == 64
+      && ix86_hard_regno_mode_ok (FIRST_SSE_REG, mode2))
+    return (GET_MODE_SIZE (mode1) == 64
+	    && ix86_hard_regno_mode_ok (FIRST_SSE_REG, mode1));
+  if (GET_MODE_SIZE (mode2) == 32
+      && ix86_hard_regno_mode_ok (FIRST_SSE_REG, mode2))
+    return (GET_MODE_SIZE (mode1) == 32
+	    && ix86_hard_regno_mode_ok (FIRST_SSE_REG, mode1));
+  if (GET_MODE_SIZE (mode2) == 16
+      && ix86_hard_regno_mode_ok (FIRST_SSE_REG, mode2))
+    return (GET_MODE_SIZE (mode1) == 16
+	    && ix86_hard_regno_mode_ok (FIRST_SSE_REG, mode1));
+
+  /* If MODE2 is appropriate for an MMX register, then tie
+     with any other mode acceptable to MMX registers.  */
+  if (GET_MODE_SIZE (mode2) == 8
+      && ix86_hard_regno_mode_ok (FIRST_MMX_REG, mode2))
+    return (GET_MODE_SIZE (mode1) == 8
+	    && ix86_hard_regno_mode_ok (FIRST_MMX_REG, mode1));
+
+  return false;
+}
+
+/* Return the cost of moving between two registers of mode MODE.  */
+
+static int
+ix86_set_reg_reg_cost (machine_mode mode)
+{
+  unsigned int units = UNITS_PER_WORD;
+
+  switch (GET_MODE_CLASS (mode))
+    {
+    default:
+      break;
+
+    case MODE_CC:
+      units = GET_MODE_SIZE (CCmode);
+      break;
+
+    case MODE_FLOAT:
+      if ((TARGET_SSE && mode == TFmode)
+	  || (TARGET_80387 && mode == XFmode)
+	  || ((TARGET_80387 || TARGET_SSE2) && mode == DFmode)
+	  || ((TARGET_80387 || TARGET_SSE) && mode == SFmode))
+	units = GET_MODE_SIZE (mode);
+      break;
+
+    case MODE_COMPLEX_FLOAT:
+      if ((TARGET_SSE && mode == TCmode)
+	  || (TARGET_80387 && mode == XCmode)
+	  || ((TARGET_80387 || TARGET_SSE2) && mode == DCmode)
+	  || ((TARGET_80387 || TARGET_SSE) && mode == SCmode))
+	units = GET_MODE_SIZE (mode);
+      break;
+
+    case MODE_VECTOR_INT:
+    case MODE_VECTOR_FLOAT:
+      if ((TARGET_AVX512F && VALID_AVX512F_REG_MODE (mode))
+	  || (TARGET_AVX && VALID_AVX256_REG_MODE (mode))
+	  || (TARGET_SSE2 && VALID_SSE2_REG_MODE (mode))
+	  || (TARGET_SSE && VALID_SSE_REG_MODE (mode))
+	  || ((TARGET_MMX || TARGET_MMX_WITH_SSE)
+	      && VALID_MMX_REG_MODE (mode)))
+	units = GET_MODE_SIZE (mode);
+    }
+
+  /* Return the cost of moving between two registers of mode MODE,
+     assuming that the move will be in pieces of at most UNITS bytes.  */
+  return COSTS_N_INSNS (CEIL (GET_MODE_SIZE (mode), units));
+}
+
+/* Return cost of vector operation in MODE given that scalar version has
+   COST.  */
+
+static int
+ix86_vec_cost (machine_mode mode, int cost)
+{
+  if (!VECTOR_MODE_P (mode))
+    return cost;
+
+  if (GET_MODE_BITSIZE (mode) == 128
+      && TARGET_SSE_SPLIT_REGS)
+    return cost * 2;
+  if (GET_MODE_BITSIZE (mode) > 128
+      && TARGET_AVX256_SPLIT_REGS)
+    return cost * GET_MODE_BITSIZE (mode) / 128;
+  return cost;
+}
+
+/* Return cost of multiplication in MODE.  */
+
+static int
+ix86_multiplication_cost (const struct processor_costs *cost,
+			  enum machine_mode mode)
+{
+  machine_mode inner_mode = mode;
+  if (VECTOR_MODE_P (mode))
+    inner_mode = GET_MODE_INNER (mode);
+
+  if (SSE_FLOAT_MODE_P (mode) && TARGET_SSE_MATH)
+    return inner_mode == DFmode ? cost->mulsd : cost->mulss;
+  else if (X87_FLOAT_MODE_P (mode))
+    return cost->fmul;
+  else if (FLOAT_MODE_P (mode))
+    return  ix86_vec_cost (mode,
+			   inner_mode == DFmode ? cost->mulsd : cost->mulss);
+  else if (GET_MODE_CLASS (mode) == MODE_VECTOR_INT)
+    {
+      /* vpmullq is used in this case. No emulation is needed.  */
+      if (TARGET_AVX512DQ)
+	return ix86_vec_cost (mode, cost->mulss);
+
+      /* V*QImode is emulated with 7-13 insns.  */
+      if (mode == V16QImode || mode == V32QImode)
+	{
+	  int extra = 11;
+	  if (TARGET_XOP && mode == V16QImode)
+	    extra = 5;
+	  else if (TARGET_SSSE3)
+	    extra = 6;
+	  return ix86_vec_cost (mode, cost->mulss * 2 + cost->sse_op * extra);
+	}
+      /* V*DImode is emulated with 5-8 insns.  */
+      else if (mode == V2DImode || mode == V4DImode)
+	{
+	  if (TARGET_XOP && mode == V2DImode)
+	    return ix86_vec_cost (mode, cost->mulss * 2 + cost->sse_op * 3);
+	  else
+	    return ix86_vec_cost (mode, cost->mulss * 3 + cost->sse_op * 5);
+	}
+      /* Without sse4.1, we don't have PMULLD; it's emulated with 7
+	 insns, including two PMULUDQ.  */
+      else if (mode == V4SImode && !(TARGET_SSE4_1 || TARGET_AVX))
+	return ix86_vec_cost (mode, cost->mulss * 2 + cost->sse_op * 5);
+      else
+	return ix86_vec_cost (mode, cost->mulss);
+    }
+  else
+    return (cost->mult_init[MODE_INDEX (mode)] + cost->mult_bit * 7);
+}
+
+/* Return cost of multiplication in MODE.  */
+
+static int
+ix86_division_cost (const struct processor_costs *cost,
+			  enum machine_mode mode)
+{
+  machine_mode inner_mode = mode;
+  if (VECTOR_MODE_P (mode))
+    inner_mode = GET_MODE_INNER (mode);
+
+  if (SSE_FLOAT_MODE_P (mode) && TARGET_SSE_MATH)
+    return inner_mode == DFmode ? cost->divsd : cost->divss;
+  else if (X87_FLOAT_MODE_P (mode))
+    return cost->fdiv;
+  else if (FLOAT_MODE_P (mode))
+    return ix86_vec_cost (mode,
+			  inner_mode == DFmode ? cost->divsd : cost->divss);
+  else
+    return cost->divide[MODE_INDEX (mode)];
+}
+
+#define COSTS_N_BYTES(N) ((N) * 2)
+
+/* Return cost of shift in MODE.
+   If CONSTANT_OP1 is true, the op1 value is known and set in OP1_VAL.
+   AND_IN_OP1 specify in op1 is result of and and SHIFT_AND_TRUNCATE
+   if op1 is a result of subreg.
+
+   SKIP_OP0/1 is set to true if cost of OP0/1 should be ignored.  */
+
+static int
+ix86_shift_rotate_cost (const struct processor_costs *cost,
+			enum machine_mode mode, bool constant_op1,
+			HOST_WIDE_INT op1_val,
+			bool speed,
+			bool and_in_op1,
+			bool shift_and_truncate,
+			bool *skip_op0, bool *skip_op1)
+{
+  if (skip_op0)
+    *skip_op0 = *skip_op1 = false;
+  if (GET_MODE_CLASS (mode) == MODE_VECTOR_INT)
+    {
+      /* V*QImode is emulated with 1-11 insns.  */
+      if (mode == V16QImode || mode == V32QImode)
+	{
+	  int count = 11;
+	  if (TARGET_XOP && mode == V16QImode)
+	    {
+	      /* For XOP we use vpshab, which requires a broadcast of the
+		 value to the variable shift insn.  For constants this
+		 means a V16Q const in mem; even when we can perform the
+		 shift with one insn set the cost to prefer paddb.  */
+	      if (constant_op1)
+		{
+		  if (skip_op1)
+		    *skip_op1 = true;
+		  return ix86_vec_cost (mode,
+					cost->sse_op
+					+ (speed
+					   ? 2
+					   : COSTS_N_BYTES
+					       (GET_MODE_UNIT_SIZE (mode))));
+		}
+	      count = 3;
+	    }
+	  else if (TARGET_SSSE3)
+	    count = 7;
+	  return ix86_vec_cost (mode, cost->sse_op * count);
+	}
+      else
+	return ix86_vec_cost (mode, cost->sse_op);
+    }
+  if (GET_MODE_SIZE (mode) > UNITS_PER_WORD)
+    {
+      if (constant_op1)
+	{
+	  if (op1_val > 32)
+	    return cost->shift_const + COSTS_N_INSNS (2);
+	  else
+	    return cost->shift_const * 2;
+	}
+      else
+	{
+	  if (and_in_op1)
+	    return cost->shift_var * 2;
+	  else
+	    return cost->shift_var * 6 + COSTS_N_INSNS (2);
+	}
+    }
+  else
+    {
+      if (constant_op1)
+	return cost->shift_const;
+      else if (shift_and_truncate)
+	{
+	  if (skip_op0)
+	    *skip_op0 = *skip_op1 = true;
+	  /* Return the cost after shift-and truncation.  */
+	  return cost->shift_var;
+	}
+      else
+	return cost->shift_var;
+    }
+  return cost->shift_const;
+}
+
+/* Compute a (partial) cost for rtx X.  Return true if the complete
+   cost has been computed, and false if subexpressions should be
+   scanned.  In either case, *TOTAL contains the cost result.  */
+
+static bool
+ix86_rtx_costs (rtx x, machine_mode mode, int outer_code_i, int opno,
+		int *total, bool speed)
+{
+  rtx mask;
+  enum rtx_code code = GET_CODE (x);
+  enum rtx_code outer_code = (enum rtx_code) outer_code_i;
+  const struct processor_costs *cost
+    = speed ? ix86_tune_cost : &ix86_size_cost;
+  int src_cost;
+
+  switch (code)
+    {
+    case SET:
+      if (register_operand (SET_DEST (x), VOIDmode)
+	  && register_operand (SET_SRC (x), VOIDmode))
+	{
+	  *total = ix86_set_reg_reg_cost (GET_MODE (SET_DEST (x)));
+	  return true;
+	}
+
+      if (register_operand (SET_SRC (x), VOIDmode))
+	/* Avoid potentially incorrect high cost from rtx_costs
+	   for non-tieable SUBREGs.  */
+	src_cost = 0;
+      else
+	{
+	  src_cost = rtx_cost (SET_SRC (x), mode, SET, 1, speed);
+
+	  if (CONSTANT_P (SET_SRC (x)))
+	    /* Constant costs assume a base value of COSTS_N_INSNS (1) and add
+	       a small value, possibly zero for cheap constants.  */
+	    src_cost += COSTS_N_INSNS (1);
+	}
+
+      *total = src_cost + rtx_cost (SET_DEST (x), mode, SET, 0, speed);
+      return true;
+
+    case CONST_INT:
+    case CONST:
+    case LABEL_REF:
+    case SYMBOL_REF:
+      if (x86_64_immediate_operand (x, VOIDmode))
+	*total = 0;
+     else
+	*total = 1;
+      return true;
+
+    case CONST_DOUBLE:
+      if (IS_STACK_MODE (mode))
+	switch (standard_80387_constant_p (x))
+	  {
+	  case -1:
+	  case 0:
+	    break;
+	  case 1: /* 0.0 */
+	    *total = 1;
+	    return true;
+	  default: /* Other constants */
+	    *total = 2;
+	    return true;
+	  }
+      /* FALLTHRU */
+
+    case CONST_VECTOR:
+      switch (standard_sse_constant_p (x, mode))
+	{
+	case 0:
+	  break;
+	case 1:  /* 0: xor eliminates false dependency */
+	  *total = 0;
+	  return true;
+	default: /* -1: cmp contains false dependency */
+	  *total = 1;
+	  return true;
+	}
+      /* FALLTHRU */
+
+    case CONST_WIDE_INT:
+      /* Fall back to (MEM (SYMBOL_REF)), since that's where
+	 it'll probably end up.  Add a penalty for size.  */
+      *total = (COSTS_N_INSNS (1)
+		+ (!TARGET_64BIT && flag_pic)
+		+ (GET_MODE_SIZE (mode) <= 4
+		   ? 0 : GET_MODE_SIZE (mode) <= 8 ? 1 : 2));
+      return true;
+
+    case ZERO_EXTEND:
+      /* The zero extensions is often completely free on x86_64, so make
+	 it as cheap as possible.  */
+      if (TARGET_64BIT && mode == DImode
+	  && GET_MODE (XEXP (x, 0)) == SImode)
+	*total = 1;
+      else if (TARGET_ZERO_EXTEND_WITH_AND)
+	*total = cost->add;
+      else
+	*total = cost->movzx;
+      return false;
+
+    case SIGN_EXTEND:
+      *total = cost->movsx;
+      return false;
+
+    case ASHIFT:
+      if (SCALAR_INT_MODE_P (mode)
+	  && GET_MODE_SIZE (mode) < UNITS_PER_WORD
+	  && CONST_INT_P (XEXP (x, 1)))
+	{
+	  HOST_WIDE_INT value = INTVAL (XEXP (x, 1));
+	  if (value == 1)
+	    {
+	      *total = cost->add;
+	      return false;
+	    }
+	  if ((value == 2 || value == 3)
+	      && cost->lea <= cost->shift_const)
+	    {
+	      *total = cost->lea;
+	      return false;
+	    }
+	}
+      /* FALLTHRU */
+
+    case ROTATE:
+    case ASHIFTRT:
+    case LSHIFTRT:
+    case ROTATERT:
+      bool skip_op0, skip_op1;
+      *total = ix86_shift_rotate_cost (cost, mode, CONSTANT_P (XEXP (x, 1)),
+				       CONST_INT_P (XEXP (x, 1))
+					 ? INTVAL (XEXP (x, 1)) : -1,
+				       speed,
+				       GET_CODE (XEXP (x, 1)) == AND,
+				       SUBREG_P (XEXP (x, 1))
+				       && GET_CODE (XEXP (XEXP (x, 1), 0)) == AND,
+				       &skip_op0, &skip_op1);
+      if (skip_op0 || skip_op1)
+	{
+	  if (!skip_op0)
+	    *total += rtx_cost (XEXP (x, 0), mode, code, 0, speed);
+	  if (!skip_op1)
+	    *total += rtx_cost (XEXP (x, 1), mode, code, 0, speed);
+	  return true;
+	}
+      return false;
+
+    case FMA:
+      {
+	rtx sub;
+
+        gcc_assert (FLOAT_MODE_P (mode));
+        gcc_assert (TARGET_FMA || TARGET_FMA4 || TARGET_AVX512F);
+
+        *total = ix86_vec_cost (mode,
+				GET_MODE_INNER (mode) == SFmode
+				? cost->fmass : cost->fmasd);
+	*total += rtx_cost (XEXP (x, 1), mode, FMA, 1, speed);
+
+        /* Negate in op0 or op2 is free: FMS, FNMA, FNMS.  */
+	sub = XEXP (x, 0);
+	if (GET_CODE (sub) == NEG)
+	  sub = XEXP (sub, 0);
+	*total += rtx_cost (sub, mode, FMA, 0, speed);
+
+	sub = XEXP (x, 2);
+	if (GET_CODE (sub) == NEG)
+	  sub = XEXP (sub, 0);
+	*total += rtx_cost (sub, mode, FMA, 2, speed);
+	return true;
+      }
+
+    case MULT:
+      if (!FLOAT_MODE_P (mode) && !VECTOR_MODE_P (mode))
+	{
+	  rtx op0 = XEXP (x, 0);
+	  rtx op1 = XEXP (x, 1);
+	  int nbits;
+	  if (CONST_INT_P (XEXP (x, 1)))
+	    {
+	      unsigned HOST_WIDE_INT value = INTVAL (XEXP (x, 1));
+	      for (nbits = 0; value != 0; value &= value - 1)
+	        nbits++;
+	    }
+	  else
+	    /* This is arbitrary.  */
+	    nbits = 7;
+
+	  /* Compute costs correctly for widening multiplication.  */
+	  if ((GET_CODE (op0) == SIGN_EXTEND || GET_CODE (op0) == ZERO_EXTEND)
+	      && GET_MODE_SIZE (GET_MODE (XEXP (op0, 0))) * 2
+	         == GET_MODE_SIZE (mode))
+	    {
+	      int is_mulwiden = 0;
+	      machine_mode inner_mode = GET_MODE (op0);
+
+	      if (GET_CODE (op0) == GET_CODE (op1))
+		is_mulwiden = 1, op1 = XEXP (op1, 0);
+	      else if (CONST_INT_P (op1))
+		{
+		  if (GET_CODE (op0) == SIGN_EXTEND)
+		    is_mulwiden = trunc_int_for_mode (INTVAL (op1), inner_mode)
+			          == INTVAL (op1);
+		  else
+		    is_mulwiden = !(INTVAL (op1) & ~GET_MODE_MASK (inner_mode));
+	        }
+
+	      if (is_mulwiden)
+	        op0 = XEXP (op0, 0), mode = GET_MODE (op0);
+	    }
+
+  	  *total = (cost->mult_init[MODE_INDEX (mode)]
+		    + nbits * cost->mult_bit
+	            + rtx_cost (op0, mode, outer_code, opno, speed)
+		    + rtx_cost (op1, mode, outer_code, opno, speed));
+
+          return true;
+	}
+      *total = ix86_multiplication_cost (cost, mode);
+      return false;
+
+    case DIV:
+    case UDIV:
+    case MOD:
+    case UMOD:
+      *total = ix86_division_cost (cost, mode);
+      return false;
+
+    case PLUS:
+      if (GET_MODE_CLASS (mode) == MODE_INT
+	  && GET_MODE_SIZE (mode) <= UNITS_PER_WORD)
+	{
+	  if (GET_CODE (XEXP (x, 0)) == PLUS
+	      && GET_CODE (XEXP (XEXP (x, 0), 0)) == MULT
+	      && CONST_INT_P (XEXP (XEXP (XEXP (x, 0), 0), 1))
+	      && CONSTANT_P (XEXP (x, 1)))
+	    {
+	      HOST_WIDE_INT val = INTVAL (XEXP (XEXP (XEXP (x, 0), 0), 1));
+	      if (val == 2 || val == 4 || val == 8)
+		{
+		  *total = cost->lea;
+		  *total += rtx_cost (XEXP (XEXP (x, 0), 1), mode,
+				      outer_code, opno, speed);
+		  *total += rtx_cost (XEXP (XEXP (XEXP (x, 0), 0), 0), mode,
+				      outer_code, opno, speed);
+		  *total += rtx_cost (XEXP (x, 1), mode,
+				      outer_code, opno, speed);
+		  return true;
+		}
+	    }
+	  else if (GET_CODE (XEXP (x, 0)) == MULT
+		   && CONST_INT_P (XEXP (XEXP (x, 0), 1)))
+	    {
+	      HOST_WIDE_INT val = INTVAL (XEXP (XEXP (x, 0), 1));
+	      if (val == 2 || val == 4 || val == 8)
+		{
+		  *total = cost->lea;
+		  *total += rtx_cost (XEXP (XEXP (x, 0), 0), mode,
+				      outer_code, opno, speed);
+		  *total += rtx_cost (XEXP (x, 1), mode,
+				      outer_code, opno, speed);
+		  return true;
+		}
+	    }
+	  else if (GET_CODE (XEXP (x, 0)) == PLUS)
+	    {
+	      /* Add with carry, ignore the cost of adding a carry flag.  */
+	      if (ix86_carry_flag_operator (XEXP (XEXP (x, 0), 0), mode))
+		*total = cost->add;
+	      else
+		{
+		  *total = cost->lea;
+		  *total += rtx_cost (XEXP (XEXP (x, 0), 0), mode,
+				      outer_code, opno, speed);
+		}
+
+	      *total += rtx_cost (XEXP (XEXP (x, 0), 1), mode,
+				  outer_code, opno, speed);
+	      *total += rtx_cost (XEXP (x, 1), mode,
+				  outer_code, opno, speed);
+	      return true;
+	    }
+	}
+      /* FALLTHRU */
+
+    case MINUS:
+      /* Subtract with borrow, ignore the cost of subtracting a carry flag.  */
+      if (GET_MODE_CLASS (mode) == MODE_INT
+	  && GET_MODE_SIZE (mode) <= UNITS_PER_WORD
+	  && GET_CODE (XEXP (x, 0)) == MINUS
+	  && ix86_carry_flag_operator (XEXP (XEXP (x, 0), 1), mode))
+	{
+	  *total = cost->add;
+	  *total += rtx_cost (XEXP (XEXP (x, 0), 0), mode,
+			      outer_code, opno, speed);
+	  *total += rtx_cost (XEXP (x, 1), mode,
+			      outer_code, opno, speed);
+	  return true;
+	}
+
+      if (SSE_FLOAT_MODE_P (mode) && TARGET_SSE_MATH)
+	{
+	  *total = cost->addss;
+	  return false;
+	}
+      else if (X87_FLOAT_MODE_P (mode))
+	{
+	  *total = cost->fadd;
+	  return false;
+	}
+      else if (FLOAT_MODE_P (mode))
+	{
+	  *total = ix86_vec_cost (mode, cost->addss);
+	  return false;
+	}
+      /* FALLTHRU */
+
+    case AND:
+    case IOR:
+    case XOR:
+      if (GET_MODE_CLASS (mode) == MODE_INT
+	  && GET_MODE_SIZE (mode) > UNITS_PER_WORD)
+	{
+	  *total = (cost->add * 2
+		    + (rtx_cost (XEXP (x, 0), mode, outer_code, opno, speed)
+		       << (GET_MODE (XEXP (x, 0)) != DImode))
+		    + (rtx_cost (XEXP (x, 1), mode, outer_code, opno, speed)
+	               << (GET_MODE (XEXP (x, 1)) != DImode)));
+	  return true;
+	}
+      /* FALLTHRU */
+
+    case NEG:
+      if (SSE_FLOAT_MODE_P (mode) && TARGET_SSE_MATH)
+	{
+	  *total = cost->sse_op;
+	  return false;
+	}
+      else if (X87_FLOAT_MODE_P (mode))
+	{
+	  *total = cost->fchs;
+	  return false;
+	}
+      else if (FLOAT_MODE_P (mode))
+	{
+	  *total = ix86_vec_cost (mode, cost->sse_op);
+	  return false;
+	}
+      /* FALLTHRU */
+
+    case NOT:
+      if (GET_MODE_CLASS (mode) == MODE_VECTOR_INT)
+	*total = ix86_vec_cost (mode, cost->sse_op);
+      else if (GET_MODE_SIZE (mode) > UNITS_PER_WORD)
+	*total = cost->add * 2;
+      else
+	*total = cost->add;
+      return false;
+
+    case COMPARE:
+      rtx op0, op1;
+      op0 = XEXP (x, 0);
+      op1 = XEXP (x, 1);
+      if (GET_CODE (op0) == ZERO_EXTRACT
+	  && XEXP (op0, 1) == const1_rtx
+	  && CONST_INT_P (XEXP (op0, 2))
+	  && op1 == const0_rtx)
+	{
+	  /* This kind of construct is implemented using test[bwl].
+	     Treat it as if we had an AND.  */
+	  mode = GET_MODE (XEXP (op0, 0));
+	  *total = (cost->add
+		    + rtx_cost (XEXP (op0, 0), mode, outer_code,
+				opno, speed)
+		    + rtx_cost (const1_rtx, mode, outer_code, opno, speed));
+	  return true;
+	}
+
+      if (GET_CODE (op0) == PLUS && rtx_equal_p (XEXP (op0, 0), op1))
+	{
+	  /* This is an overflow detection, count it as a normal compare.  */
+	  *total = rtx_cost (op0, GET_MODE (op0), COMPARE, 0, speed);
+	  return true;
+	}
+
+      rtx geu;
+      /* Match x
+	 (compare:CCC (neg:QI (geu:QI (reg:CC_CCC FLAGS_REG) (const_int 0)))
+		      (ltu:QI (reg:CC_CCC FLAGS_REG) (const_int 0)))  */
+      if (mode == CCCmode
+	  && GET_CODE (op0) == NEG
+	  && GET_CODE (geu = XEXP (op0, 0)) == GEU
+	  && REG_P (XEXP (geu, 0))
+	  && (GET_MODE (XEXP (geu, 0)) == CCCmode
+	      || GET_MODE (XEXP (geu, 0)) == CCmode)
+	  && REGNO (XEXP (geu, 0)) == FLAGS_REG
+	  && XEXP (geu, 1) == const0_rtx
+	  && GET_CODE (op1) == LTU
+	  && REG_P (XEXP (op1, 0))
+	  && GET_MODE (XEXP (op1, 0)) == GET_MODE (XEXP (geu, 0))
+	  && REGNO (XEXP (op1, 0)) == FLAGS_REG
+	  && XEXP (op1, 1) == const0_rtx)
+	{
+	  /* This is *setcc_qi_addqi3_cconly_overflow_1_* patterns, a nop.  */
+	  *total = 0;
+	  return true;
+	}
+
+      /* The embedded comparison operand is completely free.  */
+      if (!general_operand (op0, GET_MODE (op0)) && op1 == const0_rtx)
+	*total = 0;
+
+      return false;
+
+    case FLOAT_EXTEND:
+      if (!(SSE_FLOAT_MODE_P (mode) && TARGET_SSE_MATH))
+	*total = 0;
+      else
+        *total = ix86_vec_cost (mode, cost->addss);
+      return false;
+
+    case FLOAT_TRUNCATE:
+      if (!(SSE_FLOAT_MODE_P (mode) && TARGET_SSE_MATH))
+	*total = cost->fadd;
+      else
+        *total = ix86_vec_cost (mode, cost->addss);
+      return false;
+
+    case ABS:
+      /* SSE requires memory load for the constant operand. It may make
+	 sense to account for this.  Of course the constant operand may or
+	 may not be reused. */
+      if (SSE_FLOAT_MODE_P (mode) && TARGET_SSE_MATH)
+	*total = cost->sse_op;
+      else if (X87_FLOAT_MODE_P (mode))
+	*total = cost->fabs;
+      else if (FLOAT_MODE_P (mode))
+	*total = ix86_vec_cost (mode, cost->sse_op);
+      return false;
+
+    case SQRT:
+      if (SSE_FLOAT_MODE_P (mode) && TARGET_SSE_MATH)
+	*total = mode == SFmode ? cost->sqrtss : cost->sqrtsd;
+      else if (X87_FLOAT_MODE_P (mode))
+	*total = cost->fsqrt;
+      else if (FLOAT_MODE_P (mode))
+	*total = ix86_vec_cost (mode,
+				mode == SFmode ? cost->sqrtss : cost->sqrtsd);
+      return false;
+
+    case UNSPEC:
+      if (XINT (x, 1) == UNSPEC_TP)
+	*total = 0;
+      return false;
+
+    case VEC_SELECT:
+    case VEC_CONCAT:
+    case VEC_DUPLICATE:
+      /* ??? Assume all of these vector manipulation patterns are
+	 recognizable.  In which case they all pretty much have the
+	 same cost.  */
+     *total = cost->sse_op;
+     return true;
+    case VEC_MERGE:
+      mask = XEXP (x, 2);
+      /* This is masked instruction, assume the same cost,
+	 as nonmasked variant.  */
+      if (TARGET_AVX512F && register_operand (mask, GET_MODE (mask)))
+	*total = rtx_cost (XEXP (x, 0), mode, outer_code, opno, speed);
+      else
+	*total = cost->sse_op;
+      return true;
+
+    default:
+      return false;
+    }
+}
+
+#if TARGET_MACHO
+
+static int current_machopic_label_num;
+
+/* Given a symbol name and its associated stub, write out the
+   definition of the stub.  */
+
+void
+machopic_output_stub (FILE *file, const char *symb, const char *stub)
+{
+  unsigned int length;
+  char *binder_name, *symbol_name, lazy_ptr_name[32];
+  int label = ++current_machopic_label_num;
+
+  /* For 64-bit we shouldn't get here.  */
+  gcc_assert (!TARGET_64BIT);
+
+  /* Lose our funky encoding stuff so it doesn't contaminate the stub.  */
+  symb = targetm.strip_name_encoding (symb);
+
+  length = strlen (stub);
+  binder_name = XALLOCAVEC (char, length + 32);
+  GEN_BINDER_NAME_FOR_STUB (binder_name, stub, length);
+
+  length = strlen (symb);
+  symbol_name = XALLOCAVEC (char, length + 32);
+  GEN_SYMBOL_NAME_FOR_SYMBOL (symbol_name, symb, length);
+
+  sprintf (lazy_ptr_name, "L%d$lz", label);
+
+  if (MACHOPIC_ATT_STUB)
+    switch_to_section (darwin_sections[machopic_picsymbol_stub3_section]);
+  else if (MACHOPIC_PURE)
+    switch_to_section (darwin_sections[machopic_picsymbol_stub2_section]);
+  else
+    switch_to_section (darwin_sections[machopic_symbol_stub_section]);
+
+  fprintf (file, "%s:\n", stub);
+  fprintf (file, "\t.indirect_symbol %s\n", symbol_name);
+
+  if (MACHOPIC_ATT_STUB)
+    {
+      fprintf (file, "\thlt ; hlt ; hlt ; hlt ; hlt\n");
+    }
+  else if (MACHOPIC_PURE)
+    {
+      /* PIC stub.  */
+      /* 25-byte PIC stub using "CALL get_pc_thunk".  */
+      rtx tmp = gen_rtx_REG (SImode, 2 /* ECX */);
+      output_set_got (tmp, NULL_RTX);	/* "CALL ___<cpu>.get_pc_thunk.cx".  */
+      fprintf (file, "LPC$%d:\tmovl\t%s-LPC$%d(%%ecx),%%ecx\n",
+	       label, lazy_ptr_name, label);
+      fprintf (file, "\tjmp\t*%%ecx\n");
+    }
+  else
+    fprintf (file, "\tjmp\t*%s\n", lazy_ptr_name);
+
+  /* The AT&T-style ("self-modifying") stub is not lazily bound, thus
+     it needs no stub-binding-helper.  */
+  if (MACHOPIC_ATT_STUB)
+    return;
+
+  fprintf (file, "%s:\n", binder_name);
+
+  if (MACHOPIC_PURE)
+    {
+      fprintf (file, "\tlea\t%s-%s(%%ecx),%%ecx\n", lazy_ptr_name, binder_name);
+      fprintf (file, "\tpushl\t%%ecx\n");
+    }
+  else
+    fprintf (file, "\tpushl\t$%s\n", lazy_ptr_name);
+
+  fputs ("\tjmp\tdyld_stub_binding_helper\n", file);
+
+  /* N.B. Keep the correspondence of these
+     'symbol_ptr/symbol_ptr2/symbol_ptr3' sections consistent with the
+     old-pic/new-pic/non-pic stubs; altering this will break
+     compatibility with existing dylibs.  */
+  if (MACHOPIC_PURE)
+    {
+      /* 25-byte PIC stub using "CALL get_pc_thunk".  */
+      switch_to_section (darwin_sections[machopic_lazy_symbol_ptr2_section]);
+    }
+  else
+    /* 16-byte -mdynamic-no-pic stub.  */
+    switch_to_section(darwin_sections[machopic_lazy_symbol_ptr3_section]);
+
+  fprintf (file, "%s:\n", lazy_ptr_name);
+  fprintf (file, "\t.indirect_symbol %s\n", symbol_name);
+  fprintf (file, ASM_LONG "%s\n", binder_name);
+}
+#endif /* TARGET_MACHO */
+
+/* Order the registers for register allocator.  */
+
+void
+x86_order_regs_for_local_alloc (void)
+{
+   int pos = 0;
+   int i;
+
+   /* First allocate the local general purpose registers.  */
+   for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
+     if (GENERAL_REGNO_P (i) && call_used_or_fixed_reg_p (i))
+	reg_alloc_order [pos++] = i;
+
+   /* Global general purpose registers.  */
+   for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
+     if (GENERAL_REGNO_P (i) && !call_used_or_fixed_reg_p (i))
+	reg_alloc_order [pos++] = i;
+
+   /* x87 registers come first in case we are doing FP math
+      using them.  */
+   if (!TARGET_SSE_MATH)
+     for (i = FIRST_STACK_REG; i <= LAST_STACK_REG; i++)
+       reg_alloc_order [pos++] = i;
+
+   /* SSE registers.  */
+   for (i = FIRST_SSE_REG; i <= LAST_SSE_REG; i++)
+     reg_alloc_order [pos++] = i;
+   for (i = FIRST_REX_SSE_REG; i <= LAST_REX_SSE_REG; i++)
+     reg_alloc_order [pos++] = i;
+
+   /* Extended REX SSE registers.  */
+   for (i = FIRST_EXT_REX_SSE_REG; i <= LAST_EXT_REX_SSE_REG; i++)
+     reg_alloc_order [pos++] = i;
+
+   /* Mask register.  */
+   for (i = FIRST_MASK_REG; i <= LAST_MASK_REG; i++)
+     reg_alloc_order [pos++] = i;
+
+   /* x87 registers.  */
+   if (TARGET_SSE_MATH)
+     for (i = FIRST_STACK_REG; i <= LAST_STACK_REG; i++)
+       reg_alloc_order [pos++] = i;
+
+   for (i = FIRST_MMX_REG; i <= LAST_MMX_REG; i++)
+     reg_alloc_order [pos++] = i;
+
+   /* Initialize the rest of array as we do not allocate some registers
+      at all.  */
+   while (pos < FIRST_PSEUDO_REGISTER)
+     reg_alloc_order [pos++] = 0;
+}
+
+static bool
+ix86_ms_bitfield_layout_p (const_tree record_type)
+{
+  return ((TARGET_MS_BITFIELD_LAYOUT
+	   && !lookup_attribute ("gcc_struct", TYPE_ATTRIBUTES (record_type)))
+          || lookup_attribute ("ms_struct", TYPE_ATTRIBUTES (record_type)));
+}
+
+/* Returns an expression indicating where the this parameter is
+   located on entry to the FUNCTION.  */
+
+static rtx
+x86_this_parameter (tree function)
+{
+  tree type = TREE_TYPE (function);
+  bool aggr = aggregate_value_p (TREE_TYPE (type), type) != 0;
+  int nregs;
+
+  if (TARGET_64BIT)
+    {
+      const int *parm_regs;
+
+      if (ix86_function_type_abi (type) == MS_ABI)
+        parm_regs = x86_64_ms_abi_int_parameter_registers;
+      else
+        parm_regs = x86_64_int_parameter_registers;
+      return gen_rtx_REG (Pmode, parm_regs[aggr]);
+    }
+
+  nregs = ix86_function_regparm (type, function);
+
+  if (nregs > 0 && !stdarg_p (type))
+    {
+      int regno;
+      unsigned int ccvt = ix86_get_callcvt (type);
+
+      if ((ccvt & IX86_CALLCVT_FASTCALL) != 0)
+	regno = aggr ? DX_REG : CX_REG;
+      else if ((ccvt & IX86_CALLCVT_THISCALL) != 0)
+        {
+	  regno = CX_REG;
+	  if (aggr)
+	    return gen_rtx_MEM (SImode,
+				plus_constant (Pmode, stack_pointer_rtx, 4));
+	}
+      else
+        {
+	  regno = AX_REG;
+	  if (aggr)
+	    {
+	      regno = DX_REG;
+	      if (nregs == 1)
+		return gen_rtx_MEM (SImode,
+				    plus_constant (Pmode,
+						   stack_pointer_rtx, 4));
+	    }
+	}
+      return gen_rtx_REG (SImode, regno);
+    }
+
+  return gen_rtx_MEM (SImode, plus_constant (Pmode, stack_pointer_rtx,
+					     aggr ? 8 : 4));
+}
+
+/* Determine whether x86_output_mi_thunk can succeed.  */
+
+static bool
+x86_can_output_mi_thunk (const_tree, HOST_WIDE_INT, HOST_WIDE_INT vcall_offset,
+			 const_tree function)
+{
+  /* 64-bit can handle anything.  */
+  if (TARGET_64BIT)
+    return true;
+
+  /* For 32-bit, everything's fine if we have one free register.  */
+  if (ix86_function_regparm (TREE_TYPE (function), function) < 3)
+    return true;
+
+  /* Need a free register for vcall_offset.  */
+  if (vcall_offset)
+    return false;
+
+  /* Need a free register for GOT references.  */
+  if (flag_pic && !targetm.binds_local_p (function))
+    return false;
+
+  /* Otherwise ok.  */
+  return true;
+}
+
+/* Output the assembler code for a thunk function.  THUNK_DECL is the
+   declaration for the thunk function itself, FUNCTION is the decl for
+   the target function.  DELTA is an immediate constant offset to be
+   added to THIS.  If VCALL_OFFSET is nonzero, the word at
+   *(*this + vcall_offset) should be added to THIS.  */
+
+static void
+x86_output_mi_thunk (FILE *file, tree thunk_fndecl, HOST_WIDE_INT delta,
+		     HOST_WIDE_INT vcall_offset, tree function)
+{
+  const char *fnname = IDENTIFIER_POINTER (DECL_ASSEMBLER_NAME (thunk_fndecl));
+  rtx this_param = x86_this_parameter (function);
+  rtx this_reg, tmp, fnaddr;
+  unsigned int tmp_regno;
+  rtx_insn *insn;
+
+  if (TARGET_64BIT)
+    tmp_regno = R10_REG;
+  else
+    {
+      unsigned int ccvt = ix86_get_callcvt (TREE_TYPE (function));
+      if ((ccvt & IX86_CALLCVT_FASTCALL) != 0)
+	tmp_regno = AX_REG;
+      else if ((ccvt & IX86_CALLCVT_THISCALL) != 0)
+	tmp_regno = DX_REG;
+      else
+	tmp_regno = CX_REG;
+    }
+
+  emit_note (NOTE_INSN_PROLOGUE_END);
+
+  /* CET is enabled, insert EB instruction.  */
+  if ((flag_cf_protection & CF_BRANCH))
+    emit_insn (gen_nop_endbr ());
+
+  /* If VCALL_OFFSET, we'll need THIS in a register.  Might as well
+     pull it in now and let DELTA benefit.  */
+  if (REG_P (this_param))
+    this_reg = this_param;
+  else if (vcall_offset)
+    {
+      /* Put the this parameter into %eax.  */
+      this_reg = gen_rtx_REG (Pmode, AX_REG);
+      emit_move_insn (this_reg, this_param);
+    }
+  else
+    this_reg = NULL_RTX;
+
+  /* Adjust the this parameter by a fixed constant.  */
+  if (delta)
+    {
+      rtx delta_rtx = GEN_INT (delta);
+      rtx delta_dst = this_reg ? this_reg : this_param;
+
+      if (TARGET_64BIT)
+	{
+	  if (!x86_64_general_operand (delta_rtx, Pmode))
+	    {
+	      tmp = gen_rtx_REG (Pmode, tmp_regno);
+	      emit_move_insn (tmp, delta_rtx);
+	      delta_rtx = tmp;
+	    }
+	}
+
+      ix86_emit_binop (PLUS, Pmode, delta_dst, delta_rtx);
+    }
+
+  /* Adjust the this parameter by a value stored in the vtable.  */
+  if (vcall_offset)
+    {
+      rtx vcall_addr, vcall_mem, this_mem;
+
+      tmp = gen_rtx_REG (Pmode, tmp_regno);
+
+      this_mem = gen_rtx_MEM (ptr_mode, this_reg);
+      if (Pmode != ptr_mode)
+	this_mem = gen_rtx_ZERO_EXTEND (Pmode, this_mem);
+      emit_move_insn (tmp, this_mem);
+
+      /* Adjust the this parameter.  */
+      vcall_addr = plus_constant (Pmode, tmp, vcall_offset);
+      if (TARGET_64BIT
+	  && !ix86_legitimate_address_p (ptr_mode, vcall_addr, true))
+	{
+	  rtx tmp2 = gen_rtx_REG (Pmode, R11_REG);
+	  emit_move_insn (tmp2, GEN_INT (vcall_offset));
+	  vcall_addr = gen_rtx_PLUS (Pmode, tmp, tmp2);
+	}
+
+      vcall_mem = gen_rtx_MEM (ptr_mode, vcall_addr);
+      if (Pmode != ptr_mode)
+	emit_insn (gen_addsi_1_zext (this_reg,
+				     gen_rtx_REG (ptr_mode,
+						  REGNO (this_reg)),
+				     vcall_mem));
+      else
+	ix86_emit_binop (PLUS, Pmode, this_reg, vcall_mem);
+    }
+
+  /* If necessary, drop THIS back to its stack slot.  */
+  if (this_reg && this_reg != this_param)
+    emit_move_insn (this_param, this_reg);
+
+  fnaddr = XEXP (DECL_RTL (function), 0);
+  if (TARGET_64BIT)
+    {
+      if (!flag_pic || targetm.binds_local_p (function)
+	  || TARGET_PECOFF)
+	;
+      else
+	{
+	  tmp = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, fnaddr), UNSPEC_GOTPCREL);
+	  tmp = gen_rtx_CONST (Pmode, tmp);
+	  fnaddr = gen_const_mem (Pmode, tmp);
+	}
+    }
+  else
+    {
+      if (!flag_pic || targetm.binds_local_p (function))
+	;
+#if TARGET_MACHO
+      else if (TARGET_MACHO)
+	{
+	  fnaddr = machopic_indirect_call_target (DECL_RTL (function));
+	  fnaddr = XEXP (fnaddr, 0);
+	}
+#endif /* TARGET_MACHO */
+      else
+	{
+	  tmp = gen_rtx_REG (Pmode, CX_REG);
+	  output_set_got (tmp, NULL_RTX);
+
+	  fnaddr = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, fnaddr), UNSPEC_GOT);
+	  fnaddr = gen_rtx_CONST (Pmode, fnaddr);
+	  fnaddr = gen_rtx_PLUS (Pmode, tmp, fnaddr);
+	  fnaddr = gen_const_mem (Pmode, fnaddr);
+	}
+    }
+
+  /* Our sibling call patterns do not allow memories, because we have no
+     predicate that can distinguish between frame and non-frame memory.
+     For our purposes here, we can get away with (ab)using a jump pattern,
+     because we're going to do no optimization.  */
+  if (MEM_P (fnaddr))
+    {
+      if (sibcall_insn_operand (fnaddr, word_mode))
+	{
+	  fnaddr = XEXP (DECL_RTL (function), 0);
+	  tmp = gen_rtx_MEM (QImode, fnaddr);
+	  tmp = gen_rtx_CALL (VOIDmode, tmp, const0_rtx);
+	  tmp = emit_call_insn (tmp);
+	  SIBLING_CALL_P (tmp) = 1;
+	}
+      else
+	emit_jump_insn (gen_indirect_jump (fnaddr));
+    }
+  else
+    {
+      if (ix86_cmodel == CM_LARGE_PIC && SYMBOLIC_CONST (fnaddr))
+	{
+	  // CM_LARGE_PIC always uses pseudo PIC register which is
+	  // uninitialized.  Since FUNCTION is local and calling it
+	  // doesn't go through PLT, we use scratch register %r11 as
+	  // PIC register and initialize it here.
+	  pic_offset_table_rtx = gen_rtx_REG (Pmode, R11_REG);
+	  ix86_init_large_pic_reg (tmp_regno);
+	  fnaddr = legitimize_pic_address (fnaddr,
+					   gen_rtx_REG (Pmode, tmp_regno));
+	}
+
+      if (!sibcall_insn_operand (fnaddr, word_mode))
+	{
+	  tmp = gen_rtx_REG (word_mode, tmp_regno);
+	  if (GET_MODE (fnaddr) != word_mode)
+	    fnaddr = gen_rtx_ZERO_EXTEND (word_mode, fnaddr);
+	  emit_move_insn (tmp, fnaddr);
+	  fnaddr = tmp;
+	}
+
+      tmp = gen_rtx_MEM (QImode, fnaddr);
+      tmp = gen_rtx_CALL (VOIDmode, tmp, const0_rtx);
+      tmp = emit_call_insn (tmp);
+      SIBLING_CALL_P (tmp) = 1;
+    }
+  emit_barrier ();
+
+  /* Emit just enough of rest_of_compilation to get the insns emitted.  */
+  insn = get_insns ();
+  shorten_branches (insn);
+  assemble_start_function (thunk_fndecl, fnname);
+  final_start_function (insn, file, 1);
+  final (insn, file, 1);
+  final_end_function ();
+  assemble_end_function (thunk_fndecl, fnname);
+}
+
+static void
+x86_file_start (void)
+{
+  default_file_start ();
+  if (TARGET_16BIT)
+    fputs ("\t.code16gcc\n", asm_out_file);
+#if TARGET_MACHO
+  darwin_file_start ();
+#endif
+  if (X86_FILE_START_VERSION_DIRECTIVE)
+    fputs ("\t.version\t\"01.01\"\n", asm_out_file);
+  if (X86_FILE_START_FLTUSED)
+    fputs ("\t.global\t__fltused\n", asm_out_file);
+  if (ix86_asm_dialect == ASM_INTEL)
+    fputs ("\t.intel_syntax noprefix\n", asm_out_file);
+}
+
+int
+x86_field_alignment (tree type, int computed)
+{
+  machine_mode mode;
+
+  if (TARGET_64BIT || TARGET_ALIGN_DOUBLE)
+    return computed;
+  if (TARGET_IAMCU)
+    return iamcu_alignment (type, computed);
+  type = strip_array_types (type);
+  mode = TYPE_MODE (type);
+  if (mode == DFmode || mode == DCmode
+      || GET_MODE_CLASS (mode) == MODE_INT
+      || GET_MODE_CLASS (mode) == MODE_COMPLEX_INT)
+    {
+      if (TYPE_ATOMIC (type) && computed > 32)
+	{
+	  static bool warned;
+
+	  if (!warned && warn_psabi)
+	    {
+	      const char *url
+		= CHANGES_ROOT_URL "gcc-11/changes.html#ia32_atomic";
+
+	      warned = true;
+	      inform (input_location, "the alignment of %<_Atomic %T%> "
+				      "fields changed in %{GCC 11.1%}",
+		      TYPE_MAIN_VARIANT (type), url);
+	    }
+	}
+      else
+      return MIN (32, computed);
+    }
+  return computed;
+}
+
+/* Print call to TARGET to FILE.  */
+
+static void
+x86_print_call_or_nop (FILE *file, const char *target)
+{
+  if (flag_nop_mcount || !strcmp (target, "nop"))
+    /* 5 byte nop: nopl 0(%[re]ax,%[re]ax,1) */
+    fprintf (file, "1:" ASM_BYTE "0x0f, 0x1f, 0x44, 0x00, 0x00\n");
+  else
+    fprintf (file, "1:\tcall\t%s\n", target);
+}
+
+static bool
+current_fentry_name (const char **name)
+{
+  tree attr = lookup_attribute ("fentry_name",
+				DECL_ATTRIBUTES (current_function_decl));
+  if (!attr)
+    return false;
+  *name = TREE_STRING_POINTER (TREE_VALUE (TREE_VALUE (attr)));
+  return true;
+}
+
+static bool
+current_fentry_section (const char **name)
+{
+  tree attr = lookup_attribute ("fentry_section",
+				DECL_ATTRIBUTES (current_function_decl));
+  if (!attr)
+    return false;
+  *name = TREE_STRING_POINTER (TREE_VALUE (TREE_VALUE (attr)));
+  return true;
+}
+
+/* Output assembler code to FILE to increment profiler label # LABELNO
+   for profiling a function entry.  */
+void
+x86_function_profiler (FILE *file, int labelno ATTRIBUTE_UNUSED)
+{
+  if (cfun->machine->insn_queued_at_entrance)
+    {
+      if (cfun->machine->insn_queued_at_entrance == TYPE_ENDBR)
+	fprintf (file, "\t%s\n", TARGET_64BIT ? "endbr64" : "endbr32");
+      unsigned int patch_area_size
+	= crtl->patch_area_size - crtl->patch_area_entry;
+      if (patch_area_size)
+	ix86_output_patchable_area (patch_area_size,
+				    crtl->patch_area_entry == 0);
+    }
+
+  const char *mcount_name = MCOUNT_NAME;
+
+  if (current_fentry_name (&mcount_name))
+    ;
+  else if (fentry_name)
+    mcount_name = fentry_name;
+  else if (flag_fentry)
+    mcount_name = MCOUNT_NAME_BEFORE_PROLOGUE;
+
+  if (TARGET_64BIT)
+    {
+#ifndef NO_PROFILE_COUNTERS
+      fprintf (file, "\tleaq\t%sP%d(%%rip),%%r11\n", LPREFIX, labelno);
+#endif
+
+      if (!TARGET_PECOFF)
+	{
+	  switch (ix86_cmodel)
+	    {
+	    case CM_LARGE:
+	      /* NB: R10 is caller-saved.  Although it can be used as a
+		 static chain register, it is preserved when calling
+		 mcount for nested functions.  */
+	      fprintf (file, "1:\tmovabsq\t$%s, %%r10\n\tcall\t*%%r10\n",
+		       mcount_name);
+	      break;
+	    case CM_LARGE_PIC:
+#ifdef NO_PROFILE_COUNTERS
+	      fprintf (file, "1:\tmovabsq\t$_GLOBAL_OFFSET_TABLE_-1b, %%r11\n");
+	      fprintf (file, "\tleaq\t1b(%%rip), %%r10\n");
+	      fprintf (file, "\taddq\t%%r11, %%r10\n");
+	      fprintf (file, "\tmovabsq\t$%s@PLTOFF, %%r11\n", mcount_name);
+	      fprintf (file, "\taddq\t%%r11, %%r10\n");
+	      fprintf (file, "\tcall\t*%%r10\n");
+#else
+	      sorry ("profiling %<-mcmodel=large%> with PIC is not supported");
+#endif
+	      break;
+	    case CM_SMALL_PIC:
+	    case CM_MEDIUM_PIC:
+	      fprintf (file, "1:\tcall\t*%s@GOTPCREL(%%rip)\n", mcount_name);
+	      break;
+	    default:
+	      x86_print_call_or_nop (file, mcount_name);
+	      break;
+	    }
+	}
+      else
+	x86_print_call_or_nop (file, mcount_name);
+    }
+  else if (flag_pic)
+    {
+#ifndef NO_PROFILE_COUNTERS
+      fprintf (file, "\tleal\t%sP%d@GOTOFF(%%ebx),%%" PROFILE_COUNT_REGISTER "\n",
+	       LPREFIX, labelno);
+#endif
+      fprintf (file, "1:\tcall\t*%s@GOT(%%ebx)\n", mcount_name);
+    }
+  else
+    {
+#ifndef NO_PROFILE_COUNTERS
+      fprintf (file, "\tmovl\t$%sP%d,%%" PROFILE_COUNT_REGISTER "\n",
+	       LPREFIX, labelno);
+#endif
+      x86_print_call_or_nop (file, mcount_name);
+    }
+
+  if (flag_record_mcount
+	|| lookup_attribute ("fentry_section",
+                                DECL_ATTRIBUTES (current_function_decl)))
+    {
+      const char *sname = "__mcount_loc";
+
+      if (current_fentry_section (&sname))
+	;
+      else if (fentry_section)
+	sname = fentry_section;
+
+      fprintf (file, "\t.section %s, \"a\",@progbits\n", sname);
+      fprintf (file, "\t.%s 1b\n", TARGET_64BIT ? "quad" : "long");
+      fprintf (file, "\t.previous\n");
+    }
+}
+
+/* We don't have exact information about the insn sizes, but we may assume
+   quite safely that we are informed about all 1 byte insns and memory
+   address sizes.  This is enough to eliminate unnecessary padding in
+   99% of cases.  */
+
+int
+ix86_min_insn_size (rtx_insn *insn)
+{
+  int l = 0, len;
+
+  if (!INSN_P (insn) || !active_insn_p (insn))
+    return 0;
+
+  /* Discard alignments we've emit and jump instructions.  */
+  if (GET_CODE (PATTERN (insn)) == UNSPEC_VOLATILE
+      && XINT (PATTERN (insn), 1) == UNSPECV_ALIGN)
+    return 0;
+
+  /* Important case - calls are always 5 bytes.
+     It is common to have many calls in the row.  */
+  if (CALL_P (insn)
+      && symbolic_reference_mentioned_p (PATTERN (insn))
+      && !SIBLING_CALL_P (insn))
+    return 5;
+  len = get_attr_length (insn);
+  if (len <= 1)
+    return 1;
+
+  /* For normal instructions we rely on get_attr_length being exact,
+     with a few exceptions.  */
+  if (!JUMP_P (insn))
+    {
+      enum attr_type type = get_attr_type (insn);
+
+      switch (type)
+	{
+	case TYPE_MULTI:
+	  if (GET_CODE (PATTERN (insn)) == ASM_INPUT
+	      || asm_noperands (PATTERN (insn)) >= 0)
+	    return 0;
+	  break;
+	case TYPE_OTHER:
+	case TYPE_FCMP:
+	  break;
+	default:
+	  /* Otherwise trust get_attr_length.  */
+	  return len;
+	}
+
+      l = get_attr_length_address (insn);
+      if (l < 4 && symbolic_reference_mentioned_p (PATTERN (insn)))
+	l = 4;
+    }
+  if (l)
+    return 1+l;
+  else
+    return 2;
+}
+
+#ifdef ASM_OUTPUT_MAX_SKIP_PAD
+
+/* AMD K8 core mispredicts jumps when there are more than 3 jumps in 16 byte
+   window.  */
+
+static void
+ix86_avoid_jump_mispredicts (void)
+{
+  rtx_insn *insn, *start = get_insns ();
+  int nbytes = 0, njumps = 0;
+  bool isjump = false;
+
+  /* Look for all minimal intervals of instructions containing 4 jumps.
+     The intervals are bounded by START and INSN.  NBYTES is the total
+     size of instructions in the interval including INSN and not including
+     START.  When the NBYTES is smaller than 16 bytes, it is possible
+     that the end of START and INSN ends up in the same 16byte page.
+
+     The smallest offset in the page INSN can start is the case where START
+     ends on the offset 0.  Offset of INSN is then NBYTES - sizeof (INSN).
+     We add p2align to 16byte window with maxskip 15 - NBYTES + sizeof (INSN).
+
+     Don't consider asm goto as jump, while it can contain a jump, it doesn't
+     have to, control transfer to label(s) can be performed through other
+     means, and also we estimate minimum length of all asm stmts as 0.  */
+  for (insn = start; insn; insn = NEXT_INSN (insn))
+    {
+      int min_size;
+
+      if (LABEL_P (insn))
+	{
+	  align_flags alignment = label_to_alignment (insn);
+	  int align = alignment.levels[0].log;
+	  int max_skip = alignment.levels[0].maxskip;
+
+	  if (max_skip > 15)
+	    max_skip = 15;
+	  /* If align > 3, only up to 16 - max_skip - 1 bytes can be
+	     already in the current 16 byte page, because otherwise
+	     ASM_OUTPUT_MAX_SKIP_ALIGN could skip max_skip or fewer
+	     bytes to reach 16 byte boundary.  */
+	  if (align <= 0
+	      || (align <= 3 && max_skip != (1 << align) - 1))
+	    max_skip = 0;
+	  if (dump_file)
+	    fprintf (dump_file, "Label %i with max_skip %i\n",
+		     INSN_UID (insn), max_skip);
+	  if (max_skip)
+	    {
+	      while (nbytes + max_skip >= 16)
+		{
+		  start = NEXT_INSN (start);
+		  if ((JUMP_P (start) && asm_noperands (PATTERN (start)) < 0)
+		      || CALL_P (start))
+		    njumps--, isjump = true;
+		  else
+		    isjump = false;
+		  nbytes -= ix86_min_insn_size (start);
+		}
+	    }
+	  continue;
+	}
+
+      min_size = ix86_min_insn_size (insn);
+      nbytes += min_size;
+      if (dump_file)
+	fprintf (dump_file, "Insn %i estimated to %i bytes\n",
+		 INSN_UID (insn), min_size);
+      if ((JUMP_P (insn) && asm_noperands (PATTERN (insn)) < 0)
+	  || CALL_P (insn))
+	njumps++;
+      else
+	continue;
+
+      while (njumps > 3)
+	{
+	  start = NEXT_INSN (start);
+	  if ((JUMP_P (start) && asm_noperands (PATTERN (start)) < 0)
+	      || CALL_P (start))
+	    njumps--, isjump = true;
+	  else
+	    isjump = false;
+	  nbytes -= ix86_min_insn_size (start);
+	}
+      gcc_assert (njumps >= 0);
+      if (dump_file)
+        fprintf (dump_file, "Interval %i to %i has %i bytes\n",
+		 INSN_UID (start), INSN_UID (insn), nbytes);
+
+      if (njumps == 3 && isjump && nbytes < 16)
+	{
+	  int padsize = 15 - nbytes + ix86_min_insn_size (insn);
+
+	  if (dump_file)
+	    fprintf (dump_file, "Padding insn %i by %i bytes!\n",
+		     INSN_UID (insn), padsize);
+          emit_insn_before (gen_pad (GEN_INT (padsize)), insn);
+	}
+    }
+}
+#endif
+
+/* AMD Athlon works faster
+   when RET is not destination of conditional jump or directly preceded
+   by other jump instruction.  We avoid the penalty by inserting NOP just
+   before the RET instructions in such cases.  */
+static void
+ix86_pad_returns (void)
+{
+  edge e;
+  edge_iterator ei;
+
+  FOR_EACH_EDGE (e, ei, EXIT_BLOCK_PTR_FOR_FN (cfun)->preds)
+    {
+      basic_block bb = e->src;
+      rtx_insn *ret = BB_END (bb);
+      rtx_insn *prev;
+      bool replace = false;
+
+      if (!JUMP_P (ret) || !ANY_RETURN_P (PATTERN (ret))
+	  || optimize_bb_for_size_p (bb))
+	continue;
+      for (prev = PREV_INSN (ret); prev; prev = PREV_INSN (prev))
+	if (active_insn_p (prev) || LABEL_P (prev))
+	  break;
+      if (prev && LABEL_P (prev))
+	{
+	  edge e;
+	  edge_iterator ei;
+
+	  FOR_EACH_EDGE (e, ei, bb->preds)
+	    if (EDGE_FREQUENCY (e) && e->src->index >= 0
+		&& !(e->flags & EDGE_FALLTHRU))
+	      {
+		replace = true;
+		break;
+	      }
+	}
+      if (!replace)
+	{
+	  prev = prev_active_insn (ret);
+	  if (prev
+	      && ((JUMP_P (prev) && any_condjump_p (prev))
+		  || CALL_P (prev)))
+	    replace = true;
+	  /* Empty functions get branch mispredict even when
+	     the jump destination is not visible to us.  */
+	  if (!prev && !optimize_function_for_size_p (cfun))
+	    replace = true;
+	}
+      if (replace)
+	{
+	  emit_jump_insn_before (gen_simple_return_internal_long (), ret);
+	  delete_insn (ret);
+	}
+    }
+}
+
+/* Count the minimum number of instructions in BB.  Return 4 if the
+   number of instructions >= 4.  */
+
+static int
+ix86_count_insn_bb (basic_block bb)
+{
+  rtx_insn *insn;
+  int insn_count = 0;
+
+  /* Count number of instructions in this block.  Return 4 if the number
+     of instructions >= 4.  */
+  FOR_BB_INSNS (bb, insn)
+    {
+      /* Only happen in exit blocks.  */
+      if (JUMP_P (insn)
+	  && ANY_RETURN_P (PATTERN (insn)))
+	break;
+
+      if (NONDEBUG_INSN_P (insn)
+	  && GET_CODE (PATTERN (insn)) != USE
+	  && GET_CODE (PATTERN (insn)) != CLOBBER)
+	{
+	  insn_count++;
+	  if (insn_count >= 4)
+	    return insn_count;
+	}
+    }
+
+  return insn_count;
+}
+
+
+/* Count the minimum number of instructions in code path in BB.
+   Return 4 if the number of instructions >= 4.  */
+
+static int
+ix86_count_insn (basic_block bb)
+{
+  edge e;
+  edge_iterator ei;
+  int min_prev_count;
+
+  /* Only bother counting instructions along paths with no
+     more than 2 basic blocks between entry and exit.  Given
+     that BB has an edge to exit, determine if a predecessor
+     of BB has an edge from entry.  If so, compute the number
+     of instructions in the predecessor block.  If there
+     happen to be multiple such blocks, compute the minimum.  */
+  min_prev_count = 4;
+  FOR_EACH_EDGE (e, ei, bb->preds)
+    {
+      edge prev_e;
+      edge_iterator prev_ei;
+
+      if (e->src == ENTRY_BLOCK_PTR_FOR_FN (cfun))
+	{
+	  min_prev_count = 0;
+	  break;
+	}
+      FOR_EACH_EDGE (prev_e, prev_ei, e->src->preds)
+	{
+	  if (prev_e->src == ENTRY_BLOCK_PTR_FOR_FN (cfun))
+	    {
+	      int count = ix86_count_insn_bb (e->src);
+	      if (count < min_prev_count)
+		min_prev_count = count;
+	      break;
+	    }
+	}
+    }
+
+  if (min_prev_count < 4)
+    min_prev_count += ix86_count_insn_bb (bb);
+
+  return min_prev_count;
+}
+
+/* Pad short function to 4 instructions.   */
+
+static void
+ix86_pad_short_function (void)
+{
+  edge e;
+  edge_iterator ei;
+
+  FOR_EACH_EDGE (e, ei, EXIT_BLOCK_PTR_FOR_FN (cfun)->preds)
+    {
+      rtx_insn *ret = BB_END (e->src);
+      if (JUMP_P (ret) && ANY_RETURN_P (PATTERN (ret)))
+	{
+	  int insn_count = ix86_count_insn (e->src);
+
+	  /* Pad short function.  */
+	  if (insn_count < 4)
+	    {
+	      rtx_insn *insn = ret;
+
+	      /* Find epilogue.  */
+	      while (insn
+		     && (!NOTE_P (insn)
+			 || NOTE_KIND (insn) != NOTE_INSN_EPILOGUE_BEG))
+		insn = PREV_INSN (insn);
+
+	      if (!insn)
+		insn = ret;
+
+	      /* Two NOPs count as one instruction.  */
+	      insn_count = 2 * (4 - insn_count);
+	      emit_insn_before (gen_nops (GEN_INT (insn_count)), insn);
+	    }
+	}
+    }
+}
+
+/* Fix up a Windows system unwinder issue.  If an EH region falls through into
+   the epilogue, the Windows system unwinder will apply epilogue logic and
+   produce incorrect offsets.  This can be avoided by adding a nop between
+   the last insn that can throw and the first insn of the epilogue.  */
+
+static void
+ix86_seh_fixup_eh_fallthru (void)
+{
+  edge e;
+  edge_iterator ei;
+
+  FOR_EACH_EDGE (e, ei, EXIT_BLOCK_PTR_FOR_FN (cfun)->preds)
+    {
+      rtx_insn *insn, *next;
+
+      /* Find the beginning of the epilogue.  */
+      for (insn = BB_END (e->src); insn != NULL; insn = PREV_INSN (insn))
+	if (NOTE_P (insn) && NOTE_KIND (insn) == NOTE_INSN_EPILOGUE_BEG)
+	  break;
+      if (insn == NULL)
+	continue;
+
+      /* We only care about preceding insns that can throw.  */
+      insn = prev_active_insn (insn);
+      if (insn == NULL || !can_throw_internal (insn))
+	continue;
+
+      /* Do not separate calls from their debug information.  */
+      for (next = NEXT_INSN (insn); next != NULL; next = NEXT_INSN (next))
+	if (NOTE_P (next) && NOTE_KIND (next) == NOTE_INSN_VAR_LOCATION)
+	  insn = next;
+	else
+	  break;
+
+      emit_insn_after (gen_nops (const1_rtx), insn);
+    }
+}
+
+/* Implement machine specific optimizations.  We implement padding of returns
+   for K8 CPUs and pass to avoid 4 jumps in the single 16 byte window.  */
+static void
+ix86_reorg (void)
+{
+  /* We are freeing block_for_insn in the toplev to keep compatibility
+     with old MDEP_REORGS that are not CFG based.  Recompute it now.  */
+  compute_bb_for_insn ();
+
+  if (TARGET_SEH && current_function_has_exception_handlers ())
+    ix86_seh_fixup_eh_fallthru ();
+
+  if (optimize && optimize_function_for_speed_p (cfun))
+    {
+      if (TARGET_PAD_SHORT_FUNCTION)
+	ix86_pad_short_function ();
+      else if (TARGET_PAD_RETURNS)
+	ix86_pad_returns ();
+#ifdef ASM_OUTPUT_MAX_SKIP_PAD
+      if (TARGET_FOUR_JUMP_LIMIT)
+	ix86_avoid_jump_mispredicts ();
+#endif
+    }
+}
+
+/* Return nonzero when QImode register that must be represented via REX prefix
+   is used.  */
+bool
+x86_extended_QIreg_mentioned_p (rtx_insn *insn)
+{
+  int i;
+  extract_insn_cached (insn);
+  for (i = 0; i < recog_data.n_operands; i++)
+    if (GENERAL_REG_P (recog_data.operand[i])
+	&& !QI_REGNO_P (REGNO (recog_data.operand[i])))
+       return true;
+  return false;
+}
+
+/* Return true when INSN mentions register that must be encoded using REX
+   prefix.  */
+bool
+x86_extended_reg_mentioned_p (rtx insn)
+{
+  subrtx_iterator::array_type array;
+  FOR_EACH_SUBRTX (iter, array, INSN_P (insn) ? PATTERN (insn) : insn, NONCONST)
+    {
+      const_rtx x = *iter;
+      if (REG_P (x)
+	  && (REX_INT_REGNO_P (REGNO (x)) || REX_SSE_REGNO_P (REGNO (x))))
+	return true;
+    }
+  return false;
+}
+
+/* If profitable, negate (without causing overflow) integer constant
+   of mode MODE at location LOC.  Return true in this case.  */
+bool
+x86_maybe_negate_const_int (rtx *loc, machine_mode mode)
+{
+  HOST_WIDE_INT val;
+
+  if (!CONST_INT_P (*loc))
+    return false;
+
+  switch (mode)
+    {
+    case E_DImode:
+      /* DImode x86_64 constants must fit in 32 bits.  */
+      gcc_assert (x86_64_immediate_operand (*loc, mode));
+
+      mode = SImode;
+      break;
+
+    case E_SImode:
+    case E_HImode:
+    case E_QImode:
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  /* Avoid overflows.  */
+  if (mode_signbit_p (mode, *loc))
+    return false;
+
+  val = INTVAL (*loc);
+
+  /* Make things pretty and `subl $4,%eax' rather than `addl $-4,%eax'.
+     Exceptions: -128 encodes smaller than 128, so swap sign and op.  */
+  if ((val < 0 && val != -128)
+      || val == 128)
+    {
+      *loc = GEN_INT (-val);
+      return true;
+    }
+
+  return false;
+}
+
+/* Generate an unsigned DImode/SImode to FP conversion.  This is the same code
+   optabs would emit if we didn't have TFmode patterns.  */
+
+void
+x86_emit_floatuns (rtx operands[2])
+{
+  rtx_code_label *neglab, *donelab;
+  rtx i0, i1, f0, in, out;
+  machine_mode mode, inmode;
+
+  inmode = GET_MODE (operands[1]);
+  gcc_assert (inmode == SImode || inmode == DImode);
+
+  out = operands[0];
+  in = force_reg (inmode, operands[1]);
+  mode = GET_MODE (out);
+  neglab = gen_label_rtx ();
+  donelab = gen_label_rtx ();
+  f0 = gen_reg_rtx (mode);
+
+  emit_cmp_and_jump_insns (in, const0_rtx, LT, const0_rtx, inmode, 0, neglab);
+
+  expand_float (out, in, 0);
+
+  emit_jump_insn (gen_jump (donelab));
+  emit_barrier ();
+
+  emit_label (neglab);
+
+  i0 = expand_simple_binop (inmode, LSHIFTRT, in, const1_rtx, NULL,
+			    1, OPTAB_DIRECT);
+  i1 = expand_simple_binop (inmode, AND, in, const1_rtx, NULL,
+			    1, OPTAB_DIRECT);
+  i0 = expand_simple_binop (inmode, IOR, i0, i1, i0, 1, OPTAB_DIRECT);
+
+  expand_float (f0, i0, 0);
+
+  emit_insn (gen_rtx_SET (out, gen_rtx_PLUS (mode, f0, f0)));
+
+  emit_label (donelab);
+}
+
+/* Target hook for scalar_mode_supported_p.  */
+static bool
+ix86_scalar_mode_supported_p (scalar_mode mode)
+{
+  if (DECIMAL_FLOAT_MODE_P (mode))
+    return default_decimal_float_supported_p ();
+  else if (mode == TFmode)
+    return true;
+  else
+    return default_scalar_mode_supported_p (mode);
+}
+
+/* Implements target hook vector_mode_supported_p.  */
+static bool
+ix86_vector_mode_supported_p (machine_mode mode)
+{
+  if (TARGET_SSE && VALID_SSE_REG_MODE (mode))
+    return true;
+  if (TARGET_SSE2 && VALID_SSE2_REG_MODE (mode))
+    return true;
+  if (TARGET_AVX && VALID_AVX256_REG_MODE (mode))
+    return true;
+  if (TARGET_AVX512F && VALID_AVX512F_REG_MODE (mode))
+    return true;
+  if ((TARGET_MMX || TARGET_MMX_WITH_SSE)
+      && VALID_MMX_REG_MODE (mode))
+    return true;
+  if ((TARGET_3DNOW || TARGET_MMX_WITH_SSE)
+      && VALID_MMX_REG_MODE_3DNOW (mode))
+    return true;
+  return false;
+}
+
+/* Target hook for c_mode_for_suffix.  */
+static machine_mode
+ix86_c_mode_for_suffix (char suffix)
+{
+  if (suffix == 'q')
+    return TFmode;
+  if (suffix == 'w')
+    return XFmode;
+
+  return VOIDmode;
+}
+
+/* Worker function for TARGET_MD_ASM_ADJUST.
+
+   We implement asm flag outputs, and maintain source compatibility
+   with the old cc0-based compiler.  */
+
+static rtx_insn *
+ix86_md_asm_adjust (vec<rtx> &outputs, vec<rtx> & /*inputs*/,
+		    vec<machine_mode> & /*input_modes*/,
+		    vec<const char *> &constraints, vec<rtx> &clobbers,
+		    HARD_REG_SET &clobbered_regs)
+{
+  bool saw_asm_flag = false;
+
+  start_sequence ();
+  for (unsigned i = 0, n = outputs.length (); i < n; ++i)
+    {
+      const char *con = constraints[i];
+      if (strncmp (con, "=@cc", 4) != 0)
+	continue;
+      con += 4;
+      if (strchr (con, ',') != NULL)
+	{
+	  error ("alternatives not allowed in %<asm%> flag output");
+	  continue;
+	}
+
+      bool invert = false;
+      if (con[0] == 'n')
+	invert = true, con++;
+
+      machine_mode mode = CCmode;
+      rtx_code code = UNKNOWN;
+
+      switch (con[0])
+	{
+	case 'a':
+	  if (con[1] == 0)
+	    mode = CCAmode, code = EQ;
+	  else if (con[1] == 'e' && con[2] == 0)
+	    mode = CCCmode, code = NE;
+	  break;
+	case 'b':
+	  if (con[1] == 0)
+	    mode = CCCmode, code = EQ;
+	  else if (con[1] == 'e' && con[2] == 0)
+	    mode = CCAmode, code = NE;
+	  break;
+	case 'c':
+	  if (con[1] == 0)
+	    mode = CCCmode, code = EQ;
+	  break;
+	case 'e':
+	  if (con[1] == 0)
+	    mode = CCZmode, code = EQ;
+	  break;
+	case 'g':
+	  if (con[1] == 0)
+	    mode = CCGCmode, code = GT;
+	  else if (con[1] == 'e' && con[2] == 0)
+	    mode = CCGCmode, code = GE;
+	  break;
+	case 'l':
+	  if (con[1] == 0)
+	    mode = CCGCmode, code = LT;
+	  else if (con[1] == 'e' && con[2] == 0)
+	    mode = CCGCmode, code = LE;
+	  break;
+	case 'o':
+	  if (con[1] == 0)
+	    mode = CCOmode, code = EQ;
+	  break;
+	case 'p':
+	  if (con[1] == 0)
+	    mode = CCPmode, code = EQ;
+	  break;
+	case 's':
+	  if (con[1] == 0)
+	    mode = CCSmode, code = EQ;
+	  break;
+	case 'z':
+	  if (con[1] == 0)
+	    mode = CCZmode, code = EQ;
+	  break;
+	}
+      if (code == UNKNOWN)
+	{
+	  error ("unknown %<asm%> flag output %qs", constraints[i]);
+	  continue;
+	}
+      if (invert)
+	code = reverse_condition (code);
+
+      rtx dest = outputs[i];
+      if (!saw_asm_flag)
+	{
+	  /* This is the first asm flag output.  Here we put the flags
+	     register in as the real output and adjust the condition to
+	     allow it.  */
+	  constraints[i] = "=Bf";
+	  outputs[i] = gen_rtx_REG (CCmode, FLAGS_REG);
+	  saw_asm_flag = true;
+	}
+      else
+	{
+	  /* We don't need the flags register as output twice.  */
+	  constraints[i] = "=X";
+	  outputs[i] = gen_rtx_SCRATCH (SImode);
+	}
+
+      rtx x = gen_rtx_REG (mode, FLAGS_REG);
+      x = gen_rtx_fmt_ee (code, QImode, x, const0_rtx);
+
+      machine_mode dest_mode = GET_MODE (dest);
+      if (!SCALAR_INT_MODE_P (dest_mode))
+	{
+	  error ("invalid type for %<asm%> flag output");
+	  continue;
+	}
+
+      if (dest_mode == QImode)
+	emit_insn (gen_rtx_SET (dest, x));
+      else
+	{
+	  rtx reg = gen_reg_rtx (QImode);
+	  emit_insn (gen_rtx_SET (reg, x));
+
+	  reg = convert_to_mode (dest_mode, reg, 1);
+	  emit_move_insn (dest, reg);
+	}
+    }
+
+  rtx_insn *seq = get_insns ();
+  end_sequence ();
+
+  if (saw_asm_flag)
+    return seq;
+  else
+    {
+      /* If we had no asm flag outputs, clobber the flags.  */
+      clobbers.safe_push (gen_rtx_REG (CCmode, FLAGS_REG));
+      SET_HARD_REG_BIT (clobbered_regs, FLAGS_REG);
+      return NULL;
+    }
+}
+
+/* Implements target vector targetm.asm.encode_section_info.  */
+
+static void ATTRIBUTE_UNUSED
+ix86_encode_section_info (tree decl, rtx rtl, int first)
+{
+  default_encode_section_info (decl, rtl, first);
+
+  if (ix86_in_large_data_p (decl))
+    SYMBOL_REF_FLAGS (XEXP (rtl, 0)) |= SYMBOL_FLAG_FAR_ADDR;
+}
+
+/* Worker function for REVERSE_CONDITION.  */
+
+enum rtx_code
+ix86_reverse_condition (enum rtx_code code, machine_mode mode)
+{
+  return (mode == CCFPmode
+	  ? reverse_condition_maybe_unordered (code)
+	  : reverse_condition (code));
+}
+
+/* Output code to perform an x87 FP register move, from OPERANDS[1]
+   to OPERANDS[0].  */
+
+const char *
+output_387_reg_move (rtx_insn *insn, rtx *operands)
+{
+  if (REG_P (operands[0]))
+    {
+      if (REG_P (operands[1])
+	  && find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
+	{
+	  if (REGNO (operands[0]) == FIRST_STACK_REG)
+	    return output_387_ffreep (operands, 0);
+	  return "fstp\t%y0";
+	}
+      if (STACK_TOP_P (operands[0]))
+	return "fld%Z1\t%y1";
+      return "fst\t%y0";
+    }
+  else if (MEM_P (operands[0]))
+    {
+      gcc_assert (REG_P (operands[1]));
+      if (find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
+	return "fstp%Z0\t%y0";
+      else
+	{
+	  /* There is no non-popping store to memory for XFmode.
+	     So if we need one, follow the store with a load.  */
+	  if (GET_MODE (operands[0]) == XFmode)
+	    return "fstp%Z0\t%y0\n\tfld%Z0\t%y0";
+	  else
+	    return "fst%Z0\t%y0";
+	}
+    }
+  else
+    gcc_unreachable();
+}
+#ifdef TARGET_SOLARIS
+/* Solaris implementation of TARGET_ASM_NAMED_SECTION.  */
+
+static void
+i386_solaris_elf_named_section (const char *name, unsigned int flags,
+				tree decl)
+{
+  /* With Binutils 2.15, the "@unwind" marker must be specified on
+     every occurrence of the ".eh_frame" section, not just the first
+     one.  */
+  if (TARGET_64BIT
+      && strcmp (name, ".eh_frame") == 0)
+    {
+      fprintf (asm_out_file, "\t.section\t%s,\"%s\",@unwind\n", name,
+	       flags & SECTION_WRITE ? "aw" : "a");
+      return;
+    }
+
+#ifndef USE_GAS
+  if (HAVE_COMDAT_GROUP && flags & SECTION_LINKONCE)
+    {
+      solaris_elf_asm_comdat_section (name, flags, decl);
+      return;
+    }
+
+  /* Solaris/x86 as uses the same syntax for the SHF_EXCLUDE flags as the
+     SPARC assembler.  One cannot mix single-letter flags and #exclude, so
+     only emit the latter here.  */
+  if (flags & SECTION_EXCLUDE)
+    {
+      fprintf (asm_out_file, "\t.section\t%s,#exclude\n", name);
+      return;
+    }
+#endif
+
+  default_elf_asm_named_section (name, flags, decl);
+}
+#endif /* TARGET_SOLARIS */
+
+/* Return the mangling of TYPE if it is an extended fundamental type.  */
+
+static const char *
+ix86_mangle_type (const_tree type)
+{
+  type = TYPE_MAIN_VARIANT (type);
+
+  if (TREE_CODE (type) != VOID_TYPE && TREE_CODE (type) != BOOLEAN_TYPE
+      && TREE_CODE (type) != INTEGER_TYPE && TREE_CODE (type) != REAL_TYPE)
+    return NULL;
+
+  switch (TYPE_MODE (type))
+    {
+    case E_TFmode:
+      /* __float128 is "g".  */
+      return "g";
+    case E_XFmode:
+      /* "long double" or __float80 is "e".  */
+      return "e";
+    default:
+      return NULL;
+    }
+}
+
+static GTY(()) tree ix86_tls_stack_chk_guard_decl;
+
+static tree
+ix86_stack_protect_guard (void)
+{
+  if (TARGET_SSP_TLS_GUARD)
+    {
+      tree type_node = lang_hooks.types.type_for_mode (ptr_mode, 1);
+      int qual = ENCODE_QUAL_ADDR_SPACE (ix86_stack_protector_guard_reg);
+      tree type = build_qualified_type (type_node, qual);
+      tree t;
+
+      if (global_options_set.x_ix86_stack_protector_guard_symbol_str)
+	{
+	  t = ix86_tls_stack_chk_guard_decl;
+
+	  if (t == NULL)
+	    {
+	      rtx x;
+
+	      t = build_decl
+		(UNKNOWN_LOCATION, VAR_DECL,
+		 get_identifier (ix86_stack_protector_guard_symbol_str),
+		 type);
+	      TREE_STATIC (t) = 1;
+	      TREE_PUBLIC (t) = 1;
+	      DECL_EXTERNAL (t) = 1;
+	      TREE_USED (t) = 1;
+	      TREE_THIS_VOLATILE (t) = 1;
+	      DECL_ARTIFICIAL (t) = 1;
+	      DECL_IGNORED_P (t) = 1;
+
+	      /* Do not share RTL as the declaration is visible outside of
+		 current function.  */
+	      x = DECL_RTL (t);
+	      RTX_FLAG (x, used) = 1;
+
+	      ix86_tls_stack_chk_guard_decl = t;
+	    }
+	}
+      else
+	{
+	  tree asptrtype = build_pointer_type (type);
+
+	  t = build_int_cst (asptrtype, ix86_stack_protector_guard_offset);
+	  t = build2 (MEM_REF, asptrtype, t,
+		      build_int_cst (asptrtype, 0));
+	  TREE_THIS_VOLATILE (t) = 1;
+	}
+
+      return t;
+    }
+
+  return default_stack_protect_guard ();
+}
+
+/* For 32-bit code we can save PIC register setup by using
+   __stack_chk_fail_local hidden function instead of calling
+   __stack_chk_fail directly.  64-bit code doesn't need to setup any PIC
+   register, so it is better to call __stack_chk_fail directly.  */
+
+static tree ATTRIBUTE_UNUSED
+ix86_stack_protect_fail (void)
+{
+  return TARGET_64BIT
+	 ? default_external_stack_protect_fail ()
+	 : default_hidden_stack_protect_fail ();
+}
+
+/* Select a format to encode pointers in exception handling data.  CODE
+   is 0 for data, 1 for code labels, 2 for function pointers.  GLOBAL is
+   true if the symbol may be affected by dynamic relocations.
+
+   ??? All x86 object file formats are capable of representing this.
+   After all, the relocation needed is the same as for the call insn.
+   Whether or not a particular assembler allows us to enter such, I
+   guess we'll have to see.  */
+
+int
+asm_preferred_eh_data_format (int code, int global)
+{
+  /* PE-COFF is effectively always -fPIC because of the .reloc section.  */
+  if (flag_pic || TARGET_PECOFF)
+    {
+      int type = DW_EH_PE_sdata8;
+      if (!TARGET_64BIT
+	  || ix86_cmodel == CM_SMALL_PIC
+	  || (ix86_cmodel == CM_MEDIUM_PIC && (global || code)))
+	type = DW_EH_PE_sdata4;
+      return (global ? DW_EH_PE_indirect : 0) | DW_EH_PE_pcrel | type;
+    }
+
+  if (ix86_cmodel == CM_SMALL
+      || (ix86_cmodel == CM_MEDIUM && code))
+    return DW_EH_PE_udata4;
+
+  return DW_EH_PE_absptr;
+}
+
+/* Implement targetm.vectorize.builtin_vectorization_cost.  */
+static int
+ix86_builtin_vectorization_cost (enum vect_cost_for_stmt type_of_cost,
+                                 tree vectype, int)
+{
+  bool fp = false;
+  machine_mode mode = TImode;
+  int index;
+  if (vectype != NULL)
+    {
+      fp = FLOAT_TYPE_P (vectype);
+      mode = TYPE_MODE (vectype);
+    }
+
+  switch (type_of_cost)
+    {
+      case scalar_stmt:
+        return fp ? ix86_cost->addss : COSTS_N_INSNS (1);
+
+      case scalar_load:
+	/* load/store costs are relative to register move which is 2. Recompute
+ 	   it to COSTS_N_INSNS so everything have same base.  */
+        return COSTS_N_INSNS (fp ? ix86_cost->sse_load[0]
+			      : ix86_cost->int_load [2]) / 2;
+
+      case scalar_store:
+        return COSTS_N_INSNS (fp ? ix86_cost->sse_store[0]
+			      : ix86_cost->int_store [2]) / 2;
+
+      case vector_stmt:
+        return ix86_vec_cost (mode,
+			      fp ? ix86_cost->addss : ix86_cost->sse_op);
+
+      case vector_load:
+	index = sse_store_index (mode);
+	/* See PR82713 - we may end up being called on non-vector type.  */
+	if (index < 0)
+	  index = 2;
+        return COSTS_N_INSNS (ix86_cost->sse_load[index]) / 2;
+
+      case vector_store:
+	index = sse_store_index (mode);
+	/* See PR82713 - we may end up being called on non-vector type.  */
+	if (index < 0)
+	  index = 2;
+        return COSTS_N_INSNS (ix86_cost->sse_store[index]) / 2;
+
+      case vec_to_scalar:
+      case scalar_to_vec:
+        return ix86_vec_cost (mode, ix86_cost->sse_op);
+
+      /* We should have separate costs for unaligned loads and gather/scatter.
+	 Do that incrementally.  */
+      case unaligned_load:
+	index = sse_store_index (mode);
+	/* See PR82713 - we may end up being called on non-vector type.  */
+	if (index < 0)
+	  index = 2;
+        return COSTS_N_INSNS (ix86_cost->sse_unaligned_load[index]) / 2;
+
+      case unaligned_store:
+	index = sse_store_index (mode);
+	/* See PR82713 - we may end up being called on non-vector type.  */
+	if (index < 0)
+	  index = 2;
+        return COSTS_N_INSNS (ix86_cost->sse_unaligned_store[index]) / 2;
+
+      case vector_gather_load:
+        return ix86_vec_cost (mode,
+			      COSTS_N_INSNS
+				 (ix86_cost->gather_static
+				  + ix86_cost->gather_per_elt
+				    * TYPE_VECTOR_SUBPARTS (vectype)) / 2);
+
+      case vector_scatter_store:
+        return ix86_vec_cost (mode,
+			      COSTS_N_INSNS
+				 (ix86_cost->scatter_static
+				  + ix86_cost->scatter_per_elt
+				    * TYPE_VECTOR_SUBPARTS (vectype)) / 2);
+
+      case cond_branch_taken:
+        return ix86_cost->cond_taken_branch_cost;
+
+      case cond_branch_not_taken:
+        return ix86_cost->cond_not_taken_branch_cost;
+
+      case vec_perm:
+      case vec_promote_demote:
+        return ix86_vec_cost (mode, ix86_cost->sse_op);
+
+      case vec_construct:
+	{
+	  /* N element inserts into SSE vectors.  */
+	  int cost = TYPE_VECTOR_SUBPARTS (vectype) * ix86_cost->sse_op;
+	  /* One vinserti128 for combining two SSE vectors for AVX256.  */
+	  if (GET_MODE_BITSIZE (mode) == 256)
+	    cost += ix86_vec_cost (mode, ix86_cost->addss);
+	  /* One vinserti64x4 and two vinserti128 for combining SSE
+	     and AVX256 vectors to AVX512.  */
+	  else if (GET_MODE_BITSIZE (mode) == 512)
+	    cost += 3 * ix86_vec_cost (mode, ix86_cost->addss);
+	  return cost;
+	}
+
+      default:
+        gcc_unreachable ();
+    }
+}
+
+
+/* This function returns the calling abi specific va_list type node.
+   It returns  the FNDECL specific va_list type.  */
+
+static tree
+ix86_fn_abi_va_list (tree fndecl)
+{
+  if (!TARGET_64BIT)
+    return va_list_type_node;
+  gcc_assert (fndecl != NULL_TREE);
+
+  if (ix86_function_abi ((const_tree) fndecl) == MS_ABI)
+    return ms_va_list_type_node;
+  else
+    return sysv_va_list_type_node;
+}
+
+/* Returns the canonical va_list type specified by TYPE. If there
+   is no valid TYPE provided, it return NULL_TREE.  */
+
+static tree
+ix86_canonical_va_list_type (tree type)
+{
+  if (TARGET_64BIT)
+    {
+      if (lookup_attribute ("ms_abi va_list", TYPE_ATTRIBUTES (type)))
+	return ms_va_list_type_node;
+
+      if ((TREE_CODE (type) == ARRAY_TYPE
+	   && integer_zerop (array_type_nelts (type)))
+	  || POINTER_TYPE_P (type))
+	{
+	  tree elem_type = TREE_TYPE (type);
+	  if (TREE_CODE (elem_type) == RECORD_TYPE
+	      && lookup_attribute ("sysv_abi va_list",
+				   TYPE_ATTRIBUTES (elem_type)))
+	    return sysv_va_list_type_node;
+	}
+
+      return NULL_TREE;
+    }
+
+  return std_canonical_va_list_type (type);
+}
+
+/* Iterate through the target-specific builtin types for va_list.
+   IDX denotes the iterator, *PTREE is set to the result type of
+   the va_list builtin, and *PNAME to its internal type.
+   Returns zero if there is no element for this index, otherwise
+   IDX should be increased upon the next call.
+   Note, do not iterate a base builtin's name like __builtin_va_list.
+   Used from c_common_nodes_and_builtins.  */
+
+static int
+ix86_enum_va_list (int idx, const char **pname, tree *ptree)
+{
+  if (TARGET_64BIT)
+    {
+      switch (idx)
+	{
+	default:
+	  break;
+
+	case 0:
+	  *ptree = ms_va_list_type_node;
+	  *pname = "__builtin_ms_va_list";
+	  return 1;
+
+	case 1:
+	  *ptree = sysv_va_list_type_node;
+	  *pname = "__builtin_sysv_va_list";
+	  return 1;
+	}
+    }
+
+  return 0;
+}
+
+#undef TARGET_SCHED_DISPATCH
+#define TARGET_SCHED_DISPATCH ix86_bd_has_dispatch
+#undef TARGET_SCHED_DISPATCH_DO
+#define TARGET_SCHED_DISPATCH_DO ix86_bd_do_dispatch
+#undef TARGET_SCHED_REASSOCIATION_WIDTH
+#define TARGET_SCHED_REASSOCIATION_WIDTH ix86_reassociation_width
+#undef TARGET_SCHED_REORDER
+#define TARGET_SCHED_REORDER ix86_atom_sched_reorder
+#undef TARGET_SCHED_ADJUST_PRIORITY
+#define TARGET_SCHED_ADJUST_PRIORITY ix86_adjust_priority
+#undef TARGET_SCHED_DEPENDENCIES_EVALUATION_HOOK
+#define TARGET_SCHED_DEPENDENCIES_EVALUATION_HOOK \
+  ix86_dependencies_evaluation_hook
+
+
+/* Implementation of reassociation_width target hook used by
+   reassoc phase to identify parallelism level in reassociated
+   tree.  Statements tree_code is passed in OPC.  Arguments type
+   is passed in MODE.  */
+
+static int
+ix86_reassociation_width (unsigned int op, machine_mode mode)
+{
+  int width = 1;
+  /* Vector part.  */
+  if (VECTOR_MODE_P (mode))
+    {
+      int div = 1;
+      if (INTEGRAL_MODE_P (mode))
+	width = ix86_cost->reassoc_vec_int;
+      else if (FLOAT_MODE_P (mode))
+	width = ix86_cost->reassoc_vec_fp;
+
+      if (width == 1)
+	return 1;
+
+      /* Integer vector instructions execute in FP unit
+	 and can execute 3 additions and one multiplication per cycle.  */
+      if ((ix86_tune == PROCESSOR_ZNVER1 || ix86_tune == PROCESSOR_ZNVER2
+	   || ix86_tune == PROCESSOR_ZNVER3)
+   	  && INTEGRAL_MODE_P (mode) && op != PLUS && op != MINUS)
+	return 1;
+
+      /* Account for targets that splits wide vectors into multiple parts.  */
+      if (TARGET_AVX256_SPLIT_REGS && GET_MODE_BITSIZE (mode) > 128)
+	div = GET_MODE_BITSIZE (mode) / 128;
+      else if (TARGET_SSE_SPLIT_REGS && GET_MODE_BITSIZE (mode) > 64)
+	div = GET_MODE_BITSIZE (mode) / 64;
+      width = (width + div - 1) / div;
+    }
+  /* Scalar part.  */
+  else if (INTEGRAL_MODE_P (mode))
+    width = ix86_cost->reassoc_int;
+  else if (FLOAT_MODE_P (mode))
+    width = ix86_cost->reassoc_fp;
+
+  /* Avoid using too many registers in 32bit mode.  */
+  if (!TARGET_64BIT && width > 2)
+    width = 2;
+  return width;
+}
+
+/* ??? No autovectorization into MMX or 3DNOW until we can reliably
+   place emms and femms instructions.  */
+
+static machine_mode
+ix86_preferred_simd_mode (scalar_mode mode)
+{
+  if (!TARGET_SSE)
+    return word_mode;
+
+  switch (mode)
+    {
+    case E_QImode:
+      if (TARGET_AVX512BW && !TARGET_PREFER_AVX256)
+	return V64QImode;
+      else if (TARGET_AVX && !TARGET_PREFER_AVX128)
+	return V32QImode;
+      else
+	return V16QImode;
+
+    case E_HImode:
+      if (TARGET_AVX512BW && !TARGET_PREFER_AVX256)
+	return V32HImode;
+      else if (TARGET_AVX && !TARGET_PREFER_AVX128)
+	return V16HImode;
+      else
+	return V8HImode;
+
+    case E_SImode:
+      if (TARGET_AVX512F && !TARGET_PREFER_AVX256)
+	return V16SImode;
+      else if (TARGET_AVX && !TARGET_PREFER_AVX128)
+	return V8SImode;
+      else
+	return V4SImode;
+
+    case E_DImode:
+      if (TARGET_AVX512F && !TARGET_PREFER_AVX256)
+	return V8DImode;
+      else if (TARGET_AVX && !TARGET_PREFER_AVX128)
+	return V4DImode;
+      else
+	return V2DImode;
+
+    case E_SFmode:
+      if (TARGET_AVX512F && !TARGET_PREFER_AVX256)
+	return V16SFmode;
+      else if (TARGET_AVX && !TARGET_PREFER_AVX128)
+	return V8SFmode;
+      else
+	return V4SFmode;
+
+    case E_DFmode:
+      if (TARGET_AVX512F && !TARGET_PREFER_AVX256)
+	return V8DFmode;
+      else if (TARGET_AVX && !TARGET_PREFER_AVX128)
+	return V4DFmode;
+      else if (TARGET_SSE2)
+	return V2DFmode;
+      /* FALLTHRU */
+
+    default:
+      return word_mode;
+    }
+}
+
+/* If AVX is enabled then try vectorizing with both 256bit and 128bit
+   vectors.  If AVX512F is enabled then try vectorizing with 512bit,
+   256bit and 128bit vectors.  */
+
+static unsigned int
+ix86_autovectorize_vector_modes (vector_modes *modes, bool all)
+{
+  if (TARGET_AVX512F && !TARGET_PREFER_AVX256)
+    {
+      modes->safe_push (V64QImode);
+      modes->safe_push (V32QImode);
+      modes->safe_push (V16QImode);
+    }
+  else if (TARGET_AVX512F && all)
+    {
+      modes->safe_push (V32QImode);
+      modes->safe_push (V16QImode);
+      modes->safe_push (V64QImode);
+    }
+  else if (TARGET_AVX && !TARGET_PREFER_AVX128)
+    {
+      modes->safe_push (V32QImode);
+      modes->safe_push (V16QImode);
+    }
+  else if (TARGET_AVX && all)
+    {
+      modes->safe_push (V16QImode);
+      modes->safe_push (V32QImode);
+    }
+  else if (TARGET_MMX_WITH_SSE)
+    modes->safe_push (V16QImode);
+
+  if (TARGET_MMX_WITH_SSE)
+    modes->safe_push (V8QImode);
+
+  return 0;
+}
+
+/* Implemenation of targetm.vectorize.get_mask_mode.  */
+
+static opt_machine_mode
+ix86_get_mask_mode (machine_mode data_mode)
+{
+  unsigned vector_size = GET_MODE_SIZE (data_mode);
+  unsigned nunits = GET_MODE_NUNITS (data_mode);
+  unsigned elem_size = vector_size / nunits;
+
+  /* Scalar mask case.  */
+  if ((TARGET_AVX512F && vector_size == 64)
+      || (TARGET_AVX512VL && (vector_size == 32 || vector_size == 16)))
+    {
+      if (elem_size == 4
+	  || elem_size == 8
+	  || (TARGET_AVX512BW && (elem_size == 1 || elem_size == 2)))
+	return smallest_int_mode_for_size (nunits);
+    }
+
+  scalar_int_mode elem_mode
+    = smallest_int_mode_for_size (elem_size * BITS_PER_UNIT);
+
+  gcc_assert (elem_size * nunits == vector_size);
+
+  return mode_for_vector (elem_mode, nunits);
+}
+
+
+
+/* Return class of registers which could be used for pseudo of MODE
+   and of class RCLASS for spilling instead of memory.  Return NO_REGS
+   if it is not possible or non-profitable.  */
+
+/* Disabled due to PRs 70902, 71453, 71555, 71596 and 71657.  */
+
+static reg_class_t
+ix86_spill_class (reg_class_t rclass, machine_mode mode)
+{
+  if (0 && TARGET_GENERAL_REGS_SSE_SPILL
+      && TARGET_SSE2
+      && TARGET_INTER_UNIT_MOVES_TO_VEC
+      && TARGET_INTER_UNIT_MOVES_FROM_VEC
+      && (mode == SImode || (TARGET_64BIT && mode == DImode))
+      && INTEGER_CLASS_P (rclass))
+    return ALL_SSE_REGS;
+  return NO_REGS;
+}
+
+/* Implement TARGET_MAX_NOCE_IFCVT_SEQ_COST.  Like the default implementation,
+   but returns a lower bound.  */
+
+static unsigned int
+ix86_max_noce_ifcvt_seq_cost (edge e)
+{
+  bool predictable_p = predictable_edge_p (e);
+  if (predictable_p)
+    {
+      if (global_options_set.x_param_max_rtl_if_conversion_predictable_cost)
+	return param_max_rtl_if_conversion_predictable_cost;
+    }
+  else
+    {
+      if (global_options_set.x_param_max_rtl_if_conversion_unpredictable_cost)
+	return param_max_rtl_if_conversion_unpredictable_cost;
+    }
+
+  return BRANCH_COST (true, predictable_p) * COSTS_N_INSNS (2);
+}
+
+/* Return true if SEQ is a good candidate as a replacement for the
+   if-convertible sequence described in IF_INFO.  */
+
+static bool
+ix86_noce_conversion_profitable_p (rtx_insn *seq, struct noce_if_info *if_info)
+{
+  if (TARGET_ONE_IF_CONV_INSN && if_info->speed_p)
+    {
+      int cmov_cnt = 0;
+      /* Punt if SEQ contains more than one CMOV or FCMOV instruction.
+	 Maybe we should allow even more conditional moves as long as they
+	 are used far enough not to stall the CPU, or also consider
+	 IF_INFO->TEST_BB succ edge probabilities.  */
+      for (rtx_insn *insn = seq; insn; insn = NEXT_INSN (insn))
+	{
+	  rtx set = single_set (insn);
+	  if (!set)
+	    continue;
+	  if (GET_CODE (SET_SRC (set)) != IF_THEN_ELSE)
+	    continue;
+	  rtx src = SET_SRC (set);
+	  machine_mode mode = GET_MODE (src);
+	  if (GET_MODE_CLASS (mode) != MODE_INT
+	      && GET_MODE_CLASS (mode) != MODE_FLOAT)
+	    continue;
+	  if ((!REG_P (XEXP (src, 1)) && !MEM_P (XEXP (src, 1)))
+	      || (!REG_P (XEXP (src, 2)) && !MEM_P (XEXP (src, 2))))
+	    continue;
+	  /* insn is CMOV or FCMOV.  */
+	  if (++cmov_cnt > 1)
+	    return false;
+	}
+    }
+  return default_noce_conversion_profitable_p (seq, if_info);
+}
+
+/* Implement targetm.vectorize.init_cost.  */
+
+static void *
+ix86_init_cost (class loop *)
+{
+  unsigned *cost = XNEWVEC (unsigned, 3);
+  cost[vect_prologue] = cost[vect_body] = cost[vect_epilogue] = 0;
+  return cost;
+}
+
+/* Implement targetm.vectorize.add_stmt_cost.  */
+
+static unsigned
+ix86_add_stmt_cost (class vec_info *vinfo, void *data, int count,
+		    enum vect_cost_for_stmt kind,
+		    class _stmt_vec_info *stmt_info, tree vectype,
+		    int misalign,
+		    enum vect_cost_model_location where)
+{
+  unsigned *cost = (unsigned *) data;
+  unsigned retval = 0;
+  bool scalar_p
+    = (kind == scalar_stmt || kind == scalar_load || kind == scalar_store);
+  int stmt_cost = - 1;
+
+  bool fp = false;
+  machine_mode mode = scalar_p ? SImode : TImode;
+
+  if (vectype != NULL)
+    {
+      fp = FLOAT_TYPE_P (vectype);
+      mode = TYPE_MODE (vectype);
+      if (scalar_p)
+	mode = TYPE_MODE (TREE_TYPE (vectype));
+    }
+
+  if ((kind == vector_stmt || kind == scalar_stmt)
+      && stmt_info
+      && stmt_info->stmt && gimple_code (stmt_info->stmt) == GIMPLE_ASSIGN)
+    {
+      tree_code subcode = gimple_assign_rhs_code (stmt_info->stmt);
+      /*machine_mode inner_mode = mode;
+      if (VECTOR_MODE_P (mode))
+	inner_mode = GET_MODE_INNER (mode);*/
+
+      switch (subcode)
+	{
+	case PLUS_EXPR:
+	case POINTER_PLUS_EXPR:
+	case MINUS_EXPR:
+	  if (kind == scalar_stmt)
+	    {
+	      if (SSE_FLOAT_MODE_P (mode) && TARGET_SSE_MATH)
+		stmt_cost = ix86_cost->addss;
+	      else if (X87_FLOAT_MODE_P (mode))
+		stmt_cost = ix86_cost->fadd;
+	      else
+	        stmt_cost = ix86_cost->add;
+	    }
+	  else
+	    stmt_cost = ix86_vec_cost (mode, fp ? ix86_cost->addss
+				       : ix86_cost->sse_op);
+	  break;
+
+	case MULT_EXPR:
+	case WIDEN_MULT_EXPR:
+	case MULT_HIGHPART_EXPR:
+	  stmt_cost = ix86_multiplication_cost (ix86_cost, mode);
+	  break;
+	case NEGATE_EXPR:
+	  if (SSE_FLOAT_MODE_P (mode) && TARGET_SSE_MATH)
+	    stmt_cost = ix86_cost->sse_op;
+	  else if (X87_FLOAT_MODE_P (mode))
+	    stmt_cost = ix86_cost->fchs;
+	  else if (VECTOR_MODE_P (mode))
+	    stmt_cost = ix86_vec_cost (mode, ix86_cost->sse_op);
+	  else
+	    stmt_cost = ix86_cost->add;
+	  break;
+	case TRUNC_DIV_EXPR:
+	case CEIL_DIV_EXPR:
+	case FLOOR_DIV_EXPR:
+	case ROUND_DIV_EXPR:
+	case TRUNC_MOD_EXPR:
+	case CEIL_MOD_EXPR:
+	case FLOOR_MOD_EXPR:
+	case RDIV_EXPR:
+	case ROUND_MOD_EXPR:
+	case EXACT_DIV_EXPR:
+	  stmt_cost = ix86_division_cost (ix86_cost, mode);
+	  break;
+
+	case RSHIFT_EXPR:
+	case LSHIFT_EXPR:
+	case LROTATE_EXPR:
+	case RROTATE_EXPR:
+	  {
+	    tree op2 = gimple_assign_rhs2 (stmt_info->stmt);
+	    stmt_cost = ix86_shift_rotate_cost
+			   (ix86_cost, mode,
+		            TREE_CODE (op2) == INTEGER_CST,
+			    cst_and_fits_in_hwi (op2) ? int_cst_value (op2) : -1,
+		            true, false, false, NULL, NULL);
+	  }
+	  break;
+	case NOP_EXPR:
+	  /* Only sign-conversions are free.  */
+	  if (tree_nop_conversion_p
+	        (TREE_TYPE (gimple_assign_lhs (stmt_info->stmt)),
+		 TREE_TYPE (gimple_assign_rhs1 (stmt_info->stmt))))
+	    stmt_cost = 0;
+	  break;
+
+	case BIT_IOR_EXPR:
+	case ABS_EXPR:
+	case ABSU_EXPR:
+	case MIN_EXPR:
+	case MAX_EXPR:
+	case BIT_XOR_EXPR:
+	case BIT_AND_EXPR:
+	case BIT_NOT_EXPR:
+	  if (SSE_FLOAT_MODE_P (mode) && TARGET_SSE_MATH)
+	    stmt_cost = ix86_cost->sse_op;
+	  else if (VECTOR_MODE_P (mode))
+	    stmt_cost = ix86_vec_cost (mode, ix86_cost->sse_op);
+	  else
+	    stmt_cost = ix86_cost->add;
+	  break;
+	default:
+	  break;
+	}
+    }
+
+  combined_fn cfn;
+  if ((kind == vector_stmt || kind == scalar_stmt)
+      && stmt_info
+      && stmt_info->stmt
+      && (cfn = gimple_call_combined_fn (stmt_info->stmt)) != CFN_LAST)
+    switch (cfn)
+      {
+      case CFN_FMA:
+	stmt_cost = ix86_vec_cost (mode,
+				   mode == SFmode ? ix86_cost->fmass
+				   : ix86_cost->fmasd);
+	break;
+      default:
+	break;
+      }
+
+  /* If we do elementwise loads into a vector then we are bound by
+     latency and execution resources for the many scalar loads
+     (AGU and load ports).  Try to account for this by scaling the
+     construction cost by the number of elements involved.  */
+  if ((kind == vec_construct || kind == vec_to_scalar)
+      && stmt_info
+      && (STMT_VINFO_TYPE (stmt_info) == load_vec_info_type
+	  || STMT_VINFO_TYPE (stmt_info) == store_vec_info_type)
+      && STMT_VINFO_MEMORY_ACCESS_TYPE (stmt_info) == VMAT_ELEMENTWISE
+      && TREE_CODE (DR_STEP (STMT_VINFO_DATA_REF (stmt_info))) != INTEGER_CST)
+    {
+      stmt_cost = ix86_builtin_vectorization_cost (kind, vectype, misalign);
+      stmt_cost *= (TYPE_VECTOR_SUBPARTS (vectype) + 1);
+    }
+  if (stmt_cost == -1)
+    stmt_cost = ix86_builtin_vectorization_cost (kind, vectype, misalign);
+
+  /* Penalize DFmode vector operations for Bonnell.  */
+  if (TARGET_BONNELL && kind == vector_stmt
+      && vectype && GET_MODE_INNER (TYPE_MODE (vectype)) == DFmode)
+    stmt_cost *= 5;  /* FIXME: The value here is arbitrary.  */
+
+  /* Statements in an inner loop relative to the loop being
+     vectorized are weighted more heavily.  The value here is
+     arbitrary and could potentially be improved with analysis.  */
+  if (where == vect_body && stmt_info
+      && stmt_in_inner_loop_p (vinfo, stmt_info))
+    count *= 50;  /* FIXME.  */
+
+  retval = (unsigned) (count * stmt_cost);
+
+  /* We need to multiply all vector stmt cost by 1.7 (estimated cost)
+     for Silvermont as it has out of order integer pipeline and can execute
+     2 scalar instruction per tick, but has in order SIMD pipeline.  */
+  if ((TARGET_SILVERMONT || TARGET_GOLDMONT || TARGET_GOLDMONT_PLUS
+       || TARGET_TREMONT || TARGET_INTEL) && stmt_info && stmt_info->stmt)
+    {
+      tree lhs_op = gimple_get_lhs (stmt_info->stmt);
+      if (lhs_op && TREE_CODE (TREE_TYPE (lhs_op)) == INTEGER_TYPE)
+	retval = (retval * 17) / 10;
+    }
+
+  cost[where] += retval;
+
+  return retval;
+}
+
+/* Implement targetm.vectorize.finish_cost.  */
+
+static void
+ix86_finish_cost (void *data, unsigned *prologue_cost,
+		  unsigned *body_cost, unsigned *epilogue_cost)
+{
+  unsigned *cost = (unsigned *) data;
+  *prologue_cost = cost[vect_prologue];
+  *body_cost     = cost[vect_body];
+  *epilogue_cost = cost[vect_epilogue];
+}
+
+/* Implement targetm.vectorize.destroy_cost_data.  */
+
+static void
+ix86_destroy_cost_data (void *data)
+{
+  free (data);
+}
+
+/* Validate target specific memory model bits in VAL. */
+
+static unsigned HOST_WIDE_INT
+ix86_memmodel_check (unsigned HOST_WIDE_INT val)
+{
+  enum memmodel model = memmodel_from_int (val);
+  bool strong;
+
+  if (val & ~(unsigned HOST_WIDE_INT)(IX86_HLE_ACQUIRE|IX86_HLE_RELEASE
+				      |MEMMODEL_MASK)
+      || ((val & IX86_HLE_ACQUIRE) && (val & IX86_HLE_RELEASE)))
+    {
+      warning (OPT_Winvalid_memory_model,
+	       "unknown architecture specific memory model");
+      return MEMMODEL_SEQ_CST;
+    }
+  strong = (is_mm_acq_rel (model) || is_mm_seq_cst (model));
+  if (val & IX86_HLE_ACQUIRE && !(is_mm_acquire (model) || strong))
+    {
+      warning (OPT_Winvalid_memory_model,
+	      "%<HLE_ACQUIRE%> not used with %<ACQUIRE%> or stronger "
+	       "memory model");
+      return MEMMODEL_SEQ_CST | IX86_HLE_ACQUIRE;
+    }
+  if (val & IX86_HLE_RELEASE && !(is_mm_release (model) || strong))
+    {
+      warning (OPT_Winvalid_memory_model,
+	      "%<HLE_RELEASE%> not used with %<RELEASE%> or stronger "
+	       "memory model");
+      return MEMMODEL_SEQ_CST | IX86_HLE_RELEASE;
+    }
+  return val;
+}
+
+/* Set CLONEI->vecsize_mangle, CLONEI->mask_mode, CLONEI->vecsize_int,
+   CLONEI->vecsize_float and if CLONEI->simdlen is 0, also
+   CLONEI->simdlen.  Return 0 if SIMD clones shouldn't be emitted,
+   or number of vecsize_mangle variants that should be emitted.  */
+
+static int
+ix86_simd_clone_compute_vecsize_and_simdlen (struct cgraph_node *node,
+					     struct cgraph_simd_clone *clonei,
+					     tree base_type, int num)
+{
+  int ret = 1;
+
+  if (clonei->simdlen
+      && (clonei->simdlen < 2
+	  || clonei->simdlen > 1024
+	  || (clonei->simdlen & (clonei->simdlen - 1)) != 0))
+    {
+      warning_at (DECL_SOURCE_LOCATION (node->decl), 0,
+		  "unsupported simdlen %wd", clonei->simdlen.to_constant ());
+      return 0;
+    }
+
+  tree ret_type = TREE_TYPE (TREE_TYPE (node->decl));
+  if (TREE_CODE (ret_type) != VOID_TYPE)
+    switch (TYPE_MODE (ret_type))
+      {
+      case E_QImode:
+      case E_HImode:
+      case E_SImode:
+      case E_DImode:
+      case E_SFmode:
+      case E_DFmode:
+      /* case E_SCmode: */
+      /* case E_DCmode: */
+	if (!AGGREGATE_TYPE_P (ret_type))
+	  break;
+	/* FALLTHRU */
+      default:
+	warning_at (DECL_SOURCE_LOCATION (node->decl), 0,
+		    "unsupported return type %qT for simd", ret_type);
+	return 0;
+      }
+
+  tree t;
+  int i;
+  tree type_arg_types = TYPE_ARG_TYPES (TREE_TYPE (node->decl));
+  bool decl_arg_p = (node->definition || type_arg_types == NULL_TREE);
+
+  for (t = (decl_arg_p ? DECL_ARGUMENTS (node->decl) : type_arg_types), i = 0;
+       t && t != void_list_node; t = TREE_CHAIN (t), i++)
+    {
+      tree arg_type = decl_arg_p ? TREE_TYPE (t) : TREE_VALUE (t);
+      switch (TYPE_MODE (arg_type))
+	{
+	case E_QImode:
+	case E_HImode:
+	case E_SImode:
+	case E_DImode:
+	case E_SFmode:
+	case E_DFmode:
+	/* case E_SCmode: */
+	/* case E_DCmode: */
+	  if (!AGGREGATE_TYPE_P (arg_type))
+	    break;
+	  /* FALLTHRU */
+	default:
+	  if (clonei->args[i].arg_type == SIMD_CLONE_ARG_TYPE_UNIFORM)
+	    break;
+	  warning_at (DECL_SOURCE_LOCATION (node->decl), 0,
+		      "unsupported argument type %qT for simd", arg_type);
+	  return 0;
+	}
+    }
+
+  if (!TREE_PUBLIC (node->decl))
+    {
+      /* If the function isn't exported, we can pick up just one ISA
+	 for the clones.  */
+      if (TARGET_AVX512F)
+	clonei->vecsize_mangle = 'e';
+      else if (TARGET_AVX2)
+	clonei->vecsize_mangle = 'd';
+      else if (TARGET_AVX)
+	clonei->vecsize_mangle = 'c';
+      else
+	clonei->vecsize_mangle = 'b';
+      ret = 1;
+    }
+  else
+    {
+      clonei->vecsize_mangle = "bcde"[num];
+      ret = 4;
+    }
+  clonei->mask_mode = VOIDmode;
+  switch (clonei->vecsize_mangle)
+    {
+    case 'b':
+      clonei->vecsize_int = 128;
+      clonei->vecsize_float = 128;
+      break;
+    case 'c':
+      clonei->vecsize_int = 128;
+      clonei->vecsize_float = 256;
+      break;
+    case 'd':
+      clonei->vecsize_int = 256;
+      clonei->vecsize_float = 256;
+      break;
+    case 'e':
+      clonei->vecsize_int = 512;
+      clonei->vecsize_float = 512;
+      if (TYPE_MODE (base_type) == QImode)
+	clonei->mask_mode = DImode;
+      else
+	clonei->mask_mode = SImode;
+      break;
+    }
+  if (clonei->simdlen == 0)
+    {
+      if (SCALAR_INT_MODE_P (TYPE_MODE (base_type)))
+	clonei->simdlen = clonei->vecsize_int;
+      else
+	clonei->simdlen = clonei->vecsize_float;
+      clonei->simdlen = clonei->simdlen
+			/ GET_MODE_BITSIZE (TYPE_MODE (base_type));
+    }
+  else if (clonei->simdlen > 16)
+    {
+      /* For compatibility with ICC, use the same upper bounds
+	 for simdlen.  In particular, for CTYPE below, use the return type,
+	 unless the function returns void, in that case use the characteristic
+	 type.  If it is possible for given SIMDLEN to pass CTYPE value
+	 in registers (8 [XYZ]MM* regs for 32-bit code, 16 [XYZ]MM* regs
+	 for 64-bit code), accept that SIMDLEN, otherwise warn and don't
+	 emit corresponding clone.  */
+      tree ctype = ret_type;
+      if (TREE_CODE (ret_type) == VOID_TYPE)
+	ctype = base_type;
+      int cnt = GET_MODE_BITSIZE (TYPE_MODE (ctype)) * clonei->simdlen;
+      if (SCALAR_INT_MODE_P (TYPE_MODE (ctype)))
+	cnt /= clonei->vecsize_int;
+      else
+	cnt /= clonei->vecsize_float;
+      if (cnt > (TARGET_64BIT ? 16 : 8))
+	{
+	  warning_at (DECL_SOURCE_LOCATION (node->decl), 0,
+		      "unsupported simdlen %wd",
+		      clonei->simdlen.to_constant ());
+	  return 0;
+	}
+      }
+  return ret;
+}
+
+/* If SIMD clone NODE can't be used in a vectorized loop
+   in current function, return -1, otherwise return a badness of using it
+   (0 if it is most desirable from vecsize_mangle point of view, 1
+   slightly less desirable, etc.).  */
+
+static int
+ix86_simd_clone_usable (struct cgraph_node *node)
+{
+  switch (node->simdclone->vecsize_mangle)
+    {
+    case 'b':
+      if (!TARGET_SSE2)
+	return -1;
+      if (!TARGET_AVX)
+	return 0;
+      return TARGET_AVX512F ? 3 : TARGET_AVX2 ? 2 : 1;
+    case 'c':
+      if (!TARGET_AVX)
+	return -1;
+      return TARGET_AVX512F ? 2 : TARGET_AVX2 ? 1 : 0;
+    case 'd':
+      if (!TARGET_AVX2)
+	return -1;
+      return TARGET_AVX512F ? 1 : 0;
+    case 'e':
+      if (!TARGET_AVX512F)
+	return -1;
+      return 0;
+    default:
+      gcc_unreachable ();
+    }
+}
+
+/* This function adjusts the unroll factor based on
+   the hardware capabilities. For ex, bdver3 has
+   a loop buffer which makes unrolling of smaller
+   loops less important. This function decides the
+   unroll factor using number of memory references
+   (value 32 is used) as a heuristic. */
+
+static unsigned
+ix86_loop_unroll_adjust (unsigned nunroll, class loop *loop)
+{
+  basic_block *bbs;
+  rtx_insn *insn;
+  unsigned i;
+  unsigned mem_count = 0;
+
+  if (!TARGET_ADJUST_UNROLL)
+     return nunroll;
+
+  /* Count the number of memory references within the loop body.
+     This value determines the unrolling factor for bdver3 and bdver4
+     architectures. */
+  subrtx_iterator::array_type array;
+  bbs = get_loop_body (loop);
+  for (i = 0; i < loop->num_nodes; i++)
+    FOR_BB_INSNS (bbs[i], insn)
+      if (NONDEBUG_INSN_P (insn))
+	FOR_EACH_SUBRTX (iter, array, PATTERN (insn), NONCONST)
+	  if (const_rtx x = *iter)
+	    if (MEM_P (x))
+	      {
+		machine_mode mode = GET_MODE (x);
+		unsigned int n_words = GET_MODE_SIZE (mode) / UNITS_PER_WORD;
+		if (n_words > 4)
+		  mem_count += 2;
+		else
+		  mem_count += 1;
+	      }
+  free (bbs);
+
+  if (mem_count && mem_count <=32)
+    return MIN (nunroll, 32 / mem_count);
+
+  return nunroll;
+}
+
+
+/* Implement TARGET_FLOAT_EXCEPTIONS_ROUNDING_SUPPORTED_P.  */
+
+static bool
+ix86_float_exceptions_rounding_supported_p (void)
+{
+  /* For x87 floating point with standard excess precision handling,
+     there is no adddf3 pattern (since x87 floating point only has
+     XFmode operations) so the default hook implementation gets this
+     wrong.  */
+  return TARGET_80387 || (TARGET_SSE && TARGET_SSE_MATH);
+}
+
+/* Implement TARGET_ATOMIC_ASSIGN_EXPAND_FENV.  */
+
+static void
+ix86_atomic_assign_expand_fenv (tree *hold, tree *clear, tree *update)
+{
+  if (!TARGET_80387 && !(TARGET_SSE && TARGET_SSE_MATH))
+    return;
+  tree exceptions_var = create_tmp_var_raw (integer_type_node);
+  if (TARGET_80387)
+    {
+      tree fenv_index_type = build_index_type (size_int (6));
+      tree fenv_type = build_array_type (unsigned_type_node, fenv_index_type);
+      tree fenv_var = create_tmp_var_raw (fenv_type);
+      TREE_ADDRESSABLE (fenv_var) = 1;
+      tree fenv_ptr = build_pointer_type (fenv_type);
+      tree fenv_addr = build1 (ADDR_EXPR, fenv_ptr, fenv_var);
+      fenv_addr = fold_convert (ptr_type_node, fenv_addr);
+      tree fnstenv = get_ix86_builtin (IX86_BUILTIN_FNSTENV);
+      tree fldenv = get_ix86_builtin (IX86_BUILTIN_FLDENV);
+      tree fnstsw = get_ix86_builtin (IX86_BUILTIN_FNSTSW);
+      tree fnclex = get_ix86_builtin (IX86_BUILTIN_FNCLEX);
+      tree hold_fnstenv = build_call_expr (fnstenv, 1, fenv_addr);
+      tree hold_fnclex = build_call_expr (fnclex, 0);
+      fenv_var = build4 (TARGET_EXPR, fenv_type, fenv_var, hold_fnstenv,
+			 NULL_TREE, NULL_TREE);
+      *hold = build2 (COMPOUND_EXPR, void_type_node, fenv_var,
+		      hold_fnclex);
+      *clear = build_call_expr (fnclex, 0);
+      tree sw_var = create_tmp_var_raw (short_unsigned_type_node);
+      tree fnstsw_call = build_call_expr (fnstsw, 0);
+      tree sw_mod = build4 (TARGET_EXPR, short_unsigned_type_node, sw_var,
+			    fnstsw_call, NULL_TREE, NULL_TREE);
+      tree exceptions_x87 = fold_convert (integer_type_node, sw_var);
+      tree update_mod = build4 (TARGET_EXPR, integer_type_node,
+				exceptions_var, exceptions_x87,
+				NULL_TREE, NULL_TREE);
+      *update = build2 (COMPOUND_EXPR, integer_type_node,
+			sw_mod, update_mod);
+      tree update_fldenv = build_call_expr (fldenv, 1, fenv_addr);
+      *update = build2 (COMPOUND_EXPR, void_type_node, *update, update_fldenv);
+    }
+  if (TARGET_SSE && TARGET_SSE_MATH)
+    {
+      tree mxcsr_orig_var = create_tmp_var_raw (unsigned_type_node);
+      tree mxcsr_mod_var = create_tmp_var_raw (unsigned_type_node);
+      tree stmxcsr = get_ix86_builtin (IX86_BUILTIN_STMXCSR);
+      tree ldmxcsr = get_ix86_builtin (IX86_BUILTIN_LDMXCSR);
+      tree stmxcsr_hold_call = build_call_expr (stmxcsr, 0);
+      tree hold_assign_orig = build4 (TARGET_EXPR, unsigned_type_node,
+				      mxcsr_orig_var, stmxcsr_hold_call,
+				      NULL_TREE, NULL_TREE);
+      tree hold_mod_val = build2 (BIT_IOR_EXPR, unsigned_type_node,
+				  mxcsr_orig_var,
+				  build_int_cst (unsigned_type_node, 0x1f80));
+      hold_mod_val = build2 (BIT_AND_EXPR, unsigned_type_node, hold_mod_val,
+			     build_int_cst (unsigned_type_node, 0xffffffc0));
+      tree hold_assign_mod = build4 (TARGET_EXPR, unsigned_type_node,
+				     mxcsr_mod_var, hold_mod_val,
+				     NULL_TREE, NULL_TREE);
+      tree ldmxcsr_hold_call = build_call_expr (ldmxcsr, 1, mxcsr_mod_var);
+      tree hold_all = build2 (COMPOUND_EXPR, unsigned_type_node,
+			      hold_assign_orig, hold_assign_mod);
+      hold_all = build2 (COMPOUND_EXPR, void_type_node, hold_all,
+			 ldmxcsr_hold_call);
+      if (*hold)
+	*hold = build2 (COMPOUND_EXPR, void_type_node, *hold, hold_all);
+      else
+	*hold = hold_all;
+      tree ldmxcsr_clear_call = build_call_expr (ldmxcsr, 1, mxcsr_mod_var);
+      if (*clear)
+	*clear = build2 (COMPOUND_EXPR, void_type_node, *clear,
+			 ldmxcsr_clear_call);
+      else
+	*clear = ldmxcsr_clear_call;
+      tree stxmcsr_update_call = build_call_expr (stmxcsr, 0);
+      tree exceptions_sse = fold_convert (integer_type_node,
+					  stxmcsr_update_call);
+      if (*update)
+	{
+	  tree exceptions_mod = build2 (BIT_IOR_EXPR, integer_type_node,
+					exceptions_var, exceptions_sse);
+	  tree exceptions_assign = build2 (MODIFY_EXPR, integer_type_node,
+					   exceptions_var, exceptions_mod);
+	  *update = build2 (COMPOUND_EXPR, integer_type_node, *update,
+			    exceptions_assign);
+	}
+      else
+	*update = build4 (TARGET_EXPR, integer_type_node, exceptions_var,
+			  exceptions_sse, NULL_TREE, NULL_TREE);
+      tree ldmxcsr_update_call = build_call_expr (ldmxcsr, 1, mxcsr_orig_var);
+      *update = build2 (COMPOUND_EXPR, void_type_node, *update,
+			ldmxcsr_update_call);
+    }
+  tree atomic_feraiseexcept
+    = builtin_decl_implicit (BUILT_IN_ATOMIC_FERAISEEXCEPT);
+  tree atomic_feraiseexcept_call = build_call_expr (atomic_feraiseexcept,
+						    1, exceptions_var);
+  *update = build2 (COMPOUND_EXPR, void_type_node, *update,
+		    atomic_feraiseexcept_call);
+}
+
+#if !TARGET_MACHO && !TARGET_DLLIMPORT_DECL_ATTRIBUTES
+/* For i386, common symbol is local only for non-PIE binaries.  For
+   x86-64, common symbol is local only for non-PIE binaries or linker
+   supports copy reloc in PIE binaries.   */
+
+static bool
+ix86_binds_local_p (const_tree exp)
+{
+  return default_binds_local_p_3 (exp, flag_shlib != 0, true, true,
+				  (!flag_pic
+				   || (TARGET_64BIT
+				       && HAVE_LD_PIE_COPYRELOC != 0)));
+}
+#endif
+
+/* If MEM is in the form of [base+offset], extract the two parts
+   of address and set to BASE and OFFSET, otherwise return false.  */
+
+static bool
+extract_base_offset_in_addr (rtx mem, rtx *base, rtx *offset)
+{
+  rtx addr;
+
+  gcc_assert (MEM_P (mem));
+
+  addr = XEXP (mem, 0);
+  
+  if (GET_CODE (addr) == CONST)
+    addr = XEXP (addr, 0);
+
+  if (REG_P (addr) || GET_CODE (addr) == SYMBOL_REF)
+    {
+      *base = addr;
+      *offset = const0_rtx;
+      return true;
+    }
+
+  if (GET_CODE (addr) == PLUS
+      && (REG_P (XEXP (addr, 0))
+	  || GET_CODE (XEXP (addr, 0)) == SYMBOL_REF)
+      && CONST_INT_P (XEXP (addr, 1)))
+    {
+      *base = XEXP (addr, 0);
+      *offset = XEXP (addr, 1);
+      return true;
+    }
+
+  return false;
+}
+
+/* Given OPERANDS of consecutive load/store, check if we can merge
+   them into move multiple.  LOAD is true if they are load instructions.
+   MODE is the mode of memory operands.  */
+
+bool
+ix86_operands_ok_for_move_multiple (rtx *operands, bool load,
+				    machine_mode mode)
+{
+  HOST_WIDE_INT offval_1, offval_2, msize;
+  rtx mem_1, mem_2, reg_1, reg_2, base_1, base_2, offset_1, offset_2;
+
+  if (load)
+    {
+      mem_1 = operands[1];
+      mem_2 = operands[3];
+      reg_1 = operands[0];
+      reg_2 = operands[2];
+    }
+  else
+    {
+      mem_1 = operands[0];
+      mem_2 = operands[2];
+      reg_1 = operands[1];
+      reg_2 = operands[3];
+    }
+
+  gcc_assert (REG_P (reg_1) && REG_P (reg_2));
+
+  if (REGNO (reg_1) != REGNO (reg_2))
+    return false;
+
+  /* Check if the addresses are in the form of [base+offset].  */
+  if (!extract_base_offset_in_addr (mem_1, &base_1, &offset_1))
+    return false;
+  if (!extract_base_offset_in_addr (mem_2, &base_2, &offset_2))
+    return false;
+
+  /* Check if the bases are the same.  */
+  if (!rtx_equal_p (base_1, base_2))
+    return false;
+
+  offval_1 = INTVAL (offset_1);
+  offval_2 = INTVAL (offset_2);
+  msize = GET_MODE_SIZE (mode);
+  /* Check if mem_1 is adjacent to mem_2 and mem_1 has lower address.  */
+  if (offval_1 + msize != offval_2)
+    return false;
+
+  return true;
+}
+
+/* Implement the TARGET_OPTAB_SUPPORTED_P hook.  */
+
+static bool
+ix86_optab_supported_p (int op, machine_mode mode1, machine_mode,
+			optimization_type opt_type)
+{
+  switch (op)
+    {
+    case asin_optab:
+    case acos_optab:
+    case log1p_optab:
+    case exp_optab:
+    case exp10_optab:
+    case exp2_optab:
+    case expm1_optab:
+    case ldexp_optab:
+    case scalb_optab:
+    case round_optab:
+      return opt_type == OPTIMIZE_FOR_SPEED;
+
+    case rint_optab:
+      if (SSE_FLOAT_MODE_P (mode1)
+	  && TARGET_SSE_MATH
+	  && !flag_trapping_math
+	  && !TARGET_SSE4_1)
+	return opt_type == OPTIMIZE_FOR_SPEED;
+      return true;
+
+    case floor_optab:
+    case ceil_optab:
+    case btrunc_optab:
+      if (SSE_FLOAT_MODE_P (mode1)
+	  && TARGET_SSE_MATH
+	  && !flag_trapping_math
+	  && TARGET_SSE4_1)
+	return true;
+      return opt_type == OPTIMIZE_FOR_SPEED;
+
+    case rsqrt_optab:
+      return opt_type == OPTIMIZE_FOR_SPEED && use_rsqrt_p ();
+
+    default:
+      return true;
+    }
+}
+
+/* Address space support.
+
+   This is not "far pointers" in the 16-bit sense, but an easy way
+   to use %fs and %gs segment prefixes.  Therefore:
+
+    (a) All address spaces have the same modes,
+    (b) All address spaces have the same addresss forms,
+    (c) While %fs and %gs are technically subsets of the generic
+        address space, they are probably not subsets of each other.
+    (d) Since we have no access to the segment base register values
+        without resorting to a system call, we cannot convert a
+        non-default address space to a default address space.
+        Therefore we do not claim %fs or %gs are subsets of generic.
+
+   Therefore we can (mostly) use the default hooks.  */
+
+/* All use of segmentation is assumed to make address 0 valid.  */
+
+static bool
+ix86_addr_space_zero_address_valid (addr_space_t as)
+{
+  return as != ADDR_SPACE_GENERIC;
+}
+
+static void
+ix86_init_libfuncs (void)
+{
+  if (TARGET_64BIT)
+    {
+      set_optab_libfunc (sdivmod_optab, TImode, "__divmodti4");
+      set_optab_libfunc (udivmod_optab, TImode, "__udivmodti4");
+    }
+  else
+    {
+      set_optab_libfunc (sdivmod_optab, DImode, "__divmoddi4");
+      set_optab_libfunc (udivmod_optab, DImode, "__udivmoddi4");
+    }
+
+#if TARGET_MACHO
+  darwin_rename_builtins ();
+#endif
+}
+
+/* Set the value of FLT_EVAL_METHOD in float.h.  When using only the
+   FPU, assume that the fpcw is set to extended precision; when using
+   only SSE, rounding is correct; when using both SSE and the FPU,
+   the rounding precision is indeterminate, since either may be chosen
+   apparently at random.  */
+
+static enum flt_eval_method
+ix86_get_excess_precision (enum excess_precision_type type)
+{
+  switch (type)
+    {
+      case EXCESS_PRECISION_TYPE_FAST:
+	/* The fastest type to promote to will always be the native type,
+	   whether that occurs with implicit excess precision or
+	   otherwise.  */
+	return FLT_EVAL_METHOD_PROMOTE_TO_FLOAT;
+      case EXCESS_PRECISION_TYPE_STANDARD:
+      case EXCESS_PRECISION_TYPE_IMPLICIT:
+	/* Otherwise, the excess precision we want when we are
+	   in a standards compliant mode, and the implicit precision we
+	   provide would be identical were it not for the unpredictable
+	   cases.  */
+	if (!TARGET_80387)
+	  return FLT_EVAL_METHOD_PROMOTE_TO_FLOAT;
+	else if (!TARGET_MIX_SSE_I387)
+	  {
+	    if (!(TARGET_SSE && TARGET_SSE_MATH))
+	      return FLT_EVAL_METHOD_PROMOTE_TO_LONG_DOUBLE;
+	    else if (TARGET_SSE2)
+	      return FLT_EVAL_METHOD_PROMOTE_TO_FLOAT;
+	  }
+
+	/* If we are in standards compliant mode, but we know we will
+	   calculate in unpredictable precision, return
+	   FLT_EVAL_METHOD_FLOAT.  There is no reason to introduce explicit
+	   excess precision if the target can't guarantee it will honor
+	   it.  */
+	return (type == EXCESS_PRECISION_TYPE_STANDARD
+		? FLT_EVAL_METHOD_PROMOTE_TO_FLOAT
+		: FLT_EVAL_METHOD_UNPREDICTABLE);
+      default:
+	gcc_unreachable ();
+    }
+
+  return FLT_EVAL_METHOD_UNPREDICTABLE;
+}
+
+/* Implement PUSH_ROUNDING.  On 386, we have pushw instruction that
+   decrements by exactly 2 no matter what the position was, there is no pushb.
+
+   But as CIE data alignment factor on this arch is -4 for 32bit targets
+   and -8 for 64bit targets, we need to make sure all stack pointer adjustments
+   are in multiple of 4 for 32bit targets and 8 for 64bit targets.  */
+
+poly_int64
+ix86_push_rounding (poly_int64 bytes)
+{
+  return ROUND_UP (bytes, UNITS_PER_WORD);
+}
+
+/* Target-specific selftests.  */
+
+#if CHECKING_P
+
+namespace selftest {
+
+/* Verify that hard regs are dumped as expected (in compact mode).  */
+
+static void
+ix86_test_dumping_hard_regs ()
+{
+  ASSERT_RTL_DUMP_EQ ("(reg:SI ax)", gen_raw_REG (SImode, 0));
+  ASSERT_RTL_DUMP_EQ ("(reg:SI dx)", gen_raw_REG (SImode, 1));
+}
+
+/* Test dumping an insn with repeated references to the same SCRATCH,
+   to verify the rtx_reuse code.  */
+
+static void
+ix86_test_dumping_memory_blockage ()
+{
+  set_new_first_and_last_insn (NULL, NULL);
+
+  rtx pat = gen_memory_blockage ();
+  rtx_reuse_manager r;
+  r.preprocess (pat);
+
+  /* Verify that the repeated references to the SCRATCH show use
+     reuse IDS.  The first should be prefixed with a reuse ID,
+     and the second should be dumped as a "reuse_rtx" of that ID.
+     The expected string assumes Pmode == DImode.  */
+  if (Pmode == DImode)
+    ASSERT_RTL_DUMP_EQ_WITH_REUSE
+      ("(cinsn 1 (set (mem/v:BLK (0|scratch:DI) [0  A8])\n"
+       "        (unspec:BLK [\n"
+       "                (mem/v:BLK (reuse_rtx 0) [0  A8])\n"
+       "            ] UNSPEC_MEMORY_BLOCKAGE)))\n", pat, &r);
+}
+
+/* Verify loading an RTL dump; specifically a dump of copying
+   a param on x86_64 from a hard reg into the frame.
+   This test is target-specific since the dump contains target-specific
+   hard reg names.  */
+
+static void
+ix86_test_loading_dump_fragment_1 ()
+{
+  rtl_dump_test t (SELFTEST_LOCATION,
+		   locate_file ("x86_64/copy-hard-reg-into-frame.rtl"));
+
+  rtx_insn *insn = get_insn_by_uid (1);
+
+  /* The block structure and indentation here is purely for
+     readability; it mirrors the structure of the rtx.  */
+  tree mem_expr;
+  {
+    rtx pat = PATTERN (insn);
+    ASSERT_EQ (SET, GET_CODE (pat));
+    {
+      rtx dest = SET_DEST (pat);
+      ASSERT_EQ (MEM, GET_CODE (dest));
+      /* Verify the "/c" was parsed.  */
+      ASSERT_TRUE (RTX_FLAG (dest, call));
+      ASSERT_EQ (SImode, GET_MODE (dest));
+      {
+	rtx addr = XEXP (dest, 0);
+	ASSERT_EQ (PLUS, GET_CODE (addr));
+	ASSERT_EQ (DImode, GET_MODE (addr));
+	{
+	  rtx lhs = XEXP (addr, 0);
+	  /* Verify that the "frame" REG was consolidated.  */
+	  ASSERT_RTX_PTR_EQ (frame_pointer_rtx, lhs);
+	}
+	{
+	  rtx rhs = XEXP (addr, 1);
+	  ASSERT_EQ (CONST_INT, GET_CODE (rhs));
+	  ASSERT_EQ (-4, INTVAL (rhs));
+	}
+      }
+      /* Verify the "[1 i+0 S4 A32]" was parsed.  */
+      ASSERT_EQ (1, MEM_ALIAS_SET (dest));
+      /* "i" should have been handled by synthesizing a global int
+	 variable named "i".  */
+      mem_expr = MEM_EXPR (dest);
+      ASSERT_NE (mem_expr, NULL);
+      ASSERT_EQ (VAR_DECL, TREE_CODE (mem_expr));
+      ASSERT_EQ (integer_type_node, TREE_TYPE (mem_expr));
+      ASSERT_EQ (IDENTIFIER_NODE, TREE_CODE (DECL_NAME (mem_expr)));
+      ASSERT_STREQ ("i", IDENTIFIER_POINTER (DECL_NAME (mem_expr)));
+      /* "+0".  */
+      ASSERT_TRUE (MEM_OFFSET_KNOWN_P (dest));
+      ASSERT_EQ (0, MEM_OFFSET (dest));
+      /* "S4".  */
+      ASSERT_EQ (4, MEM_SIZE (dest));
+      /* "A32.  */
+      ASSERT_EQ (32, MEM_ALIGN (dest));
+    }
+    {
+      rtx src = SET_SRC (pat);
+      ASSERT_EQ (REG, GET_CODE (src));
+      ASSERT_EQ (SImode, GET_MODE (src));
+      ASSERT_EQ (5, REGNO (src));
+      tree reg_expr = REG_EXPR (src);
+      /* "i" here should point to the same var as for the MEM_EXPR.  */
+      ASSERT_EQ (reg_expr, mem_expr);
+    }
+  }
+}
+
+/* Verify that the RTL loader copes with a call_insn dump.
+   This test is target-specific since the dump contains a target-specific
+   hard reg name.  */
+
+static void
+ix86_test_loading_call_insn ()
+{
+  /* The test dump includes register "xmm0", where requires TARGET_SSE
+     to exist.  */
+  if (!TARGET_SSE)
+    return;
+
+  rtl_dump_test t (SELFTEST_LOCATION, locate_file ("x86_64/call-insn.rtl"));
+
+  rtx_insn *insn = get_insns ();
+  ASSERT_EQ (CALL_INSN, GET_CODE (insn));
+
+  /* "/j".  */
+  ASSERT_TRUE (RTX_FLAG (insn, jump));
+
+  rtx pat = PATTERN (insn);
+  ASSERT_EQ (CALL, GET_CODE (SET_SRC (pat)));
+
+  /* Verify REG_NOTES.  */
+  {
+    /* "(expr_list:REG_CALL_DECL".   */
+    ASSERT_EQ (EXPR_LIST, GET_CODE (REG_NOTES (insn)));
+    rtx_expr_list *note0 = as_a <rtx_expr_list *> (REG_NOTES (insn));
+    ASSERT_EQ (REG_CALL_DECL, REG_NOTE_KIND (note0));
+
+    /* "(expr_list:REG_EH_REGION (const_int 0 [0])".  */
+    rtx_expr_list *note1 = note0->next ();
+    ASSERT_EQ (REG_EH_REGION, REG_NOTE_KIND (note1));
+
+    ASSERT_EQ (NULL, note1->next ());
+  }
+
+  /* Verify CALL_INSN_FUNCTION_USAGE.  */
+  {
+    /* "(expr_list:DF (use (reg:DF 21 xmm0))".  */
+    rtx_expr_list *usage
+      = as_a <rtx_expr_list *> (CALL_INSN_FUNCTION_USAGE (insn));
+    ASSERT_EQ (EXPR_LIST, GET_CODE (usage));
+    ASSERT_EQ (DFmode, GET_MODE (usage));
+    ASSERT_EQ (USE, GET_CODE (usage->element ()));
+    ASSERT_EQ (NULL, usage->next ());
+  }
+}
+
+/* Verify that the RTL loader copes a dump from print_rtx_function.
+   This test is target-specific since the dump contains target-specific
+   hard reg names.  */
+
+static void
+ix86_test_loading_full_dump ()
+{
+  rtl_dump_test t (SELFTEST_LOCATION, locate_file ("x86_64/times-two.rtl"));
+
+  ASSERT_STREQ ("times_two", IDENTIFIER_POINTER (DECL_NAME (cfun->decl)));
+
+  rtx_insn *insn_1 = get_insn_by_uid (1);
+  ASSERT_EQ (NOTE, GET_CODE (insn_1));
+
+  rtx_insn *insn_7 = get_insn_by_uid (7);
+  ASSERT_EQ (INSN, GET_CODE (insn_7));
+  ASSERT_EQ (PARALLEL, GET_CODE (PATTERN (insn_7)));
+
+  rtx_insn *insn_15 = get_insn_by_uid (15);
+  ASSERT_EQ (INSN, GET_CODE (insn_15));
+  ASSERT_EQ (USE, GET_CODE (PATTERN (insn_15)));
+
+  /* Verify crtl->return_rtx.  */
+  ASSERT_EQ (REG, GET_CODE (crtl->return_rtx));
+  ASSERT_EQ (0, REGNO (crtl->return_rtx));
+  ASSERT_EQ (SImode, GET_MODE (crtl->return_rtx));
+}
+
+/* Verify that the RTL loader copes with UNSPEC and UNSPEC_VOLATILE insns.
+   In particular, verify that it correctly loads the 2nd operand.
+   This test is target-specific since these are machine-specific
+   operands (and enums).  */
+
+static void
+ix86_test_loading_unspec ()
+{
+  rtl_dump_test t (SELFTEST_LOCATION, locate_file ("x86_64/unspec.rtl"));
+
+  ASSERT_STREQ ("test_unspec", IDENTIFIER_POINTER (DECL_NAME (cfun->decl)));
+
+  ASSERT_TRUE (cfun);
+
+  /* Test of an UNSPEC.  */
+   rtx_insn *insn = get_insns ();
+  ASSERT_EQ (INSN, GET_CODE (insn));
+  rtx set = single_set (insn);
+  ASSERT_NE (NULL, set);
+  rtx dst = SET_DEST (set);
+  ASSERT_EQ (MEM, GET_CODE (dst));
+  rtx src = SET_SRC (set);
+  ASSERT_EQ (UNSPEC, GET_CODE (src));
+  ASSERT_EQ (BLKmode, GET_MODE (src));
+  ASSERT_EQ (UNSPEC_MEMORY_BLOCKAGE, XINT (src, 1));
+
+  rtx v0 = XVECEXP (src, 0, 0);
+
+  /* Verify that the two uses of the first SCRATCH have pointer
+     equality.  */
+  rtx scratch_a = XEXP (dst, 0);
+  ASSERT_EQ (SCRATCH, GET_CODE (scratch_a));
+
+  rtx scratch_b = XEXP (v0, 0);
+  ASSERT_EQ (SCRATCH, GET_CODE (scratch_b));
+
+  ASSERT_EQ (scratch_a, scratch_b);
+
+  /* Verify that the two mems are thus treated as equal.  */
+  ASSERT_TRUE (rtx_equal_p (dst, v0));
+
+  /* Verify that the insn is recognized.  */
+  ASSERT_NE(-1, recog_memoized (insn));
+
+  /* Test of an UNSPEC_VOLATILE, which has its own enum values.  */
+  insn = NEXT_INSN (insn);
+  ASSERT_EQ (INSN, GET_CODE (insn));
+
+  set = single_set (insn);
+  ASSERT_NE (NULL, set);
+
+  src = SET_SRC (set);
+  ASSERT_EQ (UNSPEC_VOLATILE, GET_CODE (src));
+  ASSERT_EQ (UNSPECV_RDTSCP, XINT (src, 1));
+}
+
+/* Run all target-specific selftests.  */
+
+static void
+ix86_run_selftests (void)
+{
+  ix86_test_dumping_hard_regs ();
+  ix86_test_dumping_memory_blockage ();
+
+  /* Various tests of loading RTL dumps, here because they contain
+     ix86-isms (e.g. names of hard regs).  */
+  ix86_test_loading_dump_fragment_1 ();
+  ix86_test_loading_call_insn ();
+  ix86_test_loading_full_dump ();
+  ix86_test_loading_unspec ();
+}
+
+} // namespace selftest
+
+#endif /* CHECKING_P */
+
+/* Initialize the GCC target structure.  */
+#undef TARGET_RETURN_IN_MEMORY
+#define TARGET_RETURN_IN_MEMORY ix86_return_in_memory
+
+#undef TARGET_LEGITIMIZE_ADDRESS
+#define TARGET_LEGITIMIZE_ADDRESS ix86_legitimize_address
+
+#undef TARGET_ATTRIBUTE_TABLE
+#define TARGET_ATTRIBUTE_TABLE ix86_attribute_table
+#undef TARGET_FUNCTION_ATTRIBUTE_INLINABLE_P
+#define TARGET_FUNCTION_ATTRIBUTE_INLINABLE_P hook_bool_const_tree_true
+#if TARGET_DLLIMPORT_DECL_ATTRIBUTES
+#  undef TARGET_MERGE_DECL_ATTRIBUTES
+#  define TARGET_MERGE_DECL_ATTRIBUTES merge_dllimport_decl_attributes
+#endif
+
+#undef TARGET_COMP_TYPE_ATTRIBUTES
+#define TARGET_COMP_TYPE_ATTRIBUTES ix86_comp_type_attributes
+
+#undef TARGET_INIT_BUILTINS
+#define TARGET_INIT_BUILTINS ix86_init_builtins
+#undef TARGET_BUILTIN_DECL
+#define TARGET_BUILTIN_DECL ix86_builtin_decl
+#undef TARGET_EXPAND_BUILTIN
+#define TARGET_EXPAND_BUILTIN ix86_expand_builtin
+
+#undef TARGET_VECTORIZE_BUILTIN_VECTORIZED_FUNCTION
+#define TARGET_VECTORIZE_BUILTIN_VECTORIZED_FUNCTION \
+  ix86_builtin_vectorized_function
+
+#undef TARGET_VECTORIZE_BUILTIN_GATHER
+#define TARGET_VECTORIZE_BUILTIN_GATHER ix86_vectorize_builtin_gather
+
+#undef TARGET_VECTORIZE_BUILTIN_SCATTER
+#define TARGET_VECTORIZE_BUILTIN_SCATTER ix86_vectorize_builtin_scatter
+
+#undef TARGET_BUILTIN_RECIPROCAL
+#define TARGET_BUILTIN_RECIPROCAL ix86_builtin_reciprocal
+
+#undef TARGET_ASM_FUNCTION_EPILOGUE
+#define TARGET_ASM_FUNCTION_EPILOGUE ix86_output_function_epilogue
+
+#undef TARGET_ASM_PRINT_PATCHABLE_FUNCTION_ENTRY
+#define TARGET_ASM_PRINT_PATCHABLE_FUNCTION_ENTRY \
+  ix86_print_patchable_function_entry
+
+#undef TARGET_ENCODE_SECTION_INFO
+#ifndef SUBTARGET_ENCODE_SECTION_INFO
+#define TARGET_ENCODE_SECTION_INFO ix86_encode_section_info
+#else
+#define TARGET_ENCODE_SECTION_INFO SUBTARGET_ENCODE_SECTION_INFO
+#endif
+
+#undef TARGET_ASM_OPEN_PAREN
+#define TARGET_ASM_OPEN_PAREN ""
+#undef TARGET_ASM_CLOSE_PAREN
+#define TARGET_ASM_CLOSE_PAREN ""
+
+#undef TARGET_ASM_BYTE_OP
+#define TARGET_ASM_BYTE_OP ASM_BYTE
+
+#undef TARGET_ASM_ALIGNED_HI_OP
+#define TARGET_ASM_ALIGNED_HI_OP ASM_SHORT
+#undef TARGET_ASM_ALIGNED_SI_OP
+#define TARGET_ASM_ALIGNED_SI_OP ASM_LONG
+#ifdef ASM_QUAD
+#undef TARGET_ASM_ALIGNED_DI_OP
+#define TARGET_ASM_ALIGNED_DI_OP ASM_QUAD
+#endif
+
+#undef TARGET_PROFILE_BEFORE_PROLOGUE
+#define TARGET_PROFILE_BEFORE_PROLOGUE ix86_profile_before_prologue
+
+#undef TARGET_MANGLE_DECL_ASSEMBLER_NAME
+#define TARGET_MANGLE_DECL_ASSEMBLER_NAME ix86_mangle_decl_assembler_name
+
+#undef TARGET_ASM_UNALIGNED_HI_OP
+#define TARGET_ASM_UNALIGNED_HI_OP TARGET_ASM_ALIGNED_HI_OP
+#undef TARGET_ASM_UNALIGNED_SI_OP
+#define TARGET_ASM_UNALIGNED_SI_OP TARGET_ASM_ALIGNED_SI_OP
+#undef TARGET_ASM_UNALIGNED_DI_OP
+#define TARGET_ASM_UNALIGNED_DI_OP TARGET_ASM_ALIGNED_DI_OP
+
+#undef TARGET_PRINT_OPERAND
+#define TARGET_PRINT_OPERAND ix86_print_operand
+#undef TARGET_PRINT_OPERAND_ADDRESS
+#define TARGET_PRINT_OPERAND_ADDRESS ix86_print_operand_address
+#undef TARGET_PRINT_OPERAND_PUNCT_VALID_P
+#define TARGET_PRINT_OPERAND_PUNCT_VALID_P ix86_print_operand_punct_valid_p
+#undef TARGET_ASM_OUTPUT_ADDR_CONST_EXTRA
+#define TARGET_ASM_OUTPUT_ADDR_CONST_EXTRA i386_asm_output_addr_const_extra
+
+#undef TARGET_SCHED_INIT_GLOBAL
+#define TARGET_SCHED_INIT_GLOBAL ix86_sched_init_global
+#undef TARGET_SCHED_ADJUST_COST
+#define TARGET_SCHED_ADJUST_COST ix86_adjust_cost
+#undef TARGET_SCHED_ISSUE_RATE
+#define TARGET_SCHED_ISSUE_RATE ix86_issue_rate
+#undef TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD
+#define TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD \
+  ia32_multipass_dfa_lookahead
+#undef TARGET_SCHED_MACRO_FUSION_P
+#define TARGET_SCHED_MACRO_FUSION_P ix86_macro_fusion_p
+#undef TARGET_SCHED_MACRO_FUSION_PAIR_P
+#define TARGET_SCHED_MACRO_FUSION_PAIR_P ix86_macro_fusion_pair_p
+
+#undef TARGET_FUNCTION_OK_FOR_SIBCALL
+#define TARGET_FUNCTION_OK_FOR_SIBCALL ix86_function_ok_for_sibcall
+
+#undef TARGET_MEMMODEL_CHECK
+#define TARGET_MEMMODEL_CHECK ix86_memmodel_check
+
+#undef TARGET_ATOMIC_ASSIGN_EXPAND_FENV
+#define TARGET_ATOMIC_ASSIGN_EXPAND_FENV ix86_atomic_assign_expand_fenv
+
+#ifdef HAVE_AS_TLS
+#undef TARGET_HAVE_TLS
+#define TARGET_HAVE_TLS true
+#endif
+#undef TARGET_CANNOT_FORCE_CONST_MEM
+#define TARGET_CANNOT_FORCE_CONST_MEM ix86_cannot_force_const_mem
+#undef TARGET_USE_BLOCKS_FOR_CONSTANT_P
+#define TARGET_USE_BLOCKS_FOR_CONSTANT_P hook_bool_mode_const_rtx_true
+
+#undef TARGET_DELEGITIMIZE_ADDRESS
+#define TARGET_DELEGITIMIZE_ADDRESS ix86_delegitimize_address
+
+#undef TARGET_CONST_NOT_OK_FOR_DEBUG_P
+#define TARGET_CONST_NOT_OK_FOR_DEBUG_P ix86_const_not_ok_for_debug_p
+
+#undef TARGET_MS_BITFIELD_LAYOUT_P
+#define TARGET_MS_BITFIELD_LAYOUT_P ix86_ms_bitfield_layout_p
+
+#if TARGET_MACHO
+#undef TARGET_BINDS_LOCAL_P
+#define TARGET_BINDS_LOCAL_P darwin_binds_local_p
+#else
+#undef TARGET_BINDS_LOCAL_P
+#define TARGET_BINDS_LOCAL_P ix86_binds_local_p
+#endif
+#if TARGET_DLLIMPORT_DECL_ATTRIBUTES
+#undef TARGET_BINDS_LOCAL_P
+#define TARGET_BINDS_LOCAL_P i386_pe_binds_local_p
+#endif
+
+#undef TARGET_ASM_OUTPUT_MI_THUNK
+#define TARGET_ASM_OUTPUT_MI_THUNK x86_output_mi_thunk
+#undef TARGET_ASM_CAN_OUTPUT_MI_THUNK
+#define TARGET_ASM_CAN_OUTPUT_MI_THUNK x86_can_output_mi_thunk
+
+#undef TARGET_ASM_FILE_START
+#define TARGET_ASM_FILE_START x86_file_start
+
+#undef TARGET_OPTION_OVERRIDE
+#define TARGET_OPTION_OVERRIDE ix86_option_override
+
+#undef TARGET_REGISTER_MOVE_COST
+#define TARGET_REGISTER_MOVE_COST ix86_register_move_cost
+#undef TARGET_MEMORY_MOVE_COST
+#define TARGET_MEMORY_MOVE_COST ix86_memory_move_cost
+#undef TARGET_RTX_COSTS
+#define TARGET_RTX_COSTS ix86_rtx_costs
+#undef TARGET_ADDRESS_COST
+#define TARGET_ADDRESS_COST ix86_address_cost
+
+#undef TARGET_FLAGS_REGNUM
+#define TARGET_FLAGS_REGNUM FLAGS_REG
+#undef TARGET_FIXED_CONDITION_CODE_REGS
+#define TARGET_FIXED_CONDITION_CODE_REGS ix86_fixed_condition_code_regs
+#undef TARGET_CC_MODES_COMPATIBLE
+#define TARGET_CC_MODES_COMPATIBLE ix86_cc_modes_compatible
+
+#undef TARGET_MACHINE_DEPENDENT_REORG
+#define TARGET_MACHINE_DEPENDENT_REORG ix86_reorg
+
+#undef TARGET_BUILD_BUILTIN_VA_LIST
+#define TARGET_BUILD_BUILTIN_VA_LIST ix86_build_builtin_va_list
+
+#undef TARGET_FOLD_BUILTIN
+#define TARGET_FOLD_BUILTIN ix86_fold_builtin
+
+#undef TARGET_GIMPLE_FOLD_BUILTIN
+#define TARGET_GIMPLE_FOLD_BUILTIN ix86_gimple_fold_builtin
+
+#undef TARGET_COMPARE_VERSION_PRIORITY
+#define TARGET_COMPARE_VERSION_PRIORITY ix86_compare_version_priority
+
+#undef TARGET_GENERATE_VERSION_DISPATCHER_BODY
+#define TARGET_GENERATE_VERSION_DISPATCHER_BODY \
+  ix86_generate_version_dispatcher_body
+
+#undef TARGET_GET_FUNCTION_VERSIONS_DISPATCHER
+#define TARGET_GET_FUNCTION_VERSIONS_DISPATCHER \
+  ix86_get_function_versions_dispatcher
+
+#undef TARGET_ENUM_VA_LIST_P
+#define TARGET_ENUM_VA_LIST_P ix86_enum_va_list
+
+#undef TARGET_FN_ABI_VA_LIST
+#define TARGET_FN_ABI_VA_LIST ix86_fn_abi_va_list
+
+#undef TARGET_CANONICAL_VA_LIST_TYPE
+#define TARGET_CANONICAL_VA_LIST_TYPE ix86_canonical_va_list_type
+
+#undef TARGET_EXPAND_BUILTIN_VA_START
+#define TARGET_EXPAND_BUILTIN_VA_START ix86_va_start
+
+#undef TARGET_MD_ASM_ADJUST
+#define TARGET_MD_ASM_ADJUST ix86_md_asm_adjust
+
+#undef TARGET_C_EXCESS_PRECISION
+#define TARGET_C_EXCESS_PRECISION ix86_get_excess_precision
+#undef TARGET_PROMOTE_PROTOTYPES
+#define TARGET_PROMOTE_PROTOTYPES hook_bool_const_tree_true
+#undef TARGET_SETUP_INCOMING_VARARGS
+#define TARGET_SETUP_INCOMING_VARARGS ix86_setup_incoming_varargs
+#undef TARGET_MUST_PASS_IN_STACK
+#define TARGET_MUST_PASS_IN_STACK ix86_must_pass_in_stack
+#undef TARGET_ALLOCATE_STACK_SLOTS_FOR_ARGS
+#define TARGET_ALLOCATE_STACK_SLOTS_FOR_ARGS ix86_allocate_stack_slots_for_args
+#undef TARGET_FUNCTION_ARG_ADVANCE
+#define TARGET_FUNCTION_ARG_ADVANCE ix86_function_arg_advance
+#undef TARGET_FUNCTION_ARG
+#define TARGET_FUNCTION_ARG ix86_function_arg
+#undef TARGET_INIT_PIC_REG
+#define TARGET_INIT_PIC_REG ix86_init_pic_reg
+#undef TARGET_USE_PSEUDO_PIC_REG
+#define TARGET_USE_PSEUDO_PIC_REG ix86_use_pseudo_pic_reg
+#undef TARGET_FUNCTION_ARG_BOUNDARY
+#define TARGET_FUNCTION_ARG_BOUNDARY ix86_function_arg_boundary
+#undef TARGET_PASS_BY_REFERENCE
+#define TARGET_PASS_BY_REFERENCE ix86_pass_by_reference
+#undef TARGET_INTERNAL_ARG_POINTER
+#define TARGET_INTERNAL_ARG_POINTER ix86_internal_arg_pointer
+#undef TARGET_UPDATE_STACK_BOUNDARY
+#define TARGET_UPDATE_STACK_BOUNDARY ix86_update_stack_boundary
+#undef TARGET_GET_DRAP_RTX
+#define TARGET_GET_DRAP_RTX ix86_get_drap_rtx
+#undef TARGET_STRICT_ARGUMENT_NAMING
+#define TARGET_STRICT_ARGUMENT_NAMING hook_bool_CUMULATIVE_ARGS_true
+#undef TARGET_STATIC_CHAIN
+#define TARGET_STATIC_CHAIN ix86_static_chain
+#undef TARGET_TRAMPOLINE_INIT
+#define TARGET_TRAMPOLINE_INIT ix86_trampoline_init
+#undef TARGET_RETURN_POPS_ARGS
+#define TARGET_RETURN_POPS_ARGS ix86_return_pops_args
+
+#undef TARGET_WARN_FUNC_RETURN
+#define TARGET_WARN_FUNC_RETURN ix86_warn_func_return
+
+#undef TARGET_LEGITIMATE_COMBINED_INSN
+#define TARGET_LEGITIMATE_COMBINED_INSN ix86_legitimate_combined_insn
+
+#undef TARGET_ASAN_SHADOW_OFFSET
+#define TARGET_ASAN_SHADOW_OFFSET ix86_asan_shadow_offset
+
+#undef TARGET_GIMPLIFY_VA_ARG_EXPR
+#define TARGET_GIMPLIFY_VA_ARG_EXPR ix86_gimplify_va_arg
+
+#undef TARGET_SCALAR_MODE_SUPPORTED_P
+#define TARGET_SCALAR_MODE_SUPPORTED_P ix86_scalar_mode_supported_p
+
+#undef TARGET_VECTOR_MODE_SUPPORTED_P
+#define TARGET_VECTOR_MODE_SUPPORTED_P ix86_vector_mode_supported_p
+
+#undef TARGET_C_MODE_FOR_SUFFIX
+#define TARGET_C_MODE_FOR_SUFFIX ix86_c_mode_for_suffix
+
+#ifdef HAVE_AS_TLS
+#undef TARGET_ASM_OUTPUT_DWARF_DTPREL
+#define TARGET_ASM_OUTPUT_DWARF_DTPREL i386_output_dwarf_dtprel
+#endif
+
+#ifdef SUBTARGET_INSERT_ATTRIBUTES
+#undef TARGET_INSERT_ATTRIBUTES
+#define TARGET_INSERT_ATTRIBUTES SUBTARGET_INSERT_ATTRIBUTES
+#endif
+
+#undef TARGET_MANGLE_TYPE
+#define TARGET_MANGLE_TYPE ix86_mangle_type
+
+#undef TARGET_STACK_PROTECT_GUARD
+#define TARGET_STACK_PROTECT_GUARD ix86_stack_protect_guard
+
+#if !TARGET_MACHO
+#undef TARGET_STACK_PROTECT_FAIL
+#define TARGET_STACK_PROTECT_FAIL ix86_stack_protect_fail
+#endif
+
+#undef TARGET_FUNCTION_VALUE
+#define TARGET_FUNCTION_VALUE ix86_function_value
+
+#undef TARGET_FUNCTION_VALUE_REGNO_P
+#define TARGET_FUNCTION_VALUE_REGNO_P ix86_function_value_regno_p
+
+#undef TARGET_ZERO_CALL_USED_REGS
+#define TARGET_ZERO_CALL_USED_REGS ix86_zero_call_used_regs
+
+#undef TARGET_PROMOTE_FUNCTION_MODE
+#define TARGET_PROMOTE_FUNCTION_MODE ix86_promote_function_mode
+
+#undef  TARGET_OVERRIDE_OPTIONS_AFTER_CHANGE
+#define TARGET_OVERRIDE_OPTIONS_AFTER_CHANGE ix86_override_options_after_change
+
+#undef TARGET_MEMBER_TYPE_FORCES_BLK
+#define TARGET_MEMBER_TYPE_FORCES_BLK ix86_member_type_forces_blk
+
+#undef TARGET_INSTANTIATE_DECLS
+#define TARGET_INSTANTIATE_DECLS ix86_instantiate_decls
+
+#undef TARGET_SECONDARY_RELOAD
+#define TARGET_SECONDARY_RELOAD ix86_secondary_reload
+#undef TARGET_SECONDARY_MEMORY_NEEDED
+#define TARGET_SECONDARY_MEMORY_NEEDED ix86_secondary_memory_needed
+#undef TARGET_SECONDARY_MEMORY_NEEDED_MODE
+#define TARGET_SECONDARY_MEMORY_NEEDED_MODE ix86_secondary_memory_needed_mode
+
+#undef TARGET_CLASS_MAX_NREGS
+#define TARGET_CLASS_MAX_NREGS ix86_class_max_nregs
+
+#undef TARGET_PREFERRED_RELOAD_CLASS
+#define TARGET_PREFERRED_RELOAD_CLASS ix86_preferred_reload_class
+#undef TARGET_PREFERRED_OUTPUT_RELOAD_CLASS
+#define TARGET_PREFERRED_OUTPUT_RELOAD_CLASS ix86_preferred_output_reload_class
+#undef TARGET_CLASS_LIKELY_SPILLED_P
+#define TARGET_CLASS_LIKELY_SPILLED_P ix86_class_likely_spilled_p
+
+#undef TARGET_VECTORIZE_BUILTIN_VECTORIZATION_COST
+#define TARGET_VECTORIZE_BUILTIN_VECTORIZATION_COST \
+  ix86_builtin_vectorization_cost
+#undef TARGET_VECTORIZE_VEC_PERM_CONST
+#define TARGET_VECTORIZE_VEC_PERM_CONST ix86_vectorize_vec_perm_const
+#undef TARGET_VECTORIZE_PREFERRED_SIMD_MODE
+#define TARGET_VECTORIZE_PREFERRED_SIMD_MODE \
+  ix86_preferred_simd_mode
+#undef TARGET_VECTORIZE_SPLIT_REDUCTION
+#define TARGET_VECTORIZE_SPLIT_REDUCTION \
+  ix86_split_reduction
+#undef TARGET_VECTORIZE_AUTOVECTORIZE_VECTOR_MODES
+#define TARGET_VECTORIZE_AUTOVECTORIZE_VECTOR_MODES \
+  ix86_autovectorize_vector_modes
+#undef TARGET_VECTORIZE_GET_MASK_MODE
+#define TARGET_VECTORIZE_GET_MASK_MODE ix86_get_mask_mode
+#undef TARGET_VECTORIZE_INIT_COST
+#define TARGET_VECTORIZE_INIT_COST ix86_init_cost
+#undef TARGET_VECTORIZE_ADD_STMT_COST
+#define TARGET_VECTORIZE_ADD_STMT_COST ix86_add_stmt_cost
+#undef TARGET_VECTORIZE_FINISH_COST
+#define TARGET_VECTORIZE_FINISH_COST ix86_finish_cost
+#undef TARGET_VECTORIZE_DESTROY_COST_DATA
+#define TARGET_VECTORIZE_DESTROY_COST_DATA ix86_destroy_cost_data
+
+#undef TARGET_SET_CURRENT_FUNCTION
+#define TARGET_SET_CURRENT_FUNCTION ix86_set_current_function
+
+#undef TARGET_OPTION_VALID_ATTRIBUTE_P
+#define TARGET_OPTION_VALID_ATTRIBUTE_P ix86_valid_target_attribute_p
+
+#undef TARGET_OPTION_SAVE
+#define TARGET_OPTION_SAVE ix86_function_specific_save
+
+#undef TARGET_OPTION_RESTORE
+#define TARGET_OPTION_RESTORE ix86_function_specific_restore
+
+#undef TARGET_OPTION_POST_STREAM_IN
+#define TARGET_OPTION_POST_STREAM_IN ix86_function_specific_post_stream_in
+
+#undef TARGET_OPTION_PRINT
+#define TARGET_OPTION_PRINT ix86_function_specific_print
+
+#undef TARGET_OPTION_FUNCTION_VERSIONS
+#define TARGET_OPTION_FUNCTION_VERSIONS common_function_versions
+
+#undef TARGET_CAN_INLINE_P
+#define TARGET_CAN_INLINE_P ix86_can_inline_p
+
+#undef TARGET_LEGITIMATE_ADDRESS_P
+#define TARGET_LEGITIMATE_ADDRESS_P ix86_legitimate_address_p
+
+#undef TARGET_REGISTER_PRIORITY
+#define TARGET_REGISTER_PRIORITY ix86_register_priority
+
+#undef TARGET_REGISTER_USAGE_LEVELING_P
+#define TARGET_REGISTER_USAGE_LEVELING_P hook_bool_void_true
+
+#undef TARGET_LEGITIMATE_CONSTANT_P
+#define TARGET_LEGITIMATE_CONSTANT_P ix86_legitimate_constant_p
+
+#undef TARGET_COMPUTE_FRAME_LAYOUT
+#define TARGET_COMPUTE_FRAME_LAYOUT ix86_compute_frame_layout
+
+#undef TARGET_FRAME_POINTER_REQUIRED
+#define TARGET_FRAME_POINTER_REQUIRED ix86_frame_pointer_required
+
+#undef TARGET_CAN_ELIMINATE
+#define TARGET_CAN_ELIMINATE ix86_can_eliminate
+
+#undef TARGET_EXTRA_LIVE_ON_ENTRY
+#define TARGET_EXTRA_LIVE_ON_ENTRY ix86_live_on_entry
+
+#undef TARGET_ASM_CODE_END
+#define TARGET_ASM_CODE_END ix86_code_end
+
+#undef TARGET_CONDITIONAL_REGISTER_USAGE
+#define TARGET_CONDITIONAL_REGISTER_USAGE ix86_conditional_register_usage
+
+#undef TARGET_CANONICALIZE_COMPARISON
+#define TARGET_CANONICALIZE_COMPARISON ix86_canonicalize_comparison
+
+#undef TARGET_LOOP_UNROLL_ADJUST
+#define TARGET_LOOP_UNROLL_ADJUST ix86_loop_unroll_adjust
+
+/* Disabled due to PRs 70902, 71453, 71555, 71596 and 71657.  */
+#undef TARGET_SPILL_CLASS
+#define TARGET_SPILL_CLASS ix86_spill_class
+
+#undef TARGET_SIMD_CLONE_COMPUTE_VECSIZE_AND_SIMDLEN
+#define TARGET_SIMD_CLONE_COMPUTE_VECSIZE_AND_SIMDLEN \
+  ix86_simd_clone_compute_vecsize_and_simdlen
+
+#undef TARGET_SIMD_CLONE_ADJUST
+#define TARGET_SIMD_CLONE_ADJUST ix86_simd_clone_adjust
+
+#undef TARGET_SIMD_CLONE_USABLE
+#define TARGET_SIMD_CLONE_USABLE ix86_simd_clone_usable
+
+#undef TARGET_OMP_DEVICE_KIND_ARCH_ISA
+#define TARGET_OMP_DEVICE_KIND_ARCH_ISA ix86_omp_device_kind_arch_isa
+
+#undef TARGET_FLOAT_EXCEPTIONS_ROUNDING_SUPPORTED_P
+#define TARGET_FLOAT_EXCEPTIONS_ROUNDING_SUPPORTED_P \
+  ix86_float_exceptions_rounding_supported_p
+
+#undef TARGET_MODE_EMIT
+#define TARGET_MODE_EMIT ix86_emit_mode_set
+
+#undef TARGET_MODE_NEEDED
+#define TARGET_MODE_NEEDED ix86_mode_needed
+
+#undef TARGET_MODE_AFTER
+#define TARGET_MODE_AFTER ix86_mode_after
+
+#undef TARGET_MODE_ENTRY
+#define TARGET_MODE_ENTRY ix86_mode_entry
+
+#undef TARGET_MODE_EXIT
+#define TARGET_MODE_EXIT ix86_mode_exit
+
+#undef TARGET_MODE_PRIORITY
+#define TARGET_MODE_PRIORITY ix86_mode_priority
+
+#undef TARGET_CALL_FUSAGE_CONTAINS_NON_CALLEE_CLOBBERS
+#define TARGET_CALL_FUSAGE_CONTAINS_NON_CALLEE_CLOBBERS true
+
+#undef TARGET_OFFLOAD_OPTIONS
+#define TARGET_OFFLOAD_OPTIONS \
+  ix86_offload_options
+
+#undef TARGET_ABSOLUTE_BIGGEST_ALIGNMENT
+#define TARGET_ABSOLUTE_BIGGEST_ALIGNMENT 512
+
+#undef TARGET_OPTAB_SUPPORTED_P
+#define TARGET_OPTAB_SUPPORTED_P ix86_optab_supported_p
+
+#undef TARGET_HARD_REGNO_SCRATCH_OK
+#define TARGET_HARD_REGNO_SCRATCH_OK ix86_hard_regno_scratch_ok
+
+#undef TARGET_CUSTOM_FUNCTION_DESCRIPTORS
+#define TARGET_CUSTOM_FUNCTION_DESCRIPTORS 1
+
+#undef TARGET_ADDR_SPACE_ZERO_ADDRESS_VALID
+#define TARGET_ADDR_SPACE_ZERO_ADDRESS_VALID ix86_addr_space_zero_address_valid
+
+#undef TARGET_INIT_LIBFUNCS
+#define TARGET_INIT_LIBFUNCS ix86_init_libfuncs
+
+#undef TARGET_EXPAND_DIVMOD_LIBFUNC
+#define TARGET_EXPAND_DIVMOD_LIBFUNC ix86_expand_divmod_libfunc
+
+#undef TARGET_MAX_NOCE_IFCVT_SEQ_COST
+#define TARGET_MAX_NOCE_IFCVT_SEQ_COST ix86_max_noce_ifcvt_seq_cost
+
+#undef TARGET_NOCE_CONVERSION_PROFITABLE_P
+#define TARGET_NOCE_CONVERSION_PROFITABLE_P ix86_noce_conversion_profitable_p
+
+#undef TARGET_HARD_REGNO_NREGS
+#define TARGET_HARD_REGNO_NREGS ix86_hard_regno_nregs
+#undef TARGET_HARD_REGNO_MODE_OK
+#define TARGET_HARD_REGNO_MODE_OK ix86_hard_regno_mode_ok
+
+#undef TARGET_MODES_TIEABLE_P
+#define TARGET_MODES_TIEABLE_P ix86_modes_tieable_p
+
+#undef TARGET_HARD_REGNO_CALL_PART_CLOBBERED
+#define TARGET_HARD_REGNO_CALL_PART_CLOBBERED \
+  ix86_hard_regno_call_part_clobbered
+
+#undef TARGET_CAN_CHANGE_MODE_CLASS
+#define TARGET_CAN_CHANGE_MODE_CLASS ix86_can_change_mode_class
+
+#undef TARGET_LOWER_LOCAL_DECL_ALIGNMENT
+#define TARGET_LOWER_LOCAL_DECL_ALIGNMENT ix86_lower_local_decl_alignment
+
+#undef TARGET_STATIC_RTX_ALIGNMENT
+#define TARGET_STATIC_RTX_ALIGNMENT ix86_static_rtx_alignment
+#undef TARGET_CONSTANT_ALIGNMENT
+#define TARGET_CONSTANT_ALIGNMENT ix86_constant_alignment
+
+#undef TARGET_EMPTY_RECORD_P
+#define TARGET_EMPTY_RECORD_P ix86_is_empty_record
+
+#undef TARGET_WARN_PARAMETER_PASSING_ABI
+#define TARGET_WARN_PARAMETER_PASSING_ABI ix86_warn_parameter_passing_abi
+
+#undef TARGET_GET_MULTILIB_ABI_NAME
+#define TARGET_GET_MULTILIB_ABI_NAME \
+  ix86_get_multilib_abi_name
+
+static bool ix86_libc_has_fast_function (int fcode ATTRIBUTE_UNUSED)
+{
+#ifdef OPTION_GLIBC
+  if (OPTION_GLIBC)
+    return (built_in_function)fcode == BUILT_IN_MEMPCPY;
+  else
+    return false;
+#else
+  return false;
+#endif
+}
+
+#undef TARGET_LIBC_HAS_FAST_FUNCTION
+#define TARGET_LIBC_HAS_FAST_FUNCTION ix86_libc_has_fast_function
+
+#if CHECKING_P
+#undef TARGET_RUN_TARGET_SELFTESTS
+#define TARGET_RUN_TARGET_SELFTESTS selftest::ix86_run_selftests
+#endif /* #if CHECKING_P */
+
+struct gcc_target targetm = TARGET_INITIALIZER;
+
+#include "gt-i386.h"
diff -uNr gcc-11.2.0.orig/gcc/doc/tm.texi gcc-11.2.0/gcc/doc/tm.texi
--- gcc-11.2.0.orig/gcc/doc/tm.texi	2021-07-28 01:55:07.252286856 -0500
+++ gcc-11.2.0/gcc/doc/tm.texi	2021-08-06 00:34:01.879709574 -0500
@@ -6767,6 +6767,11 @@
 move would be greater than that of a library call.
 @end deftypefn
 
+@deftypefn {Target Hook} rtx TARGET_CLEAR_BY_PIECES (void *@var{}, void *@var{}, @var{HOST_WIDE_INT}, @var{scalar_int_mode})
+This target hook should return an rtx that is used to clear memory.
+The default returns @code{const0_rtx}.
+@end deftypefn
+
 @deftypefn {Target Hook} int TARGET_COMPARE_BY_PIECES_BRANCH_RATIO (machine_mode @var{mode})
 When expanding a block comparison in MODE, gcc can try to reduce the
 number of branches at the expense of more memory operations.  This hook
diff -uNr gcc-11.2.0.orig/gcc/doc/tm.texi.in gcc-11.2.0/gcc/doc/tm.texi.in
--- gcc-11.2.0.orig/gcc/doc/tm.texi.in	2021-07-28 01:55:07.252286856 -0500
+++ gcc-11.2.0/gcc/doc/tm.texi.in	2021-08-06 00:34:01.881709574 -0500
@@ -4588,6 +4588,8 @@
 
 @hook TARGET_USE_BY_PIECES_INFRASTRUCTURE_P
 
+@hook TARGET_CLEAR_BY_PIECES
+
 @hook TARGET_COMPARE_BY_PIECES_BRANCH_RATIO
 
 @defmac MOVE_MAX_PIECES
diff -uNr gcc-11.2.0.orig/gcc/doc/tm.texi.in.orig gcc-11.2.0/gcc/doc/tm.texi.in.orig
--- gcc-11.2.0.orig/gcc/doc/tm.texi.in.orig	1969-12-31 18:00:00.000000000 -0600
+++ gcc-11.2.0/gcc/doc/tm.texi.in.orig	2021-07-28 01:55:07.252286856 -0500
@@ -0,0 +1,8222 @@
+@c Copyright (C) 1988-2021 Free Software Foundation, Inc.
+@c This is part of the GCC manual.
+@c For copying conditions, see the file gcc.texi.
+
+@node Target Macros
+@chapter Target Description Macros and Functions
+@cindex machine description macros
+@cindex target description macros
+@cindex macros, target description
+@cindex @file{tm.h} macros
+
+In addition to the file @file{@var{machine}.md}, a machine description
+includes a C header file conventionally given the name
+@file{@var{machine}.h} and a C source file named @file{@var{machine}.c}.
+The header file defines numerous macros that convey the information
+about the target machine that does not fit into the scheme of the
+@file{.md} file.  The file @file{tm.h} should be a link to
+@file{@var{machine}.h}.  The header file @file{config.h} includes
+@file{tm.h} and most compiler source files include @file{config.h}.  The
+source file defines a variable @code{targetm}, which is a structure
+containing pointers to functions and data relating to the target
+machine.  @file{@var{machine}.c} should also contain their definitions,
+if they are not defined elsewhere in GCC, and other functions called
+through the macros defined in the @file{.h} file.
+
+@menu
+* Target Structure::    The @code{targetm} variable.
+* Driver::              Controlling how the driver runs the compilation passes.
+* Run-time Target::     Defining @samp{-m} options like @option{-m68000} and @option{-m68020}.
+* Per-Function Data::   Defining data structures for per-function information.
+* Storage Layout::      Defining sizes and alignments of data.
+* Type Layout::         Defining sizes and properties of basic user data types.
+* Registers::           Naming and describing the hardware registers.
+* Register Classes::    Defining the classes of hardware registers.
+* Stack and Calling::   Defining which way the stack grows and by how much.
+* Varargs::             Defining the varargs macros.
+* Trampolines::         Code set up at run time to enter a nested function.
+* Library Calls::       Controlling how library routines are implicitly called.
+* Addressing Modes::    Defining addressing modes valid for memory operands.
+* Anchored Addresses::  Defining how @option{-fsection-anchors} should work.
+* Condition Code::      Defining how insns update the condition code.
+* Costs::               Defining relative costs of different operations.
+* Scheduling::          Adjusting the behavior of the instruction scheduler.
+* Sections::            Dividing storage into text, data, and other sections.
+* PIC::                 Macros for position independent code.
+* Assembler Format::    Defining how to write insns and pseudo-ops to output.
+* Debugging Info::      Defining the format of debugging output.
+* Floating Point::      Handling floating point for cross-compilers.
+* Mode Switching::      Insertion of mode-switching instructions.
+* Target Attributes::   Defining target-specific uses of @code{__attribute__}.
+* Emulated TLS::        Emulated TLS support.
+* MIPS Coprocessors::   MIPS coprocessor support and how to customize it.
+* PCH Target::          Validity checking for precompiled headers.
+* C++ ABI::             Controlling C++ ABI changes.
+* D Language and ABI::  Controlling D ABI changes.
+* Named Address Spaces:: Adding support for named address spaces
+* Misc::                Everything else.
+@end menu
+
+@node Target Structure
+@section The Global @code{targetm} Variable
+@cindex target hooks
+@cindex target functions
+
+@deftypevar {struct gcc_target} targetm
+The target @file{.c} file must define the global @code{targetm} variable
+which contains pointers to functions and data relating to the target
+machine.  The variable is declared in @file{target.h};
+@file{target-def.h} defines the macro @code{TARGET_INITIALIZER} which is
+used to initialize the variable, and macros for the default initializers
+for elements of the structure.  The @file{.c} file should override those
+macros for which the default definition is inappropriate.  For example:
+@smallexample
+#include "target.h"
+#include "target-def.h"
+
+/* @r{Initialize the GCC target structure.}  */
+
+#undef TARGET_COMP_TYPE_ATTRIBUTES
+#define TARGET_COMP_TYPE_ATTRIBUTES @var{machine}_comp_type_attributes
+
+struct gcc_target targetm = TARGET_INITIALIZER;
+@end smallexample
+@end deftypevar
+
+Where a macro should be defined in the @file{.c} file in this manner to
+form part of the @code{targetm} structure, it is documented below as a
+``Target Hook'' with a prototype.  Many macros will change in future
+from being defined in the @file{.h} file to being part of the
+@code{targetm} structure.
+
+Similarly, there is a @code{targetcm} variable for hooks that are
+specific to front ends for C-family languages, documented as ``C
+Target Hook''.  This is declared in @file{c-family/c-target.h}, the
+initializer @code{TARGETCM_INITIALIZER} in
+@file{c-family/c-target-def.h}.  If targets initialize @code{targetcm}
+themselves, they should set @code{target_has_targetcm=yes} in
+@file{config.gcc}; otherwise a default definition is used.
+
+Similarly, there is a @code{targetm_common} variable for hooks that
+are shared between the compiler driver and the compilers proper,
+documented as ``Common Target Hook''.  This is declared in
+@file{common/common-target.h}, the initializer
+@code{TARGETM_COMMON_INITIALIZER} in
+@file{common/common-target-def.h}.  If targets initialize
+@code{targetm_common} themselves, they should set
+@code{target_has_targetm_common=yes} in @file{config.gcc}; otherwise a
+default definition is used.
+
+Similarly, there is a @code{targetdm} variable for hooks that are
+specific to the D language front end, documented as ``D Target Hook''.
+This is declared in @file{d/d-target.h}, the initializer
+@code{TARGETDM_INITIALIZER} in @file{d/d-target-def.h}.  If targets
+initialize @code{targetdm} themselves, they should set
+@code{target_has_targetdm=yes} in @file{config.gcc}; otherwise a default
+definition is used.
+
+@node Driver
+@section Controlling the Compilation Driver, @file{gcc}
+@cindex driver
+@cindex controlling the compilation driver
+
+@c prevent bad page break with this line
+You can control the compilation driver.
+
+@defmac DRIVER_SELF_SPECS
+A list of specs for the driver itself.  It should be a suitable
+initializer for an array of strings, with no surrounding braces.
+
+The driver applies these specs to its own command line between loading
+default @file{specs} files (but not command-line specified ones) and
+choosing the multilib directory or running any subcommands.  It
+applies them in the order given, so each spec can depend on the
+options added by earlier ones.  It is also possible to remove options
+using @samp{%<@var{option}} in the usual way.
+
+This macro can be useful when a port has several interdependent target
+options.  It provides a way of standardizing the command line so
+that the other specs are easier to write.
+
+Do not define this macro if it does not need to do anything.
+@end defmac
+
+@defmac OPTION_DEFAULT_SPECS
+A list of specs used to support configure-time default options (i.e.@:
+@option{--with} options) in the driver.  It should be a suitable initializer
+for an array of structures, each containing two strings, without the
+outermost pair of surrounding braces.
+
+The first item in the pair is the name of the default.  This must match
+the code in @file{config.gcc} for the target.  The second item is a spec
+to apply if a default with this name was specified.  The string
+@samp{%(VALUE)} in the spec will be replaced by the value of the default
+everywhere it occurs.
+
+The driver will apply these specs to its own command line between loading
+default @file{specs} files and processing @code{DRIVER_SELF_SPECS}, using
+the same mechanism as @code{DRIVER_SELF_SPECS}.
+
+Do not define this macro if it does not need to do anything.
+@end defmac
+
+@defmac CPP_SPEC
+A C string constant that tells the GCC driver program options to
+pass to CPP@.  It can also specify how to translate options you
+give to GCC into options for GCC to pass to the CPP@.
+
+Do not define this macro if it does not need to do anything.
+@end defmac
+
+@defmac CPLUSPLUS_CPP_SPEC
+This macro is just like @code{CPP_SPEC}, but is used for C++, rather
+than C@.  If you do not define this macro, then the value of
+@code{CPP_SPEC} (if any) will be used instead.
+@end defmac
+
+@defmac CC1_SPEC
+A C string constant that tells the GCC driver program options to
+pass to @code{cc1}, @code{cc1plus}, @code{f771}, and the other language
+front ends.
+It can also specify how to translate options you give to GCC into options
+for GCC to pass to front ends.
+
+Do not define this macro if it does not need to do anything.
+@end defmac
+
+@defmac CC1PLUS_SPEC
+A C string constant that tells the GCC driver program options to
+pass to @code{cc1plus}.  It can also specify how to translate options you
+give to GCC into options for GCC to pass to the @code{cc1plus}.
+
+Do not define this macro if it does not need to do anything.
+Note that everything defined in CC1_SPEC is already passed to
+@code{cc1plus} so there is no need to duplicate the contents of
+CC1_SPEC in CC1PLUS_SPEC@.
+@end defmac
+
+@defmac ASM_SPEC
+A C string constant that tells the GCC driver program options to
+pass to the assembler.  It can also specify how to translate options
+you give to GCC into options for GCC to pass to the assembler.
+See the file @file{sun3.h} for an example of this.
+
+Do not define this macro if it does not need to do anything.
+@end defmac
+
+@defmac ASM_FINAL_SPEC
+A C string constant that tells the GCC driver program how to
+run any programs which cleanup after the normal assembler.
+Normally, this is not needed.  See the file @file{mips.h} for
+an example of this.
+
+Do not define this macro if it does not need to do anything.
+@end defmac
+
+@defmac AS_NEEDS_DASH_FOR_PIPED_INPUT
+Define this macro, with no value, if the driver should give the assembler
+an argument consisting of a single dash, @option{-}, to instruct it to
+read from its standard input (which will be a pipe connected to the
+output of the compiler proper).  This argument is given after any
+@option{-o} option specifying the name of the output file.
+
+If you do not define this macro, the assembler is assumed to read its
+standard input if given no non-option arguments.  If your assembler
+cannot read standard input at all, use a @samp{%@{pipe:%e@}} construct;
+see @file{mips.h} for instance.
+@end defmac
+
+@defmac LINK_SPEC
+A C string constant that tells the GCC driver program options to
+pass to the linker.  It can also specify how to translate options you
+give to GCC into options for GCC to pass to the linker.
+
+Do not define this macro if it does not need to do anything.
+@end defmac
+
+@defmac LIB_SPEC
+Another C string constant used much like @code{LINK_SPEC}.  The difference
+between the two is that @code{LIB_SPEC} is used at the end of the
+command given to the linker.
+
+If this macro is not defined, a default is provided that
+loads the standard C library from the usual place.  See @file{gcc.c}.
+@end defmac
+
+@defmac LIBGCC_SPEC
+Another C string constant that tells the GCC driver program
+how and when to place a reference to @file{libgcc.a} into the
+linker command line.  This constant is placed both before and after
+the value of @code{LIB_SPEC}.
+
+If this macro is not defined, the GCC driver provides a default that
+passes the string @option{-lgcc} to the linker.
+@end defmac
+
+@defmac REAL_LIBGCC_SPEC
+By default, if @code{ENABLE_SHARED_LIBGCC} is defined, the
+@code{LIBGCC_SPEC} is not directly used by the driver program but is
+instead modified to refer to different versions of @file{libgcc.a}
+depending on the values of the command line flags @option{-static},
+@option{-shared}, @option{-static-libgcc}, and @option{-shared-libgcc}.  On
+targets where these modifications are inappropriate, define
+@code{REAL_LIBGCC_SPEC} instead.  @code{REAL_LIBGCC_SPEC} tells the
+driver how to place a reference to @file{libgcc} on the link command
+line, but, unlike @code{LIBGCC_SPEC}, it is used unmodified.
+@end defmac
+
+@defmac USE_LD_AS_NEEDED
+A macro that controls the modifications to @code{LIBGCC_SPEC}
+mentioned in @code{REAL_LIBGCC_SPEC}.  If nonzero, a spec will be
+generated that uses @option{--as-needed} or equivalent options and the
+shared @file{libgcc} in place of the
+static exception handler library, when linking without any of
+@code{-static}, @code{-static-libgcc}, or @code{-shared-libgcc}.
+@end defmac
+
+@defmac LINK_EH_SPEC
+If defined, this C string constant is added to @code{LINK_SPEC}.
+When @code{USE_LD_AS_NEEDED} is zero or undefined, it also affects
+the modifications to @code{LIBGCC_SPEC} mentioned in
+@code{REAL_LIBGCC_SPEC}.
+@end defmac
+
+@defmac STARTFILE_SPEC
+Another C string constant used much like @code{LINK_SPEC}.  The
+difference between the two is that @code{STARTFILE_SPEC} is used at
+the very beginning of the command given to the linker.
+
+If this macro is not defined, a default is provided that loads the
+standard C startup file from the usual place.  See @file{gcc.c}.
+@end defmac
+
+@defmac ENDFILE_SPEC
+Another C string constant used much like @code{LINK_SPEC}.  The
+difference between the two is that @code{ENDFILE_SPEC} is used at
+the very end of the command given to the linker.
+
+Do not define this macro if it does not need to do anything.
+@end defmac
+
+@defmac THREAD_MODEL_SPEC
+GCC @code{-v} will print the thread model GCC was configured to use.
+However, this doesn't work on platforms that are multilibbed on thread
+models, such as AIX 4.3.  On such platforms, define
+@code{THREAD_MODEL_SPEC} such that it evaluates to a string without
+blanks that names one of the recognized thread models.  @code{%*}, the
+default value of this macro, will expand to the value of
+@code{thread_file} set in @file{config.gcc}.
+@end defmac
+
+@defmac SYSROOT_SUFFIX_SPEC
+Define this macro to add a suffix to the target sysroot when GCC is
+configured with a sysroot.  This will cause GCC to search for usr/lib,
+et al, within sysroot+suffix.
+@end defmac
+
+@defmac SYSROOT_HEADERS_SUFFIX_SPEC
+Define this macro to add a headers_suffix to the target sysroot when
+GCC is configured with a sysroot.  This will cause GCC to pass the
+updated sysroot+headers_suffix to CPP, causing it to search for
+usr/include, et al, within sysroot+headers_suffix.
+@end defmac
+
+@defmac EXTRA_SPECS
+Define this macro to provide additional specifications to put in the
+@file{specs} file that can be used in various specifications like
+@code{CC1_SPEC}.
+
+The definition should be an initializer for an array of structures,
+containing a string constant, that defines the specification name, and a
+string constant that provides the specification.
+
+Do not define this macro if it does not need to do anything.
+
+@code{EXTRA_SPECS} is useful when an architecture contains several
+related targets, which have various @code{@dots{}_SPECS} which are similar
+to each other, and the maintainer would like one central place to keep
+these definitions.
+
+For example, the PowerPC System V.4 targets use @code{EXTRA_SPECS} to
+define either @code{_CALL_SYSV} when the System V calling sequence is
+used or @code{_CALL_AIX} when the older AIX-based calling sequence is
+used.
+
+The @file{config/rs6000/rs6000.h} target file defines:
+
+@smallexample
+#define EXTRA_SPECS \
+  @{ "cpp_sysv_default", CPP_SYSV_DEFAULT @},
+
+#define CPP_SYS_DEFAULT ""
+@end smallexample
+
+The @file{config/rs6000/sysv.h} target file defines:
+@smallexample
+#undef CPP_SPEC
+#define CPP_SPEC \
+"%@{posix: -D_POSIX_SOURCE @} \
+%@{mcall-sysv: -D_CALL_SYSV @} \
+%@{!mcall-sysv: %(cpp_sysv_default) @} \
+%@{msoft-float: -D_SOFT_FLOAT@} %@{mcpu=403: -D_SOFT_FLOAT@}"
+
+#undef CPP_SYSV_DEFAULT
+#define CPP_SYSV_DEFAULT "-D_CALL_SYSV"
+@end smallexample
+
+while the @file{config/rs6000/eabiaix.h} target file defines
+@code{CPP_SYSV_DEFAULT} as:
+
+@smallexample
+#undef CPP_SYSV_DEFAULT
+#define CPP_SYSV_DEFAULT "-D_CALL_AIX"
+@end smallexample
+@end defmac
+
+@defmac LINK_LIBGCC_SPECIAL_1
+Define this macro if the driver program should find the library
+@file{libgcc.a}.  If you do not define this macro, the driver program will pass
+the argument @option{-lgcc} to tell the linker to do the search.
+@end defmac
+
+@defmac LINK_GCC_C_SEQUENCE_SPEC
+The sequence in which libgcc and libc are specified to the linker.
+By default this is @code{%G %L %G}.
+@end defmac
+
+@defmac POST_LINK_SPEC
+Define this macro to add additional steps to be executed after linker.
+The default value of this macro is empty string.
+@end defmac
+
+@defmac LINK_COMMAND_SPEC
+A C string constant giving the complete command line need to execute the
+linker.  When you do this, you will need to update your port each time a
+change is made to the link command line within @file{gcc.c}.  Therefore,
+define this macro only if you need to completely redefine the command
+line for invoking the linker and there is no other way to accomplish
+the effect you need.  Overriding this macro may be avoidable by overriding
+@code{LINK_GCC_C_SEQUENCE_SPEC} instead.
+@end defmac
+
+@hook TARGET_ALWAYS_STRIP_DOTDOT
+
+@defmac MULTILIB_DEFAULTS
+Define this macro as a C expression for the initializer of an array of
+string to tell the driver program which options are defaults for this
+target and thus do not need to be handled specially when using
+@code{MULTILIB_OPTIONS}.
+
+Do not define this macro if @code{MULTILIB_OPTIONS} is not defined in
+the target makefile fragment or if none of the options listed in
+@code{MULTILIB_OPTIONS} are set by default.
+@xref{Target Fragment}.
+@end defmac
+
+@defmac RELATIVE_PREFIX_NOT_LINKDIR
+Define this macro to tell @command{gcc} that it should only translate
+a @option{-B} prefix into a @option{-L} linker option if the prefix
+indicates an absolute file name.
+@end defmac
+
+@defmac MD_EXEC_PREFIX
+If defined, this macro is an additional prefix to try after
+@code{STANDARD_EXEC_PREFIX}.  @code{MD_EXEC_PREFIX} is not searched
+when the compiler is built as a cross
+compiler.  If you define @code{MD_EXEC_PREFIX}, then be sure to add it
+to the list of directories used to find the assembler in @file{configure.ac}.
+@end defmac
+
+@defmac STANDARD_STARTFILE_PREFIX
+Define this macro as a C string constant if you wish to override the
+standard choice of @code{libdir} as the default prefix to
+try when searching for startup files such as @file{crt0.o}.
+@code{STANDARD_STARTFILE_PREFIX} is not searched when the compiler
+is built as a cross compiler.
+@end defmac
+
+@defmac STANDARD_STARTFILE_PREFIX_1
+Define this macro as a C string constant if you wish to override the
+standard choice of @code{/lib} as a prefix to try after the default prefix
+when searching for startup files such as @file{crt0.o}.
+@code{STANDARD_STARTFILE_PREFIX_1} is not searched when the compiler
+is built as a cross compiler.
+@end defmac
+
+@defmac STANDARD_STARTFILE_PREFIX_2
+Define this macro as a C string constant if you wish to override the
+standard choice of @code{/lib} as yet another prefix to try after the
+default prefix when searching for startup files such as @file{crt0.o}.
+@code{STANDARD_STARTFILE_PREFIX_2} is not searched when the compiler
+is built as a cross compiler.
+@end defmac
+
+@defmac MD_STARTFILE_PREFIX
+If defined, this macro supplies an additional prefix to try after the
+standard prefixes.  @code{MD_EXEC_PREFIX} is not searched when the
+compiler is built as a cross compiler.
+@end defmac
+
+@defmac MD_STARTFILE_PREFIX_1
+If defined, this macro supplies yet another prefix to try after the
+standard prefixes.  It is not searched when the compiler is built as a
+cross compiler.
+@end defmac
+
+@defmac INIT_ENVIRONMENT
+Define this macro as a C string constant if you wish to set environment
+variables for programs called by the driver, such as the assembler and
+loader.  The driver passes the value of this macro to @code{putenv} to
+initialize the necessary environment variables.
+@end defmac
+
+@defmac LOCAL_INCLUDE_DIR
+Define this macro as a C string constant if you wish to override the
+standard choice of @file{/usr/local/include} as the default prefix to
+try when searching for local header files.  @code{LOCAL_INCLUDE_DIR}
+comes before @code{NATIVE_SYSTEM_HEADER_DIR} (set in
+@file{config.gcc}, normally @file{/usr/include}) in the search order.
+
+Cross compilers do not search either @file{/usr/local/include} or its
+replacement.
+@end defmac
+
+@defmac NATIVE_SYSTEM_HEADER_COMPONENT
+The ``component'' corresponding to @code{NATIVE_SYSTEM_HEADER_DIR}.
+See @code{INCLUDE_DEFAULTS}, below, for the description of components.
+If you do not define this macro, no component is used.
+@end defmac
+
+@defmac INCLUDE_DEFAULTS
+Define this macro if you wish to override the entire default search path
+for include files.  For a native compiler, the default search path
+usually consists of @code{GCC_INCLUDE_DIR}, @code{LOCAL_INCLUDE_DIR},
+@code{GPLUSPLUS_INCLUDE_DIR}, and
+@code{NATIVE_SYSTEM_HEADER_DIR}.  In addition, @code{GPLUSPLUS_INCLUDE_DIR}
+and @code{GCC_INCLUDE_DIR} are defined automatically by @file{Makefile},
+and specify private search areas for GCC@.  The directory
+@code{GPLUSPLUS_INCLUDE_DIR} is used only for C++ programs.
+
+The definition should be an initializer for an array of structures.
+Each array element should have four elements: the directory name (a
+string constant), the component name (also a string constant), a flag
+for C++-only directories,
+and a flag showing that the includes in the directory don't need to be
+wrapped in @code{extern @samp{C}} when compiling C++.  Mark the end of
+the array with a null element.
+
+The component name denotes what GNU package the include file is part of,
+if any, in all uppercase letters.  For example, it might be @samp{GCC}
+or @samp{BINUTILS}.  If the package is part of a vendor-supplied
+operating system, code the component name as @samp{0}.
+
+For example, here is the definition used for VAX/VMS:
+
+@smallexample
+#define INCLUDE_DEFAULTS \
+@{                                       \
+  @{ "GNU_GXX_INCLUDE:", "G++", 1, 1@},   \
+  @{ "GNU_CC_INCLUDE:", "GCC", 0, 0@},    \
+  @{ "SYS$SYSROOT:[SYSLIB.]", 0, 0, 0@},  \
+  @{ ".", 0, 0, 0@},                      \
+  @{ 0, 0, 0, 0@}                         \
+@}
+@end smallexample
+@end defmac
+
+Here is the order of prefixes tried for exec files:
+
+@enumerate
+@item
+Any prefixes specified by the user with @option{-B}.
+
+@item
+The environment variable @code{GCC_EXEC_PREFIX} or, if @code{GCC_EXEC_PREFIX}
+is not set and the compiler has not been installed in the configure-time
+@var{prefix}, the location in which the compiler has actually been installed.
+
+@item
+The directories specified by the environment variable @code{COMPILER_PATH}.
+
+@item
+The macro @code{STANDARD_EXEC_PREFIX}, if the compiler has been installed
+in the configured-time @var{prefix}.
+
+@item
+The location @file{/usr/libexec/gcc/}, but only if this is a native compiler.
+
+@item
+The location @file{/usr/lib/gcc/}, but only if this is a native compiler.
+
+@item
+The macro @code{MD_EXEC_PREFIX}, if defined, but only if this is a native
+compiler.
+@end enumerate
+
+Here is the order of prefixes tried for startfiles:
+
+@enumerate
+@item
+Any prefixes specified by the user with @option{-B}.
+
+@item
+The environment variable @code{GCC_EXEC_PREFIX} or its automatically determined
+value based on the installed toolchain location.
+
+@item
+The directories specified by the environment variable @code{LIBRARY_PATH}
+(or port-specific name; native only, cross compilers do not use this).
+
+@item
+The macro @code{STANDARD_EXEC_PREFIX}, but only if the toolchain is installed
+in the configured @var{prefix} or this is a native compiler.
+
+@item
+The location @file{/usr/lib/gcc/}, but only if this is a native compiler.
+
+@item
+The macro @code{MD_EXEC_PREFIX}, if defined, but only if this is a native
+compiler.
+
+@item
+The macro @code{MD_STARTFILE_PREFIX}, if defined, but only if this is a
+native compiler, or we have a target system root.
+
+@item
+The macro @code{MD_STARTFILE_PREFIX_1}, if defined, but only if this is a
+native compiler, or we have a target system root.
+
+@item
+The macro @code{STANDARD_STARTFILE_PREFIX}, with any sysroot modifications.
+If this path is relative it will be prefixed by @code{GCC_EXEC_PREFIX} and
+the machine suffix or @code{STANDARD_EXEC_PREFIX} and the machine suffix.
+
+@item
+The macro @code{STANDARD_STARTFILE_PREFIX_1}, but only if this is a native
+compiler, or we have a target system root. The default for this macro is
+@file{/lib/}.
+
+@item
+The macro @code{STANDARD_STARTFILE_PREFIX_2}, but only if this is a native
+compiler, or we have a target system root. The default for this macro is
+@file{/usr/lib/}.
+@end enumerate
+
+@node Run-time Target
+@section Run-time Target Specification
+@cindex run-time target specification
+@cindex predefined macros
+@cindex target specifications
+
+@c prevent bad page break with this line
+Here are run-time target specifications.
+
+@defmac TARGET_CPU_CPP_BUILTINS ()
+This function-like macro expands to a block of code that defines
+built-in preprocessor macros and assertions for the target CPU, using
+the functions @code{builtin_define}, @code{builtin_define_std} and
+@code{builtin_assert}.  When the front end
+calls this macro it provides a trailing semicolon, and since it has
+finished command line option processing your code can use those
+results freely.
+
+@code{builtin_assert} takes a string in the form you pass to the
+command-line option @option{-A}, such as @code{cpu=mips}, and creates
+the assertion.  @code{builtin_define} takes a string in the form
+accepted by option @option{-D} and unconditionally defines the macro.
+
+@code{builtin_define_std} takes a string representing the name of an
+object-like macro.  If it doesn't lie in the user's namespace,
+@code{builtin_define_std} defines it unconditionally.  Otherwise, it
+defines a version with two leading underscores, and another version
+with two leading and trailing underscores, and defines the original
+only if an ISO standard was not requested on the command line.  For
+example, passing @code{unix} defines @code{__unix}, @code{__unix__}
+and possibly @code{unix}; passing @code{_mips} defines @code{__mips},
+@code{__mips__} and possibly @code{_mips}, and passing @code{_ABI64}
+defines only @code{_ABI64}.
+
+You can also test for the C dialect being compiled.  The variable
+@code{c_language} is set to one of @code{clk_c}, @code{clk_cplusplus}
+or @code{clk_objective_c}.  Note that if we are preprocessing
+assembler, this variable will be @code{clk_c} but the function-like
+macro @code{preprocessing_asm_p()} will return true, so you might want
+to check for that first.  If you need to check for strict ANSI, the
+variable @code{flag_iso} can be used.  The function-like macro
+@code{preprocessing_trad_p()} can be used to check for traditional
+preprocessing.
+@end defmac
+
+@defmac TARGET_OS_CPP_BUILTINS ()
+Similarly to @code{TARGET_CPU_CPP_BUILTINS} but this macro is optional
+and is used for the target operating system instead.
+@end defmac
+
+@defmac TARGET_OBJFMT_CPP_BUILTINS ()
+Similarly to @code{TARGET_CPU_CPP_BUILTINS} but this macro is optional
+and is used for the target object format.  @file{elfos.h} uses this
+macro to define @code{__ELF__}, so you probably do not need to define
+it yourself.
+@end defmac
+
+@deftypevar {extern int} target_flags
+This variable is declared in @file{options.h}, which is included before
+any target-specific headers.
+@end deftypevar
+
+@hook TARGET_DEFAULT_TARGET_FLAGS
+This variable specifies the initial value of @code{target_flags}.
+Its default setting is 0.
+@end deftypevr
+
+@cindex optional hardware or system features
+@cindex features, optional, in system conventions
+
+@hook TARGET_HANDLE_OPTION
+This hook is called whenever the user specifies one of the
+target-specific options described by the @file{.opt} definition files
+(@pxref{Options}).  It has the opportunity to do some option-specific
+processing and should return true if the option is valid.  The default
+definition does nothing but return true.
+
+@var{decoded} specifies the option and its arguments.  @var{opts} and
+@var{opts_set} are the @code{gcc_options} structures to be used for
+storing option state, and @var{loc} is the location at which the
+option was passed (@code{UNKNOWN_LOCATION} except for options passed
+via attributes).
+@end deftypefn
+
+@hook TARGET_HANDLE_C_OPTION
+This target hook is called whenever the user specifies one of the
+target-specific C language family options described by the @file{.opt}
+definition files(@pxref{Options}).  It has the opportunity to do some
+option-specific processing and should return true if the option is
+valid.  The arguments are like for @code{TARGET_HANDLE_OPTION}.  The
+default definition does nothing but return false.
+
+In general, you should use @code{TARGET_HANDLE_OPTION} to handle
+options.  However, if processing an option requires routines that are
+only available in the C (and related language) front ends, then you
+should use @code{TARGET_HANDLE_C_OPTION} instead.
+@end deftypefn
+
+@hook TARGET_OBJC_CONSTRUCT_STRING_OBJECT
+
+@hook TARGET_OBJC_DECLARE_UNRESOLVED_CLASS_REFERENCE
+
+@hook TARGET_OBJC_DECLARE_CLASS_DEFINITION
+
+@hook TARGET_STRING_OBJECT_REF_TYPE_P
+
+@hook TARGET_CHECK_STRING_OBJECT_FORMAT_ARG
+
+@hook TARGET_OVERRIDE_OPTIONS_AFTER_CHANGE
+
+@defmac C_COMMON_OVERRIDE_OPTIONS
+This is similar to the @code{TARGET_OPTION_OVERRIDE} hook
+but is only used in the C
+language frontends (C, Objective-C, C++, Objective-C++) and so can be
+used to alter option flag variables which only exist in those
+frontends.
+@end defmac
+
+@hook TARGET_OPTION_OPTIMIZATION_TABLE
+Some machines may desire to change what optimizations are performed for
+various optimization levels.   This variable, if defined, describes
+options to enable at particular sets of optimization levels.  These
+options are processed once
+just after the optimization level is determined and before the remainder
+of the command options have been parsed, so may be overridden by other
+options passed explicitly.
+
+This processing is run once at program startup and when the optimization
+options are changed via @code{#pragma GCC optimize} or by using the
+@code{optimize} attribute.
+@end deftypevr
+
+@hook TARGET_OPTION_INIT_STRUCT
+
+@defmac SWITCHABLE_TARGET
+Some targets need to switch between substantially different subtargets
+during compilation.  For example, the MIPS target has one subtarget for
+the traditional MIPS architecture and another for MIPS16.  Source code
+can switch between these two subarchitectures using the @code{mips16}
+and @code{nomips16} attributes.
+
+Such subtargets can differ in things like the set of available
+registers, the set of available instructions, the costs of various
+operations, and so on.  GCC caches a lot of this type of information
+in global variables, and recomputing them for each subtarget takes a
+significant amount of time.  The compiler therefore provides a facility
+for maintaining several versions of the global variables and quickly
+switching between them; see @file{target-globals.h} for details.
+
+Define this macro to 1 if your target needs this facility.  The default
+is 0.
+@end defmac
+
+@hook TARGET_FLOAT_EXCEPTIONS_ROUNDING_SUPPORTED_P
+
+@node Per-Function Data
+@section Defining data structures for per-function information.
+@cindex per-function data
+@cindex data structures
+
+If the target needs to store information on a per-function basis, GCC
+provides a macro and a couple of variables to allow this.  Note, just
+using statics to store the information is a bad idea, since GCC supports
+nested functions, so you can be halfway through encoding one function
+when another one comes along.
+
+GCC defines a data structure called @code{struct function} which
+contains all of the data specific to an individual function.  This
+structure contains a field called @code{machine} whose type is
+@code{struct machine_function *}, which can be used by targets to point
+to their own specific data.
+
+If a target needs per-function specific data it should define the type
+@code{struct machine_function} and also the macro @code{INIT_EXPANDERS}.
+This macro should be used to initialize the function pointer
+@code{init_machine_status}.  This pointer is explained below.
+
+One typical use of per-function, target specific data is to create an
+RTX to hold the register containing the function's return address.  This
+RTX can then be used to implement the @code{__builtin_return_address}
+function, for level 0.
+
+Note---earlier implementations of GCC used a single data area to hold
+all of the per-function information.  Thus when processing of a nested
+function began the old per-function data had to be pushed onto a
+stack, and when the processing was finished, it had to be popped off the
+stack.  GCC used to provide function pointers called
+@code{save_machine_status} and @code{restore_machine_status} to handle
+the saving and restoring of the target specific information.  Since the
+single data area approach is no longer used, these pointers are no
+longer supported.
+
+@defmac INIT_EXPANDERS
+Macro called to initialize any target specific information.  This macro
+is called once per function, before generation of any RTL has begun.
+The intention of this macro is to allow the initialization of the
+function pointer @code{init_machine_status}.
+@end defmac
+
+@deftypevar {void (*)(struct function *)} init_machine_status
+If this function pointer is non-@code{NULL} it will be called once per
+function, before function compilation starts, in order to allow the
+target to perform any target specific initialization of the
+@code{struct function} structure.  It is intended that this would be
+used to initialize the @code{machine} of that structure.
+
+@code{struct machine_function} structures are expected to be freed by GC@.
+Generally, any memory that they reference must be allocated by using
+GC allocation, including the structure itself.
+@end deftypevar
+
+@node Storage Layout
+@section Storage Layout
+@cindex storage layout
+
+Note that the definitions of the macros in this table which are sizes or
+alignments measured in bits do not need to be constant.  They can be C
+expressions that refer to static variables, such as the @code{target_flags}.
+@xref{Run-time Target}.
+
+@defmac BITS_BIG_ENDIAN
+Define this macro to have the value 1 if the most significant bit in a
+byte has the lowest number; otherwise define it to have the value zero.
+This means that bit-field instructions count from the most significant
+bit.  If the machine has no bit-field instructions, then this must still
+be defined, but it doesn't matter which value it is defined to.  This
+macro need not be a constant.
+
+This macro does not affect the way structure fields are packed into
+bytes or words; that is controlled by @code{BYTES_BIG_ENDIAN}.
+@end defmac
+
+@defmac BYTES_BIG_ENDIAN
+Define this macro to have the value 1 if the most significant byte in a
+word has the lowest number.  This macro need not be a constant.
+@end defmac
+
+@defmac WORDS_BIG_ENDIAN
+Define this macro to have the value 1 if, in a multiword object, the
+most significant word has the lowest number.  This applies to both
+memory locations and registers; see @code{REG_WORDS_BIG_ENDIAN} if the
+order of words in memory is not the same as the order in registers.  This
+macro need not be a constant.
+@end defmac
+
+@defmac REG_WORDS_BIG_ENDIAN
+On some machines, the order of words in a multiword object differs between
+registers in memory.  In such a situation, define this macro to describe
+the order of words in a register.  The macro @code{WORDS_BIG_ENDIAN} controls
+the order of words in memory.
+@end defmac
+
+@defmac FLOAT_WORDS_BIG_ENDIAN
+Define this macro to have the value 1 if @code{DFmode}, @code{XFmode} or
+@code{TFmode} floating point numbers are stored in memory with the word
+containing the sign bit at the lowest address; otherwise define it to
+have the value 0.  This macro need not be a constant.
+
+You need not define this macro if the ordering is the same as for
+multi-word integers.
+@end defmac
+
+@defmac BITS_PER_WORD
+Number of bits in a word.  If you do not define this macro, the default
+is @code{BITS_PER_UNIT * UNITS_PER_WORD}.
+@end defmac
+
+@defmac MAX_BITS_PER_WORD
+Maximum number of bits in a word.  If this is undefined, the default is
+@code{BITS_PER_WORD}.  Otherwise, it is the constant value that is the
+largest value that @code{BITS_PER_WORD} can have at run-time.
+@end defmac
+
+@defmac UNITS_PER_WORD
+Number of storage units in a word; normally the size of a general-purpose
+register, a power of two from 1 or 8.
+@end defmac
+
+@defmac MIN_UNITS_PER_WORD
+Minimum number of units in a word.  If this is undefined, the default is
+@code{UNITS_PER_WORD}.  Otherwise, it is the constant value that is the
+smallest value that @code{UNITS_PER_WORD} can have at run-time.
+@end defmac
+
+@defmac POINTER_SIZE
+Width of a pointer, in bits.  You must specify a value no wider than the
+width of @code{Pmode}.  If it is not equal to the width of @code{Pmode},
+you must define @code{POINTERS_EXTEND_UNSIGNED}.  If you do not specify
+a value the default is @code{BITS_PER_WORD}.
+@end defmac
+
+@defmac POINTERS_EXTEND_UNSIGNED
+A C expression that determines how pointers should be extended from
+@code{ptr_mode} to either @code{Pmode} or @code{word_mode}.  It is
+greater than zero if pointers should be zero-extended, zero if they
+should be sign-extended, and negative if some other sort of conversion
+is needed.  In the last case, the extension is done by the target's
+@code{ptr_extend} instruction.
+
+You need not define this macro if the @code{ptr_mode}, @code{Pmode}
+and @code{word_mode} are all the same width.
+@end defmac
+
+@defmac PROMOTE_MODE (@var{m}, @var{unsignedp}, @var{type})
+A macro to update @var{m} and @var{unsignedp} when an object whose type
+is @var{type} and which has the specified mode and signedness is to be
+stored in a register.  This macro is only called when @var{type} is a
+scalar type.
+
+On most RISC machines, which only have operations that operate on a full
+register, define this macro to set @var{m} to @code{word_mode} if
+@var{m} is an integer mode narrower than @code{BITS_PER_WORD}.  In most
+cases, only integer modes should be widened because wider-precision
+floating-point operations are usually more expensive than their narrower
+counterparts.
+
+For most machines, the macro definition does not change @var{unsignedp}.
+However, some machines, have instructions that preferentially handle
+either signed or unsigned quantities of certain modes.  For example, on
+the DEC Alpha, 32-bit loads from memory and 32-bit add instructions
+sign-extend the result to 64 bits.  On such machines, set
+@var{unsignedp} according to which kind of extension is more efficient.
+
+Do not define this macro if it would never modify @var{m}.
+@end defmac
+
+@hook TARGET_C_EXCESS_PRECISION
+
+@hook TARGET_PROMOTE_FUNCTION_MODE
+
+@defmac PARM_BOUNDARY
+Normal alignment required for function parameters on the stack, in
+bits.  All stack parameters receive at least this much alignment
+regardless of data type.  On most machines, this is the same as the
+size of an integer.
+@end defmac
+
+@defmac STACK_BOUNDARY
+Define this macro to the minimum alignment enforced by hardware for the
+stack pointer on this machine.  The definition is a C expression for the
+desired alignment (measured in bits).  This value is used as a default
+if @code{PREFERRED_STACK_BOUNDARY} is not defined.  On most machines,
+this should be the same as @code{PARM_BOUNDARY}.
+@end defmac
+
+@defmac PREFERRED_STACK_BOUNDARY
+Define this macro if you wish to preserve a certain alignment for the
+stack pointer, greater than what the hardware enforces.  The definition
+is a C expression for the desired alignment (measured in bits).  This
+macro must evaluate to a value equal to or larger than
+@code{STACK_BOUNDARY}.
+@end defmac
+
+@defmac INCOMING_STACK_BOUNDARY
+Define this macro if the incoming stack boundary may be different
+from @code{PREFERRED_STACK_BOUNDARY}.  This macro must evaluate
+to a value equal to or larger than @code{STACK_BOUNDARY}.
+@end defmac
+
+@defmac FUNCTION_BOUNDARY
+Alignment required for a function entry point, in bits.
+@end defmac
+
+@defmac BIGGEST_ALIGNMENT
+Biggest alignment that any data type can require on this machine, in
+bits.  Note that this is not the biggest alignment that is supported,
+just the biggest alignment that, when violated, may cause a fault.
+@end defmac
+
+@hook TARGET_ABSOLUTE_BIGGEST_ALIGNMENT
+
+@defmac MALLOC_ABI_ALIGNMENT
+Alignment, in bits, a C conformant malloc implementation has to
+provide.  If not defined, the default value is @code{BITS_PER_WORD}.
+@end defmac
+
+@defmac ATTRIBUTE_ALIGNED_VALUE
+Alignment used by the @code{__attribute__ ((aligned))} construct.  If
+not defined, the default value is @code{BIGGEST_ALIGNMENT}.
+@end defmac
+
+@defmac MINIMUM_ATOMIC_ALIGNMENT
+If defined, the smallest alignment, in bits, that can be given to an
+object that can be referenced in one operation, without disturbing any
+nearby object.  Normally, this is @code{BITS_PER_UNIT}, but may be larger
+on machines that don't have byte or half-word store operations.
+@end defmac
+
+@defmac BIGGEST_FIELD_ALIGNMENT
+Biggest alignment that any structure or union field can require on this
+machine, in bits.  If defined, this overrides @code{BIGGEST_ALIGNMENT} for
+structure and union fields only, unless the field alignment has been set
+by the @code{__attribute__ ((aligned (@var{n})))} construct.
+@end defmac
+
+@defmac ADJUST_FIELD_ALIGN (@var{field}, @var{type}, @var{computed})
+An expression for the alignment of a structure field @var{field} of
+type @var{type} if the alignment computed in the usual way (including
+applying of @code{BIGGEST_ALIGNMENT} and @code{BIGGEST_FIELD_ALIGNMENT} to the
+alignment) is @var{computed}.  It overrides alignment only if the
+field alignment has not been set by the
+@code{__attribute__ ((aligned (@var{n})))} construct.  Note that @var{field}
+may be @code{NULL_TREE} in case we just query for the minimum alignment
+of a field of type @var{type} in structure context.
+@end defmac
+
+@defmac MAX_STACK_ALIGNMENT
+Biggest stack alignment guaranteed by the backend.  Use this macro
+to specify the maximum alignment of a variable on stack.
+
+If not defined, the default value is @code{STACK_BOUNDARY}.
+
+@c FIXME: The default should be @code{PREFERRED_STACK_BOUNDARY}.
+@c But the fix for PR 32893 indicates that we can only guarantee
+@c maximum stack alignment on stack up to @code{STACK_BOUNDARY}, not
+@c @code{PREFERRED_STACK_BOUNDARY}, if stack alignment isn't supported.
+@end defmac
+
+@defmac MAX_OFILE_ALIGNMENT
+Biggest alignment supported by the object file format of this machine.
+Use this macro to limit the alignment which can be specified using the
+@code{__attribute__ ((aligned (@var{n})))} construct for functions and
+objects with static storage duration.  The alignment of automatic
+objects may exceed the object file format maximum up to the maximum
+supported by GCC.  If not defined, the default value is
+@code{BIGGEST_ALIGNMENT}.
+
+On systems that use ELF, the default (in @file{config/elfos.h}) is
+the largest supported 32-bit ELF section alignment representable on
+a 32-bit host e.g.@: @samp{(((uint64_t) 1 << 28) * 8)}.
+On 32-bit ELF the largest supported section alignment in bits is
+@samp{(0x80000000 * 8)}, but this is not representable on 32-bit hosts.
+@end defmac
+
+@hook TARGET_LOWER_LOCAL_DECL_ALIGNMENT
+
+@hook TARGET_STATIC_RTX_ALIGNMENT
+
+@defmac DATA_ALIGNMENT (@var{type}, @var{basic-align})
+If defined, a C expression to compute the alignment for a variable in
+the static store.  @var{type} is the data type, and @var{basic-align} is
+the alignment that the object would ordinarily have.  The value of this
+macro is used instead of that alignment to align the object.
+
+If this macro is not defined, then @var{basic-align} is used.
+
+@findex strcpy
+One use of this macro is to increase alignment of medium-size data to
+make it all fit in fewer cache lines.  Another is to cause character
+arrays to be word-aligned so that @code{strcpy} calls that copy
+constants to character arrays can be done inline.
+@end defmac
+
+@defmac DATA_ABI_ALIGNMENT (@var{type}, @var{basic-align})
+Similar to @code{DATA_ALIGNMENT}, but for the cases where the ABI mandates
+some alignment increase, instead of optimization only purposes.  E.g.@
+AMD x86-64 psABI says that variables with array type larger than 15 bytes
+must be aligned to 16 byte boundaries.
+
+If this macro is not defined, then @var{basic-align} is used.
+@end defmac
+
+@hook TARGET_CONSTANT_ALIGNMENT
+
+@defmac LOCAL_ALIGNMENT (@var{type}, @var{basic-align})
+If defined, a C expression to compute the alignment for a variable in
+the local store.  @var{type} is the data type, and @var{basic-align} is
+the alignment that the object would ordinarily have.  The value of this
+macro is used instead of that alignment to align the object.
+
+If this macro is not defined, then @var{basic-align} is used.
+
+One use of this macro is to increase alignment of medium-size data to
+make it all fit in fewer cache lines.
+
+If the value of this macro has a type, it should be an unsigned type.
+@end defmac
+
+@hook TARGET_VECTOR_ALIGNMENT
+
+@defmac STACK_SLOT_ALIGNMENT (@var{type}, @var{mode}, @var{basic-align})
+If defined, a C expression to compute the alignment for stack slot.
+@var{type} is the data type, @var{mode} is the widest mode available,
+and @var{basic-align} is the alignment that the slot would ordinarily
+have.  The value of this macro is used instead of that alignment to
+align the slot.
+
+If this macro is not defined, then @var{basic-align} is used when
+@var{type} is @code{NULL}.  Otherwise, @code{LOCAL_ALIGNMENT} will
+be used.
+
+This macro is to set alignment of stack slot to the maximum alignment
+of all possible modes which the slot may have.
+
+If the value of this macro has a type, it should be an unsigned type.
+@end defmac
+
+@defmac LOCAL_DECL_ALIGNMENT (@var{decl})
+If defined, a C expression to compute the alignment for a local
+variable @var{decl}.
+
+If this macro is not defined, then
+@code{LOCAL_ALIGNMENT (TREE_TYPE (@var{decl}), DECL_ALIGN (@var{decl}))}
+is used.
+
+One use of this macro is to increase alignment of medium-size data to
+make it all fit in fewer cache lines.
+
+If the value of this macro has a type, it should be an unsigned type.
+@end defmac
+
+@defmac MINIMUM_ALIGNMENT (@var{exp}, @var{mode}, @var{align})
+If defined, a C expression to compute the minimum required alignment
+for dynamic stack realignment purposes for @var{exp} (a type or decl),
+@var{mode}, assuming normal alignment @var{align}.
+
+If this macro is not defined, then @var{align} will be used.
+@end defmac
+
+@defmac EMPTY_FIELD_BOUNDARY
+Alignment in bits to be given to a structure bit-field that follows an
+empty field such as @code{int : 0;}.
+
+If @code{PCC_BITFIELD_TYPE_MATTERS} is true, it overrides this macro.
+@end defmac
+
+@defmac STRUCTURE_SIZE_BOUNDARY
+Number of bits which any structure or union's size must be a multiple of.
+Each structure or union's size is rounded up to a multiple of this.
+
+If you do not define this macro, the default is the same as
+@code{BITS_PER_UNIT}.
+@end defmac
+
+@defmac STRICT_ALIGNMENT
+Define this macro to be the value 1 if instructions will fail to work
+if given data not on the nominal alignment.  If instructions will merely
+go slower in that case, define this macro as 0.
+@end defmac
+
+@defmac PCC_BITFIELD_TYPE_MATTERS
+Define this if you wish to imitate the way many other C compilers handle
+alignment of bit-fields and the structures that contain them.
+
+The behavior is that the type written for a named bit-field (@code{int},
+@code{short}, or other integer type) imposes an alignment for the entire
+structure, as if the structure really did contain an ordinary field of
+that type.  In addition, the bit-field is placed within the structure so
+that it would fit within such a field, not crossing a boundary for it.
+
+Thus, on most machines, a named bit-field whose type is written as
+@code{int} would not cross a four-byte boundary, and would force
+four-byte alignment for the whole structure.  (The alignment used may
+not be four bytes; it is controlled by the other alignment parameters.)
+
+An unnamed bit-field will not affect the alignment of the containing
+structure.
+
+If the macro is defined, its definition should be a C expression;
+a nonzero value for the expression enables this behavior.
+
+Note that if this macro is not defined, or its value is zero, some
+bit-fields may cross more than one alignment boundary.  The compiler can
+support such references if there are @samp{insv}, @samp{extv}, and
+@samp{extzv} insns that can directly reference memory.
+
+The other known way of making bit-fields work is to define
+@code{STRUCTURE_SIZE_BOUNDARY} as large as @code{BIGGEST_ALIGNMENT}.
+Then every structure can be accessed with fullwords.
+
+Unless the machine has bit-field instructions or you define
+@code{STRUCTURE_SIZE_BOUNDARY} that way, you must define
+@code{PCC_BITFIELD_TYPE_MATTERS} to have a nonzero value.
+
+If your aim is to make GCC use the same conventions for laying out
+bit-fields as are used by another compiler, here is how to investigate
+what the other compiler does.  Compile and run this program:
+
+@smallexample
+struct foo1
+@{
+  char x;
+  char :0;
+  char y;
+@};
+
+struct foo2
+@{
+  char x;
+  int :0;
+  char y;
+@};
+
+main ()
+@{
+  printf ("Size of foo1 is %d\n",
+          sizeof (struct foo1));
+  printf ("Size of foo2 is %d\n",
+          sizeof (struct foo2));
+  exit (0);
+@}
+@end smallexample
+
+If this prints 2 and 5, then the compiler's behavior is what you would
+get from @code{PCC_BITFIELD_TYPE_MATTERS}.
+@end defmac
+
+@defmac BITFIELD_NBYTES_LIMITED
+Like @code{PCC_BITFIELD_TYPE_MATTERS} except that its effect is limited
+to aligning a bit-field within the structure.
+@end defmac
+
+@hook TARGET_ALIGN_ANON_BITFIELD
+
+@hook TARGET_NARROW_VOLATILE_BITFIELD
+
+@hook TARGET_MEMBER_TYPE_FORCES_BLK
+
+@defmac ROUND_TYPE_ALIGN (@var{type}, @var{computed}, @var{specified})
+Define this macro as an expression for the alignment of a type (given
+by @var{type} as a tree node) if the alignment computed in the usual
+way is @var{computed} and the alignment explicitly specified was
+@var{specified}.
+
+The default is to use @var{specified} if it is larger; otherwise, use
+the smaller of @var{computed} and @code{BIGGEST_ALIGNMENT}
+@end defmac
+
+@defmac MAX_FIXED_MODE_SIZE
+An integer expression for the size in bits of the largest integer
+machine mode that should actually be used.  All integer machine modes of
+this size or smaller can be used for structures and unions with the
+appropriate sizes.  If this macro is undefined, @code{GET_MODE_BITSIZE
+(DImode)} is assumed.
+@end defmac
+
+@defmac STACK_SAVEAREA_MODE (@var{save_level})
+If defined, an expression of type @code{machine_mode} that
+specifies the mode of the save area operand of a
+@code{save_stack_@var{level}} named pattern (@pxref{Standard Names}).
+@var{save_level} is one of @code{SAVE_BLOCK}, @code{SAVE_FUNCTION}, or
+@code{SAVE_NONLOCAL} and selects which of the three named patterns is
+having its mode specified.
+
+You need not define this macro if it always returns @code{Pmode}.  You
+would most commonly define this macro if the
+@code{save_stack_@var{level}} patterns need to support both a 32- and a
+64-bit mode.
+@end defmac
+
+@defmac STACK_SIZE_MODE
+If defined, an expression of type @code{machine_mode} that
+specifies the mode of the size increment operand of an
+@code{allocate_stack} named pattern (@pxref{Standard Names}).
+
+You need not define this macro if it always returns @code{word_mode}.
+You would most commonly define this macro if the @code{allocate_stack}
+pattern needs to support both a 32- and a 64-bit mode.
+@end defmac
+
+@hook TARGET_LIBGCC_CMP_RETURN_MODE
+
+@hook TARGET_LIBGCC_SHIFT_COUNT_MODE
+
+@hook TARGET_UNWIND_WORD_MODE
+
+@hook TARGET_MS_BITFIELD_LAYOUT_P
+
+@hook TARGET_DECIMAL_FLOAT_SUPPORTED_P
+
+@hook TARGET_FIXED_POINT_SUPPORTED_P
+
+@hook TARGET_EXPAND_TO_RTL_HOOK
+
+@hook TARGET_INSTANTIATE_DECLS
+
+@hook TARGET_MANGLE_TYPE
+
+@node Type Layout
+@section Layout of Source Language Data Types
+
+These macros define the sizes and other characteristics of the standard
+basic data types used in programs being compiled.  Unlike the macros in
+the previous section, these apply to specific features of C and related
+languages, rather than to fundamental aspects of storage layout.
+
+@defmac INT_TYPE_SIZE
+A C expression for the size in bits of the type @code{int} on the
+target machine.  If you don't define this, the default is one word.
+@end defmac
+
+@defmac SHORT_TYPE_SIZE
+A C expression for the size in bits of the type @code{short} on the
+target machine.  If you don't define this, the default is half a word.
+(If this would be less than one storage unit, it is rounded up to one
+unit.)
+@end defmac
+
+@defmac LONG_TYPE_SIZE
+A C expression for the size in bits of the type @code{long} on the
+target machine.  If you don't define this, the default is one word.
+@end defmac
+
+@defmac ADA_LONG_TYPE_SIZE
+On some machines, the size used for the Ada equivalent of the type
+@code{long} by a native Ada compiler differs from that used by C@.  In
+that situation, define this macro to be a C expression to be used for
+the size of that type.  If you don't define this, the default is the
+value of @code{LONG_TYPE_SIZE}.
+@end defmac
+
+@defmac LONG_LONG_TYPE_SIZE
+A C expression for the size in bits of the type @code{long long} on the
+target machine.  If you don't define this, the default is two
+words.  If you want to support GNU Ada on your machine, the value of this
+macro must be at least 64.
+@end defmac
+
+@defmac CHAR_TYPE_SIZE
+A C expression for the size in bits of the type @code{char} on the
+target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT}.
+@end defmac
+
+@defmac BOOL_TYPE_SIZE
+A C expression for the size in bits of the C++ type @code{bool} and
+C99 type @code{_Bool} on the target machine.  If you don't define
+this, and you probably shouldn't, the default is @code{CHAR_TYPE_SIZE}.
+@end defmac
+
+@defmac FLOAT_TYPE_SIZE
+A C expression for the size in bits of the type @code{float} on the
+target machine.  If you don't define this, the default is one word.
+@end defmac
+
+@defmac DOUBLE_TYPE_SIZE
+A C expression for the size in bits of the type @code{double} on the
+target machine.  If you don't define this, the default is two
+words.
+@end defmac
+
+@defmac LONG_DOUBLE_TYPE_SIZE
+A C expression for the size in bits of the type @code{long double} on
+the target machine.  If you don't define this, the default is two
+words.
+@end defmac
+
+@defmac SHORT_FRACT_TYPE_SIZE
+A C expression for the size in bits of the type @code{short _Fract} on
+the target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT}.
+@end defmac
+
+@defmac FRACT_TYPE_SIZE
+A C expression for the size in bits of the type @code{_Fract} on
+the target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT * 2}.
+@end defmac
+
+@defmac LONG_FRACT_TYPE_SIZE
+A C expression for the size in bits of the type @code{long _Fract} on
+the target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT * 4}.
+@end defmac
+
+@defmac LONG_LONG_FRACT_TYPE_SIZE
+A C expression for the size in bits of the type @code{long long _Fract} on
+the target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT * 8}.
+@end defmac
+
+@defmac SHORT_ACCUM_TYPE_SIZE
+A C expression for the size in bits of the type @code{short _Accum} on
+the target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT * 2}.
+@end defmac
+
+@defmac ACCUM_TYPE_SIZE
+A C expression for the size in bits of the type @code{_Accum} on
+the target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT * 4}.
+@end defmac
+
+@defmac LONG_ACCUM_TYPE_SIZE
+A C expression for the size in bits of the type @code{long _Accum} on
+the target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT * 8}.
+@end defmac
+
+@defmac LONG_LONG_ACCUM_TYPE_SIZE
+A C expression for the size in bits of the type @code{long long _Accum} on
+the target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT * 16}.
+@end defmac
+
+@defmac LIBGCC2_GNU_PREFIX
+This macro corresponds to the @code{TARGET_LIBFUNC_GNU_PREFIX} target
+hook and should be defined if that hook is overriden to be true.  It
+causes function names in libgcc to be changed to use a @code{__gnu_}
+prefix for their name rather than the default @code{__}.  A port which
+uses this macro should also arrange to use @file{t-gnu-prefix} in
+the libgcc @file{config.host}.
+@end defmac
+
+@defmac WIDEST_HARDWARE_FP_SIZE
+A C expression for the size in bits of the widest floating-point format
+supported by the hardware.  If you define this macro, you must specify a
+value less than or equal to the value of @code{LONG_DOUBLE_TYPE_SIZE}.
+If you do not define this macro, the value of @code{LONG_DOUBLE_TYPE_SIZE}
+is the default.
+@end defmac
+
+@defmac DEFAULT_SIGNED_CHAR
+An expression whose value is 1 or 0, according to whether the type
+@code{char} should be signed or unsigned by default.  The user can
+always override this default with the options @option{-fsigned-char}
+and @option{-funsigned-char}.
+@end defmac
+
+@hook TARGET_DEFAULT_SHORT_ENUMS
+
+@defmac SIZE_TYPE
+A C expression for a string describing the name of the data type to use
+for size values.  The typedef name @code{size_t} is defined using the
+contents of the string.
+
+The string can contain more than one keyword.  If so, separate them with
+spaces, and write first any length keyword, then @code{unsigned} if
+appropriate, and finally @code{int}.  The string must exactly match one
+of the data type names defined in the function
+@code{c_common_nodes_and_builtins} in the file @file{c-family/c-common.c}.
+You may not omit @code{int} or change the order---that would cause the
+compiler to crash on startup.
+
+If you don't define this macro, the default is @code{"long unsigned
+int"}.
+@end defmac
+
+@defmac SIZETYPE
+GCC defines internal types (@code{sizetype}, @code{ssizetype},
+@code{bitsizetype} and @code{sbitsizetype}) for expressions
+dealing with size.  This macro is a C expression for a string describing
+the name of the data type from which the precision of @code{sizetype}
+is extracted.
+
+The string has the same restrictions as @code{SIZE_TYPE} string.
+
+If you don't define this macro, the default is @code{SIZE_TYPE}.
+@end defmac
+
+@defmac PTRDIFF_TYPE
+A C expression for a string describing the name of the data type to use
+for the result of subtracting two pointers.  The typedef name
+@code{ptrdiff_t} is defined using the contents of the string.  See
+@code{SIZE_TYPE} above for more information.
+
+If you don't define this macro, the default is @code{"long int"}.
+@end defmac
+
+@defmac WCHAR_TYPE
+A C expression for a string describing the name of the data type to use
+for wide characters.  The typedef name @code{wchar_t} is defined using
+the contents of the string.  See @code{SIZE_TYPE} above for more
+information.
+
+If you don't define this macro, the default is @code{"int"}.
+@end defmac
+
+@defmac WCHAR_TYPE_SIZE
+A C expression for the size in bits of the data type for wide
+characters.  This is used in @code{cpp}, which cannot make use of
+@code{WCHAR_TYPE}.
+@end defmac
+
+@defmac WINT_TYPE
+A C expression for a string describing the name of the data type to
+use for wide characters passed to @code{printf} and returned from
+@code{getwc}.  The typedef name @code{wint_t} is defined using the
+contents of the string.  See @code{SIZE_TYPE} above for more
+information.
+
+If you don't define this macro, the default is @code{"unsigned int"}.
+@end defmac
+
+@defmac INTMAX_TYPE
+A C expression for a string describing the name of the data type that
+can represent any value of any standard or extended signed integer type.
+The typedef name @code{intmax_t} is defined using the contents of the
+string.  See @code{SIZE_TYPE} above for more information.
+
+If you don't define this macro, the default is the first of
+@code{"int"}, @code{"long int"}, or @code{"long long int"} that has as
+much precision as @code{long long int}.
+@end defmac
+
+@defmac UINTMAX_TYPE
+A C expression for a string describing the name of the data type that
+can represent any value of any standard or extended unsigned integer
+type.  The typedef name @code{uintmax_t} is defined using the contents
+of the string.  See @code{SIZE_TYPE} above for more information.
+
+If you don't define this macro, the default is the first of
+@code{"unsigned int"}, @code{"long unsigned int"}, or @code{"long long
+unsigned int"} that has as much precision as @code{long long unsigned
+int}.
+@end defmac
+
+@defmac SIG_ATOMIC_TYPE
+@defmacx INT8_TYPE
+@defmacx INT16_TYPE
+@defmacx INT32_TYPE
+@defmacx INT64_TYPE
+@defmacx UINT8_TYPE
+@defmacx UINT16_TYPE
+@defmacx UINT32_TYPE
+@defmacx UINT64_TYPE
+@defmacx INT_LEAST8_TYPE
+@defmacx INT_LEAST16_TYPE
+@defmacx INT_LEAST32_TYPE
+@defmacx INT_LEAST64_TYPE
+@defmacx UINT_LEAST8_TYPE
+@defmacx UINT_LEAST16_TYPE
+@defmacx UINT_LEAST32_TYPE
+@defmacx UINT_LEAST64_TYPE
+@defmacx INT_FAST8_TYPE
+@defmacx INT_FAST16_TYPE
+@defmacx INT_FAST32_TYPE
+@defmacx INT_FAST64_TYPE
+@defmacx UINT_FAST8_TYPE
+@defmacx UINT_FAST16_TYPE
+@defmacx UINT_FAST32_TYPE
+@defmacx UINT_FAST64_TYPE
+@defmacx INTPTR_TYPE
+@defmacx UINTPTR_TYPE
+C expressions for the standard types @code{sig_atomic_t},
+@code{int8_t}, @code{int16_t}, @code{int32_t}, @code{int64_t},
+@code{uint8_t}, @code{uint16_t}, @code{uint32_t}, @code{uint64_t},
+@code{int_least8_t}, @code{int_least16_t}, @code{int_least32_t},
+@code{int_least64_t}, @code{uint_least8_t}, @code{uint_least16_t},
+@code{uint_least32_t}, @code{uint_least64_t}, @code{int_fast8_t},
+@code{int_fast16_t}, @code{int_fast32_t}, @code{int_fast64_t},
+@code{uint_fast8_t}, @code{uint_fast16_t}, @code{uint_fast32_t},
+@code{uint_fast64_t}, @code{intptr_t}, and @code{uintptr_t}.  See
+@code{SIZE_TYPE} above for more information.
+
+If any of these macros evaluates to a null pointer, the corresponding
+type is not supported; if GCC is configured to provide
+@code{<stdint.h>} in such a case, the header provided may not conform
+to C99, depending on the type in question.  The defaults for all of
+these macros are null pointers.
+@end defmac
+
+@defmac TARGET_PTRMEMFUNC_VBIT_LOCATION
+The C++ compiler represents a pointer-to-member-function with a struct
+that looks like:
+
+@smallexample
+  struct @{
+    union @{
+      void (*fn)();
+      ptrdiff_t vtable_index;
+    @};
+    ptrdiff_t delta;
+  @};
+@end smallexample
+
+@noindent
+The C++ compiler must use one bit to indicate whether the function that
+will be called through a pointer-to-member-function is virtual.
+Normally, we assume that the low-order bit of a function pointer must
+always be zero.  Then, by ensuring that the vtable_index is odd, we can
+distinguish which variant of the union is in use.  But, on some
+platforms function pointers can be odd, and so this doesn't work.  In
+that case, we use the low-order bit of the @code{delta} field, and shift
+the remainder of the @code{delta} field to the left.
+
+GCC will automatically make the right selection about where to store
+this bit using the @code{FUNCTION_BOUNDARY} setting for your platform.
+However, some platforms such as ARM/Thumb have @code{FUNCTION_BOUNDARY}
+set such that functions always start at even addresses, but the lowest
+bit of pointers to functions indicate whether the function at that
+address is in ARM or Thumb mode.  If this is the case of your
+architecture, you should define this macro to
+@code{ptrmemfunc_vbit_in_delta}.
+
+In general, you should not have to define this macro.  On architectures
+in which function addresses are always even, according to
+@code{FUNCTION_BOUNDARY}, GCC will automatically define this macro to
+@code{ptrmemfunc_vbit_in_pfn}.
+@end defmac
+
+@defmac TARGET_VTABLE_USES_DESCRIPTORS
+Normally, the C++ compiler uses function pointers in vtables.  This
+macro allows the target to change to use ``function descriptors''
+instead.  Function descriptors are found on targets for whom a
+function pointer is actually a small data structure.  Normally the
+data structure consists of the actual code address plus a data
+pointer to which the function's data is relative.
+
+If vtables are used, the value of this macro should be the number
+of words that the function descriptor occupies.
+@end defmac
+
+@defmac TARGET_VTABLE_ENTRY_ALIGN
+By default, the vtable entries are void pointers, the so the alignment
+is the same as pointer alignment.  The value of this macro specifies
+the alignment of the vtable entry in bits.  It should be defined only
+when special alignment is necessary. */
+@end defmac
+
+@defmac TARGET_VTABLE_DATA_ENTRY_DISTANCE
+There are a few non-descriptor entries in the vtable at offsets below
+zero.  If these entries must be padded (say, to preserve the alignment
+specified by @code{TARGET_VTABLE_ENTRY_ALIGN}), set this to the number
+of words in each data entry.
+@end defmac
+
+@node Registers
+@section Register Usage
+@cindex register usage
+
+This section explains how to describe what registers the target machine
+has, and how (in general) they can be used.
+
+The description of which registers a specific instruction can use is
+done with register classes; see @ref{Register Classes}.  For information
+on using registers to access a stack frame, see @ref{Frame Registers}.
+For passing values in registers, see @ref{Register Arguments}.
+For returning values in registers, see @ref{Scalar Return}.
+
+@menu
+* Register Basics::             Number and kinds of registers.
+* Allocation Order::            Order in which registers are allocated.
+* Values in Registers::         What kinds of values each reg can hold.
+* Leaf Functions::              Renumbering registers for leaf functions.
+* Stack Registers::             Handling a register stack such as 80387.
+@end menu
+
+@node Register Basics
+@subsection Basic Characteristics of Registers
+
+@c prevent bad page break with this line
+Registers have various characteristics.
+
+@defmac FIRST_PSEUDO_REGISTER
+Number of hardware registers known to the compiler.  They receive
+numbers 0 through @code{FIRST_PSEUDO_REGISTER-1}; thus, the first
+pseudo register's number really is assigned the number
+@code{FIRST_PSEUDO_REGISTER}.
+@end defmac
+
+@defmac FIXED_REGISTERS
+@cindex fixed register
+An initializer that says which registers are used for fixed purposes
+all throughout the compiled code and are therefore not available for
+general allocation.  These would include the stack pointer, the frame
+pointer (except on machines where that can be used as a general
+register when no frame pointer is needed), the program counter on
+machines where that is considered one of the addressable registers,
+and any other numbered register with a standard use.
+
+This information is expressed as a sequence of numbers, separated by
+commas and surrounded by braces.  The @var{n}th number is 1 if
+register @var{n} is fixed, 0 otherwise.
+
+The table initialized from this macro, and the table initialized by
+the following one, may be overridden at run time either automatically,
+by the actions of the macro @code{CONDITIONAL_REGISTER_USAGE}, or by
+the user with the command options @option{-ffixed-@var{reg}},
+@option{-fcall-used-@var{reg}} and @option{-fcall-saved-@var{reg}}.
+@end defmac
+
+@defmac CALL_USED_REGISTERS
+@cindex call-used register
+@cindex call-clobbered register
+@cindex call-saved register
+Like @code{FIXED_REGISTERS} but has 1 for each register that is
+clobbered (in general) by function calls as well as for fixed
+registers.  This macro therefore identifies the registers that are not
+available for general allocation of values that must live across
+function calls.
+
+If a register has 0 in @code{CALL_USED_REGISTERS}, the compiler
+automatically saves it on function entry and restores it on function
+exit, if the register is used within the function.
+
+Exactly one of @code{CALL_USED_REGISTERS} and @code{CALL_REALLY_USED_REGISTERS}
+must be defined.  Modern ports should define @code{CALL_REALLY_USED_REGISTERS}.
+@end defmac
+
+@defmac CALL_REALLY_USED_REGISTERS
+@cindex call-used register
+@cindex call-clobbered register
+@cindex call-saved register
+Like @code{CALL_USED_REGISTERS} except this macro doesn't require
+that the entire set of @code{FIXED_REGISTERS} be included.
+(@code{CALL_USED_REGISTERS} must be a superset of @code{FIXED_REGISTERS}).
+
+Exactly one of @code{CALL_USED_REGISTERS} and @code{CALL_REALLY_USED_REGISTERS}
+must be defined.  Modern ports should define @code{CALL_REALLY_USED_REGISTERS}.
+@end defmac
+
+@cindex call-used register
+@cindex call-clobbered register
+@cindex call-saved register
+@hook TARGET_FNTYPE_ABI
+
+@hook TARGET_INSN_CALLEE_ABI
+
+@cindex call-used register
+@cindex call-clobbered register
+@cindex call-saved register
+@hook TARGET_HARD_REGNO_CALL_PART_CLOBBERED
+
+@hook TARGET_GET_MULTILIB_ABI_NAME
+
+@findex fixed_regs
+@findex call_used_regs
+@findex global_regs
+@findex reg_names
+@findex reg_class_contents
+@hook TARGET_CONDITIONAL_REGISTER_USAGE
+
+@defmac INCOMING_REGNO (@var{out})
+Define this macro if the target machine has register windows.  This C
+expression returns the register number as seen by the called function
+corresponding to the register number @var{out} as seen by the calling
+function.  Return @var{out} if register number @var{out} is not an
+outbound register.
+@end defmac
+
+@defmac OUTGOING_REGNO (@var{in})
+Define this macro if the target machine has register windows.  This C
+expression returns the register number as seen by the calling function
+corresponding to the register number @var{in} as seen by the called
+function.  Return @var{in} if register number @var{in} is not an inbound
+register.
+@end defmac
+
+@defmac LOCAL_REGNO (@var{regno})
+Define this macro if the target machine has register windows.  This C
+expression returns true if the register is call-saved but is in the
+register window.  Unlike most call-saved registers, such registers
+need not be explicitly restored on function exit or during non-local
+gotos.
+@end defmac
+
+@defmac PC_REGNUM
+If the program counter has a register number, define this as that
+register number.  Otherwise, do not define it.
+@end defmac
+
+@node Allocation Order
+@subsection Order of Allocation of Registers
+@cindex order of register allocation
+@cindex register allocation order
+
+@c prevent bad page break with this line
+Registers are allocated in order.
+
+@defmac REG_ALLOC_ORDER
+If defined, an initializer for a vector of integers, containing the
+numbers of hard registers in the order in which GCC should prefer
+to use them (from most preferred to least).
+
+If this macro is not defined, registers are used lowest numbered first
+(all else being equal).
+
+One use of this macro is on machines where the highest numbered
+registers must always be saved and the save-multiple-registers
+instruction supports only sequences of consecutive registers.  On such
+machines, define @code{REG_ALLOC_ORDER} to be an initializer that lists
+the highest numbered allocable register first.
+@end defmac
+
+@defmac ADJUST_REG_ALLOC_ORDER
+A C statement (sans semicolon) to choose the order in which to allocate
+hard registers for pseudo-registers local to a basic block.
+
+Store the desired register order in the array @code{reg_alloc_order}.
+Element 0 should be the register to allocate first; element 1, the next
+register; and so on.
+
+The macro body should not assume anything about the contents of
+@code{reg_alloc_order} before execution of the macro.
+
+On most machines, it is not necessary to define this macro.
+@end defmac
+
+@defmac HONOR_REG_ALLOC_ORDER
+Normally, IRA tries to estimate the costs for saving a register in the
+prologue and restoring it in the epilogue.  This discourages it from
+using call-saved registers.  If a machine wants to ensure that IRA
+allocates registers in the order given by REG_ALLOC_ORDER even if some
+call-saved registers appear earlier than call-used ones, then define this
+macro as a C expression to nonzero. Default is 0.
+@end defmac
+
+@defmac IRA_HARD_REGNO_ADD_COST_MULTIPLIER (@var{regno})
+In some case register allocation order is not enough for the
+Integrated Register Allocator (@acronym{IRA}) to generate a good code.
+If this macro is defined, it should return a floating point value
+based on @var{regno}.  The cost of using @var{regno} for a pseudo will
+be increased by approximately the pseudo's usage frequency times the
+value returned by this macro.  Not defining this macro is equivalent
+to having it always return @code{0.0}.
+
+On most machines, it is not necessary to define this macro.
+@end defmac
+
+@node Values in Registers
+@subsection How Values Fit in Registers
+
+This section discusses the macros that describe which kinds of values
+(specifically, which machine modes) each register can hold, and how many
+consecutive registers are needed for a given mode.
+
+@hook TARGET_HARD_REGNO_NREGS
+
+@defmac HARD_REGNO_NREGS_HAS_PADDING (@var{regno}, @var{mode})
+A C expression that is nonzero if a value of mode @var{mode}, stored
+in memory, ends with padding that causes it to take up more space than
+in registers starting at register number @var{regno} (as determined by
+multiplying GCC's notion of the size of the register when containing
+this mode by the number of registers returned by
+@code{TARGET_HARD_REGNO_NREGS}).  By default this is zero.
+
+For example, if a floating-point value is stored in three 32-bit
+registers but takes up 128 bits in memory, then this would be
+nonzero.
+
+This macros only needs to be defined if there are cases where
+@code{subreg_get_info}
+would otherwise wrongly determine that a @code{subreg} can be
+represented by an offset to the register number, when in fact such a
+@code{subreg} would contain some of the padding not stored in
+registers and so not be representable.
+@end defmac
+
+@defmac HARD_REGNO_NREGS_WITH_PADDING (@var{regno}, @var{mode})
+For values of @var{regno} and @var{mode} for which
+@code{HARD_REGNO_NREGS_HAS_PADDING} returns nonzero, a C expression
+returning the greater number of registers required to hold the value
+including any padding.  In the example above, the value would be four.
+@end defmac
+
+@defmac REGMODE_NATURAL_SIZE (@var{mode})
+Define this macro if the natural size of registers that hold values
+of mode @var{mode} is not the word size.  It is a C expression that
+should give the natural size in bytes for the specified mode.  It is
+used by the register allocator to try to optimize its results.  This
+happens for example on SPARC 64-bit where the natural size of
+floating-point registers is still 32-bit.
+@end defmac
+
+@hook TARGET_HARD_REGNO_MODE_OK
+
+@defmac HARD_REGNO_RENAME_OK (@var{from}, @var{to})
+A C expression that is nonzero if it is OK to rename a hard register
+@var{from} to another hard register @var{to}.
+
+One common use of this macro is to prevent renaming of a register to
+another register that is not saved by a prologue in an interrupt
+handler.
+
+The default is always nonzero.
+@end defmac
+
+@hook TARGET_MODES_TIEABLE_P
+
+@hook TARGET_HARD_REGNO_SCRATCH_OK
+
+@defmac AVOID_CCMODE_COPIES
+Define this macro if the compiler should avoid copies to/from @code{CCmode}
+registers.  You should only define this macro if support for copying to/from
+@code{CCmode} is incomplete.
+@end defmac
+
+@node Leaf Functions
+@subsection Handling Leaf Functions
+
+@cindex leaf functions
+@cindex functions, leaf
+On some machines, a leaf function (i.e., one which makes no calls) can run
+more efficiently if it does not make its own register window.  Often this
+means it is required to receive its arguments in the registers where they
+are passed by the caller, instead of the registers where they would
+normally arrive.
+
+The special treatment for leaf functions generally applies only when
+other conditions are met; for example, often they may use only those
+registers for its own variables and temporaries.  We use the term ``leaf
+function'' to mean a function that is suitable for this special
+handling, so that functions with no calls are not necessarily ``leaf
+functions''.
+
+GCC assigns register numbers before it knows whether the function is
+suitable for leaf function treatment.  So it needs to renumber the
+registers in order to output a leaf function.  The following macros
+accomplish this.
+
+@defmac LEAF_REGISTERS
+Name of a char vector, indexed by hard register number, which
+contains 1 for a register that is allowable in a candidate for leaf
+function treatment.
+
+If leaf function treatment involves renumbering the registers, then the
+registers marked here should be the ones before renumbering---those that
+GCC would ordinarily allocate.  The registers which will actually be
+used in the assembler code, after renumbering, should not be marked with 1
+in this vector.
+
+Define this macro only if the target machine offers a way to optimize
+the treatment of leaf functions.
+@end defmac
+
+@defmac LEAF_REG_REMAP (@var{regno})
+A C expression whose value is the register number to which @var{regno}
+should be renumbered, when a function is treated as a leaf function.
+
+If @var{regno} is a register number which should not appear in a leaf
+function before renumbering, then the expression should yield @minus{}1, which
+will cause the compiler to abort.
+
+Define this macro only if the target machine offers a way to optimize the
+treatment of leaf functions, and registers need to be renumbered to do
+this.
+@end defmac
+
+@findex current_function_is_leaf
+@findex current_function_uses_only_leaf_regs
+@code{TARGET_ASM_FUNCTION_PROLOGUE} and
+@code{TARGET_ASM_FUNCTION_EPILOGUE} must usually treat leaf functions
+specially.  They can test the C variable @code{current_function_is_leaf}
+which is nonzero for leaf functions.  @code{current_function_is_leaf} is
+set prior to local register allocation and is valid for the remaining
+compiler passes.  They can also test the C variable
+@code{current_function_uses_only_leaf_regs} which is nonzero for leaf
+functions which only use leaf registers.
+@code{current_function_uses_only_leaf_regs} is valid after all passes
+that modify the instructions have been run and is only useful if
+@code{LEAF_REGISTERS} is defined.
+@c changed this to fix overfull.  ALSO:  why the "it" at the beginning
+@c of the next paragraph?!  --mew 2feb93
+
+@node Stack Registers
+@subsection Registers That Form a Stack
+
+There are special features to handle computers where some of the
+``registers'' form a stack.  Stack registers are normally written by
+pushing onto the stack, and are numbered relative to the top of the
+stack.
+
+Currently, GCC can only handle one group of stack-like registers, and
+they must be consecutively numbered.  Furthermore, the existing
+support for stack-like registers is specific to the 80387 floating
+point coprocessor.  If you have a new architecture that uses
+stack-like registers, you will need to do substantial work on
+@file{reg-stack.c} and write your machine description to cooperate
+with it, as well as defining these macros.
+
+@defmac STACK_REGS
+Define this if the machine has any stack-like registers.
+@end defmac
+
+@defmac STACK_REG_COVER_CLASS
+This is a cover class containing the stack registers.  Define this if
+the machine has any stack-like registers.
+@end defmac
+
+@defmac FIRST_STACK_REG
+The number of the first stack-like register.  This one is the top
+of the stack.
+@end defmac
+
+@defmac LAST_STACK_REG
+The number of the last stack-like register.  This one is the bottom of
+the stack.
+@end defmac
+
+@node Register Classes
+@section Register Classes
+@cindex register class definitions
+@cindex class definitions, register
+
+On many machines, the numbered registers are not all equivalent.
+For example, certain registers may not be allowed for indexed addressing;
+certain registers may not be allowed in some instructions.  These machine
+restrictions are described to the compiler using @dfn{register classes}.
+
+You define a number of register classes, giving each one a name and saying
+which of the registers belong to it.  Then you can specify register classes
+that are allowed as operands to particular instruction patterns.
+
+@findex ALL_REGS
+@findex NO_REGS
+In general, each register will belong to several classes.  In fact, one
+class must be named @code{ALL_REGS} and contain all the registers.  Another
+class must be named @code{NO_REGS} and contain no registers.  Often the
+union of two classes will be another class; however, this is not required.
+
+@findex GENERAL_REGS
+One of the classes must be named @code{GENERAL_REGS}.  There is nothing
+terribly special about the name, but the operand constraint letters
+@samp{r} and @samp{g} specify this class.  If @code{GENERAL_REGS} is
+the same as @code{ALL_REGS}, just define it as a macro which expands
+to @code{ALL_REGS}.
+
+Order the classes so that if class @var{x} is contained in class @var{y}
+then @var{x} has a lower class number than @var{y}.
+
+The way classes other than @code{GENERAL_REGS} are specified in operand
+constraints is through machine-dependent operand constraint letters.
+You can define such letters to correspond to various classes, then use
+them in operand constraints.
+
+You must define the narrowest register classes for allocatable
+registers, so that each class either has no subclasses, or that for
+some mode, the move cost between registers within the class is
+cheaper than moving a register in the class to or from memory
+(@pxref{Costs}).
+
+You should define a class for the union of two classes whenever some
+instruction allows both classes.  For example, if an instruction allows
+either a floating point (coprocessor) register or a general register for a
+certain operand, you should define a class @code{FLOAT_OR_GENERAL_REGS}
+which includes both of them.  Otherwise you will get suboptimal code,
+or even internal compiler errors when reload cannot find a register in the
+class computed via @code{reg_class_subunion}.
+
+You must also specify certain redundant information about the register
+classes: for each class, which classes contain it and which ones are
+contained in it; for each pair of classes, the largest class contained
+in their union.
+
+When a value occupying several consecutive registers is expected in a
+certain class, all the registers used must belong to that class.
+Therefore, register classes cannot be used to enforce a requirement for
+a register pair to start with an even-numbered register.  The way to
+specify this requirement is with @code{TARGET_HARD_REGNO_MODE_OK}.
+
+Register classes used for input-operands of bitwise-and or shift
+instructions have a special requirement: each such class must have, for
+each fixed-point machine mode, a subclass whose registers can transfer that
+mode to or from memory.  For example, on some machines, the operations for
+single-byte values (@code{QImode}) are limited to certain registers.  When
+this is so, each register class that is used in a bitwise-and or shift
+instruction must have a subclass consisting of registers from which
+single-byte values can be loaded or stored.  This is so that
+@code{PREFERRED_RELOAD_CLASS} can always have a possible value to return.
+
+@deftp {Data type} {enum reg_class}
+An enumerated type that must be defined with all the register class names
+as enumerated values.  @code{NO_REGS} must be first.  @code{ALL_REGS}
+must be the last register class, followed by one more enumerated value,
+@code{LIM_REG_CLASSES}, which is not a register class but rather
+tells how many classes there are.
+
+Each register class has a number, which is the value of casting
+the class name to type @code{int}.  The number serves as an index
+in many of the tables described below.
+@end deftp
+
+@defmac N_REG_CLASSES
+The number of distinct register classes, defined as follows:
+
+@smallexample
+#define N_REG_CLASSES (int) LIM_REG_CLASSES
+@end smallexample
+@end defmac
+
+@defmac REG_CLASS_NAMES
+An initializer containing the names of the register classes as C string
+constants.  These names are used in writing some of the debugging dumps.
+@end defmac
+
+@defmac REG_CLASS_CONTENTS
+An initializer containing the contents of the register classes, as integers
+which are bit masks.  The @var{n}th integer specifies the contents of class
+@var{n}.  The way the integer @var{mask} is interpreted is that
+register @var{r} is in the class if @code{@var{mask} & (1 << @var{r})} is 1.
+
+When the machine has more than 32 registers, an integer does not suffice.
+Then the integers are replaced by sub-initializers, braced groupings containing
+several integers.  Each sub-initializer must be suitable as an initializer
+for the type @code{HARD_REG_SET} which is defined in @file{hard-reg-set.h}.
+In this situation, the first integer in each sub-initializer corresponds to
+registers 0 through 31, the second integer to registers 32 through 63, and
+so on.
+@end defmac
+
+@defmac REGNO_REG_CLASS (@var{regno})
+A C expression whose value is a register class containing hard register
+@var{regno}.  In general there is more than one such class; choose a class
+which is @dfn{minimal}, meaning that no smaller class also contains the
+register.
+@end defmac
+
+@defmac BASE_REG_CLASS
+A macro whose definition is the name of the class to which a valid
+base register must belong.  A base register is one used in an address
+which is the register value plus a displacement.
+@end defmac
+
+@defmac MODE_BASE_REG_CLASS (@var{mode})
+This is a variation of the @code{BASE_REG_CLASS} macro which allows
+the selection of a base register in a mode dependent manner.  If
+@var{mode} is VOIDmode then it should return the same value as
+@code{BASE_REG_CLASS}.
+@end defmac
+
+@defmac MODE_BASE_REG_REG_CLASS (@var{mode})
+A C expression whose value is the register class to which a valid
+base register must belong in order to be used in a base plus index
+register address.  You should define this macro if base plus index
+addresses have different requirements than other base register uses.
+@end defmac
+
+@defmac MODE_CODE_BASE_REG_CLASS (@var{mode}, @var{address_space}, @var{outer_code}, @var{index_code})
+A C expression whose value is the register class to which a valid
+base register for a memory reference in mode @var{mode} to address
+space @var{address_space} must belong.  @var{outer_code} and @var{index_code}
+define the context in which the base register occurs.  @var{outer_code} is
+the code of the immediately enclosing expression (@code{MEM} for the top level
+of an address, @code{ADDRESS} for something that occurs in an
+@code{address_operand}).  @var{index_code} is the code of the corresponding
+index expression if @var{outer_code} is @code{PLUS}; @code{SCRATCH} otherwise.
+@end defmac
+
+@defmac INDEX_REG_CLASS
+A macro whose definition is the name of the class to which a valid
+index register must belong.  An index register is one used in an
+address where its value is either multiplied by a scale factor or
+added to another register (as well as added to a displacement).
+@end defmac
+
+@defmac REGNO_OK_FOR_BASE_P (@var{num})
+A C expression which is nonzero if register number @var{num} is
+suitable for use as a base register in operand addresses.
+@end defmac
+
+@defmac REGNO_MODE_OK_FOR_BASE_P (@var{num}, @var{mode})
+A C expression that is just like @code{REGNO_OK_FOR_BASE_P}, except that
+that expression may examine the mode of the memory reference in
+@var{mode}.  You should define this macro if the mode of the memory
+reference affects whether a register may be used as a base register.  If
+you define this macro, the compiler will use it instead of
+@code{REGNO_OK_FOR_BASE_P}.  The mode may be @code{VOIDmode} for
+addresses that appear outside a @code{MEM}, i.e., as an
+@code{address_operand}.
+@end defmac
+
+@defmac REGNO_MODE_OK_FOR_REG_BASE_P (@var{num}, @var{mode})
+A C expression which is nonzero if register number @var{num} is suitable for
+use as a base register in base plus index operand addresses, accessing
+memory in mode @var{mode}.  It may be either a suitable hard register or a
+pseudo register that has been allocated such a hard register.  You should
+define this macro if base plus index addresses have different requirements
+than other base register uses.
+
+Use of this macro is deprecated; please use the more general
+@code{REGNO_MODE_CODE_OK_FOR_BASE_P}.
+@end defmac
+
+@defmac REGNO_MODE_CODE_OK_FOR_BASE_P (@var{num}, @var{mode}, @var{address_space}, @var{outer_code}, @var{index_code})
+A C expression which is nonzero if register number @var{num} is
+suitable for use as a base register in operand addresses, accessing
+memory in mode @var{mode} in address space @var{address_space}.
+This is similar to @code{REGNO_MODE_OK_FOR_BASE_P}, except
+that that expression may examine the context in which the register
+appears in the memory reference.  @var{outer_code} is the code of the
+immediately enclosing expression (@code{MEM} if at the top level of the
+address, @code{ADDRESS} for something that occurs in an
+@code{address_operand}).  @var{index_code} is the code of the
+corresponding index expression if @var{outer_code} is @code{PLUS};
+@code{SCRATCH} otherwise.  The mode may be @code{VOIDmode} for addresses
+that appear outside a @code{MEM}, i.e., as an @code{address_operand}.
+@end defmac
+
+@defmac REGNO_OK_FOR_INDEX_P (@var{num})
+A C expression which is nonzero if register number @var{num} is
+suitable for use as an index register in operand addresses.  It may be
+either a suitable hard register or a pseudo register that has been
+allocated such a hard register.
+
+The difference between an index register and a base register is that
+the index register may be scaled.  If an address involves the sum of
+two registers, neither one of them scaled, then either one may be
+labeled the ``base'' and the other the ``index''; but whichever
+labeling is used must fit the machine's constraints of which registers
+may serve in each capacity.  The compiler will try both labelings,
+looking for one that is valid, and will reload one or both registers
+only if neither labeling works.
+@end defmac
+
+@hook TARGET_PREFERRED_RENAME_CLASS
+
+@hook TARGET_PREFERRED_RELOAD_CLASS
+
+@defmac PREFERRED_RELOAD_CLASS (@var{x}, @var{class})
+A C expression that places additional restrictions on the register class
+to use when it is necessary to copy value @var{x} into a register in class
+@var{class}.  The value is a register class; perhaps @var{class}, or perhaps
+another, smaller class.  On many machines, the following definition is
+safe:
+
+@smallexample
+#define PREFERRED_RELOAD_CLASS(X,CLASS) CLASS
+@end smallexample
+
+Sometimes returning a more restrictive class makes better code.  For
+example, on the 68000, when @var{x} is an integer constant that is in range
+for a @samp{moveq} instruction, the value of this macro is always
+@code{DATA_REGS} as long as @var{class} includes the data registers.
+Requiring a data register guarantees that a @samp{moveq} will be used.
+
+One case where @code{PREFERRED_RELOAD_CLASS} must not return
+@var{class} is if @var{x} is a legitimate constant which cannot be
+loaded into some register class.  By returning @code{NO_REGS} you can
+force @var{x} into a memory location.  For example, rs6000 can load
+immediate values into general-purpose registers, but does not have an
+instruction for loading an immediate value into a floating-point
+register, so @code{PREFERRED_RELOAD_CLASS} returns @code{NO_REGS} when
+@var{x} is a floating-point constant.  If the constant cannot be loaded
+into any kind of register, code generation will be better if
+@code{TARGET_LEGITIMATE_CONSTANT_P} makes the constant illegitimate instead
+of using @code{TARGET_PREFERRED_RELOAD_CLASS}.
+
+If an insn has pseudos in it after register allocation, reload will go
+through the alternatives and call repeatedly @code{PREFERRED_RELOAD_CLASS}
+to find the best one.  Returning @code{NO_REGS}, in this case, makes
+reload add a @code{!} in front of the constraint: the x86 back-end uses
+this feature to discourage usage of 387 registers when math is done in
+the SSE registers (and vice versa).
+@end defmac
+
+@hook TARGET_PREFERRED_OUTPUT_RELOAD_CLASS
+
+@defmac LIMIT_RELOAD_CLASS (@var{mode}, @var{class})
+A C expression that places additional restrictions on the register class
+to use when it is necessary to be able to hold a value of mode
+@var{mode} in a reload register for which class @var{class} would
+ordinarily be used.
+
+Unlike @code{PREFERRED_RELOAD_CLASS}, this macro should be used when
+there are certain modes that simply cannot go in certain reload classes.
+
+The value is a register class; perhaps @var{class}, or perhaps another,
+smaller class.
+
+Don't define this macro unless the target machine has limitations which
+require the macro to do something nontrivial.
+@end defmac
+
+@hook TARGET_SECONDARY_RELOAD
+
+@defmac SECONDARY_RELOAD_CLASS (@var{class}, @var{mode}, @var{x})
+@defmacx SECONDARY_INPUT_RELOAD_CLASS (@var{class}, @var{mode}, @var{x})
+@defmacx SECONDARY_OUTPUT_RELOAD_CLASS (@var{class}, @var{mode}, @var{x})
+These macros are obsolete, new ports should use the target hook
+@code{TARGET_SECONDARY_RELOAD} instead.
+
+These are obsolete macros, replaced by the @code{TARGET_SECONDARY_RELOAD}
+target hook.  Older ports still define these macros to indicate to the
+reload phase that it may
+need to allocate at least one register for a reload in addition to the
+register to contain the data.  Specifically, if copying @var{x} to a
+register @var{class} in @var{mode} requires an intermediate register,
+you were supposed to define @code{SECONDARY_INPUT_RELOAD_CLASS} to return the
+largest register class all of whose registers can be used as
+intermediate registers or scratch registers.
+
+If copying a register @var{class} in @var{mode} to @var{x} requires an
+intermediate or scratch register, @code{SECONDARY_OUTPUT_RELOAD_CLASS}
+was supposed to be defined be defined to return the largest register
+class required.  If the
+requirements for input and output reloads were the same, the macro
+@code{SECONDARY_RELOAD_CLASS} should have been used instead of defining both
+macros identically.
+
+The values returned by these macros are often @code{GENERAL_REGS}.
+Return @code{NO_REGS} if no spare register is needed; i.e., if @var{x}
+can be directly copied to or from a register of @var{class} in
+@var{mode} without requiring a scratch register.  Do not define this
+macro if it would always return @code{NO_REGS}.
+
+If a scratch register is required (either with or without an
+intermediate register), you were supposed to define patterns for
+@samp{reload_in@var{m}} or @samp{reload_out@var{m}}, as required
+(@pxref{Standard Names}.  These patterns, which were normally
+implemented with a @code{define_expand}, should be similar to the
+@samp{mov@var{m}} patterns, except that operand 2 is the scratch
+register.
+
+These patterns need constraints for the reload register and scratch
+register that
+contain a single register class.  If the original reload register (whose
+class is @var{class}) can meet the constraint given in the pattern, the
+value returned by these macros is used for the class of the scratch
+register.  Otherwise, two additional reload registers are required.
+Their classes are obtained from the constraints in the insn pattern.
+
+@var{x} might be a pseudo-register or a @code{subreg} of a
+pseudo-register, which could either be in a hard register or in memory.
+Use @code{true_regnum} to find out; it will return @minus{}1 if the pseudo is
+in memory and the hard register number if it is in a register.
+
+These macros should not be used in the case where a particular class of
+registers can only be copied to memory and not to another class of
+registers.  In that case, secondary reload registers are not needed and
+would not be helpful.  Instead, a stack location must be used to perform
+the copy and the @code{mov@var{m}} pattern should use memory as an
+intermediate storage.  This case often occurs between floating-point and
+general registers.
+@end defmac
+
+@hook TARGET_SECONDARY_MEMORY_NEEDED
+
+@defmac SECONDARY_MEMORY_NEEDED_RTX (@var{mode})
+Normally when @code{TARGET_SECONDARY_MEMORY_NEEDED} is defined, the compiler
+allocates a stack slot for a memory location needed for register copies.
+If this macro is defined, the compiler instead uses the memory location
+defined by this macro.
+
+Do not define this macro if you do not define
+@code{TARGET_SECONDARY_MEMORY_NEEDED}.
+@end defmac
+
+@hook TARGET_SECONDARY_MEMORY_NEEDED_MODE
+
+@hook TARGET_SELECT_EARLY_REMAT_MODES
+
+@hook TARGET_CLASS_LIKELY_SPILLED_P
+
+@hook TARGET_CLASS_MAX_NREGS
+
+@defmac CLASS_MAX_NREGS (@var{class}, @var{mode})
+A C expression for the maximum number of consecutive registers
+of class @var{class} needed to hold a value of mode @var{mode}.
+
+This is closely related to the macro @code{TARGET_HARD_REGNO_NREGS}.  In fact,
+the value of the macro @code{CLASS_MAX_NREGS (@var{class}, @var{mode})}
+should be the maximum value of @code{TARGET_HARD_REGNO_NREGS (@var{regno},
+@var{mode})} for all @var{regno} values in the class @var{class}.
+
+This macro helps control the handling of multiple-word values
+in the reload pass.
+@end defmac
+
+@hook TARGET_CAN_CHANGE_MODE_CLASS
+
+@hook TARGET_IRA_CHANGE_PSEUDO_ALLOCNO_CLASS
+
+@hook TARGET_LRA_P
+
+@hook TARGET_REGISTER_PRIORITY
+
+@hook TARGET_REGISTER_USAGE_LEVELING_P
+
+@hook TARGET_DIFFERENT_ADDR_DISPLACEMENT_P
+
+@hook TARGET_CANNOT_SUBSTITUTE_MEM_EQUIV_P
+
+@hook TARGET_LEGITIMIZE_ADDRESS_DISPLACEMENT
+
+@hook TARGET_SPILL_CLASS
+
+@hook TARGET_ADDITIONAL_ALLOCNO_CLASS_P
+
+@hook TARGET_CSTORE_MODE
+
+@hook TARGET_COMPUTE_PRESSURE_CLASSES
+
+@node Stack and Calling
+@section Stack Layout and Calling Conventions
+@cindex calling conventions
+
+@c prevent bad page break with this line
+This describes the stack layout and calling conventions.
+
+@menu
+* Frame Layout::
+* Exception Handling::
+* Stack Checking::
+* Frame Registers::
+* Elimination::
+* Stack Arguments::
+* Register Arguments::
+* Scalar Return::
+* Aggregate Return::
+* Caller Saves::
+* Function Entry::
+* Profiling::
+* Tail Calls::
+* Shrink-wrapping separate components::
+* Stack Smashing Protection::
+* Miscellaneous Register Hooks::
+@end menu
+
+@node Frame Layout
+@subsection Basic Stack Layout
+@cindex stack frame layout
+@cindex frame layout
+
+@c prevent bad page break with this line
+Here is the basic stack layout.
+
+@defmac STACK_GROWS_DOWNWARD
+Define this macro to be true if pushing a word onto the stack moves the stack
+pointer to a smaller address, and false otherwise.
+@end defmac
+
+@defmac STACK_PUSH_CODE
+This macro defines the operation used when something is pushed
+on the stack.  In RTL, a push operation will be
+@code{(set (mem (STACK_PUSH_CODE (reg sp))) @dots{})}
+
+The choices are @code{PRE_DEC}, @code{POST_DEC}, @code{PRE_INC},
+and @code{POST_INC}.  Which of these is correct depends on
+the stack direction and on whether the stack pointer points
+to the last item on the stack or whether it points to the
+space for the next item on the stack.
+
+The default is @code{PRE_DEC} when @code{STACK_GROWS_DOWNWARD} is
+true, which is almost always right, and @code{PRE_INC} otherwise,
+which is often wrong.
+@end defmac
+
+@defmac FRAME_GROWS_DOWNWARD
+Define this macro to nonzero value if the addresses of local variable slots
+are at negative offsets from the frame pointer.
+@end defmac
+
+@defmac ARGS_GROW_DOWNWARD
+Define this macro if successive arguments to a function occupy decreasing
+addresses on the stack.
+@end defmac
+
+@hook TARGET_STARTING_FRAME_OFFSET
+
+@defmac STACK_ALIGNMENT_NEEDED
+Define to zero to disable final alignment of the stack during reload.
+The nonzero default for this macro is suitable for most ports.
+
+On ports where @code{TARGET_STARTING_FRAME_OFFSET} is nonzero or where there
+is a register save block following the local block that doesn't require
+alignment to @code{STACK_BOUNDARY}, it may be beneficial to disable
+stack alignment and do it in the backend.
+@end defmac
+
+@defmac STACK_POINTER_OFFSET
+Offset from the stack pointer register to the first location at which
+outgoing arguments are placed.  If not specified, the default value of
+zero is used.  This is the proper value for most machines.
+
+If @code{ARGS_GROW_DOWNWARD}, this is the offset to the location above
+the first location at which outgoing arguments are placed.
+@end defmac
+
+@defmac FIRST_PARM_OFFSET (@var{fundecl})
+Offset from the argument pointer register to the first argument's
+address.  On some machines it may depend on the data type of the
+function.
+
+If @code{ARGS_GROW_DOWNWARD}, this is the offset to the location above
+the first argument's address.
+@end defmac
+
+@defmac STACK_DYNAMIC_OFFSET (@var{fundecl})
+Offset from the stack pointer register to an item dynamically allocated
+on the stack, e.g., by @code{alloca}.
+
+The default value for this macro is @code{STACK_POINTER_OFFSET} plus the
+length of the outgoing arguments.  The default is correct for most
+machines.  See @file{function.c} for details.
+@end defmac
+
+@defmac INITIAL_FRAME_ADDRESS_RTX
+A C expression whose value is RTL representing the address of the initial
+stack frame. This address is passed to @code{RETURN_ADDR_RTX} and
+@code{DYNAMIC_CHAIN_ADDRESS}.  If you don't define this macro, a reasonable
+default value will be used.  Define this macro in order to make frame pointer
+elimination work in the presence of @code{__builtin_frame_address (count)} and
+@code{__builtin_return_address (count)} for @code{count} not equal to zero.
+@end defmac
+
+@defmac DYNAMIC_CHAIN_ADDRESS (@var{frameaddr})
+A C expression whose value is RTL representing the address in a stack
+frame where the pointer to the caller's frame is stored.  Assume that
+@var{frameaddr} is an RTL expression for the address of the stack frame
+itself.
+
+If you don't define this macro, the default is to return the value
+of @var{frameaddr}---that is, the stack frame address is also the
+address of the stack word that points to the previous frame.
+@end defmac
+
+@defmac SETUP_FRAME_ADDRESSES
+A C expression that produces the machine-specific code to
+setup the stack so that arbitrary frames can be accessed.  For example,
+on the SPARC, we must flush all of the register windows to the stack
+before we can access arbitrary stack frames.  You will seldom need to
+define this macro.  The default is to do nothing.
+@end defmac
+
+@hook TARGET_BUILTIN_SETJMP_FRAME_VALUE
+
+@defmac FRAME_ADDR_RTX (@var{frameaddr})
+A C expression whose value is RTL representing the value of the frame
+address for the current frame.  @var{frameaddr} is the frame pointer
+of the current frame.  This is used for __builtin_frame_address.
+You need only define this macro if the frame address is not the same
+as the frame pointer.  Most machines do not need to define it.
+@end defmac
+
+@defmac RETURN_ADDR_RTX (@var{count}, @var{frameaddr})
+A C expression whose value is RTL representing the value of the return
+address for the frame @var{count} steps up from the current frame, after
+the prologue.  @var{frameaddr} is the frame pointer of the @var{count}
+frame, or the frame pointer of the @var{count} @minus{} 1 frame if
+@code{RETURN_ADDR_IN_PREVIOUS_FRAME} is nonzero.
+
+The value of the expression must always be the correct address when
+@var{count} is zero, but may be @code{NULL_RTX} if there is no way to
+determine the return address of other frames.
+@end defmac
+
+@defmac RETURN_ADDR_IN_PREVIOUS_FRAME
+Define this macro to nonzero value if the return address of a particular
+stack frame is accessed from the frame pointer of the previous stack
+frame.  The zero default for this macro is suitable for most ports.
+@end defmac
+
+@defmac INCOMING_RETURN_ADDR_RTX
+A C expression whose value is RTL representing the location of the
+incoming return address at the beginning of any function, before the
+prologue.  This RTL is either a @code{REG}, indicating that the return
+value is saved in @samp{REG}, or a @code{MEM} representing a location in
+the stack.
+
+You only need to define this macro if you want to support call frame
+debugging information like that provided by DWARF 2.
+
+If this RTL is a @code{REG}, you should also define
+@code{DWARF_FRAME_RETURN_COLUMN} to @code{DWARF_FRAME_REGNUM (REGNO)}.
+@end defmac
+
+@defmac DWARF_ALT_FRAME_RETURN_COLUMN
+A C expression whose value is an integer giving a DWARF 2 column
+number that may be used as an alternative return column.  The column
+must not correspond to any gcc hard register (that is, it must not
+be in the range of @code{DWARF_FRAME_REGNUM}).
+
+This macro can be useful if @code{DWARF_FRAME_RETURN_COLUMN} is set to a
+general register, but an alternative column needs to be used for signal
+frames.  Some targets have also used different frame return columns
+over time.
+@end defmac
+
+@defmac DWARF_ZERO_REG
+A C expression whose value is an integer giving a DWARF 2 register
+number that is considered to always have the value zero.  This should
+only be defined if the target has an architected zero register, and
+someone decided it was a good idea to use that register number to
+terminate the stack backtrace.  New ports should avoid this.
+@end defmac
+
+@hook TARGET_DWARF_HANDLE_FRAME_UNSPEC
+
+@hook TARGET_DWARF_POLY_INDETERMINATE_VALUE
+
+@defmac INCOMING_FRAME_SP_OFFSET
+A C expression whose value is an integer giving the offset, in bytes,
+from the value of the stack pointer register to the top of the stack
+frame at the beginning of any function, before the prologue.  The top of
+the frame is defined to be the value of the stack pointer in the
+previous frame, just before the call instruction.
+
+You only need to define this macro if you want to support call frame
+debugging information like that provided by DWARF 2.
+@end defmac
+
+@defmac DEFAULT_INCOMING_FRAME_SP_OFFSET
+Like @code{INCOMING_FRAME_SP_OFFSET}, but must be the same for all
+functions of the same ABI, and when using GAS @code{.cfi_*} directives
+must also agree with the default CFI GAS emits.  Define this macro
+only if @code{INCOMING_FRAME_SP_OFFSET} can have different values
+between different functions of the same ABI or when
+@code{INCOMING_FRAME_SP_OFFSET} does not agree with GAS default CFI.
+@end defmac
+
+@defmac ARG_POINTER_CFA_OFFSET (@var{fundecl})
+A C expression whose value is an integer giving the offset, in bytes,
+from the argument pointer to the canonical frame address (cfa).  The
+final value should coincide with that calculated by
+@code{INCOMING_FRAME_SP_OFFSET}.  Which is unfortunately not usable
+during virtual register instantiation.
+
+The default value for this macro is
+@code{FIRST_PARM_OFFSET (fundecl) + crtl->args.pretend_args_size},
+which is correct for most machines; in general, the arguments are found
+immediately before the stack frame.  Note that this is not the case on
+some targets that save registers into the caller's frame, such as SPARC
+and rs6000, and so such targets need to define this macro.
+
+You only need to define this macro if the default is incorrect, and you
+want to support call frame debugging information like that provided by
+DWARF 2.
+@end defmac
+
+@defmac FRAME_POINTER_CFA_OFFSET (@var{fundecl})
+If defined, a C expression whose value is an integer giving the offset
+in bytes from the frame pointer to the canonical frame address (cfa).
+The final value should coincide with that calculated by
+@code{INCOMING_FRAME_SP_OFFSET}.
+
+Normally the CFA is calculated as an offset from the argument pointer,
+via @code{ARG_POINTER_CFA_OFFSET}, but if the argument pointer is
+variable due to the ABI, this may not be possible.  If this macro is
+defined, it implies that the virtual register instantiation should be
+based on the frame pointer instead of the argument pointer.  Only one
+of @code{FRAME_POINTER_CFA_OFFSET} and @code{ARG_POINTER_CFA_OFFSET}
+should be defined.
+@end defmac
+
+@defmac CFA_FRAME_BASE_OFFSET (@var{fundecl})
+If defined, a C expression whose value is an integer giving the offset
+in bytes from the canonical frame address (cfa) to the frame base used
+in DWARF 2 debug information.  The default is zero.  A different value
+may reduce the size of debug information on some ports.
+@end defmac
+
+@node Exception Handling
+@subsection Exception Handling Support
+@cindex exception handling
+
+@defmac EH_RETURN_DATA_REGNO (@var{N})
+A C expression whose value is the @var{N}th register number used for
+data by exception handlers, or @code{INVALID_REGNUM} if fewer than
+@var{N} registers are usable.
+
+The exception handling library routines communicate with the exception
+handlers via a set of agreed upon registers.  Ideally these registers
+should be call-clobbered; it is possible to use call-saved registers,
+but may negatively impact code size.  The target must support at least
+2 data registers, but should define 4 if there are enough free registers.
+
+You must define this macro if you want to support call frame exception
+handling like that provided by DWARF 2.
+@end defmac
+
+@defmac EH_RETURN_STACKADJ_RTX
+A C expression whose value is RTL representing a location in which
+to store a stack adjustment to be applied before function return.
+This is used to unwind the stack to an exception handler's call frame.
+It will be assigned zero on code paths that return normally.
+
+Typically this is a call-clobbered hard register that is otherwise
+untouched by the epilogue, but could also be a stack slot.
+
+Do not define this macro if the stack pointer is saved and restored
+by the regular prolog and epilog code in the call frame itself; in
+this case, the exception handling library routines will update the
+stack location to be restored in place.  Otherwise, you must define
+this macro if you want to support call frame exception handling like
+that provided by DWARF 2.
+@end defmac
+
+@defmac EH_RETURN_HANDLER_RTX
+A C expression whose value is RTL representing a location in which
+to store the address of an exception handler to which we should
+return.  It will not be assigned on code paths that return normally.
+
+Typically this is the location in the call frame at which the normal
+return address is stored.  For targets that return by popping an
+address off the stack, this might be a memory address just below
+the @emph{target} call frame rather than inside the current call
+frame.  If defined, @code{EH_RETURN_STACKADJ_RTX} will have already
+been assigned, so it may be used to calculate the location of the
+target call frame.
+
+Some targets have more complex requirements than storing to an
+address calculable during initial code generation.  In that case
+the @code{eh_return} instruction pattern should be used instead.
+
+If you want to support call frame exception handling, you must
+define either this macro or the @code{eh_return} instruction pattern.
+@end defmac
+
+@defmac RETURN_ADDR_OFFSET
+If defined, an integer-valued C expression for which rtl will be generated
+to add it to the exception handler address before it is searched in the
+exception handling tables, and to subtract it again from the address before
+using it to return to the exception handler.
+@end defmac
+
+@defmac ASM_PREFERRED_EH_DATA_FORMAT (@var{code}, @var{global})
+This macro chooses the encoding of pointers embedded in the exception
+handling sections.  If at all possible, this should be defined such
+that the exception handling section will not require dynamic relocations,
+and so may be read-only.
+
+@var{code} is 0 for data, 1 for code labels, 2 for function pointers.
+@var{global} is true if the symbol may be affected by dynamic relocations.
+The macro should return a combination of the @code{DW_EH_PE_*} defines
+as found in @file{dwarf2.h}.
+
+If this macro is not defined, pointers will not be encoded but
+represented directly.
+@end defmac
+
+@defmac ASM_MAYBE_OUTPUT_ENCODED_ADDR_RTX (@var{file}, @var{encoding}, @var{size}, @var{addr}, @var{done})
+This macro allows the target to emit whatever special magic is required
+to represent the encoding chosen by @code{ASM_PREFERRED_EH_DATA_FORMAT}.
+Generic code takes care of pc-relative and indirect encodings; this must
+be defined if the target uses text-relative or data-relative encodings.
+
+This is a C statement that branches to @var{done} if the format was
+handled.  @var{encoding} is the format chosen, @var{size} is the number
+of bytes that the format occupies, @var{addr} is the @code{SYMBOL_REF}
+to be emitted.
+@end defmac
+
+@defmac MD_FALLBACK_FRAME_STATE_FOR (@var{context}, @var{fs})
+This macro allows the target to add CPU and operating system specific
+code to the call-frame unwinder for use when there is no unwind data
+available.  The most common reason to implement this macro is to unwind
+through signal frames.
+
+This macro is called from @code{uw_frame_state_for} in
+@file{unwind-dw2.c}, @file{unwind-dw2-xtensa.c} and
+@file{unwind-ia64.c}.  @var{context} is an @code{_Unwind_Context};
+@var{fs} is an @code{_Unwind_FrameState}.  Examine @code{context->ra}
+for the address of the code being executed and @code{context->cfa} for
+the stack pointer value.  If the frame can be decoded, the register
+save addresses should be updated in @var{fs} and the macro should
+evaluate to @code{_URC_NO_REASON}.  If the frame cannot be decoded,
+the macro should evaluate to @code{_URC_END_OF_STACK}.
+
+For proper signal handling in Java this macro is accompanied by
+@code{MAKE_THROW_FRAME}, defined in @file{libjava/include/*-signal.h} headers.
+@end defmac
+
+@defmac MD_HANDLE_UNWABI (@var{context}, @var{fs})
+This macro allows the target to add operating system specific code to the
+call-frame unwinder to handle the IA-64 @code{.unwabi} unwinding directive,
+usually used for signal or interrupt frames.
+
+This macro is called from @code{uw_update_context} in libgcc's
+@file{unwind-ia64.c}.  @var{context} is an @code{_Unwind_Context};
+@var{fs} is an @code{_Unwind_FrameState}.  Examine @code{fs->unwabi}
+for the abi and context in the @code{.unwabi} directive.  If the
+@code{.unwabi} directive can be handled, the register save addresses should
+be updated in @var{fs}.
+@end defmac
+
+@defmac TARGET_USES_WEAK_UNWIND_INFO
+A C expression that evaluates to true if the target requires unwind
+info to be given comdat linkage.  Define it to be @code{1} if comdat
+linkage is necessary.  The default is @code{0}.
+@end defmac
+
+@node Stack Checking
+@subsection Specifying How Stack Checking is Done
+
+GCC will check that stack references are within the boundaries of the
+stack, if the option @option{-fstack-check} is specified, in one of
+three ways:
+
+@enumerate
+@item
+If the value of the @code{STACK_CHECK_BUILTIN} macro is nonzero, GCC
+will assume that you have arranged for full stack checking to be done
+at appropriate places in the configuration files.  GCC will not do
+other special processing.
+
+@item
+If @code{STACK_CHECK_BUILTIN} is zero and the value of the
+@code{STACK_CHECK_STATIC_BUILTIN} macro is nonzero, GCC will assume
+that you have arranged for static stack checking (checking of the
+static stack frame of functions) to be done at appropriate places
+in the configuration files.  GCC will only emit code to do dynamic
+stack checking (checking on dynamic stack allocations) using the third
+approach below.
+
+@item
+If neither of the above are true, GCC will generate code to periodically
+``probe'' the stack pointer using the values of the macros defined below.
+@end enumerate
+
+If neither STACK_CHECK_BUILTIN nor STACK_CHECK_STATIC_BUILTIN is defined,
+GCC will change its allocation strategy for large objects if the option
+@option{-fstack-check} is specified: they will always be allocated
+dynamically if their size exceeds @code{STACK_CHECK_MAX_VAR_SIZE} bytes.
+
+@defmac STACK_CHECK_BUILTIN
+A nonzero value if stack checking is done by the configuration files in a
+machine-dependent manner.  You should define this macro if stack checking
+is required by the ABI of your machine or if you would like to do stack
+checking in some more efficient way than the generic approach.  The default
+value of this macro is zero.
+@end defmac
+
+@defmac STACK_CHECK_STATIC_BUILTIN
+A nonzero value if static stack checking is done by the configuration files
+in a machine-dependent manner.  You should define this macro if you would
+like to do static stack checking in some more efficient way than the generic
+approach.  The default value of this macro is zero.
+@end defmac
+
+@defmac STACK_CHECK_PROBE_INTERVAL_EXP
+An integer specifying the interval at which GCC must generate stack probe
+instructions, defined as 2 raised to this integer.  You will normally
+define this macro so that the interval be no larger than the size of
+the ``guard pages'' at the end of a stack area.  The default value
+of 12 (4096-byte interval) is suitable for most systems.
+@end defmac
+
+@defmac STACK_CHECK_MOVING_SP
+An integer which is nonzero if GCC should move the stack pointer page by page
+when doing probes.  This can be necessary on systems where the stack pointer
+contains the bottom address of the memory area accessible to the executing
+thread at any point in time.  In this situation an alternate signal stack
+is required in order to be able to recover from a stack overflow.  The
+default value of this macro is zero.
+@end defmac
+
+@defmac STACK_CHECK_PROTECT
+The number of bytes of stack needed to recover from a stack overflow, for
+languages where such a recovery is supported.  The default value of 4KB/8KB
+with the @code{setjmp}/@code{longjmp}-based exception handling mechanism and
+8KB/12KB with other exception handling mechanisms should be adequate for most
+architectures and operating systems.
+@end defmac
+
+The following macros are relevant only if neither STACK_CHECK_BUILTIN
+nor STACK_CHECK_STATIC_BUILTIN is defined; you can omit them altogether
+in the opposite case.
+
+@defmac STACK_CHECK_MAX_FRAME_SIZE
+The maximum size of a stack frame, in bytes.  GCC will generate probe
+instructions in non-leaf functions to ensure at least this many bytes of
+stack are available.  If a stack frame is larger than this size, stack
+checking will not be reliable and GCC will issue a warning.  The
+default is chosen so that GCC only generates one instruction on most
+systems.  You should normally not change the default value of this macro.
+@end defmac
+
+@defmac STACK_CHECK_FIXED_FRAME_SIZE
+GCC uses this value to generate the above warning message.  It
+represents the amount of fixed frame used by a function, not including
+space for any callee-saved registers, temporaries and user variables.
+You need only specify an upper bound for this amount and will normally
+use the default of four words.
+@end defmac
+
+@defmac STACK_CHECK_MAX_VAR_SIZE
+The maximum size, in bytes, of an object that GCC will place in the
+fixed area of the stack frame when the user specifies
+@option{-fstack-check}.
+GCC computed the default from the values of the above macros and you will
+normally not need to override that default.
+@end defmac
+
+@hook TARGET_STACK_CLASH_PROTECTION_ALLOCA_PROBE_RANGE
+
+@need 2000
+@node Frame Registers
+@subsection Registers That Address the Stack Frame
+
+@c prevent bad page break with this line
+This discusses registers that address the stack frame.
+
+@defmac STACK_POINTER_REGNUM
+The register number of the stack pointer register, which must also be a
+fixed register according to @code{FIXED_REGISTERS}.  On most machines,
+the hardware determines which register this is.
+@end defmac
+
+@defmac FRAME_POINTER_REGNUM
+The register number of the frame pointer register, which is used to
+access automatic variables in the stack frame.  On some machines, the
+hardware determines which register this is.  On other machines, you can
+choose any register you wish for this purpose.
+@end defmac
+
+@defmac HARD_FRAME_POINTER_REGNUM
+On some machines the offset between the frame pointer and starting
+offset of the automatic variables is not known until after register
+allocation has been done (for example, because the saved registers are
+between these two locations).  On those machines, define
+@code{FRAME_POINTER_REGNUM} the number of a special, fixed register to
+be used internally until the offset is known, and define
+@code{HARD_FRAME_POINTER_REGNUM} to be the actual hard register number
+used for the frame pointer.
+
+You should define this macro only in the very rare circumstances when it
+is not possible to calculate the offset between the frame pointer and
+the automatic variables until after register allocation has been
+completed.  When this macro is defined, you must also indicate in your
+definition of @code{ELIMINABLE_REGS} how to eliminate
+@code{FRAME_POINTER_REGNUM} into either @code{HARD_FRAME_POINTER_REGNUM}
+or @code{STACK_POINTER_REGNUM}.
+
+Do not define this macro if it would be the same as
+@code{FRAME_POINTER_REGNUM}.
+@end defmac
+
+@defmac ARG_POINTER_REGNUM
+The register number of the arg pointer register, which is used to access
+the function's argument list.  On some machines, this is the same as the
+frame pointer register.  On some machines, the hardware determines which
+register this is.  On other machines, you can choose any register you
+wish for this purpose.  If this is not the same register as the frame
+pointer register, then you must mark it as a fixed register according to
+@code{FIXED_REGISTERS}, or arrange to be able to eliminate it
+(@pxref{Elimination}).
+@end defmac
+
+@defmac HARD_FRAME_POINTER_IS_FRAME_POINTER
+Define this to a preprocessor constant that is nonzero if
+@code{hard_frame_pointer_rtx} and @code{frame_pointer_rtx} should be
+the same.  The default definition is @samp{(HARD_FRAME_POINTER_REGNUM
+== FRAME_POINTER_REGNUM)}; you only need to define this macro if that
+definition is not suitable for use in preprocessor conditionals.
+@end defmac
+
+@defmac HARD_FRAME_POINTER_IS_ARG_POINTER
+Define this to a preprocessor constant that is nonzero if
+@code{hard_frame_pointer_rtx} and @code{arg_pointer_rtx} should be the
+same.  The default definition is @samp{(HARD_FRAME_POINTER_REGNUM ==
+ARG_POINTER_REGNUM)}; you only need to define this macro if that
+definition is not suitable for use in preprocessor conditionals.
+@end defmac
+
+@defmac RETURN_ADDRESS_POINTER_REGNUM
+The register number of the return address pointer register, which is used to
+access the current function's return address from the stack.  On some
+machines, the return address is not at a fixed offset from the frame
+pointer or stack pointer or argument pointer.  This register can be defined
+to point to the return address on the stack, and then be converted by
+@code{ELIMINABLE_REGS} into either the frame pointer or stack pointer.
+
+Do not define this macro unless there is no other way to get the return
+address from the stack.
+@end defmac
+
+@defmac STATIC_CHAIN_REGNUM
+@defmacx STATIC_CHAIN_INCOMING_REGNUM
+Register numbers used for passing a function's static chain pointer.  If
+register windows are used, the register number as seen by the called
+function is @code{STATIC_CHAIN_INCOMING_REGNUM}, while the register
+number as seen by the calling function is @code{STATIC_CHAIN_REGNUM}.  If
+these registers are the same, @code{STATIC_CHAIN_INCOMING_REGNUM} need
+not be defined.
+
+The static chain register need not be a fixed register.
+
+If the static chain is passed in memory, these macros should not be
+defined; instead, the @code{TARGET_STATIC_CHAIN} hook should be used.
+@end defmac
+
+@hook TARGET_STATIC_CHAIN
+
+@defmac DWARF_FRAME_REGISTERS
+This macro specifies the maximum number of hard registers that can be
+saved in a call frame.  This is used to size data structures used in
+DWARF2 exception handling.
+
+Prior to GCC 3.0, this macro was needed in order to establish a stable
+exception handling ABI in the face of adding new hard registers for ISA
+extensions.  In GCC 3.0 and later, the EH ABI is insulated from changes
+in the number of hard registers.  Nevertheless, this macro can still be
+used to reduce the runtime memory requirements of the exception handling
+routines, which can be substantial if the ISA contains a lot of
+registers that are not call-saved.
+
+If this macro is not defined, it defaults to
+@code{FIRST_PSEUDO_REGISTER}.
+@end defmac
+
+@defmac PRE_GCC3_DWARF_FRAME_REGISTERS
+
+This macro is similar to @code{DWARF_FRAME_REGISTERS}, but is provided
+for backward compatibility in pre GCC 3.0 compiled code.
+
+If this macro is not defined, it defaults to
+@code{DWARF_FRAME_REGISTERS}.
+@end defmac
+
+@defmac DWARF_REG_TO_UNWIND_COLUMN (@var{regno})
+
+Define this macro if the target's representation for dwarf registers
+is different than the internal representation for unwind column.
+Given a dwarf register, this macro should return the internal unwind
+column number to use instead.
+@end defmac
+
+@defmac DWARF_FRAME_REGNUM (@var{regno})
+
+Define this macro if the target's representation for dwarf registers
+used in .eh_frame or .debug_frame is different from that used in other
+debug info sections.  Given a GCC hard register number, this macro
+should return the .eh_frame register number.  The default is
+@code{DBX_REGISTER_NUMBER (@var{regno})}.
+
+@end defmac
+
+@defmac DWARF2_FRAME_REG_OUT (@var{regno}, @var{for_eh})
+
+Define this macro to map register numbers held in the call frame info
+that GCC has collected using @code{DWARF_FRAME_REGNUM} to those that
+should be output in .debug_frame (@code{@var{for_eh}} is zero) and
+.eh_frame (@code{@var{for_eh}} is nonzero).  The default is to
+return @code{@var{regno}}.
+
+@end defmac
+
+@defmac REG_VALUE_IN_UNWIND_CONTEXT
+
+Define this macro if the target stores register values as
+@code{_Unwind_Word} type in unwind context.  It should be defined if
+target register size is larger than the size of @code{void *}.  The
+default is to store register values as @code{void *} type.
+
+@end defmac
+
+@defmac ASSUME_EXTENDED_UNWIND_CONTEXT
+
+Define this macro to be 1 if the target always uses extended unwind
+context with version, args_size and by_value fields.  If it is undefined,
+it will be defined to 1 when @code{REG_VALUE_IN_UNWIND_CONTEXT} is
+defined and 0 otherwise.
+
+@end defmac
+
+@defmac DWARF_LAZY_REGISTER_VALUE (@var{regno}, @var{value})
+Define this macro if the target has pseudo DWARF registers whose
+values need to be computed lazily on demand by the unwinder (such as when
+referenced in a CFA expression).  The macro returns true if @var{regno}
+is such a register and stores its value in @samp{*@var{value}} if so.
+@end defmac
+
+@node Elimination
+@subsection Eliminating Frame Pointer and Arg Pointer
+
+@c prevent bad page break with this line
+This is about eliminating the frame pointer and arg pointer.
+
+@hook TARGET_FRAME_POINTER_REQUIRED
+
+@defmac ELIMINABLE_REGS
+This macro specifies a table of register pairs used to eliminate
+unneeded registers that point into the stack frame.
+
+The definition of this macro is a list of structure initializations, each
+of which specifies an original and replacement register.
+
+On some machines, the position of the argument pointer is not known until
+the compilation is completed.  In such a case, a separate hard register
+must be used for the argument pointer.  This register can be eliminated by
+replacing it with either the frame pointer or the argument pointer,
+depending on whether or not the frame pointer has been eliminated.
+
+In this case, you might specify:
+@smallexample
+#define ELIMINABLE_REGS  \
+@{@{ARG_POINTER_REGNUM, STACK_POINTER_REGNUM@}, \
+ @{ARG_POINTER_REGNUM, FRAME_POINTER_REGNUM@}, \
+ @{FRAME_POINTER_REGNUM, STACK_POINTER_REGNUM@}@}
+@end smallexample
+
+Note that the elimination of the argument pointer with the stack pointer is
+specified first since that is the preferred elimination.
+@end defmac
+
+@hook TARGET_CAN_ELIMINATE
+
+@defmac INITIAL_ELIMINATION_OFFSET (@var{from-reg}, @var{to-reg}, @var{offset-var})
+This macro returns the initial difference between the specified pair
+of registers.  The value would be computed from information
+such as the result of @code{get_frame_size ()} and the tables of
+registers @code{df_regs_ever_live_p} and @code{call_used_regs}.
+@end defmac
+
+@hook TARGET_COMPUTE_FRAME_LAYOUT
+
+@node Stack Arguments
+@subsection Passing Function Arguments on the Stack
+@cindex arguments on stack
+@cindex stack arguments
+
+The macros in this section control how arguments are passed
+on the stack.  See the following section for other macros that
+control passing certain arguments in registers.
+
+@hook TARGET_PROMOTE_PROTOTYPES
+
+@defmac PUSH_ARGS
+A C expression.  If nonzero, push insns will be used to pass
+outgoing arguments.
+If the target machine does not have a push instruction, set it to zero.
+That directs GCC to use an alternate strategy: to
+allocate the entire argument block and then store the arguments into
+it.  When @code{PUSH_ARGS} is nonzero, @code{PUSH_ROUNDING} must be defined too.
+@end defmac
+
+@defmac PUSH_ARGS_REVERSED
+A C expression.  If nonzero, function arguments will be evaluated from
+last to first, rather than from first to last.  If this macro is not
+defined, it defaults to @code{PUSH_ARGS} on targets where the stack
+and args grow in opposite directions, and 0 otherwise.
+@end defmac
+
+@defmac PUSH_ROUNDING (@var{npushed})
+A C expression that is the number of bytes actually pushed onto the
+stack when an instruction attempts to push @var{npushed} bytes.
+
+On some machines, the definition
+
+@smallexample
+#define PUSH_ROUNDING(BYTES) (BYTES)
+@end smallexample
+
+@noindent
+will suffice.  But on other machines, instructions that appear
+to push one byte actually push two bytes in an attempt to maintain
+alignment.  Then the definition should be
+
+@smallexample
+#define PUSH_ROUNDING(BYTES) (((BYTES) + 1) & ~1)
+@end smallexample
+
+If the value of this macro has a type, it should be an unsigned type.
+@end defmac
+
+@findex outgoing_args_size
+@findex crtl->outgoing_args_size
+@defmac ACCUMULATE_OUTGOING_ARGS
+A C expression.  If nonzero, the maximum amount of space required for outgoing arguments
+will be computed and placed into
+@code{crtl->outgoing_args_size}.  No space will be pushed
+onto the stack for each call; instead, the function prologue should
+increase the stack frame size by this amount.
+
+Setting both @code{PUSH_ARGS} and @code{ACCUMULATE_OUTGOING_ARGS}
+is not proper.
+@end defmac
+
+@defmac REG_PARM_STACK_SPACE (@var{fndecl})
+Define this macro if functions should assume that stack space has been
+allocated for arguments even when their values are passed in
+registers.
+
+The value of this macro is the size, in bytes, of the area reserved for
+arguments passed in registers for the function represented by @var{fndecl},
+which can be zero if GCC is calling a library function.
+The argument @var{fndecl} can be the FUNCTION_DECL, or the type itself
+of the function.
+
+This space can be allocated by the caller, or be a part of the
+machine-dependent stack frame: @code{OUTGOING_REG_PARM_STACK_SPACE} says
+which.
+@end defmac
+@c above is overfull.  not sure what to do.  --mew 5feb93  did
+@c something, not sure if it looks good.  --mew 10feb93
+
+@defmac INCOMING_REG_PARM_STACK_SPACE (@var{fndecl})
+Like @code{REG_PARM_STACK_SPACE}, but for incoming register arguments.
+Define this macro if space guaranteed when compiling a function body
+is different to space required when making a call, a situation that
+can arise with K&R style function definitions.
+@end defmac
+
+@defmac OUTGOING_REG_PARM_STACK_SPACE (@var{fntype})
+Define this to a nonzero value if it is the responsibility of the
+caller to allocate the area reserved for arguments passed in registers
+when calling a function of @var{fntype}.  @var{fntype} may be NULL
+if the function called is a library function.
+
+If @code{ACCUMULATE_OUTGOING_ARGS} is defined, this macro controls
+whether the space for these arguments counts in the value of
+@code{crtl->outgoing_args_size}.
+@end defmac
+
+@defmac STACK_PARMS_IN_REG_PARM_AREA
+Define this macro if @code{REG_PARM_STACK_SPACE} is defined, but the
+stack parameters don't skip the area specified by it.
+@c i changed this, makes more sens and it should have taken care of the
+@c overfull.. not as specific, tho.  --mew 5feb93
+
+Normally, when a parameter is not passed in registers, it is placed on the
+stack beyond the @code{REG_PARM_STACK_SPACE} area.  Defining this macro
+suppresses this behavior and causes the parameter to be passed on the
+stack in its natural location.
+@end defmac
+
+@hook TARGET_RETURN_POPS_ARGS
+
+@defmac CALL_POPS_ARGS (@var{cum})
+A C expression that should indicate the number of bytes a call sequence
+pops off the stack.  It is added to the value of @code{RETURN_POPS_ARGS}
+when compiling a function call.
+
+@var{cum} is the variable in which all arguments to the called function
+have been accumulated.
+
+On certain architectures, such as the SH5, a call trampoline is used
+that pops certain registers off the stack, depending on the arguments
+that have been passed to the function.  Since this is a property of the
+call site, not of the called function, @code{RETURN_POPS_ARGS} is not
+appropriate.
+@end defmac
+
+@node Register Arguments
+@subsection Passing Arguments in Registers
+@cindex arguments in registers
+@cindex registers arguments
+
+This section describes the macros which let you control how various
+types of arguments are passed in registers or how they are arranged in
+the stack.
+
+@hook TARGET_FUNCTION_ARG
+
+@hook TARGET_MUST_PASS_IN_STACK
+
+@hook TARGET_FUNCTION_INCOMING_ARG
+
+@hook TARGET_USE_PSEUDO_PIC_REG
+
+@hook TARGET_INIT_PIC_REG
+
+@hook TARGET_ARG_PARTIAL_BYTES
+
+@hook TARGET_PASS_BY_REFERENCE
+
+@hook TARGET_CALLEE_COPIES
+
+@defmac CUMULATIVE_ARGS
+A C type for declaring a variable that is used as the first argument
+of @code{TARGET_FUNCTION_ARG} and other related values.  For some
+target machines, the type @code{int} suffices and can hold the number
+of bytes of argument so far.
+
+There is no need to record in @code{CUMULATIVE_ARGS} anything about the
+arguments that have been passed on the stack.  The compiler has other
+variables to keep track of that.  For target machines on which all
+arguments are passed on the stack, there is no need to store anything in
+@code{CUMULATIVE_ARGS}; however, the data structure must exist and
+should not be empty, so use @code{int}.
+@end defmac
+
+@defmac OVERRIDE_ABI_FORMAT (@var{fndecl})
+If defined, this macro is called before generating any code for a
+function, but after the @var{cfun} descriptor for the function has been
+created.  The back end may use this macro to update @var{cfun} to
+reflect an ABI other than that which would normally be used by default.
+If the compiler is generating code for a compiler-generated function,
+@var{fndecl} may be @code{NULL}.
+@end defmac
+
+@defmac INIT_CUMULATIVE_ARGS (@var{cum}, @var{fntype}, @var{libname}, @var{fndecl}, @var{n_named_args})
+A C statement (sans semicolon) for initializing the variable
+@var{cum} for the state at the beginning of the argument list.  The
+variable has type @code{CUMULATIVE_ARGS}.  The value of @var{fntype}
+is the tree node for the data type of the function which will receive
+the args, or 0 if the args are to a compiler support library function.
+For direct calls that are not libcalls, @var{fndecl} contain the
+declaration node of the function.  @var{fndecl} is also set when
+@code{INIT_CUMULATIVE_ARGS} is used to find arguments for the function
+being compiled.  @var{n_named_args} is set to the number of named
+arguments, including a structure return address if it is passed as a
+parameter, when making a call.  When processing incoming arguments,
+@var{n_named_args} is set to @minus{}1.
+
+When processing a call to a compiler support library function,
+@var{libname} identifies which one.  It is a @code{symbol_ref} rtx which
+contains the name of the function, as a string.  @var{libname} is 0 when
+an ordinary C function call is being processed.  Thus, each time this
+macro is called, either @var{libname} or @var{fntype} is nonzero, but
+never both of them at once.
+@end defmac
+
+@defmac INIT_CUMULATIVE_LIBCALL_ARGS (@var{cum}, @var{mode}, @var{libname})
+Like @code{INIT_CUMULATIVE_ARGS} but only used for outgoing libcalls,
+it gets a @code{MODE} argument instead of @var{fntype}, that would be
+@code{NULL}.  @var{indirect} would always be zero, too.  If this macro
+is not defined, @code{INIT_CUMULATIVE_ARGS (cum, NULL_RTX, libname,
+0)} is used instead.
+@end defmac
+
+@defmac INIT_CUMULATIVE_INCOMING_ARGS (@var{cum}, @var{fntype}, @var{libname})
+Like @code{INIT_CUMULATIVE_ARGS} but overrides it for the purposes of
+finding the arguments for the function being compiled.  If this macro is
+undefined, @code{INIT_CUMULATIVE_ARGS} is used instead.
+
+The value passed for @var{libname} is always 0, since library routines
+with special calling conventions are never compiled with GCC@.  The
+argument @var{libname} exists for symmetry with
+@code{INIT_CUMULATIVE_ARGS}.
+@c could use "this macro" in place of @code{INIT_CUMULATIVE_ARGS}, maybe.
+@c --mew 5feb93   i switched the order of the sentences.  --mew 10feb93
+@end defmac
+
+@hook TARGET_FUNCTION_ARG_ADVANCE
+
+@hook TARGET_FUNCTION_ARG_OFFSET
+
+@hook TARGET_FUNCTION_ARG_PADDING
+
+@defmac PAD_VARARGS_DOWN
+If defined, a C expression which determines whether the default
+implementation of va_arg will attempt to pad down before reading the
+next argument, if that argument is smaller than its aligned space as
+controlled by @code{PARM_BOUNDARY}.  If this macro is not defined, all such
+arguments are padded down if @code{BYTES_BIG_ENDIAN} is true.
+@end defmac
+
+@defmac BLOCK_REG_PADDING (@var{mode}, @var{type}, @var{first})
+Specify padding for the last element of a block move between registers and
+memory.  @var{first} is nonzero if this is the only element.  Defining this
+macro allows better control of register function parameters on big-endian
+machines, without using @code{PARALLEL} rtl.  In particular,
+@code{MUST_PASS_IN_STACK} need not test padding and mode of types in
+registers, as there is no longer a "wrong" part of a register;  For example,
+a three byte aggregate may be passed in the high part of a register if so
+required.
+@end defmac
+
+@hook TARGET_FUNCTION_ARG_BOUNDARY
+
+@hook TARGET_FUNCTION_ARG_ROUND_BOUNDARY
+
+@defmac FUNCTION_ARG_REGNO_P (@var{regno})
+A C expression that is nonzero if @var{regno} is the number of a hard
+register in which function arguments are sometimes passed.  This does
+@emph{not} include implicit arguments such as the static chain and
+the structure-value address.  On many machines, no registers can be
+used for this purpose since all function arguments are pushed on the
+stack.
+@end defmac
+
+@hook TARGET_SPLIT_COMPLEX_ARG
+
+@hook TARGET_BUILD_BUILTIN_VA_LIST
+
+@hook TARGET_ENUM_VA_LIST_P
+
+@hook TARGET_FN_ABI_VA_LIST
+
+@hook TARGET_CANONICAL_VA_LIST_TYPE
+
+@hook TARGET_GIMPLIFY_VA_ARG_EXPR
+
+@hook TARGET_VALID_POINTER_MODE
+
+@hook TARGET_REF_MAY_ALIAS_ERRNO
+
+@hook TARGET_TRANSLATE_MODE_ATTRIBUTE
+
+@hook TARGET_SCALAR_MODE_SUPPORTED_P
+
+@hook TARGET_VECTOR_MODE_SUPPORTED_P
+
+@hook TARGET_COMPATIBLE_VECTOR_TYPES_P
+
+@hook TARGET_ARRAY_MODE
+
+@hook TARGET_ARRAY_MODE_SUPPORTED_P
+
+@hook TARGET_LIBGCC_FLOATING_MODE_SUPPORTED_P
+
+@hook TARGET_FLOATN_MODE
+
+@hook TARGET_FLOATN_BUILTIN_P
+
+@hook TARGET_SMALL_REGISTER_CLASSES_FOR_MODE_P
+
+@node Scalar Return
+@subsection How Scalar Function Values Are Returned
+@cindex return values in registers
+@cindex values, returned by functions
+@cindex scalars, returned as values
+
+This section discusses the macros that control returning scalars as
+values---values that can fit in registers.
+
+@hook TARGET_FUNCTION_VALUE
+
+@defmac FUNCTION_VALUE (@var{valtype}, @var{func})
+This macro has been deprecated.  Use @code{TARGET_FUNCTION_VALUE} for
+a new target instead.
+@end defmac
+
+@defmac LIBCALL_VALUE (@var{mode})
+A C expression to create an RTX representing the place where a library
+function returns a value of mode @var{mode}.
+
+Note that ``library function'' in this context means a compiler
+support routine, used to perform arithmetic, whose name is known
+specially by the compiler and was not mentioned in the C code being
+compiled.
+@end defmac
+
+@hook TARGET_LIBCALL_VALUE
+
+@defmac FUNCTION_VALUE_REGNO_P (@var{regno})
+A C expression that is nonzero if @var{regno} is the number of a hard
+register in which the values of called function may come back.
+
+A register whose use for returning values is limited to serving as the
+second of a pair (for a value of type @code{double}, say) need not be
+recognized by this macro.  So for most machines, this definition
+suffices:
+
+@smallexample
+#define FUNCTION_VALUE_REGNO_P(N) ((N) == 0)
+@end smallexample
+
+If the machine has register windows, so that the caller and the called
+function use different registers for the return value, this macro
+should recognize only the caller's register numbers.
+
+This macro has been deprecated.  Use @code{TARGET_FUNCTION_VALUE_REGNO_P}
+for a new target instead.
+@end defmac
+
+@hook TARGET_FUNCTION_VALUE_REGNO_P
+
+@defmac APPLY_RESULT_SIZE
+Define this macro if @samp{untyped_call} and @samp{untyped_return}
+need more space than is implied by @code{FUNCTION_VALUE_REGNO_P} for
+saving and restoring an arbitrary return value.
+@end defmac
+
+@hook TARGET_OMIT_STRUCT_RETURN_REG
+
+@hook TARGET_RETURN_IN_MSB
+
+@node Aggregate Return
+@subsection How Large Values Are Returned
+@cindex aggregates as return values
+@cindex large return values
+@cindex returning aggregate values
+@cindex structure value address
+
+When a function value's mode is @code{BLKmode} (and in some other
+cases), the value is not returned according to
+@code{TARGET_FUNCTION_VALUE} (@pxref{Scalar Return}).  Instead, the
+caller passes the address of a block of memory in which the value
+should be stored.  This address is called the @dfn{structure value
+address}.
+
+This section describes how to control returning structure values in
+memory.
+
+@hook TARGET_RETURN_IN_MEMORY
+
+@defmac DEFAULT_PCC_STRUCT_RETURN
+Define this macro to be 1 if all structure and union return values must be
+in memory.  Since this results in slower code, this should be defined
+only if needed for compatibility with other compilers or with an ABI@.
+If you define this macro to be 0, then the conventions used for structure
+and union return values are decided by the @code{TARGET_RETURN_IN_MEMORY}
+target hook.
+
+If not defined, this defaults to the value 1.
+@end defmac
+
+@hook TARGET_STRUCT_VALUE_RTX
+
+@defmac PCC_STATIC_STRUCT_RETURN
+Define this macro if the usual system convention on the target machine
+for returning structures and unions is for the called function to return
+the address of a static variable containing the value.
+
+Do not define this if the usual system convention is for the caller to
+pass an address to the subroutine.
+
+This macro has effect in @option{-fpcc-struct-return} mode, but it does
+nothing when you use @option{-freg-struct-return} mode.
+@end defmac
+
+@hook TARGET_GET_RAW_RESULT_MODE
+
+@hook TARGET_GET_RAW_ARG_MODE
+
+@hook TARGET_EMPTY_RECORD_P
+
+@hook TARGET_WARN_PARAMETER_PASSING_ABI
+
+@node Caller Saves
+@subsection Caller-Saves Register Allocation
+
+If you enable it, GCC can save registers around function calls.  This
+makes it possible to use call-clobbered registers to hold variables that
+must live across calls.
+
+@defmac HARD_REGNO_CALLER_SAVE_MODE (@var{regno}, @var{nregs})
+A C expression specifying which mode is required for saving @var{nregs}
+of a pseudo-register in call-clobbered hard register @var{regno}.  If
+@var{regno} is unsuitable for caller save, @code{VOIDmode} should be
+returned.  For most machines this macro need not be defined since GCC
+will select the smallest suitable mode.
+@end defmac
+
+@node Function Entry
+@subsection Function Entry and Exit
+@cindex function entry and exit
+@cindex prologue
+@cindex epilogue
+
+This section describes the macros that output function entry
+(@dfn{prologue}) and exit (@dfn{epilogue}) code.
+
+@hook TARGET_ASM_PRINT_PATCHABLE_FUNCTION_ENTRY
+
+@hook TARGET_ASM_FUNCTION_PROLOGUE
+
+@hook TARGET_ASM_FUNCTION_END_PROLOGUE
+
+@hook TARGET_ASM_FUNCTION_BEGIN_EPILOGUE
+
+@hook TARGET_ASM_FUNCTION_EPILOGUE
+
+@itemize @bullet
+@item
+@findex pretend_args_size
+@findex crtl->args.pretend_args_size
+A region of @code{crtl->args.pretend_args_size} bytes of
+uninitialized space just underneath the first argument arriving on the
+stack.  (This may not be at the very start of the allocated stack region
+if the calling sequence has pushed anything else since pushing the stack
+arguments.  But usually, on such machines, nothing else has been pushed
+yet, because the function prologue itself does all the pushing.)  This
+region is used on machines where an argument may be passed partly in
+registers and partly in memory, and, in some cases to support the
+features in @code{<stdarg.h>}.
+
+@item
+An area of memory used to save certain registers used by the function.
+The size of this area, which may also include space for such things as
+the return address and pointers to previous stack frames, is
+machine-specific and usually depends on which registers have been used
+in the function.  Machines with register windows often do not require
+a save area.
+
+@item
+A region of at least @var{size} bytes, possibly rounded up to an allocation
+boundary, to contain the local variables of the function.  On some machines,
+this region and the save area may occur in the opposite order, with the
+save area closer to the top of the stack.
+
+@item
+@cindex @code{ACCUMULATE_OUTGOING_ARGS} and stack frames
+Optionally, when @code{ACCUMULATE_OUTGOING_ARGS} is defined, a region of
+@code{crtl->outgoing_args_size} bytes to be used for outgoing
+argument lists of the function.  @xref{Stack Arguments}.
+@end itemize
+
+@defmac EXIT_IGNORE_STACK
+Define this macro as a C expression that is nonzero if the return
+instruction or the function epilogue ignores the value of the stack
+pointer; in other words, if it is safe to delete an instruction to
+adjust the stack pointer before a return from the function.  The
+default is 0.
+
+Note that this macro's value is relevant only for functions for which
+frame pointers are maintained.  It is never safe to delete a final
+stack adjustment in a function that has no frame pointer, and the
+compiler knows this regardless of @code{EXIT_IGNORE_STACK}.
+@end defmac
+
+@defmac EPILOGUE_USES (@var{regno})
+Define this macro as a C expression that is nonzero for registers that are
+used by the epilogue or the @samp{return} pattern.  The stack and frame
+pointer registers are already assumed to be used as needed.
+@end defmac
+
+@defmac EH_USES (@var{regno})
+Define this macro as a C expression that is nonzero for registers that are
+used by the exception handling mechanism, and so should be considered live
+on entry to an exception edge.
+@end defmac
+
+@hook TARGET_ASM_OUTPUT_MI_THUNK
+
+@hook TARGET_ASM_CAN_OUTPUT_MI_THUNK
+
+@node Profiling
+@subsection Generating Code for Profiling
+@cindex profiling, code generation
+
+These macros will help you generate code for profiling.
+
+@defmac FUNCTION_PROFILER (@var{file}, @var{labelno})
+A C statement or compound statement to output to @var{file} some
+assembler code to call the profiling subroutine @code{mcount}.
+
+@findex mcount
+The details of how @code{mcount} expects to be called are determined by
+your operating system environment, not by GCC@.  To figure them out,
+compile a small program for profiling using the system's installed C
+compiler and look at the assembler code that results.
+
+Older implementations of @code{mcount} expect the address of a counter
+variable to be loaded into some register.  The name of this variable is
+@samp{LP} followed by the number @var{labelno}, so you would generate
+the name using @samp{LP%d} in a @code{fprintf}.
+@end defmac
+
+@defmac PROFILE_HOOK
+A C statement or compound statement to output to @var{file} some assembly
+code to call the profiling subroutine @code{mcount} even the target does
+not support profiling.
+@end defmac
+
+@defmac NO_PROFILE_COUNTERS
+Define this macro to be an expression with a nonzero value if the
+@code{mcount} subroutine on your system does not need a counter variable
+allocated for each function.  This is true for almost all modern
+implementations.  If you define this macro, you must not use the
+@var{labelno} argument to @code{FUNCTION_PROFILER}.
+@end defmac
+
+@defmac PROFILE_BEFORE_PROLOGUE
+Define this macro if the code for function profiling should come before
+the function prologue.  Normally, the profiling code comes after.
+@end defmac
+
+@hook TARGET_KEEP_LEAF_WHEN_PROFILED
+
+@node Tail Calls
+@subsection Permitting tail calls
+@cindex tail calls
+
+@hook TARGET_FUNCTION_OK_FOR_SIBCALL
+
+@hook TARGET_EXTRA_LIVE_ON_ENTRY
+
+@hook TARGET_SET_UP_BY_PROLOGUE
+
+@hook TARGET_WARN_FUNC_RETURN
+
+@node Shrink-wrapping separate components
+@subsection Shrink-wrapping separate components
+@cindex shrink-wrapping separate components
+
+The prologue may perform a variety of target dependent tasks such as
+saving callee-saved registers, saving the return address, aligning the
+stack, creating a stack frame, initializing the PIC register, setting
+up the static chain, etc.
+
+On some targets some of these tasks may be independent of others and
+thus may be shrink-wrapped separately.  These independent tasks are
+referred to as components and are handled generically by the target
+independent parts of GCC.
+
+Using the following hooks those prologue or epilogue components can be
+shrink-wrapped separately, so that the initialization (and possibly
+teardown) those components do is not done as frequently on execution
+paths where this would unnecessary.
+
+What exactly those components are is up to the target code; the generic
+code treats them abstractly, as a bit in an @code{sbitmap}.  These
+@code{sbitmap}s are allocated by the @code{shrink_wrap.get_separate_components}
+and @code{shrink_wrap.components_for_bb} hooks, and deallocated by the
+generic code.
+
+@hook TARGET_SHRINK_WRAP_GET_SEPARATE_COMPONENTS
+
+@hook TARGET_SHRINK_WRAP_COMPONENTS_FOR_BB
+
+@hook TARGET_SHRINK_WRAP_DISQUALIFY_COMPONENTS
+
+@hook TARGET_SHRINK_WRAP_EMIT_PROLOGUE_COMPONENTS
+
+@hook TARGET_SHRINK_WRAP_EMIT_EPILOGUE_COMPONENTS
+
+@hook TARGET_SHRINK_WRAP_SET_HANDLED_COMPONENTS
+
+@node Stack Smashing Protection
+@subsection Stack smashing protection
+@cindex stack smashing protection
+
+@hook TARGET_STACK_PROTECT_GUARD
+
+@hook TARGET_STACK_PROTECT_FAIL
+
+@hook TARGET_STACK_PROTECT_RUNTIME_ENABLED_P
+
+@hook TARGET_SUPPORTS_SPLIT_STACK
+
+@hook TARGET_GET_VALID_OPTION_VALUES
+
+@node Miscellaneous Register Hooks
+@subsection Miscellaneous register hooks
+@cindex miscellaneous register hooks
+
+@hook TARGET_CALL_FUSAGE_CONTAINS_NON_CALLEE_CLOBBERS
+
+@node Varargs
+@section Implementing the Varargs Macros
+@cindex varargs implementation
+
+GCC comes with an implementation of @code{<varargs.h>} and
+@code{<stdarg.h>} that work without change on machines that pass arguments
+on the stack.  Other machines require their own implementations of
+varargs, and the two machine independent header files must have
+conditionals to include it.
+
+ISO @code{<stdarg.h>} differs from traditional @code{<varargs.h>} mainly in
+the calling convention for @code{va_start}.  The traditional
+implementation takes just one argument, which is the variable in which
+to store the argument pointer.  The ISO implementation of
+@code{va_start} takes an additional second argument.  The user is
+supposed to write the last named argument of the function here.
+
+However, @code{va_start} should not use this argument.  The way to find
+the end of the named arguments is with the built-in functions described
+below.
+
+@defmac __builtin_saveregs ()
+Use this built-in function to save the argument registers in memory so
+that the varargs mechanism can access them.  Both ISO and traditional
+versions of @code{va_start} must use @code{__builtin_saveregs}, unless
+you use @code{TARGET_SETUP_INCOMING_VARARGS} (see below) instead.
+
+On some machines, @code{__builtin_saveregs} is open-coded under the
+control of the target hook @code{TARGET_EXPAND_BUILTIN_SAVEREGS}.  On
+other machines, it calls a routine written in assembler language,
+found in @file{libgcc2.c}.
+
+Code generated for the call to @code{__builtin_saveregs} appears at the
+beginning of the function, as opposed to where the call to
+@code{__builtin_saveregs} is written, regardless of what the code is.
+This is because the registers must be saved before the function starts
+to use them for its own purposes.
+@c i rewrote the first sentence above to fix an overfull hbox. --mew
+@c 10feb93
+@end defmac
+
+@defmac __builtin_next_arg (@var{lastarg})
+This builtin returns the address of the first anonymous stack
+argument, as type @code{void *}.  If @code{ARGS_GROW_DOWNWARD}, it
+returns the address of the location above the first anonymous stack
+argument.  Use it in @code{va_start} to initialize the pointer for
+fetching arguments from the stack.  Also use it in @code{va_start} to
+verify that the second parameter @var{lastarg} is the last named argument
+of the current function.
+@end defmac
+
+@defmac __builtin_classify_type (@var{object})
+Since each machine has its own conventions for which data types are
+passed in which kind of register, your implementation of @code{va_arg}
+has to embody these conventions.  The easiest way to categorize the
+specified data type is to use @code{__builtin_classify_type} together
+with @code{sizeof} and @code{__alignof__}.
+
+@code{__builtin_classify_type} ignores the value of @var{object},
+considering only its data type.  It returns an integer describing what
+kind of type that is---integer, floating, pointer, structure, and so on.
+
+The file @file{typeclass.h} defines an enumeration that you can use to
+interpret the values of @code{__builtin_classify_type}.
+@end defmac
+
+These machine description macros help implement varargs:
+
+@hook TARGET_EXPAND_BUILTIN_SAVEREGS
+
+@hook TARGET_SETUP_INCOMING_VARARGS
+
+@hook TARGET_STRICT_ARGUMENT_NAMING
+
+@hook TARGET_CALL_ARGS
+
+@hook TARGET_END_CALL_ARGS
+
+@hook TARGET_PRETEND_OUTGOING_VARARGS_NAMED
+
+@hook TARGET_LOAD_BOUNDS_FOR_ARG
+
+@hook TARGET_STORE_BOUNDS_FOR_ARG
+
+@hook TARGET_LOAD_RETURNED_BOUNDS
+
+@hook TARGET_STORE_RETURNED_BOUNDS
+
+@node Trampolines
+@section Support for Nested Functions
+@cindex support for nested functions
+@cindex trampolines for nested functions
+@cindex descriptors for nested functions
+@cindex nested functions, support for
+
+Taking the address of a nested function requires special compiler
+handling to ensure that the static chain register is loaded when
+the function is invoked via an indirect call.
+
+GCC has traditionally supported nested functions by creating an
+executable @dfn{trampoline} at run time when the address of a nested
+function is taken.  This is a small piece of code which normally
+resides on the stack, in the stack frame of the containing function.
+The trampoline loads the static chain register and then jumps to the
+real address of the nested function.
+
+The use of trampolines requires an executable stack, which is a
+security risk.  To avoid this problem, GCC also supports another
+strategy: using descriptors for nested functions.  Under this model,
+taking the address of a nested function results in a pointer to a
+non-executable function descriptor object.  Initializing the static chain
+from the descriptor is handled at indirect call sites.
+
+On some targets, including HPPA and IA-64, function descriptors may be
+mandated by the ABI or be otherwise handled in a target-specific way
+by the back end in its code generation strategy for indirect calls.
+GCC also provides its own generic descriptor implementation to support the
+@option{-fno-trampolines} option.  In this case runtime detection of
+function descriptors at indirect call sites relies on descriptor
+pointers being tagged with a bit that is never set in bare function
+addresses.  Since GCC's generic function descriptors are
+not ABI-compliant, this option is typically used only on a
+per-language basis (notably by Ada) or when it can otherwise be
+applied to the whole program.
+
+Define the following hook if your backend either implements ABI-specified
+descriptor support, or can use GCC's generic descriptor implementation
+for nested functions.
+
+@hook TARGET_CUSTOM_FUNCTION_DESCRIPTORS
+
+The following macros tell GCC how to generate code to allocate and
+initialize an executable trampoline.  You can also use this interface
+if your back end needs to create ABI-specified non-executable descriptors; in
+this case the "trampoline" created is the descriptor containing data only.
+
+The instructions in an executable trampoline must do two things: load
+a constant address into the static chain register, and jump to the real
+address of the nested function.  On CISC machines such as the m68k,
+this requires two instructions, a move immediate and a jump.  Then the
+two addresses exist in the trampoline as word-long immediate operands.
+On RISC machines, it is often necessary to load each address into a
+register in two parts.  Then pieces of each address form separate
+immediate operands.
+
+The code generated to initialize the trampoline must store the variable
+parts---the static chain value and the function address---into the
+immediate operands of the instructions.  On a CISC machine, this is
+simply a matter of copying each address to a memory reference at the
+proper offset from the start of the trampoline.  On a RISC machine, it
+may be necessary to take out pieces of the address and store them
+separately.
+
+@hook TARGET_ASM_TRAMPOLINE_TEMPLATE
+
+@defmac TRAMPOLINE_SECTION
+Return the section into which the trampoline template is to be placed
+(@pxref{Sections}).  The default value is @code{readonly_data_section}.
+@end defmac
+
+@defmac TRAMPOLINE_SIZE
+A C expression for the size in bytes of the trampoline, as an integer.
+@end defmac
+
+@defmac TRAMPOLINE_ALIGNMENT
+Alignment required for trampolines, in bits.
+
+If you don't define this macro, the value of @code{FUNCTION_ALIGNMENT}
+is used for aligning trampolines.
+@end defmac
+
+@hook TARGET_TRAMPOLINE_INIT
+
+@hook TARGET_EMIT_CALL_BUILTIN___CLEAR_CACHE
+
+@hook TARGET_TRAMPOLINE_ADJUST_ADDRESS
+
+Implementing trampolines is difficult on many machines because they have
+separate instruction and data caches.  Writing into a stack location
+fails to clear the memory in the instruction cache, so when the program
+jumps to that location, it executes the old contents.
+
+Here are two possible solutions.  One is to clear the relevant parts of
+the instruction cache whenever a trampoline is set up.  The other is to
+make all trampolines identical, by having them jump to a standard
+subroutine.  The former technique makes trampoline execution faster; the
+latter makes initialization faster.
+
+To clear the instruction cache when a trampoline is initialized, define
+the following macro.
+
+@defmac CLEAR_INSN_CACHE (@var{beg}, @var{end})
+If defined, expands to a C expression clearing the @emph{instruction
+cache} in the specified interval.  The definition of this macro would
+typically be a series of @code{asm} statements.  Both @var{beg} and
+@var{end} are pointer expressions.
+@end defmac
+
+To use a standard subroutine, define the following macro.  In addition,
+you must make sure that the instructions in a trampoline fill an entire
+cache line with identical instructions, or else ensure that the
+beginning of the trampoline code is always aligned at the same point in
+its cache line.  Look in @file{m68k.h} as a guide.
+
+@defmac TRANSFER_FROM_TRAMPOLINE
+Define this macro if trampolines need a special subroutine to do their
+work.  The macro should expand to a series of @code{asm} statements
+which will be compiled with GCC@.  They go in a library function named
+@code{__transfer_from_trampoline}.
+
+If you need to avoid executing the ordinary prologue code of a compiled
+C function when you jump to the subroutine, you can do so by placing a
+special label of your own in the assembler code.  Use one @code{asm}
+statement to generate an assembler label, and another to make the label
+global.  Then trampolines can use that label to jump directly to your
+special assembler code.
+@end defmac
+
+@node Library Calls
+@section Implicit Calls to Library Routines
+@cindex library subroutine names
+@cindex @file{libgcc.a}
+
+@c prevent bad page break with this line
+Here is an explanation of implicit calls to library routines.
+
+@defmac DECLARE_LIBRARY_RENAMES
+This macro, if defined, should expand to a piece of C code that will get
+expanded when compiling functions for libgcc.a.  It can be used to
+provide alternate names for GCC's internal library functions if there
+are ABI-mandated names that the compiler should provide.
+@end defmac
+
+@findex set_optab_libfunc
+@findex init_one_libfunc
+@hook TARGET_INIT_LIBFUNCS
+
+@hook TARGET_LIBFUNC_GNU_PREFIX
+
+@defmac FLOAT_LIB_COMPARE_RETURNS_BOOL (@var{mode}, @var{comparison})
+This macro should return @code{true} if the library routine that
+implements the floating point comparison operator @var{comparison} in
+mode @var{mode} will return a boolean, and @var{false} if it will
+return a tristate.
+
+GCC's own floating point libraries return tristates from the
+comparison operators, so the default returns false always.  Most ports
+don't need to define this macro.
+@end defmac
+
+@defmac TARGET_LIB_INT_CMP_BIASED
+This macro should evaluate to @code{true} if the integer comparison
+functions (like @code{__cmpdi2}) return 0 to indicate that the first
+operand is smaller than the second, 1 to indicate that they are equal,
+and 2 to indicate that the first operand is greater than the second.
+If this macro evaluates to @code{false} the comparison functions return
+@minus{}1, 0, and 1 instead of 0, 1, and 2.  If the target uses the routines
+in @file{libgcc.a}, you do not need to define this macro.
+@end defmac
+
+@defmac TARGET_HAS_NO_HW_DIVIDE
+This macro should be defined if the target has no hardware divide
+instructions.  If this macro is defined, GCC will use an algorithm which
+make use of simple logical and arithmetic operations for 64-bit
+division.  If the macro is not defined, GCC will use an algorithm which
+make use of a 64-bit by 32-bit divide primitive.
+@end defmac
+
+@cindex @code{EDOM}, implicit usage
+@findex matherr
+@defmac TARGET_EDOM
+The value of @code{EDOM} on the target machine, as a C integer constant
+expression.  If you don't define this macro, GCC does not attempt to
+deposit the value of @code{EDOM} into @code{errno} directly.  Look in
+@file{/usr/include/errno.h} to find the value of @code{EDOM} on your
+system.
+
+If you do not define @code{TARGET_EDOM}, then compiled code reports
+domain errors by calling the library function and letting it report the
+error.  If mathematical functions on your system use @code{matherr} when
+there is an error, then you should leave @code{TARGET_EDOM} undefined so
+that @code{matherr} is used normally.
+@end defmac
+
+@cindex @code{errno}, implicit usage
+@defmac GEN_ERRNO_RTX
+Define this macro as a C expression to create an rtl expression that
+refers to the global ``variable'' @code{errno}.  (On certain systems,
+@code{errno} may not actually be a variable.)  If you don't define this
+macro, a reasonable default is used.
+@end defmac
+
+@hook TARGET_LIBC_HAS_FUNCTION
+
+@hook TARGET_LIBC_HAS_FAST_FUNCTION
+
+@defmac NEXT_OBJC_RUNTIME
+Set this macro to 1 to use the "NeXT" Objective-C message sending conventions
+by default.  This calling convention involves passing the object, the selector
+and the method arguments all at once to the method-lookup library function.
+This is the usual setting when targeting Darwin/Mac OS X systems, which have
+the NeXT runtime installed.
+
+If the macro is set to 0, the "GNU" Objective-C message sending convention
+will be used by default.  This convention passes just the object and the
+selector to the method-lookup function, which returns a pointer to the method.
+
+In either case, it remains possible to select code-generation for the alternate
+scheme, by means of compiler command line switches.
+@end defmac
+
+@node Addressing Modes
+@section Addressing Modes
+@cindex addressing modes
+
+@c prevent bad page break with this line
+This is about addressing modes.
+
+@defmac HAVE_PRE_INCREMENT
+@defmacx HAVE_PRE_DECREMENT
+@defmacx HAVE_POST_INCREMENT
+@defmacx HAVE_POST_DECREMENT
+A C expression that is nonzero if the machine supports pre-increment,
+pre-decrement, post-increment, or post-decrement addressing respectively.
+@end defmac
+
+@defmac HAVE_PRE_MODIFY_DISP
+@defmacx HAVE_POST_MODIFY_DISP
+A C expression that is nonzero if the machine supports pre- or
+post-address side-effect generation involving constants other than
+the size of the memory operand.
+@end defmac
+
+@defmac HAVE_PRE_MODIFY_REG
+@defmacx HAVE_POST_MODIFY_REG
+A C expression that is nonzero if the machine supports pre- or
+post-address side-effect generation involving a register displacement.
+@end defmac
+
+@defmac CONSTANT_ADDRESS_P (@var{x})
+A C expression that is 1 if the RTX @var{x} is a constant which
+is a valid address.  On most machines the default definition of
+@code{(CONSTANT_P (@var{x}) && GET_CODE (@var{x}) != CONST_DOUBLE)}
+is acceptable, but a few machines are more restrictive as to which
+constant addresses are supported.
+@end defmac
+
+@defmac CONSTANT_P (@var{x})
+@code{CONSTANT_P}, which is defined by target-independent code,
+accepts integer-values expressions whose values are not explicitly
+known, such as @code{symbol_ref}, @code{label_ref}, and @code{high}
+expressions and @code{const} arithmetic expressions, in addition to
+@code{const_int} and @code{const_double} expressions.
+@end defmac
+
+@defmac MAX_REGS_PER_ADDRESS
+A number, the maximum number of registers that can appear in a valid
+memory address.  Note that it is up to you to specify a value equal to
+the maximum number that @code{TARGET_LEGITIMATE_ADDRESS_P} would ever
+accept.
+@end defmac
+
+@hook TARGET_LEGITIMATE_ADDRESS_P
+
+@defmac TARGET_MEM_CONSTRAINT
+A single character to be used instead of the default @code{'m'}
+character for general memory addresses.  This defines the constraint
+letter which matches the memory addresses accepted by
+@code{TARGET_LEGITIMATE_ADDRESS_P}.  Define this macro if you want to
+support new address formats in your back end without changing the
+semantics of the @code{'m'} constraint.  This is necessary in order to
+preserve functionality of inline assembly constructs using the
+@code{'m'} constraint.
+@end defmac
+
+@defmac FIND_BASE_TERM (@var{x})
+A C expression to determine the base term of address @var{x},
+or to provide a simplified version of @var{x} from which @file{alias.c}
+can easily find the base term.  This macro is used in only two places:
+@code{find_base_value} and @code{find_base_term} in @file{alias.c}.
+
+It is always safe for this macro to not be defined.  It exists so
+that alias analysis can understand machine-dependent addresses.
+
+The typical use of this macro is to handle addresses containing
+a label_ref or symbol_ref within an UNSPEC@.
+@end defmac
+
+@hook TARGET_LEGITIMIZE_ADDRESS
+
+@defmac LEGITIMIZE_RELOAD_ADDRESS (@var{x}, @var{mode}, @var{opnum}, @var{type}, @var{ind_levels}, @var{win})
+A C compound statement that attempts to replace @var{x}, which is an address
+that needs reloading, with a valid memory address for an operand of mode
+@var{mode}.  @var{win} will be a C statement label elsewhere in the code.
+It is not necessary to define this macro, but it might be useful for
+performance reasons.
+
+For example, on the i386, it is sometimes possible to use a single
+reload register instead of two by reloading a sum of two pseudo
+registers into a register.  On the other hand, for number of RISC
+processors offsets are limited so that often an intermediate address
+needs to be generated in order to address a stack slot.  By defining
+@code{LEGITIMIZE_RELOAD_ADDRESS} appropriately, the intermediate addresses
+generated for adjacent some stack slots can be made identical, and thus
+be shared.
+
+@emph{Note}: This macro should be used with caution.  It is necessary
+to know something of how reload works in order to effectively use this,
+and it is quite easy to produce macros that build in too much knowledge
+of reload internals.
+
+@emph{Note}: This macro must be able to reload an address created by a
+previous invocation of this macro.  If it fails to handle such addresses
+then the compiler may generate incorrect code or abort.
+
+@findex push_reload
+The macro definition should use @code{push_reload} to indicate parts that
+need reloading; @var{opnum}, @var{type} and @var{ind_levels} are usually
+suitable to be passed unaltered to @code{push_reload}.
+
+The code generated by this macro must not alter the substructure of
+@var{x}.  If it transforms @var{x} into a more legitimate form, it
+should assign @var{x} (which will always be a C variable) a new value.
+This also applies to parts that you change indirectly by calling
+@code{push_reload}.
+
+@findex strict_memory_address_p
+The macro definition may use @code{strict_memory_address_p} to test if
+the address has become legitimate.
+
+@findex copy_rtx
+If you want to change only a part of @var{x}, one standard way of doing
+this is to use @code{copy_rtx}.  Note, however, that it unshares only a
+single level of rtl.  Thus, if the part to be changed is not at the
+top level, you'll need to replace first the top level.
+It is not necessary for this macro to come up with a legitimate
+address;  but often a machine-dependent strategy can generate better code.
+@end defmac
+
+@hook TARGET_MODE_DEPENDENT_ADDRESS_P
+
+@hook TARGET_LEGITIMATE_CONSTANT_P
+
+@hook TARGET_PRECOMPUTE_TLS_P
+
+@hook TARGET_DELEGITIMIZE_ADDRESS
+
+@hook TARGET_CONST_NOT_OK_FOR_DEBUG_P
+
+@hook TARGET_CANNOT_FORCE_CONST_MEM
+
+@hook TARGET_USE_BLOCKS_FOR_CONSTANT_P
+
+@hook TARGET_USE_BLOCKS_FOR_DECL_P
+
+@hook TARGET_BUILTIN_RECIPROCAL
+
+@hook TARGET_VECTORIZE_BUILTIN_MASK_FOR_LOAD
+
+@hook TARGET_VECTORIZE_BUILTIN_VECTORIZATION_COST
+
+@hook TARGET_VECTORIZE_PREFERRED_VECTOR_ALIGNMENT
+
+@hook TARGET_VECTORIZE_VECTOR_ALIGNMENT_REACHABLE
+
+@hook TARGET_VECTORIZE_VEC_PERM_CONST
+
+@hook TARGET_VECTORIZE_BUILTIN_VECTORIZED_FUNCTION
+
+@hook TARGET_VECTORIZE_BUILTIN_MD_VECTORIZED_FUNCTION
+
+@hook TARGET_VECTORIZE_SUPPORT_VECTOR_MISALIGNMENT
+
+@hook TARGET_VECTORIZE_PREFERRED_SIMD_MODE
+
+@hook TARGET_VECTORIZE_SPLIT_REDUCTION
+
+@hook TARGET_VECTORIZE_AUTOVECTORIZE_VECTOR_MODES
+
+@hook TARGET_VECTORIZE_RELATED_MODE
+
+@hook TARGET_VECTORIZE_GET_MASK_MODE
+
+@hook TARGET_VECTORIZE_EMPTY_MASK_IS_EXPENSIVE
+
+@hook TARGET_VECTORIZE_INIT_COST
+
+@hook TARGET_VECTORIZE_ADD_STMT_COST
+
+@hook TARGET_VECTORIZE_FINISH_COST
+
+@hook TARGET_VECTORIZE_DESTROY_COST_DATA
+
+@hook TARGET_VECTORIZE_BUILTIN_GATHER
+
+@hook TARGET_VECTORIZE_BUILTIN_SCATTER
+
+@hook TARGET_SIMD_CLONE_COMPUTE_VECSIZE_AND_SIMDLEN
+
+@hook TARGET_SIMD_CLONE_ADJUST
+
+@hook TARGET_SIMD_CLONE_USABLE
+
+@hook TARGET_SIMT_VF
+
+@hook TARGET_OMP_DEVICE_KIND_ARCH_ISA
+
+@hook TARGET_GOACC_VALIDATE_DIMS
+
+@hook TARGET_GOACC_DIM_LIMIT
+
+@hook TARGET_GOACC_FORK_JOIN
+
+@hook TARGET_GOACC_REDUCTION
+
+@hook TARGET_PREFERRED_ELSE_VALUE
+
+@node Anchored Addresses
+@section Anchored Addresses
+@cindex anchored addresses
+@cindex @option{-fsection-anchors}
+
+GCC usually addresses every static object as a separate entity.
+For example, if we have:
+
+@smallexample
+static int a, b, c;
+int foo (void) @{ return a + b + c; @}
+@end smallexample
+
+the code for @code{foo} will usually calculate three separate symbolic
+addresses: those of @code{a}, @code{b} and @code{c}.  On some targets,
+it would be better to calculate just one symbolic address and access
+the three variables relative to it.  The equivalent pseudocode would
+be something like:
+
+@smallexample
+int foo (void)
+@{
+  register int *xr = &x;
+  return xr[&a - &x] + xr[&b - &x] + xr[&c - &x];
+@}
+@end smallexample
+
+(which isn't valid C).  We refer to shared addresses like @code{x} as
+``section anchors''.  Their use is controlled by @option{-fsection-anchors}.
+
+The hooks below describe the target properties that GCC needs to know
+in order to make effective use of section anchors.  It won't use
+section anchors at all unless either @code{TARGET_MIN_ANCHOR_OFFSET}
+or @code{TARGET_MAX_ANCHOR_OFFSET} is set to a nonzero value.
+
+@hook TARGET_MIN_ANCHOR_OFFSET
+
+@hook TARGET_MAX_ANCHOR_OFFSET
+
+@hook TARGET_ASM_OUTPUT_ANCHOR
+
+@hook TARGET_USE_ANCHORS_FOR_SYMBOL_P
+
+@node Condition Code
+@section Condition Code Status
+@cindex condition code status
+
+The macros in this section can be split in two families, according to the
+two ways of representing condition codes in GCC.
+
+The first representation is the so called @code{(cc0)} representation
+(@pxref{Jump Patterns}), where all instructions can have an implicit
+clobber of the condition codes.  The second is the condition code
+register representation, which provides better schedulability for
+architectures that do have a condition code register, but on which
+most instructions do not affect it.  The latter category includes
+most RISC machines.
+
+The implicit clobbering poses a strong restriction on the placement of
+the definition and use of the condition code.  In the past the definition
+and use were always adjacent.  However, recent changes to support trapping
+arithmatic may result in the definition and user being in different blocks.
+Thus, there may be a @code{NOTE_INSN_BASIC_BLOCK} between them.  Additionally,
+the definition may be the source of exception handling edges.
+
+These restrictions can prevent important
+optimizations on some machines.  For example, on the IBM RS/6000, there
+is a delay for taken branches unless the condition code register is set
+three instructions earlier than the conditional branch.  The instruction
+scheduler cannot perform this optimization if it is not permitted to
+separate the definition and use of the condition code register.
+
+For this reason, it is possible and suggested to use a register to
+represent the condition code for new ports.  If there is a specific
+condition code register in the machine, use a hard register.  If the
+condition code or comparison result can be placed in any general register,
+or if there are multiple condition registers, use a pseudo register.
+Registers used to store the condition code value will usually have a mode
+that is in class @code{MODE_CC}.
+
+Alternatively, you can use @code{BImode} if the comparison operator is
+specified already in the compare instruction.  In this case, you are not
+interested in most macros in this section.
+
+@menu
+* CC0 Condition Codes::      Old style representation of condition codes.
+* MODE_CC Condition Codes::  Modern representation of condition codes.
+@end menu
+
+@node CC0 Condition Codes
+@subsection Representation of condition codes using @code{(cc0)}
+@findex cc0
+
+@findex cc_status
+The file @file{conditions.h} defines a variable @code{cc_status} to
+describe how the condition code was computed (in case the interpretation of
+the condition code depends on the instruction that it was set by).  This
+variable contains the RTL expressions on which the condition code is
+currently based, and several standard flags.
+
+Sometimes additional machine-specific flags must be defined in the machine
+description header file.  It can also add additional machine-specific
+information by defining @code{CC_STATUS_MDEP}.
+
+@defmac CC_STATUS_MDEP
+C code for a data type which is used for declaring the @code{mdep}
+component of @code{cc_status}.  It defaults to @code{int}.
+
+This macro is not used on machines that do not use @code{cc0}.
+@end defmac
+
+@defmac CC_STATUS_MDEP_INIT
+A C expression to initialize the @code{mdep} field to ``empty''.
+The default definition does nothing, since most machines don't use
+the field anyway.  If you want to use the field, you should probably
+define this macro to initialize it.
+
+This macro is not used on machines that do not use @code{cc0}.
+@end defmac
+
+@defmac NOTICE_UPDATE_CC (@var{exp}, @var{insn})
+A C compound statement to set the components of @code{cc_status}
+appropriately for an insn @var{insn} whose body is @var{exp}.  It is
+this macro's responsibility to recognize insns that set the condition
+code as a byproduct of other activity as well as those that explicitly
+set @code{(cc0)}.
+
+This macro is not used on machines that do not use @code{cc0}.
+
+If there are insns that do not set the condition code but do alter
+other machine registers, this macro must check to see whether they
+invalidate the expressions that the condition code is recorded as
+reflecting.  For example, on the 68000, insns that store in address
+registers do not set the condition code, which means that usually
+@code{NOTICE_UPDATE_CC} can leave @code{cc_status} unaltered for such
+insns.  But suppose that the previous insn set the condition code
+based on location @samp{a4@@(102)} and the current insn stores a new
+value in @samp{a4}.  Although the condition code is not changed by
+this, it will no longer be true that it reflects the contents of
+@samp{a4@@(102)}.  Therefore, @code{NOTICE_UPDATE_CC} must alter
+@code{cc_status} in this case to say that nothing is known about the
+condition code value.
+
+The definition of @code{NOTICE_UPDATE_CC} must be prepared to deal
+with the results of peephole optimization: insns whose patterns are
+@code{parallel} RTXs containing various @code{reg}, @code{mem} or
+constants which are just the operands.  The RTL structure of these
+insns is not sufficient to indicate what the insns actually do.  What
+@code{NOTICE_UPDATE_CC} should do when it sees one is just to run
+@code{CC_STATUS_INIT}.
+
+A possible definition of @code{NOTICE_UPDATE_CC} is to call a function
+that looks at an attribute (@pxref{Insn Attributes}) named, for example,
+@samp{cc}.  This avoids having detailed information about patterns in
+two places, the @file{md} file and in @code{NOTICE_UPDATE_CC}.
+@end defmac
+
+@node MODE_CC Condition Codes
+@subsection Representation of condition codes using registers
+@findex CCmode
+@findex MODE_CC
+
+@defmac SELECT_CC_MODE (@var{op}, @var{x}, @var{y})
+On many machines, the condition code may be produced by other instructions
+than compares, for example the branch can use directly the condition
+code set by a subtract instruction.  However, on some machines
+when the condition code is set this way some bits (such as the overflow
+bit) are not set in the same way as a test instruction, so that a different
+branch instruction must be used for some conditional branches.  When
+this happens, use the machine mode of the condition code register to
+record different formats of the condition code register.  Modes can
+also be used to record which compare instruction (e.g.@: a signed or an
+unsigned comparison) produced the condition codes.
+
+If other modes than @code{CCmode} are required, add them to
+@file{@var{machine}-modes.def} and define @code{SELECT_CC_MODE} to choose
+a mode given an operand of a compare.  This is needed because the modes
+have to be chosen not only during RTL generation but also, for example,
+by instruction combination.  The result of @code{SELECT_CC_MODE} should
+be consistent with the mode used in the patterns; for example to support
+the case of the add on the SPARC discussed above, we have the pattern
+
+@smallexample
+(define_insn ""
+  [(set (reg:CCNZ 0)
+        (compare:CCNZ
+          (plus:SI (match_operand:SI 0 "register_operand" "%r")
+                   (match_operand:SI 1 "arith_operand" "rI"))
+          (const_int 0)))]
+  ""
+  "@dots{}")
+@end smallexample
+
+@noindent
+together with a @code{SELECT_CC_MODE} that returns @code{CCNZmode}
+for comparisons whose argument is a @code{plus}:
+
+@smallexample
+#define SELECT_CC_MODE(OP,X,Y) \
+  (GET_MODE_CLASS (GET_MODE (X)) == MODE_FLOAT           \
+   ? ((OP == LT || OP == LE || OP == GT || OP == GE)     \
+      ? CCFPEmode : CCFPmode)                            \
+   : ((GET_CODE (X) == PLUS || GET_CODE (X) == MINUS     \
+       || GET_CODE (X) == NEG || GET_CODE (x) == ASHIFT) \
+      ? CCNZmode : CCmode))
+@end smallexample
+
+Another reason to use modes is to retain information on which operands
+were used by the comparison; see @code{REVERSIBLE_CC_MODE} later in
+this section.
+
+You should define this macro if and only if you define extra CC modes
+in @file{@var{machine}-modes.def}.
+@end defmac
+
+@hook TARGET_CANONICALIZE_COMPARISON
+
+@defmac REVERSIBLE_CC_MODE (@var{mode})
+A C expression whose value is one if it is always safe to reverse a
+comparison whose mode is @var{mode}.  If @code{SELECT_CC_MODE}
+can ever return @var{mode} for a floating-point inequality comparison,
+then @code{REVERSIBLE_CC_MODE (@var{mode})} must be zero.
+
+You need not define this macro if it would always returns zero or if the
+floating-point format is anything other than @code{IEEE_FLOAT_FORMAT}.
+For example, here is the definition used on the SPARC, where floating-point
+inequality comparisons are given either @code{CCFPEmode} or @code{CCFPmode}:
+
+@smallexample
+#define REVERSIBLE_CC_MODE(MODE) \
+   ((MODE) != CCFPEmode && (MODE) != CCFPmode)
+@end smallexample
+@end defmac
+
+@defmac REVERSE_CONDITION (@var{code}, @var{mode})
+A C expression whose value is reversed condition code of the @var{code} for
+comparison done in CC_MODE @var{mode}.  The macro is used only in case
+@code{REVERSIBLE_CC_MODE (@var{mode})} is nonzero.  Define this macro in case
+machine has some non-standard way how to reverse certain conditionals.  For
+instance in case all floating point conditions are non-trapping, compiler may
+freely convert unordered compares to ordered ones.  Then definition may look
+like:
+
+@smallexample
+#define REVERSE_CONDITION(CODE, MODE) \
+   ((MODE) != CCFPmode ? reverse_condition (CODE) \
+    : reverse_condition_maybe_unordered (CODE))
+@end smallexample
+@end defmac
+
+@hook TARGET_FIXED_CONDITION_CODE_REGS
+
+@hook TARGET_CC_MODES_COMPATIBLE
+
+@hook TARGET_FLAGS_REGNUM
+
+@node Costs
+@section Describing Relative Costs of Operations
+@cindex costs of instructions
+@cindex relative costs
+@cindex speed of instructions
+
+These macros let you describe the relative speed of various operations
+on the target machine.
+
+@defmac REGISTER_MOVE_COST (@var{mode}, @var{from}, @var{to})
+A C expression for the cost of moving data of mode @var{mode} from a
+register in class @var{from} to one in class @var{to}.  The classes are
+expressed using the enumeration values such as @code{GENERAL_REGS}.  A
+value of 2 is the default; other values are interpreted relative to
+that.
+
+It is not required that the cost always equal 2 when @var{from} is the
+same as @var{to}; on some machines it is expensive to move between
+registers if they are not general registers.
+
+If reload sees an insn consisting of a single @code{set} between two
+hard registers, and if @code{REGISTER_MOVE_COST} applied to their
+classes returns a value of 2, reload does not check to ensure that the
+constraints of the insn are met.  Setting a cost of other than 2 will
+allow reload to verify that the constraints are met.  You should do this
+if the @samp{mov@var{m}} pattern's constraints do not allow such copying.
+
+These macros are obsolete, new ports should use the target hook
+@code{TARGET_REGISTER_MOVE_COST} instead.
+@end defmac
+
+@hook TARGET_REGISTER_MOVE_COST
+
+@defmac MEMORY_MOVE_COST (@var{mode}, @var{class}, @var{in})
+A C expression for the cost of moving data of mode @var{mode} between a
+register of class @var{class} and memory; @var{in} is zero if the value
+is to be written to memory, nonzero if it is to be read in.  This cost
+is relative to those in @code{REGISTER_MOVE_COST}.  If moving between
+registers and memory is more expensive than between two registers, you
+should define this macro to express the relative cost.
+
+If you do not define this macro, GCC uses a default cost of 4 plus
+the cost of copying via a secondary reload register, if one is
+needed.  If your machine requires a secondary reload register to copy
+between memory and a register of @var{class} but the reload mechanism is
+more complex than copying via an intermediate, define this macro to
+reflect the actual cost of the move.
+
+GCC defines the function @code{memory_move_secondary_cost} if
+secondary reloads are needed.  It computes the costs due to copying via
+a secondary register.  If your machine copies from memory using a
+secondary register in the conventional way but the default base value of
+4 is not correct for your machine, define this macro to add some other
+value to the result of that function.  The arguments to that function
+are the same as to this macro.
+
+These macros are obsolete, new ports should use the target hook
+@code{TARGET_MEMORY_MOVE_COST} instead.
+@end defmac
+
+@hook TARGET_MEMORY_MOVE_COST
+
+@defmac BRANCH_COST (@var{speed_p}, @var{predictable_p})
+A C expression for the cost of a branch instruction.  A value of 1 is
+the default; other values are interpreted relative to that. Parameter
+@var{speed_p} is true when the branch in question should be optimized
+for speed.  When it is false, @code{BRANCH_COST} should return a value
+optimal for code size rather than performance.  @var{predictable_p} is
+true for well-predicted branches. On many architectures the
+@code{BRANCH_COST} can be reduced then.
+@end defmac
+
+Here are additional macros which do not specify precise relative costs,
+but only that certain actions are more expensive than GCC would
+ordinarily expect.
+
+@defmac SLOW_BYTE_ACCESS
+Define this macro as a C expression which is nonzero if accessing less
+than a word of memory (i.e.@: a @code{char} or a @code{short}) is no
+faster than accessing a word of memory, i.e., if such access
+require more than one instruction or if there is no difference in cost
+between byte and (aligned) word loads.
+
+When this macro is not defined, the compiler will access a field by
+finding the smallest containing object; when it is defined, a fullword
+load will be used if alignment permits.  Unless bytes accesses are
+faster than word accesses, using word accesses is preferable since it
+may eliminate subsequent memory access if subsequent accesses occur to
+other fields in the same word of the structure, but to different bytes.
+@end defmac
+
+@hook TARGET_SLOW_UNALIGNED_ACCESS
+
+@defmac MOVE_RATIO (@var{speed})
+The threshold of number of scalar memory-to-memory move insns, @emph{below}
+which a sequence of insns should be generated instead of a
+string move insn or a library call.  Increasing the value will always
+make code faster, but eventually incurs high cost in increased code size.
+
+Note that on machines where the corresponding move insn is a
+@code{define_expand} that emits a sequence of insns, this macro counts
+the number of such sequences.
+
+The parameter @var{speed} is true if the code is currently being
+optimized for speed rather than size.
+
+If you don't define this, a reasonable default is used.
+@end defmac
+
+@hook TARGET_USE_BY_PIECES_INFRASTRUCTURE_P
+
+@hook TARGET_COMPARE_BY_PIECES_BRANCH_RATIO
+
+@defmac MOVE_MAX_PIECES
+A C expression used by @code{move_by_pieces} to determine the largest unit
+a load or store used to copy memory is.  Defaults to @code{MOVE_MAX}.
+@end defmac
+
+@defmac STORE_MAX_PIECES
+A C expression used by @code{store_by_pieces} to determine the largest unit
+a store used to memory is.  Defaults to @code{MOVE_MAX_PIECES}, or two times
+the size of @code{HOST_WIDE_INT}, whichever is smaller.
+@end defmac
+
+@defmac COMPARE_MAX_PIECES
+A C expression used by @code{compare_by_pieces} to determine the largest unit
+a load or store used to compare memory is.  Defaults to
+@code{MOVE_MAX_PIECES}.
+@end defmac
+
+@defmac CLEAR_RATIO (@var{speed})
+The threshold of number of scalar move insns, @emph{below} which a sequence
+of insns should be generated to clear memory instead of a string clear insn
+or a library call.  Increasing the value will always make code faster, but
+eventually incurs high cost in increased code size.
+
+The parameter @var{speed} is true if the code is currently being
+optimized for speed rather than size.
+
+If you don't define this, a reasonable default is used.
+@end defmac
+
+@defmac SET_RATIO (@var{speed})
+The threshold of number of scalar move insns, @emph{below} which a sequence
+of insns should be generated to set memory to a constant value, instead of
+a block set insn or a library call.
+Increasing the value will always make code faster, but
+eventually incurs high cost in increased code size.
+
+The parameter @var{speed} is true if the code is currently being
+optimized for speed rather than size.
+
+If you don't define this, it defaults to the value of @code{MOVE_RATIO}.
+@end defmac
+
+@defmac USE_LOAD_POST_INCREMENT (@var{mode})
+A C expression used to determine whether a load postincrement is a good
+thing to use for a given mode.  Defaults to the value of
+@code{HAVE_POST_INCREMENT}.
+@end defmac
+
+@defmac USE_LOAD_POST_DECREMENT (@var{mode})
+A C expression used to determine whether a load postdecrement is a good
+thing to use for a given mode.  Defaults to the value of
+@code{HAVE_POST_DECREMENT}.
+@end defmac
+
+@defmac USE_LOAD_PRE_INCREMENT (@var{mode})
+A C expression used to determine whether a load preincrement is a good
+thing to use for a given mode.  Defaults to the value of
+@code{HAVE_PRE_INCREMENT}.
+@end defmac
+
+@defmac USE_LOAD_PRE_DECREMENT (@var{mode})
+A C expression used to determine whether a load predecrement is a good
+thing to use for a given mode.  Defaults to the value of
+@code{HAVE_PRE_DECREMENT}.
+@end defmac
+
+@defmac USE_STORE_POST_INCREMENT (@var{mode})
+A C expression used to determine whether a store postincrement is a good
+thing to use for a given mode.  Defaults to the value of
+@code{HAVE_POST_INCREMENT}.
+@end defmac
+
+@defmac USE_STORE_POST_DECREMENT (@var{mode})
+A C expression used to determine whether a store postdecrement is a good
+thing to use for a given mode.  Defaults to the value of
+@code{HAVE_POST_DECREMENT}.
+@end defmac
+
+@defmac USE_STORE_PRE_INCREMENT (@var{mode})
+This macro is used to determine whether a store preincrement is a good
+thing to use for a given mode.  Defaults to the value of
+@code{HAVE_PRE_INCREMENT}.
+@end defmac
+
+@defmac USE_STORE_PRE_DECREMENT (@var{mode})
+This macro is used to determine whether a store predecrement is a good
+thing to use for a given mode.  Defaults to the value of
+@code{HAVE_PRE_DECREMENT}.
+@end defmac
+
+@defmac NO_FUNCTION_CSE
+Define this macro to be true if it is as good or better to call a constant
+function address than to call an address kept in a register.
+@end defmac
+
+@defmac LOGICAL_OP_NON_SHORT_CIRCUIT
+Define this macro if a non-short-circuit operation produced by
+@samp{fold_range_test ()} is optimal.  This macro defaults to true if
+@code{BRANCH_COST} is greater than or equal to the value 2.
+@end defmac
+
+@hook TARGET_OPTAB_SUPPORTED_P
+
+@hook TARGET_RTX_COSTS
+
+@hook TARGET_ADDRESS_COST
+
+@hook TARGET_INSN_COST
+
+@hook TARGET_MAX_NOCE_IFCVT_SEQ_COST
+
+@hook TARGET_NOCE_CONVERSION_PROFITABLE_P
+
+@hook TARGET_NEW_ADDRESS_PROFITABLE_P
+
+@hook TARGET_NO_SPECULATION_IN_DELAY_SLOTS_P
+
+@hook TARGET_ESTIMATED_POLY_VALUE
+
+@node Scheduling
+@section Adjusting the Instruction Scheduler
+
+The instruction scheduler may need a fair amount of machine-specific
+adjustment in order to produce good code.  GCC provides several target
+hooks for this purpose.  It is usually enough to define just a few of
+them: try the first ones in this list first.
+
+@hook TARGET_SCHED_ISSUE_RATE
+
+@hook TARGET_SCHED_VARIABLE_ISSUE
+
+@hook TARGET_SCHED_ADJUST_COST
+
+@hook TARGET_SCHED_ADJUST_PRIORITY
+
+@hook TARGET_SCHED_REORDER
+
+@hook TARGET_SCHED_REORDER2
+
+@hook TARGET_SCHED_MACRO_FUSION_P
+
+@hook TARGET_SCHED_MACRO_FUSION_PAIR_P
+
+@hook TARGET_SCHED_DEPENDENCIES_EVALUATION_HOOK
+
+@hook TARGET_SCHED_INIT
+
+@hook TARGET_SCHED_FINISH
+
+@hook TARGET_SCHED_INIT_GLOBAL
+
+@hook TARGET_SCHED_FINISH_GLOBAL
+
+@hook TARGET_SCHED_DFA_PRE_CYCLE_INSN
+
+@hook TARGET_SCHED_INIT_DFA_PRE_CYCLE_INSN
+
+@hook TARGET_SCHED_DFA_POST_CYCLE_INSN
+
+@hook TARGET_SCHED_INIT_DFA_POST_CYCLE_INSN
+
+@hook TARGET_SCHED_DFA_PRE_ADVANCE_CYCLE
+
+@hook TARGET_SCHED_DFA_POST_ADVANCE_CYCLE
+
+@hook TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD
+
+@hook TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD
+
+@hook TARGET_SCHED_FIRST_CYCLE_MULTIPASS_BEGIN
+
+@hook TARGET_SCHED_FIRST_CYCLE_MULTIPASS_ISSUE
+
+@hook TARGET_SCHED_FIRST_CYCLE_MULTIPASS_BACKTRACK
+
+@hook TARGET_SCHED_FIRST_CYCLE_MULTIPASS_END
+
+@hook TARGET_SCHED_FIRST_CYCLE_MULTIPASS_INIT
+
+@hook TARGET_SCHED_FIRST_CYCLE_MULTIPASS_FINI
+
+@hook TARGET_SCHED_DFA_NEW_CYCLE
+
+@hook TARGET_SCHED_IS_COSTLY_DEPENDENCE
+
+@hook TARGET_SCHED_H_I_D_EXTENDED
+
+@hook TARGET_SCHED_ALLOC_SCHED_CONTEXT
+
+@hook TARGET_SCHED_INIT_SCHED_CONTEXT
+
+@hook TARGET_SCHED_SET_SCHED_CONTEXT
+
+@hook TARGET_SCHED_CLEAR_SCHED_CONTEXT
+
+@hook TARGET_SCHED_FREE_SCHED_CONTEXT
+
+@hook TARGET_SCHED_SPECULATE_INSN
+
+@hook TARGET_SCHED_NEEDS_BLOCK_P
+
+@hook TARGET_SCHED_GEN_SPEC_CHECK
+
+@hook TARGET_SCHED_SET_SCHED_FLAGS
+
+@hook TARGET_SCHED_CAN_SPECULATE_INSN
+
+@hook TARGET_SCHED_SMS_RES_MII
+
+@hook TARGET_SCHED_DISPATCH
+
+@hook TARGET_SCHED_DISPATCH_DO
+
+@hook TARGET_SCHED_EXPOSED_PIPELINE
+
+@hook TARGET_SCHED_REASSOCIATION_WIDTH
+
+@hook TARGET_SCHED_FUSION_PRIORITY
+
+@hook TARGET_EXPAND_DIVMOD_LIBFUNC
+
+@node Sections
+@section Dividing the Output into Sections (Texts, Data, @dots{})
+@c the above section title is WAY too long.  maybe cut the part between
+@c the (...)?  --mew 10feb93
+
+An object file is divided into sections containing different types of
+data.  In the most common case, there are three sections: the @dfn{text
+section}, which holds instructions and read-only data; the @dfn{data
+section}, which holds initialized writable data; and the @dfn{bss
+section}, which holds uninitialized data.  Some systems have other kinds
+of sections.
+
+@file{varasm.c} provides several well-known sections, such as
+@code{text_section}, @code{data_section} and @code{bss_section}.
+The normal way of controlling a @code{@var{foo}_section} variable
+is to define the associated @code{@var{FOO}_SECTION_ASM_OP} macro,
+as described below.  The macros are only read once, when @file{varasm.c}
+initializes itself, so their values must be run-time constants.
+They may however depend on command-line flags.
+
+@emph{Note:} Some run-time files, such @file{crtstuff.c}, also make
+use of the @code{@var{FOO}_SECTION_ASM_OP} macros, and expect them
+to be string literals.
+
+Some assemblers require a different string to be written every time a
+section is selected.  If your assembler falls into this category, you
+should define the @code{TARGET_ASM_INIT_SECTIONS} hook and use
+@code{get_unnamed_section} to set up the sections.
+
+You must always create a @code{text_section}, either by defining
+@code{TEXT_SECTION_ASM_OP} or by initializing @code{text_section}
+in @code{TARGET_ASM_INIT_SECTIONS}.  The same is true of
+@code{data_section} and @code{DATA_SECTION_ASM_OP}.  If you do not
+create a distinct @code{readonly_data_section}, the default is to
+reuse @code{text_section}.
+
+All the other @file{varasm.c} sections are optional, and are null
+if the target does not provide them.
+
+@defmac TEXT_SECTION_ASM_OP
+A C expression whose value is a string, including spacing, containing the
+assembler operation that should precede instructions and read-only data.
+Normally @code{"\t.text"} is right.
+@end defmac
+
+@defmac HOT_TEXT_SECTION_NAME
+If defined, a C string constant for the name of the section containing most
+frequently executed functions of the program.  If not defined, GCC will provide
+a default definition if the target supports named sections.
+@end defmac
+
+@defmac UNLIKELY_EXECUTED_TEXT_SECTION_NAME
+If defined, a C string constant for the name of the section containing unlikely
+executed functions in the program.
+@end defmac
+
+@defmac DATA_SECTION_ASM_OP
+A C expression whose value is a string, including spacing, containing the
+assembler operation to identify the following data as writable initialized
+data.  Normally @code{"\t.data"} is right.
+@end defmac
+
+@defmac SDATA_SECTION_ASM_OP
+If defined, a C expression whose value is a string, including spacing,
+containing the assembler operation to identify the following data as
+initialized, writable small data.
+@end defmac
+
+@defmac READONLY_DATA_SECTION_ASM_OP
+A C expression whose value is a string, including spacing, containing the
+assembler operation to identify the following data as read-only initialized
+data.
+@end defmac
+
+@defmac BSS_SECTION_ASM_OP
+If defined, a C expression whose value is a string, including spacing,
+containing the assembler operation to identify the following data as
+uninitialized global data.  If not defined, and
+@code{ASM_OUTPUT_ALIGNED_BSS} not defined,
+uninitialized global data will be output in the data section if
+@option{-fno-common} is passed, otherwise @code{ASM_OUTPUT_COMMON} will be
+used.
+@end defmac
+
+@defmac SBSS_SECTION_ASM_OP
+If defined, a C expression whose value is a string, including spacing,
+containing the assembler operation to identify the following data as
+uninitialized, writable small data.
+@end defmac
+
+@defmac TLS_COMMON_ASM_OP
+If defined, a C expression whose value is a string containing the
+assembler operation to identify the following data as thread-local
+common data.  The default is @code{".tls_common"}.
+@end defmac
+
+@defmac TLS_SECTION_ASM_FLAG
+If defined, a C expression whose value is a character constant
+containing the flag used to mark a section as a TLS section.  The
+default is @code{'T'}.
+@end defmac
+
+@defmac INIT_SECTION_ASM_OP
+If defined, a C expression whose value is a string, including spacing,
+containing the assembler operation to identify the following data as
+initialization code.  If not defined, GCC will assume such a section does
+not exist.  This section has no corresponding @code{init_section}
+variable; it is used entirely in runtime code.
+@end defmac
+
+@defmac FINI_SECTION_ASM_OP
+If defined, a C expression whose value is a string, including spacing,
+containing the assembler operation to identify the following data as
+finalization code.  If not defined, GCC will assume such a section does
+not exist.  This section has no corresponding @code{fini_section}
+variable; it is used entirely in runtime code.
+@end defmac
+
+@defmac INIT_ARRAY_SECTION_ASM_OP
+If defined, a C expression whose value is a string, including spacing,
+containing the assembler operation to identify the following data as
+part of the @code{.init_array} (or equivalent) section.  If not
+defined, GCC will assume such a section does not exist.  Do not define
+both this macro and @code{INIT_SECTION_ASM_OP}.
+@end defmac
+
+@defmac FINI_ARRAY_SECTION_ASM_OP
+If defined, a C expression whose value is a string, including spacing,
+containing the assembler operation to identify the following data as
+part of the @code{.fini_array} (or equivalent) section.  If not
+defined, GCC will assume such a section does not exist.  Do not define
+both this macro and @code{FINI_SECTION_ASM_OP}.
+@end defmac
+
+@defmac MACH_DEP_SECTION_ASM_FLAG
+If defined, a C expression whose value is a character constant
+containing the flag used to mark a machine-dependent section.  This
+corresponds to the @code{SECTION_MACH_DEP} section flag.
+@end defmac
+
+@defmac CRT_CALL_STATIC_FUNCTION (@var{section_op}, @var{function})
+If defined, an ASM statement that switches to a different section
+via @var{section_op}, calls @var{function}, and switches back to
+the text section.  This is used in @file{crtstuff.c} if
+@code{INIT_SECTION_ASM_OP} or @code{FINI_SECTION_ASM_OP} to calls
+to initialization and finalization functions from the init and fini
+sections.  By default, this macro uses a simple function call.  Some
+ports need hand-crafted assembly code to avoid dependencies on
+registers initialized in the function prologue or to ensure that
+constant pools don't end up too far way in the text section.
+@end defmac
+
+@defmac TARGET_LIBGCC_SDATA_SECTION
+If defined, a string which names the section into which small
+variables defined in crtstuff and libgcc should go.  This is useful
+when the target has options for optimizing access to small data, and
+you want the crtstuff and libgcc routines to be conservative in what
+they expect of your application yet liberal in what your application
+expects.  For example, for targets with a @code{.sdata} section (like
+MIPS), you could compile crtstuff with @code{-G 0} so that it doesn't
+require small data support from your application, but use this macro
+to put small data into @code{.sdata} so that your application can
+access these variables whether it uses small data or not.
+@end defmac
+
+@defmac FORCE_CODE_SECTION_ALIGN
+If defined, an ASM statement that aligns a code section to some
+arbitrary boundary.  This is used to force all fragments of the
+@code{.init} and @code{.fini} sections to have to same alignment
+and thus prevent the linker from having to add any padding.
+@end defmac
+
+@defmac JUMP_TABLES_IN_TEXT_SECTION
+Define this macro to be an expression with a nonzero value if jump
+tables (for @code{tablejump} insns) should be output in the text
+section, along with the assembler instructions.  Otherwise, the
+readonly data section is used.
+
+This macro is irrelevant if there is no separate readonly data section.
+@end defmac
+
+@hook TARGET_ASM_INIT_SECTIONS
+
+@hook TARGET_ASM_RELOC_RW_MASK
+
+@hook TARGET_ASM_GENERATE_PIC_ADDR_DIFF_VEC
+
+@hook TARGET_ASM_SELECT_SECTION
+
+@defmac USE_SELECT_SECTION_FOR_FUNCTIONS
+Define this macro if you wish TARGET_ASM_SELECT_SECTION to be called
+for @code{FUNCTION_DECL}s as well as for variables and constants.
+
+In the case of a @code{FUNCTION_DECL}, @var{reloc} will be zero if the
+function has been determined to be likely to be called, and nonzero if
+it is unlikely to be called.
+@end defmac
+
+@hook TARGET_ASM_UNIQUE_SECTION
+
+@hook TARGET_ASM_FUNCTION_RODATA_SECTION
+
+@hook TARGET_ASM_MERGEABLE_RODATA_PREFIX
+
+@hook TARGET_ASM_TM_CLONE_TABLE_SECTION
+
+@hook TARGET_ASM_SELECT_RTX_SECTION
+
+@hook TARGET_MANGLE_DECL_ASSEMBLER_NAME
+
+@hook TARGET_ENCODE_SECTION_INFO
+
+@hook TARGET_STRIP_NAME_ENCODING
+
+@hook TARGET_IN_SMALL_DATA_P
+
+@hook TARGET_HAVE_SRODATA_SECTION
+
+@hook TARGET_PROFILE_BEFORE_PROLOGUE
+
+@hook TARGET_BINDS_LOCAL_P
+
+@hook TARGET_HAVE_TLS
+
+
+@node PIC
+@section Position Independent Code
+@cindex position independent code
+@cindex PIC
+
+This section describes macros that help implement generation of position
+independent code.  Simply defining these macros is not enough to
+generate valid PIC; you must also add support to the hook
+@code{TARGET_LEGITIMATE_ADDRESS_P} and to the macro
+@code{PRINT_OPERAND_ADDRESS}, as well as @code{LEGITIMIZE_ADDRESS}.  You
+must modify the definition of @samp{movsi} to do something appropriate
+when the source operand contains a symbolic address.  You may also
+need to alter the handling of switch statements so that they use
+relative addresses.
+@c i rearranged the order of the macros above to try to force one of
+@c them to the next line, to eliminate an overfull hbox. --mew 10feb93
+
+@defmac PIC_OFFSET_TABLE_REGNUM
+The register number of the register used to address a table of static
+data addresses in memory.  In some cases this register is defined by a
+processor's ``application binary interface'' (ABI)@.  When this macro
+is defined, RTL is generated for this register once, as with the stack
+pointer and frame pointer registers.  If this macro is not defined, it
+is up to the machine-dependent files to allocate such a register (if
+necessary).  Note that this register must be fixed when in use (e.g.@:
+when @code{flag_pic} is true).
+@end defmac
+
+@defmac PIC_OFFSET_TABLE_REG_CALL_CLOBBERED
+A C expression that is nonzero if the register defined by
+@code{PIC_OFFSET_TABLE_REGNUM} is clobbered by calls.  If not defined,
+the default is zero.  Do not define
+this macro if @code{PIC_OFFSET_TABLE_REGNUM} is not defined.
+@end defmac
+
+@defmac LEGITIMATE_PIC_OPERAND_P (@var{x})
+A C expression that is nonzero if @var{x} is a legitimate immediate
+operand on the target machine when generating position independent code.
+You can assume that @var{x} satisfies @code{CONSTANT_P}, so you need not
+check this.  You can also assume @var{flag_pic} is true, so you need not
+check it either.  You need not define this macro if all constants
+(including @code{SYMBOL_REF}) can be immediate operands when generating
+position independent code.
+@end defmac
+
+@node Assembler Format
+@section Defining the Output Assembler Language
+
+This section describes macros whose principal purpose is to describe how
+to write instructions in assembler language---rather than what the
+instructions do.
+
+@menu
+* File Framework::       Structural information for the assembler file.
+* Data Output::          Output of constants (numbers, strings, addresses).
+* Uninitialized Data::   Output of uninitialized variables.
+* Label Output::         Output and generation of labels.
+* Initialization::       General principles of initialization
+                         and termination routines.
+* Macros for Initialization::
+                         Specific macros that control the handling of
+                         initialization and termination routines.
+* Instruction Output::   Output of actual instructions.
+* Dispatch Tables::      Output of jump tables.
+* Exception Region Output:: Output of exception region code.
+* Alignment Output::     Pseudo ops for alignment and skipping data.
+@end menu
+
+@node File Framework
+@subsection The Overall Framework of an Assembler File
+@cindex assembler format
+@cindex output of assembler code
+
+@c prevent bad page break with this line
+This describes the overall framework of an assembly file.
+
+@findex default_file_start
+@hook TARGET_ASM_FILE_START
+
+@hook TARGET_ASM_FILE_START_APP_OFF
+
+@hook TARGET_ASM_FILE_START_FILE_DIRECTIVE
+
+@hook TARGET_ASM_FILE_END
+
+@deftypefun void file_end_indicate_exec_stack ()
+Some systems use a common convention, the @samp{.note.GNU-stack}
+special section, to indicate whether or not an object file relies on
+the stack being executable.  If your system uses this convention, you
+should define @code{TARGET_ASM_FILE_END} to this function.  If you
+need to do other things in that hook, have your hook function call
+this function.
+@end deftypefun
+
+@hook TARGET_ASM_LTO_START
+
+@hook TARGET_ASM_LTO_END
+
+@hook TARGET_ASM_CODE_END
+
+@defmac ASM_COMMENT_START
+A C string constant describing how to begin a comment in the target
+assembler language.  The compiler assumes that the comment will end at
+the end of the line.
+@end defmac
+
+@defmac ASM_APP_ON
+A C string constant for text to be output before each @code{asm}
+statement or group of consecutive ones.  Normally this is
+@code{"#APP"}, which is a comment that has no effect on most
+assemblers but tells the GNU assembler that it must check the lines
+that follow for all valid assembler constructs.
+@end defmac
+
+@defmac ASM_APP_OFF
+A C string constant for text to be output after each @code{asm}
+statement or group of consecutive ones.  Normally this is
+@code{"#NO_APP"}, which tells the GNU assembler to resume making the
+time-saving assumptions that are valid for ordinary compiler output.
+@end defmac
+
+@defmac ASM_OUTPUT_SOURCE_FILENAME (@var{stream}, @var{name})
+A C statement to output COFF information or DWARF debugging information
+which indicates that filename @var{name} is the current source file to
+the stdio stream @var{stream}.
+
+This macro need not be defined if the standard form of output
+for the file format in use is appropriate.
+@end defmac
+
+@hook TARGET_ASM_OUTPUT_SOURCE_FILENAME
+
+@hook TARGET_ASM_OUTPUT_IDENT
+
+@defmac OUTPUT_QUOTED_STRING (@var{stream}, @var{string})
+A C statement to output the string @var{string} to the stdio stream
+@var{stream}.  If you do not call the function @code{output_quoted_string}
+in your config files, GCC will only call it to output filenames to
+the assembler source.  So you can use it to canonicalize the format
+of the filename using this macro.
+@end defmac
+
+@hook TARGET_ASM_NAMED_SECTION
+
+@hook TARGET_ASM_ELF_FLAGS_NUMERIC
+
+@hook TARGET_ASM_FUNCTION_SECTION
+
+@hook TARGET_ASM_FUNCTION_SWITCHED_TEXT_SECTIONS
+
+@hook TARGET_HAVE_NAMED_SECTIONS
+This flag is true if the target supports @code{TARGET_ASM_NAMED_SECTION}.
+It must not be modified by command-line option processing.
+@end deftypevr
+
+@anchor{TARGET_HAVE_SWITCHABLE_BSS_SECTIONS}
+@hook TARGET_HAVE_SWITCHABLE_BSS_SECTIONS
+
+@hook TARGET_SECTION_TYPE_FLAGS
+
+@hook TARGET_ASM_RECORD_GCC_SWITCHES
+
+@hook TARGET_ASM_RECORD_GCC_SWITCHES_SECTION
+
+@need 2000
+@node Data Output
+@subsection Output of Data
+
+
+@hook TARGET_ASM_BYTE_OP
+
+@hook TARGET_ASM_INTEGER
+
+@hook TARGET_ASM_DECL_END
+
+@hook TARGET_ASM_OUTPUT_ADDR_CONST_EXTRA
+
+@defmac ASM_OUTPUT_ASCII (@var{stream}, @var{ptr}, @var{len})
+A C statement to output to the stdio stream @var{stream} an assembler
+instruction to assemble a string constant containing the @var{len}
+bytes at @var{ptr}.  @var{ptr} will be a C expression of type
+@code{char *} and @var{len} a C expression of type @code{int}.
+
+If the assembler has a @code{.ascii} pseudo-op as found in the
+Berkeley Unix assembler, do not define the macro
+@code{ASM_OUTPUT_ASCII}.
+@end defmac
+
+@defmac ASM_OUTPUT_FDESC (@var{stream}, @var{decl}, @var{n})
+A C statement to output word @var{n} of a function descriptor for
+@var{decl}.  This must be defined if @code{TARGET_VTABLE_USES_DESCRIPTORS}
+is defined, and is otherwise unused.
+@end defmac
+
+@defmac CONSTANT_POOL_BEFORE_FUNCTION
+You may define this macro as a C expression.  You should define the
+expression to have a nonzero value if GCC should output the constant
+pool for a function before the code for the function, or a zero value if
+GCC should output the constant pool after the function.  If you do
+not define this macro, the usual case, GCC will output the constant
+pool before the function.
+@end defmac
+
+@defmac ASM_OUTPUT_POOL_PROLOGUE (@var{file}, @var{funname}, @var{fundecl}, @var{size})
+A C statement to output assembler commands to define the start of the
+constant pool for a function.  @var{funname} is a string giving
+the name of the function.  Should the return type of the function
+be required, it can be obtained via @var{fundecl}.  @var{size}
+is the size, in bytes, of the constant pool that will be written
+immediately after this call.
+
+If no constant-pool prefix is required, the usual case, this macro need
+not be defined.
+@end defmac
+
+@defmac ASM_OUTPUT_SPECIAL_POOL_ENTRY (@var{file}, @var{x}, @var{mode}, @var{align}, @var{labelno}, @var{jumpto})
+A C statement (with or without semicolon) to output a constant in the
+constant pool, if it needs special treatment.  (This macro need not do
+anything for RTL expressions that can be output normally.)
+
+The argument @var{file} is the standard I/O stream to output the
+assembler code on.  @var{x} is the RTL expression for the constant to
+output, and @var{mode} is the machine mode (in case @var{x} is a
+@samp{const_int}).  @var{align} is the required alignment for the value
+@var{x}; you should output an assembler directive to force this much
+alignment.
+
+The argument @var{labelno} is a number to use in an internal label for
+the address of this pool entry.  The definition of this macro is
+responsible for outputting the label definition at the proper place.
+Here is how to do this:
+
+@smallexample
+@code{(*targetm.asm_out.internal_label)} (@var{file}, "LC", @var{labelno});
+@end smallexample
+
+When you output a pool entry specially, you should end with a
+@code{goto} to the label @var{jumpto}.  This will prevent the same pool
+entry from being output a second time in the usual manner.
+
+You need not define this macro if it would do nothing.
+@end defmac
+
+@defmac ASM_OUTPUT_POOL_EPILOGUE (@var{file} @var{funname} @var{fundecl} @var{size})
+A C statement to output assembler commands to at the end of the constant
+pool for a function.  @var{funname} is a string giving the name of the
+function.  Should the return type of the function be required, you can
+obtain it via @var{fundecl}.  @var{size} is the size, in bytes, of the
+constant pool that GCC wrote immediately before this call.
+
+If no constant-pool epilogue is required, the usual case, you need not
+define this macro.
+@end defmac
+
+@defmac IS_ASM_LOGICAL_LINE_SEPARATOR (@var{C}, @var{STR})
+Define this macro as a C expression which is nonzero if @var{C} is
+used as a logical line separator by the assembler.  @var{STR} points
+to the position in the string where @var{C} was found; this can be used if
+a line separator uses multiple characters.
+
+If you do not define this macro, the default is that only
+the character @samp{;} is treated as a logical line separator.
+@end defmac
+
+@hook TARGET_ASM_OPEN_PAREN
+
+These macros are provided by @file{real.h} for writing the definitions
+of @code{ASM_OUTPUT_DOUBLE} and the like:
+
+@defmac REAL_VALUE_TO_TARGET_SINGLE (@var{x}, @var{l})
+@defmacx REAL_VALUE_TO_TARGET_DOUBLE (@var{x}, @var{l})
+@defmacx REAL_VALUE_TO_TARGET_LONG_DOUBLE (@var{x}, @var{l})
+@defmacx REAL_VALUE_TO_TARGET_DECIMAL32 (@var{x}, @var{l})
+@defmacx REAL_VALUE_TO_TARGET_DECIMAL64 (@var{x}, @var{l})
+@defmacx REAL_VALUE_TO_TARGET_DECIMAL128 (@var{x}, @var{l})
+These translate @var{x}, of type @code{REAL_VALUE_TYPE}, to the
+target's floating point representation, and store its bit pattern in
+the variable @var{l}.  For @code{REAL_VALUE_TO_TARGET_SINGLE} and
+@code{REAL_VALUE_TO_TARGET_DECIMAL32}, this variable should be a
+simple @code{long int}.  For the others, it should be an array of
+@code{long int}.  The number of elements in this array is determined
+by the size of the desired target floating point data type: 32 bits of
+it go in each @code{long int} array element.  Each array element holds
+32 bits of the result, even if @code{long int} is wider than 32 bits
+on the host machine.
+
+The array element values are designed so that you can print them out
+using @code{fprintf} in the order they should appear in the target
+machine's memory.
+@end defmac
+
+@node Uninitialized Data
+@subsection Output of Uninitialized Variables
+
+Each of the macros in this section is used to do the whole job of
+outputting a single uninitialized variable.
+
+@defmac ASM_OUTPUT_COMMON (@var{stream}, @var{name}, @var{size}, @var{rounded})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} the assembler definition of a common-label named
+@var{name} whose size is @var{size} bytes.  The variable @var{rounded}
+is the size rounded up to whatever alignment the caller wants.  It is
+possible that @var{size} may be zero, for instance if a struct with no
+other member than a zero-length array is defined.  In this case, the
+backend must output a symbol definition that allocates at least one
+byte, both so that the address of the resulting object does not compare
+equal to any other, and because some object formats cannot even express
+the concept of a zero-sized common symbol, as that is how they represent
+an ordinary undefined external.
+
+Use the expression @code{assemble_name (@var{stream}, @var{name})} to
+output the name itself; before and after that, output the additional
+assembler syntax for defining the name, and a newline.
+
+This macro controls how the assembler definitions of uninitialized
+common global variables are output.
+@end defmac
+
+@defmac ASM_OUTPUT_ALIGNED_COMMON (@var{stream}, @var{name}, @var{size}, @var{alignment})
+Like @code{ASM_OUTPUT_COMMON} except takes the required alignment as a
+separate, explicit argument.  If you define this macro, it is used in
+place of @code{ASM_OUTPUT_COMMON}, and gives you more flexibility in
+handling the required alignment of the variable.  The alignment is specified
+as the number of bits.
+@end defmac
+
+@defmac ASM_OUTPUT_ALIGNED_DECL_COMMON (@var{stream}, @var{decl}, @var{name}, @var{size}, @var{alignment})
+Like @code{ASM_OUTPUT_ALIGNED_COMMON} except that @var{decl} of the
+variable to be output, if there is one, or @code{NULL_TREE} if there
+is no corresponding variable.  If you define this macro, GCC will use it
+in place of both @code{ASM_OUTPUT_COMMON} and
+@code{ASM_OUTPUT_ALIGNED_COMMON}.  Define this macro when you need to see
+the variable's decl in order to chose what to output.
+@end defmac
+
+@defmac ASM_OUTPUT_ALIGNED_BSS (@var{stream}, @var{decl}, @var{name}, @var{size}, @var{alignment})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} the assembler definition of uninitialized global @var{decl} named
+@var{name} whose size is @var{size} bytes.  The variable @var{alignment}
+is the alignment specified as the number of bits.
+
+Try to use function @code{asm_output_aligned_bss} defined in file
+@file{varasm.c} when defining this macro.  If unable, use the expression
+@code{assemble_name (@var{stream}, @var{name})} to output the name itself;
+before and after that, output the additional assembler syntax for defining
+the name, and a newline.
+
+There are two ways of handling global BSS@.  One is to define this macro.
+The other is to have @code{TARGET_ASM_SELECT_SECTION} return a
+switchable BSS section (@pxref{TARGET_HAVE_SWITCHABLE_BSS_SECTIONS}).
+You do not need to do both.
+
+Some languages do not have @code{common} data, and require a
+non-common form of global BSS in order to handle uninitialized globals
+efficiently.  C++ is one example of this.  However, if the target does
+not support global BSS, the front end may choose to make globals
+common in order to save space in the object file.
+@end defmac
+
+@defmac ASM_OUTPUT_LOCAL (@var{stream}, @var{name}, @var{size}, @var{rounded})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} the assembler definition of a local-common-label named
+@var{name} whose size is @var{size} bytes.  The variable @var{rounded}
+is the size rounded up to whatever alignment the caller wants.
+
+Use the expression @code{assemble_name (@var{stream}, @var{name})} to
+output the name itself; before and after that, output the additional
+assembler syntax for defining the name, and a newline.
+
+This macro controls how the assembler definitions of uninitialized
+static variables are output.
+@end defmac
+
+@defmac ASM_OUTPUT_ALIGNED_LOCAL (@var{stream}, @var{name}, @var{size}, @var{alignment})
+Like @code{ASM_OUTPUT_LOCAL} except takes the required alignment as a
+separate, explicit argument.  If you define this macro, it is used in
+place of @code{ASM_OUTPUT_LOCAL}, and gives you more flexibility in
+handling the required alignment of the variable.  The alignment is specified
+as the number of bits.
+@end defmac
+
+@defmac ASM_OUTPUT_ALIGNED_DECL_LOCAL (@var{stream}, @var{decl}, @var{name}, @var{size}, @var{alignment})
+Like @code{ASM_OUTPUT_ALIGNED_LOCAL} except that @var{decl} of the
+variable to be output, if there is one, or @code{NULL_TREE} if there
+is no corresponding variable.  If you define this macro, GCC will use it
+in place of both @code{ASM_OUTPUT_LOCAL} and
+@code{ASM_OUTPUT_ALIGNED_LOCAL}.  Define this macro when you need to see
+the variable's decl in order to chose what to output.
+@end defmac
+
+@node Label Output
+@subsection Output and Generation of Labels
+
+@c prevent bad page break with this line
+This is about outputting labels.
+
+@findex assemble_name
+@defmac ASM_OUTPUT_LABEL (@var{stream}, @var{name})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} the assembler definition of a label named @var{name}.
+Use the expression @code{assemble_name (@var{stream}, @var{name})} to
+output the name itself; before and after that, output the additional
+assembler syntax for defining the name, and a newline.  A default
+definition of this macro is provided which is correct for most systems.
+@end defmac
+
+@defmac ASM_OUTPUT_FUNCTION_LABEL (@var{stream}, @var{name}, @var{decl})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} the assembler definition of a label named @var{name} of
+a function.
+Use the expression @code{assemble_name (@var{stream}, @var{name})} to
+output the name itself; before and after that, output the additional
+assembler syntax for defining the name, and a newline.  A default
+definition of this macro is provided which is correct for most systems.
+
+If this macro is not defined, then the function name is defined in the
+usual manner as a label (by means of @code{ASM_OUTPUT_LABEL}).
+@end defmac
+
+@findex assemble_name_raw
+@defmac ASM_OUTPUT_INTERNAL_LABEL (@var{stream}, @var{name})
+Identical to @code{ASM_OUTPUT_LABEL}, except that @var{name} is known
+to refer to a compiler-generated label.  The default definition uses
+@code{assemble_name_raw}, which is like @code{assemble_name} except
+that it is more efficient.
+@end defmac
+
+@defmac SIZE_ASM_OP
+A C string containing the appropriate assembler directive to specify the
+size of a symbol, without any arguments.  On systems that use ELF, the
+default (in @file{config/elfos.h}) is @samp{"\t.size\t"}; on other
+systems, the default is not to define this macro.
+
+Define this macro only if it is correct to use the default definitions
+of @code{ASM_OUTPUT_SIZE_DIRECTIVE} and @code{ASM_OUTPUT_MEASURED_SIZE}
+for your system.  If you need your own custom definitions of those
+macros, or if you do not need explicit symbol sizes at all, do not
+define this macro.
+@end defmac
+
+@defmac ASM_OUTPUT_SIZE_DIRECTIVE (@var{stream}, @var{name}, @var{size})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} a directive telling the assembler that the size of the
+symbol @var{name} is @var{size}.  @var{size} is a @code{HOST_WIDE_INT}.
+If you define @code{SIZE_ASM_OP}, a default definition of this macro is
+provided.
+@end defmac
+
+@defmac ASM_OUTPUT_MEASURED_SIZE (@var{stream}, @var{name})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} a directive telling the assembler to calculate the size of
+the symbol @var{name} by subtracting its address from the current
+address.
+
+If you define @code{SIZE_ASM_OP}, a default definition of this macro is
+provided.  The default assumes that the assembler recognizes a special
+@samp{.} symbol as referring to the current address, and can calculate
+the difference between this and another symbol.  If your assembler does
+not recognize @samp{.} or cannot do calculations with it, you will need
+to redefine @code{ASM_OUTPUT_MEASURED_SIZE} to use some other technique.
+@end defmac
+
+@defmac NO_DOLLAR_IN_LABEL
+Define this macro if the assembler does not accept the character
+@samp{$} in label names.  By default constructors and destructors in
+G++ have @samp{$} in the identifiers.  If this macro is defined,
+@samp{.} is used instead.
+@end defmac
+
+@defmac NO_DOT_IN_LABEL
+Define this macro if the assembler does not accept the character
+@samp{.} in label names.  By default constructors and destructors in G++
+have names that use @samp{.}.  If this macro is defined, these names
+are rewritten to avoid @samp{.}.
+@end defmac
+
+@defmac TYPE_ASM_OP
+A C string containing the appropriate assembler directive to specify the
+type of a symbol, without any arguments.  On systems that use ELF, the
+default (in @file{config/elfos.h}) is @samp{"\t.type\t"}; on other
+systems, the default is not to define this macro.
+
+Define this macro only if it is correct to use the default definition of
+@code{ASM_OUTPUT_TYPE_DIRECTIVE} for your system.  If you need your own
+custom definition of this macro, or if you do not need explicit symbol
+types at all, do not define this macro.
+@end defmac
+
+@defmac TYPE_OPERAND_FMT
+A C string which specifies (using @code{printf} syntax) the format of
+the second operand to @code{TYPE_ASM_OP}.  On systems that use ELF, the
+default (in @file{config/elfos.h}) is @samp{"@@%s"}; on other systems,
+the default is not to define this macro.
+
+Define this macro only if it is correct to use the default definition of
+@code{ASM_OUTPUT_TYPE_DIRECTIVE} for your system.  If you need your own
+custom definition of this macro, or if you do not need explicit symbol
+types at all, do not define this macro.
+@end defmac
+
+@defmac ASM_OUTPUT_TYPE_DIRECTIVE (@var{stream}, @var{type})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} a directive telling the assembler that the type of the
+symbol @var{name} is @var{type}.  @var{type} is a C string; currently,
+that string is always either @samp{"function"} or @samp{"object"}, but
+you should not count on this.
+
+If you define @code{TYPE_ASM_OP} and @code{TYPE_OPERAND_FMT}, a default
+definition of this macro is provided.
+@end defmac
+
+@defmac ASM_DECLARE_FUNCTION_NAME (@var{stream}, @var{name}, @var{decl})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} any text necessary for declaring the name @var{name} of a
+function which is being defined.  This macro is responsible for
+outputting the label definition (perhaps using
+@code{ASM_OUTPUT_FUNCTION_LABEL}).  The argument @var{decl} is the
+@code{FUNCTION_DECL} tree node representing the function.
+
+If this macro is not defined, then the function name is defined in the
+usual manner as a label (by means of @code{ASM_OUTPUT_FUNCTION_LABEL}).
+
+You may wish to use @code{ASM_OUTPUT_TYPE_DIRECTIVE} in the definition
+of this macro.
+@end defmac
+
+@defmac ASM_DECLARE_FUNCTION_SIZE (@var{stream}, @var{name}, @var{decl})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} any text necessary for declaring the size of a function
+which is being defined.  The argument @var{name} is the name of the
+function.  The argument @var{decl} is the @code{FUNCTION_DECL} tree node
+representing the function.
+
+If this macro is not defined, then the function size is not defined.
+
+You may wish to use @code{ASM_OUTPUT_MEASURED_SIZE} in the definition
+of this macro.
+@end defmac
+
+@defmac ASM_DECLARE_COLD_FUNCTION_NAME (@var{stream}, @var{name}, @var{decl})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} any text necessary for declaring the name @var{name} of a
+cold function partition which is being defined.  This macro is responsible
+for outputting the label definition (perhaps using
+@code{ASM_OUTPUT_FUNCTION_LABEL}).  The argument @var{decl} is the
+@code{FUNCTION_DECL} tree node representing the function.
+
+If this macro is not defined, then the cold partition name is defined in the
+usual manner as a label (by means of @code{ASM_OUTPUT_LABEL}).
+
+You may wish to use @code{ASM_OUTPUT_TYPE_DIRECTIVE} in the definition
+of this macro.
+@end defmac
+
+@defmac ASM_DECLARE_COLD_FUNCTION_SIZE (@var{stream}, @var{name}, @var{decl})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} any text necessary for declaring the size of a cold function
+partition which is being defined.  The argument @var{name} is the name of the
+cold partition of the function.  The argument @var{decl} is the
+@code{FUNCTION_DECL} tree node representing the function.
+
+If this macro is not defined, then the partition size is not defined.
+
+You may wish to use @code{ASM_OUTPUT_MEASURED_SIZE} in the definition
+of this macro.
+@end defmac
+
+@defmac ASM_DECLARE_OBJECT_NAME (@var{stream}, @var{name}, @var{decl})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} any text necessary for declaring the name @var{name} of an
+initialized variable which is being defined.  This macro must output the
+label definition (perhaps using @code{ASM_OUTPUT_LABEL}).  The argument
+@var{decl} is the @code{VAR_DECL} tree node representing the variable.
+
+If this macro is not defined, then the variable name is defined in the
+usual manner as a label (by means of @code{ASM_OUTPUT_LABEL}).
+
+You may wish to use @code{ASM_OUTPUT_TYPE_DIRECTIVE} and/or
+@code{ASM_OUTPUT_SIZE_DIRECTIVE} in the definition of this macro.
+@end defmac
+
+@hook TARGET_ASM_DECLARE_CONSTANT_NAME
+
+@defmac ASM_DECLARE_REGISTER_GLOBAL (@var{stream}, @var{decl}, @var{regno}, @var{name})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} any text necessary for claiming a register @var{regno}
+for a global variable @var{decl} with name @var{name}.
+
+If you don't define this macro, that is equivalent to defining it to do
+nothing.
+@end defmac
+
+@defmac ASM_FINISH_DECLARE_OBJECT (@var{stream}, @var{decl}, @var{toplevel}, @var{atend})
+A C statement (sans semicolon) to finish up declaring a variable name
+once the compiler has processed its initializer fully and thus has had a
+chance to determine the size of an array when controlled by an
+initializer.  This is used on systems where it's necessary to declare
+something about the size of the object.
+
+If you don't define this macro, that is equivalent to defining it to do
+nothing.
+
+You may wish to use @code{ASM_OUTPUT_SIZE_DIRECTIVE} and/or
+@code{ASM_OUTPUT_MEASURED_SIZE} in the definition of this macro.
+@end defmac
+
+@hook TARGET_ASM_GLOBALIZE_LABEL
+
+@hook TARGET_ASM_GLOBALIZE_DECL_NAME
+
+@hook TARGET_ASM_ASSEMBLE_UNDEFINED_DECL
+
+@defmac ASM_WEAKEN_LABEL (@var{stream}, @var{name})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} some commands that will make the label @var{name} weak;
+that is, available for reference from other files but only used if
+no other definition is available.  Use the expression
+@code{assemble_name (@var{stream}, @var{name})} to output the name
+itself; before and after that, output the additional assembler syntax
+for making that name weak, and a newline.
+
+If you don't define this macro or @code{ASM_WEAKEN_DECL}, GCC will not
+support weak symbols and you should not define the @code{SUPPORTS_WEAK}
+macro.
+@end defmac
+
+@defmac ASM_WEAKEN_DECL (@var{stream}, @var{decl}, @var{name}, @var{value})
+Combines (and replaces) the function of @code{ASM_WEAKEN_LABEL} and
+@code{ASM_OUTPUT_WEAK_ALIAS}, allowing access to the associated function
+or variable decl.  If @var{value} is not @code{NULL}, this C statement
+should output to the stdio stream @var{stream} assembler code which
+defines (equates) the weak symbol @var{name} to have the value
+@var{value}.  If @var{value} is @code{NULL}, it should output commands
+to make @var{name} weak.
+@end defmac
+
+@defmac ASM_OUTPUT_WEAKREF (@var{stream}, @var{decl}, @var{name}, @var{value})
+Outputs a directive that enables @var{name} to be used to refer to
+symbol @var{value} with weak-symbol semantics.  @code{decl} is the
+declaration of @code{name}.
+@end defmac
+
+@defmac SUPPORTS_WEAK
+A preprocessor constant expression which evaluates to true if the target
+supports weak symbols.
+
+If you don't define this macro, @file{defaults.h} provides a default
+definition.  If either @code{ASM_WEAKEN_LABEL} or @code{ASM_WEAKEN_DECL}
+is defined, the default definition is @samp{1}; otherwise, it is @samp{0}.
+@end defmac
+
+@defmac TARGET_SUPPORTS_WEAK
+A C expression which evaluates to true if the target supports weak symbols.
+
+If you don't define this macro, @file{defaults.h} provides a default
+definition.  The default definition is @samp{(SUPPORTS_WEAK)}.  Define
+this macro if you want to control weak symbol support with a compiler
+flag such as @option{-melf}.
+@end defmac
+
+@defmac MAKE_DECL_ONE_ONLY (@var{decl})
+A C statement (sans semicolon) to mark @var{decl} to be emitted as a
+public symbol such that extra copies in multiple translation units will
+be discarded by the linker.  Define this macro if your object file
+format provides support for this concept, such as the @samp{COMDAT}
+section flags in the Microsoft Windows PE/COFF format, and this support
+requires changes to @var{decl}, such as putting it in a separate section.
+@end defmac
+
+@defmac SUPPORTS_ONE_ONLY
+A C expression which evaluates to true if the target supports one-only
+semantics.
+
+If you don't define this macro, @file{varasm.c} provides a default
+definition.  If @code{MAKE_DECL_ONE_ONLY} is defined, the default
+definition is @samp{1}; otherwise, it is @samp{0}.  Define this macro if
+you want to control one-only symbol support with a compiler flag, or if
+setting the @code{DECL_ONE_ONLY} flag is enough to mark a declaration to
+be emitted as one-only.
+@end defmac
+
+@hook TARGET_ASM_ASSEMBLE_VISIBILITY
+
+@defmac TARGET_WEAK_NOT_IN_ARCHIVE_TOC
+A C expression that evaluates to true if the target's linker expects
+that weak symbols do not appear in a static archive's table of contents.
+The default is @code{0}.
+
+Leaving weak symbols out of an archive's table of contents means that,
+if a symbol will only have a definition in one translation unit and
+will have undefined references from other translation units, that
+symbol should not be weak.  Defining this macro to be nonzero will
+thus have the effect that certain symbols that would normally be weak
+(explicit template instantiations, and vtables for polymorphic classes
+with noninline key methods) will instead be nonweak.
+
+The C++ ABI requires this macro to be zero.  Define this macro for
+targets where full C++ ABI compliance is impossible and where linker
+restrictions require weak symbols to be left out of a static archive's
+table of contents.
+@end defmac
+
+@defmac ASM_OUTPUT_EXTERNAL (@var{stream}, @var{decl}, @var{name})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} any text necessary for declaring the name of an external
+symbol named @var{name} which is referenced in this compilation but
+not defined.  The value of @var{decl} is the tree node for the
+declaration.
+
+This macro need not be defined if it does not need to output anything.
+The GNU assembler and most Unix assemblers don't require anything.
+@end defmac
+
+@hook TARGET_ASM_EXTERNAL_LIBCALL
+
+@hook TARGET_ASM_MARK_DECL_PRESERVED
+
+@defmac ASM_OUTPUT_LABELREF (@var{stream}, @var{name})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} a reference in assembler syntax to a label named
+@var{name}.  This should add @samp{_} to the front of the name, if that
+is customary on your operating system, as it is in most Berkeley Unix
+systems.  This macro is used in @code{assemble_name}.
+@end defmac
+
+@hook TARGET_MANGLE_ASSEMBLER_NAME
+
+@defmac ASM_OUTPUT_SYMBOL_REF (@var{stream}, @var{sym})
+A C statement (sans semicolon) to output a reference to
+@code{SYMBOL_REF} @var{sym}.  If not defined, @code{assemble_name}
+will be used to output the name of the symbol.  This macro may be used
+to modify the way a symbol is referenced depending on information
+encoded by @code{TARGET_ENCODE_SECTION_INFO}.
+@end defmac
+
+@defmac ASM_OUTPUT_LABEL_REF (@var{stream}, @var{buf})
+A C statement (sans semicolon) to output a reference to @var{buf}, the
+result of @code{ASM_GENERATE_INTERNAL_LABEL}.  If not defined,
+@code{assemble_name} will be used to output the name of the symbol.
+This macro is not used by @code{output_asm_label}, or the @code{%l}
+specifier that calls it; the intention is that this macro should be set
+when it is necessary to output a label differently when its address is
+being taken.
+@end defmac
+
+@hook TARGET_ASM_INTERNAL_LABEL
+
+@defmac ASM_OUTPUT_DEBUG_LABEL (@var{stream}, @var{prefix}, @var{num})
+A C statement to output to the stdio stream @var{stream} a debug info
+label whose name is made from the string @var{prefix} and the number
+@var{num}.  This is useful for VLIW targets, where debug info labels
+may need to be treated differently than branch target labels.  On some
+systems, branch target labels must be at the beginning of instruction
+bundles, but debug info labels can occur in the middle of instruction
+bundles.
+
+If this macro is not defined, then @code{(*targetm.asm_out.internal_label)} will be
+used.
+@end defmac
+
+@defmac ASM_GENERATE_INTERNAL_LABEL (@var{string}, @var{prefix}, @var{num})
+A C statement to store into the string @var{string} a label whose name
+is made from the string @var{prefix} and the number @var{num}.
+
+This string, when output subsequently by @code{assemble_name}, should
+produce the output that @code{(*targetm.asm_out.internal_label)} would produce
+with the same @var{prefix} and @var{num}.
+
+If the string begins with @samp{*}, then @code{assemble_name} will
+output the rest of the string unchanged.  It is often convenient for
+@code{ASM_GENERATE_INTERNAL_LABEL} to use @samp{*} in this way.  If the
+string doesn't start with @samp{*}, then @code{ASM_OUTPUT_LABELREF} gets
+to output the string, and may change it.  (Of course,
+@code{ASM_OUTPUT_LABELREF} is also part of your machine description, so
+you should know what it does on your machine.)
+@end defmac
+
+@defmac ASM_FORMAT_PRIVATE_NAME (@var{outvar}, @var{name}, @var{number})
+A C expression to assign to @var{outvar} (which is a variable of type
+@code{char *}) a newly allocated string made from the string
+@var{name} and the number @var{number}, with some suitable punctuation
+added.  Use @code{alloca} to get space for the string.
+
+The string will be used as an argument to @code{ASM_OUTPUT_LABELREF} to
+produce an assembler label for an internal static variable whose name is
+@var{name}.  Therefore, the string must be such as to result in valid
+assembler code.  The argument @var{number} is different each time this
+macro is executed; it prevents conflicts between similarly-named
+internal static variables in different scopes.
+
+Ideally this string should not be a valid C identifier, to prevent any
+conflict with the user's own symbols.  Most assemblers allow periods
+or percent signs in assembler symbols; putting at least one of these
+between the name and the number will suffice.
+
+If this macro is not defined, a default definition will be provided
+which is correct for most systems.
+@end defmac
+
+@defmac ASM_OUTPUT_DEF (@var{stream}, @var{name}, @var{value})
+A C statement to output to the stdio stream @var{stream} assembler code
+which defines (equates) the symbol @var{name} to have the value @var{value}.
+
+@findex SET_ASM_OP
+If @code{SET_ASM_OP} is defined, a default definition is provided which is
+correct for most systems.
+@end defmac
+
+@defmac ASM_OUTPUT_DEF_FROM_DECLS (@var{stream}, @var{decl_of_name}, @var{decl_of_value})
+A C statement to output to the stdio stream @var{stream} assembler code
+which defines (equates) the symbol whose tree node is @var{decl_of_name}
+to have the value of the tree node @var{decl_of_value}.  This macro will
+be used in preference to @samp{ASM_OUTPUT_DEF} if it is defined and if
+the tree nodes are available.
+
+@findex SET_ASM_OP
+If @code{SET_ASM_OP} is defined, a default definition is provided which is
+correct for most systems.
+@end defmac
+
+@defmac TARGET_DEFERRED_OUTPUT_DEFS (@var{decl_of_name}, @var{decl_of_value})
+A C statement that evaluates to true if the assembler code which defines
+(equates) the symbol whose tree node is @var{decl_of_name} to have the value
+of the tree node @var{decl_of_value} should be emitted near the end of the
+current compilation unit.  The default is to not defer output of defines.
+This macro affects defines output by @samp{ASM_OUTPUT_DEF} and
+@samp{ASM_OUTPUT_DEF_FROM_DECLS}.
+@end defmac
+
+@defmac ASM_OUTPUT_WEAK_ALIAS (@var{stream}, @var{name}, @var{value})
+A C statement to output to the stdio stream @var{stream} assembler code
+which defines (equates) the weak symbol @var{name} to have the value
+@var{value}.  If @var{value} is @code{NULL}, it defines @var{name} as
+an undefined weak symbol.
+
+Define this macro if the target only supports weak aliases; define
+@code{ASM_OUTPUT_DEF} instead if possible.
+@end defmac
+
+@defmac OBJC_GEN_METHOD_LABEL (@var{buf}, @var{is_inst}, @var{class_name}, @var{cat_name}, @var{sel_name})
+Define this macro to override the default assembler names used for
+Objective-C methods.
+
+The default name is a unique method number followed by the name of the
+class (e.g.@: @samp{_1_Foo}).  For methods in categories, the name of
+the category is also included in the assembler name (e.g.@:
+@samp{_1_Foo_Bar}).
+
+These names are safe on most systems, but make debugging difficult since
+the method's selector is not present in the name.  Therefore, particular
+systems define other ways of computing names.
+
+@var{buf} is an expression of type @code{char *} which gives you a
+buffer in which to store the name; its length is as long as
+@var{class_name}, @var{cat_name} and @var{sel_name} put together, plus
+50 characters extra.
+
+The argument @var{is_inst} specifies whether the method is an instance
+method or a class method; @var{class_name} is the name of the class;
+@var{cat_name} is the name of the category (or @code{NULL} if the method is not
+in a category); and @var{sel_name} is the name of the selector.
+
+On systems where the assembler can handle quoted names, you can use this
+macro to provide more human-readable names.
+@end defmac
+
+@node Initialization
+@subsection How Initialization Functions Are Handled
+@cindex initialization routines
+@cindex termination routines
+@cindex constructors, output of
+@cindex destructors, output of
+
+The compiled code for certain languages includes @dfn{constructors}
+(also called @dfn{initialization routines})---functions to initialize
+data in the program when the program is started.  These functions need
+to be called before the program is ``started''---that is to say, before
+@code{main} is called.
+
+Compiling some languages generates @dfn{destructors} (also called
+@dfn{termination routines}) that should be called when the program
+terminates.
+
+To make the initialization and termination functions work, the compiler
+must output something in the assembler code to cause those functions to
+be called at the appropriate time.  When you port the compiler to a new
+system, you need to specify how to do this.
+
+There are two major ways that GCC currently supports the execution of
+initialization and termination functions.  Each way has two variants.
+Much of the structure is common to all four variations.
+
+@findex __CTOR_LIST__
+@findex __DTOR_LIST__
+The linker must build two lists of these functions---a list of
+initialization functions, called @code{__CTOR_LIST__}, and a list of
+termination functions, called @code{__DTOR_LIST__}.
+
+Each list always begins with an ignored function pointer (which may hold
+0, @minus{}1, or a count of the function pointers after it, depending on
+the environment).  This is followed by a series of zero or more function
+pointers to constructors (or destructors), followed by a function
+pointer containing zero.
+
+Depending on the operating system and its executable file format, either
+@file{crtstuff.c} or @file{libgcc2.c} traverses these lists at startup
+time and exit time.  Constructors are called in reverse order of the
+list; destructors in forward order.
+
+The best way to handle static constructors works only for object file
+formats which provide arbitrarily-named sections.  A section is set
+aside for a list of constructors, and another for a list of destructors.
+Traditionally these are called @samp{.ctors} and @samp{.dtors}.  Each
+object file that defines an initialization function also puts a word in
+the constructor section to point to that function.  The linker
+accumulates all these words into one contiguous @samp{.ctors} section.
+Termination functions are handled similarly.
+
+This method will be chosen as the default by @file{target-def.h} if
+@code{TARGET_ASM_NAMED_SECTION} is defined.  A target that does not
+support arbitrary sections, but does support special designated
+constructor and destructor sections may define @code{CTORS_SECTION_ASM_OP}
+and @code{DTORS_SECTION_ASM_OP} to achieve the same effect.
+
+When arbitrary sections are available, there are two variants, depending
+upon how the code in @file{crtstuff.c} is called.  On systems that
+support a @dfn{.init} section which is executed at program startup,
+parts of @file{crtstuff.c} are compiled into that section.  The
+program is linked by the @command{gcc} driver like this:
+
+@smallexample
+ld -o @var{output_file} crti.o crtbegin.o @dots{} -lgcc crtend.o crtn.o
+@end smallexample
+
+The prologue of a function (@code{__init}) appears in the @code{.init}
+section of @file{crti.o}; the epilogue appears in @file{crtn.o}.  Likewise
+for the function @code{__fini} in the @dfn{.fini} section.  Normally these
+files are provided by the operating system or by the GNU C library, but
+are provided by GCC for a few targets.
+
+The objects @file{crtbegin.o} and @file{crtend.o} are (for most targets)
+compiled from @file{crtstuff.c}.  They contain, among other things, code
+fragments within the @code{.init} and @code{.fini} sections that branch
+to routines in the @code{.text} section.  The linker will pull all parts
+of a section together, which results in a complete @code{__init} function
+that invokes the routines we need at startup.
+
+To use this variant, you must define the @code{INIT_SECTION_ASM_OP}
+macro properly.
+
+If no init section is available, when GCC compiles any function called
+@code{main} (or more accurately, any function designated as a program
+entry point by the language front end calling @code{expand_main_function}),
+it inserts a procedure call to @code{__main} as the first executable code
+after the function prologue.  The @code{__main} function is defined
+in @file{libgcc2.c} and runs the global constructors.
+
+In file formats that don't support arbitrary sections, there are again
+two variants.  In the simplest variant, the GNU linker (GNU @code{ld})
+and an `a.out' format must be used.  In this case,
+@code{TARGET_ASM_CONSTRUCTOR} is defined to produce a @code{.stabs}
+entry of type @samp{N_SETT}, referencing the name @code{__CTOR_LIST__},
+and with the address of the void function containing the initialization
+code as its value.  The GNU linker recognizes this as a request to add
+the value to a @dfn{set}; the values are accumulated, and are eventually
+placed in the executable as a vector in the format described above, with
+a leading (ignored) count and a trailing zero element.
+@code{TARGET_ASM_DESTRUCTOR} is handled similarly.  Since no init
+section is available, the absence of @code{INIT_SECTION_ASM_OP} causes
+the compilation of @code{main} to call @code{__main} as above, starting
+the initialization process.
+
+The last variant uses neither arbitrary sections nor the GNU linker.
+This is preferable when you want to do dynamic linking and when using
+file formats which the GNU linker does not support, such as `ECOFF'@.  In
+this case, @code{TARGET_HAVE_CTORS_DTORS} is false, initialization and
+termination functions are recognized simply by their names.  This requires
+an extra program in the linkage step, called @command{collect2}.  This program
+pretends to be the linker, for use with GCC; it does its job by running
+the ordinary linker, but also arranges to include the vectors of
+initialization and termination functions.  These functions are called
+via @code{__main} as described above.  In order to use this method,
+@code{use_collect2} must be defined in the target in @file{config.gcc}.
+
+@ifinfo
+The following section describes the specific macros that control and
+customize the handling of initialization and termination functions.
+@end ifinfo
+
+@node Macros for Initialization
+@subsection Macros Controlling Initialization Routines
+
+Here are the macros that control how the compiler handles initialization
+and termination functions:
+
+@defmac INIT_SECTION_ASM_OP
+If defined, a C string constant, including spacing, for the assembler
+operation to identify the following data as initialization code.  If not
+defined, GCC will assume such a section does not exist.  When you are
+using special sections for initialization and termination functions, this
+macro also controls how @file{crtstuff.c} and @file{libgcc2.c} arrange to
+run the initialization functions.
+@end defmac
+
+@defmac HAS_INIT_SECTION
+If defined, @code{main} will not call @code{__main} as described above.
+This macro should be defined for systems that control start-up code
+on a symbol-by-symbol basis, such as OSF/1, and should not
+be defined explicitly for systems that support @code{INIT_SECTION_ASM_OP}.
+@end defmac
+
+@defmac LD_INIT_SWITCH
+If defined, a C string constant for a switch that tells the linker that
+the following symbol is an initialization routine.
+@end defmac
+
+@defmac LD_FINI_SWITCH
+If defined, a C string constant for a switch that tells the linker that
+the following symbol is a finalization routine.
+@end defmac
+
+@defmac COLLECT_SHARED_INIT_FUNC (@var{stream}, @var{func})
+If defined, a C statement that will write a function that can be
+automatically called when a shared library is loaded.  The function
+should call @var{func}, which takes no arguments.  If not defined, and
+the object format requires an explicit initialization function, then a
+function called @code{_GLOBAL__DI} will be generated.
+
+This function and the following one are used by collect2 when linking a
+shared library that needs constructors or destructors, or has DWARF2
+exception tables embedded in the code.
+@end defmac
+
+@defmac COLLECT_SHARED_FINI_FUNC (@var{stream}, @var{func})
+If defined, a C statement that will write a function that can be
+automatically called when a shared library is unloaded.  The function
+should call @var{func}, which takes no arguments.  If not defined, and
+the object format requires an explicit finalization function, then a
+function called @code{_GLOBAL__DD} will be generated.
+@end defmac
+
+@defmac INVOKE__main
+If defined, @code{main} will call @code{__main} despite the presence of
+@code{INIT_SECTION_ASM_OP}.  This macro should be defined for systems
+where the init section is not actually run automatically, but is still
+useful for collecting the lists of constructors and destructors.
+@end defmac
+
+@defmac SUPPORTS_INIT_PRIORITY
+If nonzero, the C++ @code{init_priority} attribute is supported and the
+compiler should emit instructions to control the order of initialization
+of objects.  If zero, the compiler will issue an error message upon
+encountering an @code{init_priority} attribute.
+@end defmac
+
+@hook TARGET_HAVE_CTORS_DTORS
+
+@hook TARGET_ASM_CONSTRUCTOR
+
+@hook TARGET_ASM_DESTRUCTOR
+
+If @code{TARGET_HAVE_CTORS_DTORS} is true, the initialization routine
+generated for the generated object file will have static linkage.
+
+If your system uses @command{collect2} as the means of processing
+constructors, then that program normally uses @command{nm} to scan
+an object file for constructor functions to be called.
+
+On certain kinds of systems, you can define this macro to make
+@command{collect2} work faster (and, in some cases, make it work at all):
+
+@defmac OBJECT_FORMAT_COFF
+Define this macro if the system uses COFF (Common Object File Format)
+object files, so that @command{collect2} can assume this format and scan
+object files directly for dynamic constructor/destructor functions.
+
+This macro is effective only in a native compiler; @command{collect2} as
+part of a cross compiler always uses @command{nm} for the target machine.
+@end defmac
+
+@defmac REAL_NM_FILE_NAME
+Define this macro as a C string constant containing the file name to use
+to execute @command{nm}.  The default is to search the path normally for
+@command{nm}.
+@end defmac
+
+@defmac NM_FLAGS
+@command{collect2} calls @command{nm} to scan object files for static
+constructors and destructors and LTO info.  By default, @option{-n} is
+passed.  Define @code{NM_FLAGS} to a C string constant if other options
+are needed to get the same output format as GNU @command{nm -n}
+produces.
+@end defmac
+
+If your system supports shared libraries and has a program to list the
+dynamic dependencies of a given library or executable, you can define
+these macros to enable support for running initialization and
+termination functions in shared libraries:
+
+@defmac LDD_SUFFIX
+Define this macro to a C string constant containing the name of the program
+which lists dynamic dependencies, like @command{ldd} under SunOS 4.
+@end defmac
+
+@defmac PARSE_LDD_OUTPUT (@var{ptr})
+Define this macro to be C code that extracts filenames from the output
+of the program denoted by @code{LDD_SUFFIX}.  @var{ptr} is a variable
+of type @code{char *} that points to the beginning of a line of output
+from @code{LDD_SUFFIX}.  If the line lists a dynamic dependency, the
+code must advance @var{ptr} to the beginning of the filename on that
+line.  Otherwise, it must set @var{ptr} to @code{NULL}.
+@end defmac
+
+@defmac SHLIB_SUFFIX
+Define this macro to a C string constant containing the default shared
+library extension of the target (e.g., @samp{".so"}).  @command{collect2}
+strips version information after this suffix when generating global
+constructor and destructor names.  This define is only needed on targets
+that use @command{collect2} to process constructors and destructors.
+@end defmac
+
+@node Instruction Output
+@subsection Output of Assembler Instructions
+
+@c prevent bad page break with this line
+This describes assembler instruction output.
+
+@defmac REGISTER_NAMES
+A C initializer containing the assembler's names for the machine
+registers, each one as a C string constant.  This is what translates
+register numbers in the compiler into assembler language.
+@end defmac
+
+@defmac ADDITIONAL_REGISTER_NAMES
+If defined, a C initializer for an array of structures containing a name
+and a register number.  This macro defines additional names for hard
+registers, thus allowing the @code{asm} option in declarations to refer
+to registers using alternate names.
+@end defmac
+
+@defmac OVERLAPPING_REGISTER_NAMES
+If defined, a C initializer for an array of structures containing a
+name, a register number and a count of the number of consecutive
+machine registers the name overlaps.  This macro defines additional
+names for hard registers, thus allowing the @code{asm} option in
+declarations to refer to registers using alternate names.  Unlike
+@code{ADDITIONAL_REGISTER_NAMES}, this macro should be used when the
+register name implies multiple underlying registers.
+
+This macro should be used when it is important that a clobber in an
+@code{asm} statement clobbers all the underlying values implied by the
+register name.  For example, on ARM, clobbering the double-precision
+VFP register ``d0'' implies clobbering both single-precision registers
+``s0'' and ``s1''.
+@end defmac
+
+@defmac ASM_OUTPUT_OPCODE (@var{stream}, @var{ptr})
+Define this macro if you are using an unusual assembler that
+requires different names for the machine instructions.
+
+The definition is a C statement or statements which output an
+assembler instruction opcode to the stdio stream @var{stream}.  The
+macro-operand @var{ptr} is a variable of type @code{char *} which
+points to the opcode name in its ``internal'' form---the form that is
+written in the machine description.  The definition should output the
+opcode name to @var{stream}, performing any translation you desire, and
+increment the variable @var{ptr} to point at the end of the opcode
+so that it will not be output twice.
+
+In fact, your macro definition may process less than the entire opcode
+name, or more than the opcode name; but if you want to process text
+that includes @samp{%}-sequences to substitute operands, you must take
+care of the substitution yourself.  Just be sure to increment
+@var{ptr} over whatever text should not be output normally.
+
+@findex recog_data.operand
+If you need to look at the operand values, they can be found as the
+elements of @code{recog_data.operand}.
+
+If the macro definition does nothing, the instruction is output
+in the usual way.
+@end defmac
+
+@defmac FINAL_PRESCAN_INSN (@var{insn}, @var{opvec}, @var{noperands})
+If defined, a C statement to be executed just prior to the output of
+assembler code for @var{insn}, to modify the extracted operands so
+they will be output differently.
+
+Here the argument @var{opvec} is the vector containing the operands
+extracted from @var{insn}, and @var{noperands} is the number of
+elements of the vector which contain meaningful data for this insn.
+The contents of this vector are what will be used to convert the insn
+template into assembler code, so you can change the assembler output
+by changing the contents of the vector.
+
+This macro is useful when various assembler syntaxes share a single
+file of instruction patterns; by defining this macro differently, you
+can cause a large class of instructions to be output differently (such
+as with rearranged operands).  Naturally, variations in assembler
+syntax affecting individual insn patterns ought to be handled by
+writing conditional output routines in those patterns.
+
+If this macro is not defined, it is equivalent to a null statement.
+@end defmac
+
+@hook TARGET_ASM_FINAL_POSTSCAN_INSN
+
+@defmac PRINT_OPERAND (@var{stream}, @var{x}, @var{code})
+A C compound statement to output to stdio stream @var{stream} the
+assembler syntax for an instruction operand @var{x}.  @var{x} is an
+RTL expression.
+
+@var{code} is a value that can be used to specify one of several ways
+of printing the operand.  It is used when identical operands must be
+printed differently depending on the context.  @var{code} comes from
+the @samp{%} specification that was used to request printing of the
+operand.  If the specification was just @samp{%@var{digit}} then
+@var{code} is 0; if the specification was @samp{%@var{ltr}
+@var{digit}} then @var{code} is the ASCII code for @var{ltr}.
+
+@findex reg_names
+If @var{x} is a register, this macro should print the register's name.
+The names can be found in an array @code{reg_names} whose type is
+@code{char *[]}.  @code{reg_names} is initialized from
+@code{REGISTER_NAMES}.
+
+When the machine description has a specification @samp{%@var{punct}}
+(a @samp{%} followed by a punctuation character), this macro is called
+with a null pointer for @var{x} and the punctuation character for
+@var{code}.
+@end defmac
+
+@defmac PRINT_OPERAND_PUNCT_VALID_P (@var{code})
+A C expression which evaluates to true if @var{code} is a valid
+punctuation character for use in the @code{PRINT_OPERAND} macro.  If
+@code{PRINT_OPERAND_PUNCT_VALID_P} is not defined, it means that no
+punctuation characters (except for the standard one, @samp{%}) are used
+in this way.
+@end defmac
+
+@defmac PRINT_OPERAND_ADDRESS (@var{stream}, @var{x})
+A C compound statement to output to stdio stream @var{stream} the
+assembler syntax for an instruction operand that is a memory reference
+whose address is @var{x}.  @var{x} is an RTL expression.
+
+@cindex @code{TARGET_ENCODE_SECTION_INFO} usage
+On some machines, the syntax for a symbolic address depends on the
+section that the address refers to.  On these machines, define the hook
+@code{TARGET_ENCODE_SECTION_INFO} to store the information into the
+@code{symbol_ref}, and then check for it here.  @xref{Assembler
+Format}.
+@end defmac
+
+@findex dbr_sequence_length
+@defmac DBR_OUTPUT_SEQEND (@var{file})
+A C statement, to be executed after all slot-filler instructions have
+been output.  If necessary, call @code{dbr_sequence_length} to
+determine the number of slots filled in a sequence (zero if not
+currently outputting a sequence), to decide how many no-ops to output,
+or whatever.
+
+Don't define this macro if it has nothing to do, but it is helpful in
+reading assembly output if the extent of the delay sequence is made
+explicit (e.g.@: with white space).
+@end defmac
+
+@findex final_sequence
+Note that output routines for instructions with delay slots must be
+prepared to deal with not being output as part of a sequence
+(i.e.@: when the scheduling pass is not run, or when no slot fillers could be
+found.)  The variable @code{final_sequence} is null when not
+processing a sequence, otherwise it contains the @code{sequence} rtx
+being output.
+
+@findex asm_fprintf
+@defmac REGISTER_PREFIX
+@defmacx LOCAL_LABEL_PREFIX
+@defmacx USER_LABEL_PREFIX
+@defmacx IMMEDIATE_PREFIX
+If defined, C string expressions to be used for the @samp{%R}, @samp{%L},
+@samp{%U}, and @samp{%I} options of @code{asm_fprintf} (see
+@file{final.c}).  These are useful when a single @file{md} file must
+support multiple assembler formats.  In that case, the various @file{tm.h}
+files can define these macros differently.
+@end defmac
+
+@defmac ASM_FPRINTF_EXTENSIONS (@var{file}, @var{argptr}, @var{format})
+If defined this macro should expand to a series of @code{case}
+statements which will be parsed inside the @code{switch} statement of
+the @code{asm_fprintf} function.  This allows targets to define extra
+printf formats which may useful when generating their assembler
+statements.  Note that uppercase letters are reserved for future
+generic extensions to asm_fprintf, and so are not available to target
+specific code.  The output file is given by the parameter @var{file}.
+The varargs input pointer is @var{argptr} and the rest of the format
+string, starting the character after the one that is being switched
+upon, is pointed to by @var{format}.
+@end defmac
+
+@defmac ASSEMBLER_DIALECT
+If your target supports multiple dialects of assembler language (such as
+different opcodes), define this macro as a C expression that gives the
+numeric index of the assembler language dialect to use, with zero as the
+first variant.
+
+If this macro is defined, you may use constructs of the form
+@smallexample
+@samp{@{option0|option1|option2@dots{}@}}
+@end smallexample
+@noindent
+in the output templates of patterns (@pxref{Output Template}) or in the
+first argument of @code{asm_fprintf}.  This construct outputs
+@samp{option0}, @samp{option1}, @samp{option2}, etc., if the value of
+@code{ASSEMBLER_DIALECT} is zero, one, two, etc.  Any special characters
+within these strings retain their usual meaning.  If there are fewer
+alternatives within the braces than the value of
+@code{ASSEMBLER_DIALECT}, the construct outputs nothing. If it's needed
+to print curly braces or @samp{|} character in assembler output directly,
+@samp{%@{}, @samp{%@}} and @samp{%|} can be used.
+
+If you do not define this macro, the characters @samp{@{}, @samp{|} and
+@samp{@}} do not have any special meaning when used in templates or
+operands to @code{asm_fprintf}.
+
+Define the macros @code{REGISTER_PREFIX}, @code{LOCAL_LABEL_PREFIX},
+@code{USER_LABEL_PREFIX} and @code{IMMEDIATE_PREFIX} if you can express
+the variations in assembler language syntax with that mechanism.  Define
+@code{ASSEMBLER_DIALECT} and use the @samp{@{option0|option1@}} syntax
+if the syntax variant are larger and involve such things as different
+opcodes or operand order.
+@end defmac
+
+@defmac ASM_OUTPUT_REG_PUSH (@var{stream}, @var{regno})
+A C expression to output to @var{stream} some assembler code
+which will push hard register number @var{regno} onto the stack.
+The code need not be optimal, since this macro is used only when
+profiling.
+@end defmac
+
+@defmac ASM_OUTPUT_REG_POP (@var{stream}, @var{regno})
+A C expression to output to @var{stream} some assembler code
+which will pop hard register number @var{regno} off of the stack.
+The code need not be optimal, since this macro is used only when
+profiling.
+@end defmac
+
+@node Dispatch Tables
+@subsection Output of Dispatch Tables
+
+@c prevent bad page break with this line
+This concerns dispatch tables.
+
+@cindex dispatch table
+@defmac ASM_OUTPUT_ADDR_DIFF_ELT (@var{stream}, @var{body}, @var{value}, @var{rel})
+A C statement to output to the stdio stream @var{stream} an assembler
+pseudo-instruction to generate a difference between two labels.
+@var{value} and @var{rel} are the numbers of two internal labels.  The
+definitions of these labels are output using
+@code{(*targetm.asm_out.internal_label)}, and they must be printed in the same
+way here.  For example,
+
+@smallexample
+fprintf (@var{stream}, "\t.word L%d-L%d\n",
+         @var{value}, @var{rel})
+@end smallexample
+
+You must provide this macro on machines where the addresses in a
+dispatch table are relative to the table's own address.  If defined, GCC
+will also use this macro on all machines when producing PIC@.
+@var{body} is the body of the @code{ADDR_DIFF_VEC}; it is provided so that the
+mode and flags can be read.
+@end defmac
+
+@defmac ASM_OUTPUT_ADDR_VEC_ELT (@var{stream}, @var{value})
+This macro should be provided on machines where the addresses
+in a dispatch table are absolute.
+
+The definition should be a C statement to output to the stdio stream
+@var{stream} an assembler pseudo-instruction to generate a reference to
+a label.  @var{value} is the number of an internal label whose
+definition is output using @code{(*targetm.asm_out.internal_label)}.
+For example,
+
+@smallexample
+fprintf (@var{stream}, "\t.word L%d\n", @var{value})
+@end smallexample
+@end defmac
+
+@defmac ASM_OUTPUT_CASE_LABEL (@var{stream}, @var{prefix}, @var{num}, @var{table})
+Define this if the label before a jump-table needs to be output
+specially.  The first three arguments are the same as for
+@code{(*targetm.asm_out.internal_label)}; the fourth argument is the
+jump-table which follows (a @code{jump_table_data} containing an
+@code{addr_vec} or @code{addr_diff_vec}).
+
+This feature is used on system V to output a @code{swbeg} statement
+for the table.
+
+If this macro is not defined, these labels are output with
+@code{(*targetm.asm_out.internal_label)}.
+@end defmac
+
+@defmac ASM_OUTPUT_CASE_END (@var{stream}, @var{num}, @var{table})
+Define this if something special must be output at the end of a
+jump-table.  The definition should be a C statement to be executed
+after the assembler code for the table is written.  It should write
+the appropriate code to stdio stream @var{stream}.  The argument
+@var{table} is the jump-table insn, and @var{num} is the label-number
+of the preceding label.
+
+If this macro is not defined, nothing special is output at the end of
+the jump-table.
+@end defmac
+
+@hook TARGET_ASM_POST_CFI_STARTPROC
+
+@hook TARGET_ASM_EMIT_UNWIND_LABEL
+
+@hook TARGET_ASM_EMIT_EXCEPT_TABLE_LABEL
+
+@hook TARGET_ASM_EMIT_EXCEPT_PERSONALITY
+
+@hook TARGET_ASM_UNWIND_EMIT
+
+@hook TARGET_ASM_MAKE_EH_SYMBOL_INDIRECT
+
+@hook TARGET_ASM_UNWIND_EMIT_BEFORE_INSN
+
+@hook TARGET_ASM_SHOULD_RESTORE_CFA_STATE
+
+@node Exception Region Output
+@subsection Assembler Commands for Exception Regions
+
+@c prevent bad page break with this line
+
+This describes commands marking the start and the end of an exception
+region.
+
+@defmac EH_FRAME_SECTION_NAME
+If defined, a C string constant for the name of the section containing
+exception handling frame unwind information.  If not defined, GCC will
+provide a default definition if the target supports named sections.
+@file{crtstuff.c} uses this macro to switch to the appropriate section.
+
+You should define this symbol if your target supports DWARF 2 frame
+unwind information and the default definition does not work.
+@end defmac
+
+@defmac EH_FRAME_THROUGH_COLLECT2
+If defined, DWARF 2 frame unwind information will identified by
+specially named labels.  The collect2 process will locate these
+labels and generate code to register the frames.
+
+This might be necessary, for instance, if the system linker will not
+place the eh_frames in-between the sentinals from @file{crtstuff.c},
+or if the system linker does garbage collection and sections cannot
+be marked as not to be collected.
+@end defmac
+
+@defmac EH_TABLES_CAN_BE_READ_ONLY
+Define this macro to 1 if your target is such that no frame unwind
+information encoding used with non-PIC code will ever require a
+runtime relocation, but the linker may not support merging read-only
+and read-write sections into a single read-write section.
+@end defmac
+
+@defmac MASK_RETURN_ADDR
+An rtx used to mask the return address found via @code{RETURN_ADDR_RTX}, so
+that it does not contain any extraneous set bits in it.
+@end defmac
+
+@defmac DWARF2_UNWIND_INFO
+Define this macro to 0 if your target supports DWARF 2 frame unwind
+information, but it does not yet work with exception handling.
+Otherwise, if your target supports this information (if it defines
+@code{INCOMING_RETURN_ADDR_RTX} and @code{OBJECT_FORMAT_ELF}),
+GCC will provide a default definition of 1.
+@end defmac
+
+@hook TARGET_EXCEPT_UNWIND_INFO
+This hook defines the mechanism that will be used for exception handling
+by the target.  If the target has ABI specified unwind tables, the hook
+should return @code{UI_TARGET}.  If the target is to use the
+@code{setjmp}/@code{longjmp}-based exception handling scheme, the hook
+should return @code{UI_SJLJ}.  If the target supports DWARF 2 frame unwind
+information, the hook should return @code{UI_DWARF2}.
+
+A target may, if exceptions are disabled, choose to return @code{UI_NONE}.
+This may end up simplifying other parts of target-specific code.  The
+default implementation of this hook never returns @code{UI_NONE}.
+
+Note that the value returned by this hook should be constant.  It should
+not depend on anything except the command-line switches described by
+@var{opts}.  In particular, the
+setting @code{UI_SJLJ} must be fixed at compiler start-up as C pre-processor
+macros and builtin functions related to exception handling are set up
+depending on this setting.
+
+The default implementation of the hook first honors the
+@option{--enable-sjlj-exceptions} configure option, then
+@code{DWARF2_UNWIND_INFO}, and finally defaults to @code{UI_SJLJ}.  If
+@code{DWARF2_UNWIND_INFO} depends on command-line options, the target
+must define this hook so that @var{opts} is used correctly.
+@end deftypefn
+
+@hook TARGET_UNWIND_TABLES_DEFAULT
+This variable should be set to @code{true} if the target ABI requires unwinding
+tables even when exceptions are not used.  It must not be modified by
+command-line option processing.
+@end deftypevr
+
+@defmac DONT_USE_BUILTIN_SETJMP
+Define this macro to 1 if the @code{setjmp}/@code{longjmp}-based scheme
+should use the @code{setjmp}/@code{longjmp} functions from the C library
+instead of the @code{__builtin_setjmp}/@code{__builtin_longjmp} machinery.
+@end defmac
+
+@defmac JMP_BUF_SIZE
+This macro has no effect unless @code{DONT_USE_BUILTIN_SETJMP} is also
+defined.  Define this macro if the default size of @code{jmp_buf} buffer
+for the @code{setjmp}/@code{longjmp}-based exception handling mechanism
+is not large enough, or if it is much too large.
+The default size is @code{FIRST_PSEUDO_REGISTER * sizeof(void *)}.
+@end defmac
+
+@defmac DWARF_CIE_DATA_ALIGNMENT
+This macro need only be defined if the target might save registers in the
+function prologue at an offset to the stack pointer that is not aligned to
+@code{UNITS_PER_WORD}.  The definition should be the negative minimum
+alignment if @code{STACK_GROWS_DOWNWARD} is true, and the positive
+minimum alignment otherwise.  @xref{DWARF}.  Only applicable if
+the target supports DWARF 2 frame unwind information.
+@end defmac
+
+@hook TARGET_TERMINATE_DW2_EH_FRAME_INFO
+
+@hook TARGET_DWARF_REGISTER_SPAN
+
+@hook TARGET_DWARF_FRAME_REG_MODE
+
+@hook TARGET_INIT_DWARF_REG_SIZES_EXTRA
+
+@hook TARGET_ASM_TTYPE
+
+@hook TARGET_ARM_EABI_UNWINDER
+
+@node Alignment Output
+@subsection Assembler Commands for Alignment
+
+@c prevent bad page break with this line
+This describes commands for alignment.
+
+@defmac JUMP_ALIGN (@var{label})
+The alignment (log base 2) to put in front of @var{label}, which is
+a common destination of jumps and has no fallthru incoming edge.
+
+This macro need not be defined if you don't want any special alignment
+to be done at such a time.  Most machine descriptions do not currently
+define the macro.
+
+Unless it's necessary to inspect the @var{label} parameter, it is better
+to set the variable @var{align_jumps} in the target's
+@code{TARGET_OPTION_OVERRIDE}.  Otherwise, you should try to honor the user's
+selection in @var{align_jumps} in a @code{JUMP_ALIGN} implementation.
+@end defmac
+
+@defmac LABEL_ALIGN_AFTER_BARRIER (@var{label})
+The alignment (log base 2) to put in front of @var{label}, which follows
+a @code{BARRIER}.
+
+This macro need not be defined if you don't want any special alignment
+to be done at such a time.  Most machine descriptions do not currently
+define the macro.
+@end defmac
+
+@defmac LOOP_ALIGN (@var{label})
+The alignment (log base 2) to put in front of @var{label} that heads
+a frequently executed basic block (usually the header of a loop).
+
+This macro need not be defined if you don't want any special alignment
+to be done at such a time.  Most machine descriptions do not currently
+define the macro.
+
+Unless it's necessary to inspect the @var{label} parameter, it is better
+to set the variable @code{align_loops} in the target's
+@code{TARGET_OPTION_OVERRIDE}.  Otherwise, you should try to honor the user's
+selection in @code{align_loops} in a @code{LOOP_ALIGN} implementation.
+@end defmac
+
+@defmac LABEL_ALIGN (@var{label})
+The alignment (log base 2) to put in front of @var{label}.
+If @code{LABEL_ALIGN_AFTER_BARRIER} / @code{LOOP_ALIGN} specify a different alignment,
+the maximum of the specified values is used.
+
+Unless it's necessary to inspect the @var{label} parameter, it is better
+to set the variable @code{align_labels} in the target's
+@code{TARGET_OPTION_OVERRIDE}.  Otherwise, you should try to honor the user's
+selection in @code{align_labels} in a @code{LABEL_ALIGN} implementation.
+@end defmac
+
+@defmac ASM_OUTPUT_SKIP (@var{stream}, @var{nbytes})
+A C statement to output to the stdio stream @var{stream} an assembler
+instruction to advance the location counter by @var{nbytes} bytes.
+Those bytes should be zero when loaded.  @var{nbytes} will be a C
+expression of type @code{unsigned HOST_WIDE_INT}.
+@end defmac
+
+@defmac ASM_NO_SKIP_IN_TEXT
+Define this macro if @code{ASM_OUTPUT_SKIP} should not be used in the
+text section because it fails to put zeros in the bytes that are skipped.
+This is true on many Unix systems, where the pseudo--op to skip bytes
+produces no-op instructions rather than zeros when used in the text
+section.
+@end defmac
+
+@defmac ASM_OUTPUT_ALIGN (@var{stream}, @var{power})
+A C statement to output to the stdio stream @var{stream} an assembler
+command to advance the location counter to a multiple of 2 to the
+@var{power} bytes.  @var{power} will be a C expression of type @code{int}.
+@end defmac
+
+@defmac ASM_OUTPUT_ALIGN_WITH_NOP (@var{stream}, @var{power})
+Like @code{ASM_OUTPUT_ALIGN}, except that the ``nop'' instruction is used
+for padding, if necessary.
+@end defmac
+
+@defmac ASM_OUTPUT_MAX_SKIP_ALIGN (@var{stream}, @var{power}, @var{max_skip})
+A C statement to output to the stdio stream @var{stream} an assembler
+command to advance the location counter to a multiple of 2 to the
+@var{power} bytes, but only if @var{max_skip} or fewer bytes are needed to
+satisfy the alignment request.  @var{power} and @var{max_skip} will be
+a C expression of type @code{int}.
+@end defmac
+
+@need 3000
+@node Debugging Info
+@section Controlling Debugging Information Format
+
+@c prevent bad page break with this line
+This describes how to specify debugging information.
+
+@menu
+* All Debuggers::      Macros that affect all debugging formats uniformly.
+* DBX Options::        Macros enabling specific options in DBX format.
+* DBX Hooks::          Hook macros for varying DBX format.
+* File Names and DBX:: Macros controlling output of file names in DBX format.
+* DWARF::              Macros for DWARF format.
+* VMS Debug::          Macros for VMS debug format.
+@end menu
+
+@node All Debuggers
+@subsection Macros Affecting All Debugging Formats
+
+@c prevent bad page break with this line
+These macros affect all debugging formats.
+
+@defmac DBX_REGISTER_NUMBER (@var{regno})
+A C expression that returns the DBX register number for the compiler
+register number @var{regno}.  In the default macro provided, the value
+of this expression will be @var{regno} itself.  But sometimes there are
+some registers that the compiler knows about and DBX does not, or vice
+versa.  In such cases, some register may need to have one number in the
+compiler and another for DBX@.
+
+If two registers have consecutive numbers inside GCC, and they can be
+used as a pair to hold a multiword value, then they @emph{must} have
+consecutive numbers after renumbering with @code{DBX_REGISTER_NUMBER}.
+Otherwise, debuggers will be unable to access such a pair, because they
+expect register pairs to be consecutive in their own numbering scheme.
+
+If you find yourself defining @code{DBX_REGISTER_NUMBER} in way that
+does not preserve register pairs, then what you must do instead is
+redefine the actual register numbering scheme.
+@end defmac
+
+@defmac DEBUGGER_AUTO_OFFSET (@var{x})
+A C expression that returns the integer offset value for an automatic
+variable having address @var{x} (an RTL expression).  The default
+computation assumes that @var{x} is based on the frame-pointer and
+gives the offset from the frame-pointer.  This is required for targets
+that produce debugging output for DBX and allow the frame-pointer to be
+eliminated when the @option{-g} option is used.
+@end defmac
+
+@defmac DEBUGGER_ARG_OFFSET (@var{offset}, @var{x})
+A C expression that returns the integer offset value for an argument
+having address @var{x} (an RTL expression).  The nominal offset is
+@var{offset}.
+@end defmac
+
+@defmac PREFERRED_DEBUGGING_TYPE
+A C expression that returns the type of debugging output GCC should
+produce when the user specifies just @option{-g}.  Define
+this if you have arranged for GCC to support more than one format of
+debugging output.  Currently, the allowable values are @code{DBX_DEBUG},
+@code{DWARF2_DEBUG}, @code{XCOFF_DEBUG}, @code{VMS_DEBUG},
+and @code{VMS_AND_DWARF2_DEBUG}.
+
+When the user specifies @option{-ggdb}, GCC normally also uses the
+value of this macro to select the debugging output format, but with two
+exceptions.  If @code{DWARF2_DEBUGGING_INFO} is defined, GCC uses the
+value @code{DWARF2_DEBUG}.  Otherwise, if @code{DBX_DEBUGGING_INFO} is
+defined, GCC uses @code{DBX_DEBUG}.
+
+The value of this macro only affects the default debugging output; the
+user can always get a specific type of output by using @option{-gstabs},
+@option{-gdwarf-2}, @option{-gxcoff}, or @option{-gvms}.
+@end defmac
+
+@node DBX Options
+@subsection Specific Options for DBX Output
+
+@c prevent bad page break with this line
+These are specific options for DBX output.
+
+@defmac DBX_DEBUGGING_INFO
+Define this macro if GCC should produce debugging output for DBX
+in response to the @option{-g} option.
+@end defmac
+
+@defmac XCOFF_DEBUGGING_INFO
+Define this macro if GCC should produce XCOFF format debugging output
+in response to the @option{-g} option.  This is a variant of DBX format.
+@end defmac
+
+@defmac DEFAULT_GDB_EXTENSIONS
+Define this macro to control whether GCC should by default generate
+GDB's extended version of DBX debugging information (assuming DBX-format
+debugging information is enabled at all).  If you don't define the
+macro, the default is 1: always generate the extended information
+if there is any occasion to.
+@end defmac
+
+@defmac DEBUG_SYMS_TEXT
+Define this macro if all @code{.stabs} commands should be output while
+in the text section.
+@end defmac
+
+@defmac ASM_STABS_OP
+A C string constant, including spacing, naming the assembler pseudo op to
+use instead of @code{"\t.stabs\t"} to define an ordinary debugging symbol.
+If you don't define this macro, @code{"\t.stabs\t"} is used.  This macro
+applies only to DBX debugging information format.
+@end defmac
+
+@defmac ASM_STABD_OP
+A C string constant, including spacing, naming the assembler pseudo op to
+use instead of @code{"\t.stabd\t"} to define a debugging symbol whose
+value is the current location.  If you don't define this macro,
+@code{"\t.stabd\t"} is used.  This macro applies only to DBX debugging
+information format.
+@end defmac
+
+@defmac ASM_STABN_OP
+A C string constant, including spacing, naming the assembler pseudo op to
+use instead of @code{"\t.stabn\t"} to define a debugging symbol with no
+name.  If you don't define this macro, @code{"\t.stabn\t"} is used.  This
+macro applies only to DBX debugging information format.
+@end defmac
+
+@defmac DBX_NO_XREFS
+Define this macro if DBX on your system does not support the construct
+@samp{xs@var{tagname}}.  On some systems, this construct is used to
+describe a forward reference to a structure named @var{tagname}.
+On other systems, this construct is not supported at all.
+@end defmac
+
+@defmac DBX_CONTIN_LENGTH
+A symbol name in DBX-format debugging information is normally
+continued (split into two separate @code{.stabs} directives) when it
+exceeds a certain length (by default, 80 characters).  On some
+operating systems, DBX requires this splitting; on others, splitting
+must not be done.  You can inhibit splitting by defining this macro
+with the value zero.  You can override the default splitting-length by
+defining this macro as an expression for the length you desire.
+@end defmac
+
+@defmac DBX_CONTIN_CHAR
+Normally continuation is indicated by adding a @samp{\} character to
+the end of a @code{.stabs} string when a continuation follows.  To use
+a different character instead, define this macro as a character
+constant for the character you want to use.  Do not define this macro
+if backslash is correct for your system.
+@end defmac
+
+@defmac DBX_STATIC_STAB_DATA_SECTION
+Define this macro if it is necessary to go to the data section before
+outputting the @samp{.stabs} pseudo-op for a non-global static
+variable.
+@end defmac
+
+@defmac DBX_TYPE_DECL_STABS_CODE
+The value to use in the ``code'' field of the @code{.stabs} directive
+for a typedef.  The default is @code{N_LSYM}.
+@end defmac
+
+@defmac DBX_STATIC_CONST_VAR_CODE
+The value to use in the ``code'' field of the @code{.stabs} directive
+for a static variable located in the text section.  DBX format does not
+provide any ``right'' way to do this.  The default is @code{N_FUN}.
+@end defmac
+
+@defmac DBX_REGPARM_STABS_CODE
+The value to use in the ``code'' field of the @code{.stabs} directive
+for a parameter passed in registers.  DBX format does not provide any
+``right'' way to do this.  The default is @code{N_RSYM}.
+@end defmac
+
+@defmac DBX_REGPARM_STABS_LETTER
+The letter to use in DBX symbol data to identify a symbol as a parameter
+passed in registers.  DBX format does not customarily provide any way to
+do this.  The default is @code{'P'}.
+@end defmac
+
+@defmac DBX_FUNCTION_FIRST
+Define this macro if the DBX information for a function and its
+arguments should precede the assembler code for the function.  Normally,
+in DBX format, the debugging information entirely follows the assembler
+code.
+@end defmac
+
+@defmac DBX_BLOCKS_FUNCTION_RELATIVE
+Define this macro, with value 1, if the value of a symbol describing
+the scope of a block (@code{N_LBRAC} or @code{N_RBRAC}) should be
+relative to the start of the enclosing function.  Normally, GCC uses
+an absolute address.
+@end defmac
+
+@defmac DBX_LINES_FUNCTION_RELATIVE
+Define this macro, with value 1, if the value of a symbol indicating
+the current line number (@code{N_SLINE}) should be relative to the
+start of the enclosing function.  Normally, GCC uses an absolute address.
+@end defmac
+
+@defmac DBX_USE_BINCL
+Define this macro if GCC should generate @code{N_BINCL} and
+@code{N_EINCL} stabs for included header files, as on Sun systems.  This
+macro also directs GCC to output a type number as a pair of a file
+number and a type number within the file.  Normally, GCC does not
+generate @code{N_BINCL} or @code{N_EINCL} stabs, and it outputs a single
+number for a type number.
+@end defmac
+
+@node DBX Hooks
+@subsection Open-Ended Hooks for DBX Format
+
+@c prevent bad page break with this line
+These are hooks for DBX format.
+
+@defmac DBX_OUTPUT_SOURCE_LINE (@var{stream}, @var{line}, @var{counter})
+A C statement to output DBX debugging information before code for line
+number @var{line} of the current source file to the stdio stream
+@var{stream}.  @var{counter} is the number of time the macro was
+invoked, including the current invocation; it is intended to generate
+unique labels in the assembly output.
+
+This macro should not be defined if the default output is correct, or
+if it can be made correct by defining @code{DBX_LINES_FUNCTION_RELATIVE}.
+@end defmac
+
+@defmac NO_DBX_FUNCTION_END
+Some stabs encapsulation formats (in particular ECOFF), cannot handle the
+@code{.stabs "",N_FUN,,0,0,Lscope-function-1} gdb dbx extension construct.
+On those machines, define this macro to turn this feature off without
+disturbing the rest of the gdb extensions.
+@end defmac
+
+@defmac NO_DBX_BNSYM_ENSYM
+Some assemblers cannot handle the @code{.stabd BNSYM/ENSYM,0,0} gdb dbx
+extension construct.  On those machines, define this macro to turn this
+feature off without disturbing the rest of the gdb extensions.
+@end defmac
+
+@node File Names and DBX
+@subsection File Names in DBX Format
+
+@c prevent bad page break with this line
+This describes file names in DBX format.
+
+@defmac DBX_OUTPUT_MAIN_SOURCE_FILENAME (@var{stream}, @var{name})
+A C statement to output DBX debugging information to the stdio stream
+@var{stream}, which indicates that file @var{name} is the main source
+file---the file specified as the input file for compilation.
+This macro is called only once, at the beginning of compilation.
+
+This macro need not be defined if the standard form of output
+for DBX debugging information is appropriate.
+
+It may be necessary to refer to a label equal to the beginning of the
+text section.  You can use @samp{assemble_name (stream, ltext_label_name)}
+to do so.  If you do this, you must also set the variable
+@var{used_ltext_label_name} to @code{true}.
+@end defmac
+
+@defmac NO_DBX_MAIN_SOURCE_DIRECTORY
+Define this macro, with value 1, if GCC should not emit an indication
+of the current directory for compilation and current source language at
+the beginning of the file.
+@end defmac
+
+@defmac NO_DBX_GCC_MARKER
+Define this macro, with value 1, if GCC should not emit an indication
+that this object file was compiled by GCC@.  The default is to emit
+an @code{N_OPT} stab at the beginning of every source file, with
+@samp{gcc2_compiled.} for the string and value 0.
+@end defmac
+
+@defmac DBX_OUTPUT_MAIN_SOURCE_FILE_END (@var{stream}, @var{name})
+A C statement to output DBX debugging information at the end of
+compilation of the main source file @var{name}.  Output should be
+written to the stdio stream @var{stream}.
+
+If you don't define this macro, nothing special is output at the end
+of compilation, which is correct for most machines.
+@end defmac
+
+@defmac DBX_OUTPUT_NULL_N_SO_AT_MAIN_SOURCE_FILE_END
+Define this macro @emph{instead of} defining
+@code{DBX_OUTPUT_MAIN_SOURCE_FILE_END}, if what needs to be output at
+the end of compilation is an @code{N_SO} stab with an empty string,
+whose value is the highest absolute text address in the file.
+@end defmac
+
+@need 2000
+@node DWARF
+@subsection Macros for DWARF Output
+
+@c prevent bad page break with this line
+Here are macros for DWARF output.
+
+@defmac DWARF2_DEBUGGING_INFO
+Define this macro if GCC should produce dwarf version 2 format
+debugging output in response to the @option{-g} option.
+
+@hook TARGET_DWARF_CALLING_CONVENTION
+
+To support optional call frame debugging information, you must also
+define @code{INCOMING_RETURN_ADDR_RTX} and either set
+@code{RTX_FRAME_RELATED_P} on the prologue insns if you use RTL for the
+prologue, or call @code{dwarf2out_def_cfa} and @code{dwarf2out_reg_save}
+as appropriate from @code{TARGET_ASM_FUNCTION_PROLOGUE} if you don't.
+@end defmac
+
+@defmac DWARF2_FRAME_INFO
+Define this macro to a nonzero value if GCC should always output
+Dwarf 2 frame information.  If @code{TARGET_EXCEPT_UNWIND_INFO}
+(@pxref{Exception Region Output}) returns @code{UI_DWARF2}, and
+exceptions are enabled, GCC will output this information not matter
+how you define @code{DWARF2_FRAME_INFO}.
+@end defmac
+
+@hook TARGET_DEBUG_UNWIND_INFO
+
+@defmac DWARF2_ASM_LINE_DEBUG_INFO
+Define this macro to be a nonzero value if the assembler can generate Dwarf 2
+line debug info sections.  This will result in much more compact line number
+tables, and hence is desirable if it works.
+@end defmac
+
+@defmac DWARF2_ASM_VIEW_DEBUG_INFO
+Define this macro to be a nonzero value if the assembler supports view
+assignment and verification in @code{.loc}.  If it does not, but the
+user enables location views, the compiler may have to fallback to
+internal line number tables.
+@end defmac
+
+@hook TARGET_RESET_LOCATION_VIEW
+
+@hook TARGET_WANT_DEBUG_PUB_SECTIONS
+
+@hook TARGET_DELAY_SCHED2
+
+@hook TARGET_DELAY_VARTRACK
+
+@hook TARGET_NO_REGISTER_ALLOCATION
+
+@defmac ASM_OUTPUT_DWARF_DELTA (@var{stream}, @var{size}, @var{label1}, @var{label2})
+A C statement to issue assembly directives that create a difference
+@var{lab1} minus @var{lab2}, using an integer of the given @var{size}.
+@end defmac
+
+@defmac ASM_OUTPUT_DWARF_VMS_DELTA (@var{stream}, @var{size}, @var{label1}, @var{label2})
+A C statement to issue assembly directives that create a difference
+between the two given labels in system defined units, e.g.@: instruction
+slots on IA64 VMS, using an integer of the given size.
+@end defmac
+
+@defmac ASM_OUTPUT_DWARF_OFFSET (@var{stream}, @var{size}, @var{label}, @var{offset}, @var{section})
+A C statement to issue assembly directives that create a
+section-relative reference to the given @var{label} plus @var{offset}, using
+an integer of the given @var{size}.  The label is known to be defined in the
+given @var{section}.
+@end defmac
+
+@defmac ASM_OUTPUT_DWARF_PCREL (@var{stream}, @var{size}, @var{label})
+A C statement to issue assembly directives that create a self-relative
+reference to the given @var{label}, using an integer of the given @var{size}.
+@end defmac
+
+@defmac ASM_OUTPUT_DWARF_DATAREL (@var{stream}, @var{size}, @var{label})
+A C statement to issue assembly directives that create a reference to the
+given @var{label} relative to the dbase, using an integer of the given @var{size}.
+@end defmac
+
+@defmac ASM_OUTPUT_DWARF_TABLE_REF (@var{label})
+A C statement to issue assembly directives that create a reference to
+the DWARF table identifier @var{label} from the current section.  This
+is used on some systems to avoid garbage collecting a DWARF table which
+is referenced by a function.
+@end defmac
+
+@hook TARGET_ASM_OUTPUT_DWARF_DTPREL
+
+@need 2000
+@node VMS Debug
+@subsection Macros for VMS Debug Format
+
+@c prevent bad page break with this line
+Here are macros for VMS debug format.
+
+@defmac VMS_DEBUGGING_INFO
+Define this macro if GCC should produce debugging output for VMS
+in response to the @option{-g} option.  The default behavior for VMS
+is to generate minimal debug info for a traceback in the absence of
+@option{-g} unless explicitly overridden with @option{-g0}.  This
+behavior is controlled by @code{TARGET_OPTION_OPTIMIZATION} and
+@code{TARGET_OPTION_OVERRIDE}.
+@end defmac
+
+@node Floating Point
+@section Cross Compilation and Floating Point
+@cindex cross compilation and floating point
+@cindex floating point and cross compilation
+
+While all modern machines use twos-complement representation for integers,
+there are a variety of representations for floating point numbers.  This
+means that in a cross-compiler the representation of floating point numbers
+in the compiled program may be different from that used in the machine
+doing the compilation.
+
+Because different representation systems may offer different amounts of
+range and precision, all floating point constants must be represented in
+the target machine's format.  Therefore, the cross compiler cannot
+safely use the host machine's floating point arithmetic; it must emulate
+the target's arithmetic.  To ensure consistency, GCC always uses
+emulation to work with floating point values, even when the host and
+target floating point formats are identical.
+
+The following macros are provided by @file{real.h} for the compiler to
+use.  All parts of the compiler which generate or optimize
+floating-point calculations must use these macros.  They may evaluate
+their operands more than once, so operands must not have side effects.
+
+@defmac REAL_VALUE_TYPE
+The C data type to be used to hold a floating point value in the target
+machine's format.  Typically this is a @code{struct} containing an
+array of @code{HOST_WIDE_INT}, but all code should treat it as an opaque
+quantity.
+@end defmac
+
+@deftypefn Macro HOST_WIDE_INT REAL_VALUE_FIX (REAL_VALUE_TYPE @var{x})
+Truncates @var{x} to a signed integer, rounding toward zero.
+@end deftypefn
+
+@deftypefn Macro {unsigned HOST_WIDE_INT} REAL_VALUE_UNSIGNED_FIX (REAL_VALUE_TYPE @var{x})
+Truncates @var{x} to an unsigned integer, rounding toward zero.  If
+@var{x} is negative, returns zero.
+@end deftypefn
+
+@deftypefn Macro REAL_VALUE_TYPE REAL_VALUE_ATOF (const char *@var{string}, machine_mode @var{mode})
+Converts @var{string} into a floating point number in the target machine's
+representation for mode @var{mode}.  This routine can handle both
+decimal and hexadecimal floating point constants, using the syntax
+defined by the C language for both.
+@end deftypefn
+
+@deftypefn Macro int REAL_VALUE_NEGATIVE (REAL_VALUE_TYPE @var{x})
+Returns 1 if @var{x} is negative (including negative zero), 0 otherwise.
+@end deftypefn
+
+@deftypefn Macro int REAL_VALUE_ISINF (REAL_VALUE_TYPE @var{x})
+Determines whether @var{x} represents infinity (positive or negative).
+@end deftypefn
+
+@deftypefn Macro int REAL_VALUE_ISNAN (REAL_VALUE_TYPE @var{x})
+Determines whether @var{x} represents a ``NaN'' (not-a-number).
+@end deftypefn
+
+@deftypefn Macro REAL_VALUE_TYPE REAL_VALUE_NEGATE (REAL_VALUE_TYPE @var{x})
+Returns the negative of the floating point value @var{x}.
+@end deftypefn
+
+@deftypefn Macro REAL_VALUE_TYPE REAL_VALUE_ABS (REAL_VALUE_TYPE @var{x})
+Returns the absolute value of @var{x}.
+@end deftypefn
+
+@node Mode Switching
+@section Mode Switching Instructions
+@cindex mode switching
+The following macros control mode switching optimizations:
+
+@defmac OPTIMIZE_MODE_SWITCHING (@var{entity})
+Define this macro if the port needs extra instructions inserted for mode
+switching in an optimizing compilation.
+
+For an example, the SH4 can perform both single and double precision
+floating point operations, but to perform a single precision operation,
+the FPSCR PR bit has to be cleared, while for a double precision
+operation, this bit has to be set.  Changing the PR bit requires a general
+purpose register as a scratch register, hence these FPSCR sets have to
+be inserted before reload, i.e.@: you cannot put this into instruction emitting
+or @code{TARGET_MACHINE_DEPENDENT_REORG}.
+
+You can have multiple entities that are mode-switched, and select at run time
+which entities actually need it.  @code{OPTIMIZE_MODE_SWITCHING} should
+return nonzero for any @var{entity} that needs mode-switching.
+If you define this macro, you also have to define
+@code{NUM_MODES_FOR_MODE_SWITCHING}, @code{TARGET_MODE_NEEDED},
+@code{TARGET_MODE_PRIORITY} and @code{TARGET_MODE_EMIT}.
+@code{TARGET_MODE_AFTER}, @code{TARGET_MODE_ENTRY}, and @code{TARGET_MODE_EXIT}
+are optional.
+@end defmac
+
+@defmac NUM_MODES_FOR_MODE_SWITCHING
+If you define @code{OPTIMIZE_MODE_SWITCHING}, you have to define this as
+initializer for an array of integers.  Each initializer element
+N refers to an entity that needs mode switching, and specifies the number
+of different modes that might need to be set for this entity.
+The position of the initializer in the initializer---starting counting at
+zero---determines the integer that is used to refer to the mode-switched
+entity in question.
+In macros that take mode arguments / yield a mode result, modes are
+represented as numbers 0 @dots{} N @minus{} 1.  N is used to specify that no mode
+switch is needed / supplied.
+@end defmac
+
+@hook TARGET_MODE_EMIT
+
+@hook TARGET_MODE_NEEDED
+
+@hook TARGET_MODE_AFTER
+
+@hook TARGET_MODE_ENTRY
+
+@hook TARGET_MODE_EXIT
+
+@hook TARGET_MODE_PRIORITY
+
+@node Target Attributes
+@section Defining target-specific uses of @code{__attribute__}
+@cindex target attributes
+@cindex machine attributes
+@cindex attributes, target-specific
+
+Target-specific attributes may be defined for functions, data and types.
+These are described using the following target hooks; they also need to
+be documented in @file{extend.texi}.
+
+@hook TARGET_ATTRIBUTE_TABLE
+
+@hook TARGET_ATTRIBUTE_TAKES_IDENTIFIER_P
+
+@hook TARGET_COMP_TYPE_ATTRIBUTES
+
+@hook TARGET_SET_DEFAULT_TYPE_ATTRIBUTES
+
+@hook TARGET_MERGE_TYPE_ATTRIBUTES
+
+@hook TARGET_MERGE_DECL_ATTRIBUTES
+
+@hook TARGET_VALID_DLLIMPORT_ATTRIBUTE_P
+
+@defmac TARGET_DECLSPEC
+Define this macro to a nonzero value if you want to treat
+@code{__declspec(X)} as equivalent to @code{__attribute((X))}.  By
+default, this behavior is enabled only for targets that define
+@code{TARGET_DLLIMPORT_DECL_ATTRIBUTES}.  The current implementation
+of @code{__declspec} is via a built-in macro, but you should not rely
+on this implementation detail.
+@end defmac
+
+@hook TARGET_INSERT_ATTRIBUTES
+
+@hook TARGET_HANDLE_GENERIC_ATTRIBUTE
+
+@hook TARGET_FUNCTION_ATTRIBUTE_INLINABLE_P
+
+@hook TARGET_OPTION_VALID_ATTRIBUTE_P
+
+@hook TARGET_OPTION_SAVE
+
+@hook TARGET_OPTION_RESTORE
+
+@hook TARGET_OPTION_POST_STREAM_IN
+
+@hook TARGET_OPTION_PRINT
+
+@hook TARGET_OPTION_PRAGMA_PARSE
+
+@hook TARGET_OPTION_OVERRIDE
+
+@hook TARGET_OPTION_FUNCTION_VERSIONS
+
+@hook TARGET_CAN_INLINE_P
+
+@hook TARGET_RELAYOUT_FUNCTION
+
+@node Emulated TLS
+@section Emulating TLS
+@cindex Emulated TLS
+
+For targets whose psABI does not provide Thread Local Storage via
+specific relocations and instruction sequences, an emulation layer is
+used.  A set of target hooks allows this emulation layer to be
+configured for the requirements of a particular target.  For instance
+the psABI may in fact specify TLS support in terms of an emulation
+layer.
+
+The emulation layer works by creating a control object for every TLS
+object.  To access the TLS object, a lookup function is provided
+which, when given the address of the control object, will return the
+address of the current thread's instance of the TLS object.
+
+@hook TARGET_EMUTLS_GET_ADDRESS
+
+@hook TARGET_EMUTLS_REGISTER_COMMON
+
+@hook TARGET_EMUTLS_VAR_SECTION
+
+@hook TARGET_EMUTLS_TMPL_SECTION
+
+@hook TARGET_EMUTLS_VAR_PREFIX
+
+@hook TARGET_EMUTLS_TMPL_PREFIX
+
+@hook TARGET_EMUTLS_VAR_FIELDS
+
+@hook TARGET_EMUTLS_VAR_INIT
+
+@hook TARGET_EMUTLS_VAR_ALIGN_FIXED
+
+@hook TARGET_EMUTLS_DEBUG_FORM_TLS_ADDRESS
+
+@node MIPS Coprocessors
+@section Defining coprocessor specifics for MIPS targets.
+@cindex MIPS coprocessor-definition macros
+
+The MIPS specification allows MIPS implementations to have as many as 4
+coprocessors, each with as many as 32 private registers.  GCC supports
+accessing these registers and transferring values between the registers
+and memory using asm-ized variables.  For example:
+
+@smallexample
+  register unsigned int cp0count asm ("c0r1");
+  unsigned int d;
+
+  d = cp0count + 3;
+@end smallexample
+
+(``c0r1'' is the default name of register 1 in coprocessor 0; alternate
+names may be added as described below, or the default names may be
+overridden entirely in @code{SUBTARGET_CONDITIONAL_REGISTER_USAGE}.)
+
+Coprocessor registers are assumed to be epilogue-used; sets to them will
+be preserved even if it does not appear that the register is used again
+later in the function.
+
+Another note: according to the MIPS spec, coprocessor 1 (if present) is
+the FPU@.  One accesses COP1 registers through standard mips
+floating-point support; they are not included in this mechanism.
+
+@node PCH Target
+@section Parameters for Precompiled Header Validity Checking
+@cindex parameters, precompiled headers
+
+@hook TARGET_GET_PCH_VALIDITY
+
+@hook TARGET_PCH_VALID_P
+
+@hook TARGET_CHECK_PCH_TARGET_FLAGS
+
+@hook TARGET_PREPARE_PCH_SAVE
+
+@node C++ ABI
+@section C++ ABI parameters
+@cindex parameters, c++ abi
+
+@hook TARGET_CXX_GUARD_TYPE
+
+@hook TARGET_CXX_GUARD_MASK_BIT
+
+@hook TARGET_CXX_GET_COOKIE_SIZE
+
+@hook TARGET_CXX_COOKIE_HAS_SIZE
+
+@hook TARGET_CXX_IMPORT_EXPORT_CLASS
+
+@hook TARGET_CXX_CDTOR_RETURNS_THIS
+
+@hook TARGET_CXX_KEY_METHOD_MAY_BE_INLINE
+
+@hook TARGET_CXX_DETERMINE_CLASS_DATA_VISIBILITY
+
+@hook TARGET_CXX_CLASS_DATA_ALWAYS_COMDAT
+
+@hook TARGET_CXX_LIBRARY_RTTI_COMDAT
+
+@hook TARGET_CXX_USE_AEABI_ATEXIT
+
+@hook TARGET_CXX_USE_ATEXIT_FOR_CXA_ATEXIT
+
+@hook TARGET_CXX_ADJUST_CLASS_AT_DEFINITION
+
+@hook TARGET_CXX_DECL_MANGLING_CONTEXT
+
+@node D Language and ABI
+@section D ABI parameters
+@cindex parameters, d abi
+
+@hook TARGET_D_CPU_VERSIONS
+
+@hook TARGET_D_OS_VERSIONS
+
+@hook TARGET_D_REGISTER_CPU_TARGET_INFO
+
+@hook TARGET_D_REGISTER_OS_TARGET_INFO
+
+@hook TARGET_D_MINFO_SECTION
+
+@hook TARGET_D_MINFO_START_NAME
+
+@hook TARGET_D_MINFO_END_NAME
+
+@hook TARGET_D_HAS_STDCALL_CONVENTION
+
+@hook TARGET_D_TEMPLATES_ALWAYS_COMDAT
+
+@node Named Address Spaces
+@section Adding support for named address spaces
+@cindex named address spaces
+
+The draft technical report of the ISO/IEC JTC1 S22 WG14 N1275
+standards committee, @cite{Programming Languages - C - Extensions to
+support embedded processors}, specifies a syntax for embedded
+processors to specify alternate address spaces.  You can configure a
+GCC port to support section 5.1 of the draft report to add support for
+address spaces other than the default address space.  These address
+spaces are new keywords that are similar to the @code{volatile} and
+@code{const} type attributes.
+
+Pointers to named address spaces can have a different size than
+pointers to the generic address space.
+
+For example, the SPU port uses the @code{__ea} address space to refer
+to memory in the host processor, rather than memory local to the SPU
+processor.  Access to memory in the @code{__ea} address space involves
+issuing DMA operations to move data between the host processor and the
+local processor memory address space.  Pointers in the @code{__ea}
+address space are either 32 bits or 64 bits based on the
+@option{-mea32} or @option{-mea64} switches (native SPU pointers are
+always 32 bits).
+
+Internally, address spaces are represented as a small integer in the
+range 0 to 15 with address space 0 being reserved for the generic
+address space.
+
+To register a named address space qualifier keyword with the C front end,
+the target may call the @code{c_register_addr_space} routine.  For example,
+the SPU port uses the following to declare @code{__ea} as the keyword for
+named address space #1:
+@smallexample
+#define ADDR_SPACE_EA 1
+c_register_addr_space ("__ea", ADDR_SPACE_EA);
+@end smallexample
+
+@hook TARGET_ADDR_SPACE_POINTER_MODE
+
+@hook TARGET_ADDR_SPACE_ADDRESS_MODE
+
+@hook TARGET_ADDR_SPACE_VALID_POINTER_MODE
+
+@hook TARGET_ADDR_SPACE_LEGITIMATE_ADDRESS_P
+
+@hook TARGET_ADDR_SPACE_LEGITIMIZE_ADDRESS
+
+@hook TARGET_ADDR_SPACE_SUBSET_P
+
+@hook TARGET_ADDR_SPACE_ZERO_ADDRESS_VALID
+
+@hook TARGET_ADDR_SPACE_CONVERT
+
+@hook TARGET_ADDR_SPACE_DEBUG
+
+@hook TARGET_ADDR_SPACE_DIAGNOSE_USAGE
+
+@node Misc
+@section Miscellaneous Parameters
+@cindex parameters, miscellaneous
+
+@c prevent bad page break with this line
+Here are several miscellaneous parameters.
+
+@defmac HAS_LONG_COND_BRANCH
+Define this boolean macro to indicate whether or not your architecture
+has conditional branches that can span all of memory.  It is used in
+conjunction with an optimization that partitions hot and cold basic
+blocks into separate sections of the executable.  If this macro is
+set to false, gcc will convert any conditional branches that attempt
+to cross between sections into unconditional branches or indirect jumps.
+@end defmac
+
+@defmac HAS_LONG_UNCOND_BRANCH
+Define this boolean macro to indicate whether or not your architecture
+has unconditional branches that can span all of memory.  It is used in
+conjunction with an optimization that partitions hot and cold basic
+blocks into separate sections of the executable.  If this macro is
+set to false, gcc will convert any unconditional branches that attempt
+to cross between sections into indirect jumps.
+@end defmac
+
+@defmac CASE_VECTOR_MODE
+An alias for a machine mode name.  This is the machine mode that
+elements of a jump-table should have.
+@end defmac
+
+@defmac CASE_VECTOR_SHORTEN_MODE (@var{min_offset}, @var{max_offset}, @var{body})
+Optional: return the preferred mode for an @code{addr_diff_vec}
+when the minimum and maximum offset are known.  If you define this,
+it enables extra code in branch shortening to deal with @code{addr_diff_vec}.
+To make this work, you also have to define @code{INSN_ALIGN} and
+make the alignment for @code{addr_diff_vec} explicit.
+The @var{body} argument is provided so that the offset_unsigned and scale
+flags can be updated.
+@end defmac
+
+@defmac CASE_VECTOR_PC_RELATIVE
+Define this macro to be a C expression to indicate when jump-tables
+should contain relative addresses.  You need not define this macro if
+jump-tables never contain relative addresses, or jump-tables should
+contain relative addresses only when @option{-fPIC} or @option{-fPIC}
+is in effect.
+@end defmac
+
+@hook TARGET_CASE_VALUES_THRESHOLD
+
+@defmac WORD_REGISTER_OPERATIONS
+Define this macro to 1 if operations between registers with integral mode
+smaller than a word are always performed on the entire register.  To be
+more explicit, if you start with a pair of @code{word_mode} registers with
+known values and you do a subword, for example @code{QImode}, addition on
+the low part of the registers, then the compiler may consider that the
+result has a known value in @code{word_mode} too if the macro is defined
+to 1.  Most RISC machines have this property and most CISC machines do not.
+@end defmac
+
+@hook TARGET_MIN_ARITHMETIC_PRECISION
+
+@defmac LOAD_EXTEND_OP (@var{mem_mode})
+Define this macro to be a C expression indicating when insns that read
+memory in @var{mem_mode}, an integral mode narrower than a word, set the
+bits outside of @var{mem_mode} to be either the sign-extension or the
+zero-extension of the data read.  Return @code{SIGN_EXTEND} for values
+of @var{mem_mode} for which the
+insn sign-extends, @code{ZERO_EXTEND} for which it zero-extends, and
+@code{UNKNOWN} for other modes.
+
+This macro is not called with @var{mem_mode} non-integral or with a width
+greater than or equal to @code{BITS_PER_WORD}, so you may return any
+value in this case.  Do not define this macro if it would always return
+@code{UNKNOWN}.  On machines where this macro is defined, you will normally
+define it as the constant @code{SIGN_EXTEND} or @code{ZERO_EXTEND}.
+
+You may return a non-@code{UNKNOWN} value even if for some hard registers
+the sign extension is not performed, if for the @code{REGNO_REG_CLASS}
+of these hard registers @code{TARGET_CAN_CHANGE_MODE_CLASS} returns false
+when the @var{from} mode is @var{mem_mode} and the @var{to} mode is any
+integral mode larger than this but not larger than @code{word_mode}.
+
+You must return @code{UNKNOWN} if for some hard registers that allow this
+mode, @code{TARGET_CAN_CHANGE_MODE_CLASS} says that they cannot change to
+@code{word_mode}, but that they can change to another integral mode that
+is larger then @var{mem_mode} but still smaller than @code{word_mode}.
+@end defmac
+
+@defmac SHORT_IMMEDIATES_SIGN_EXTEND
+Define this macro to 1 if loading short immediate values into registers sign
+extends.
+@end defmac
+
+@hook TARGET_MIN_DIVISIONS_FOR_RECIP_MUL
+
+@defmac MOVE_MAX
+The maximum number of bytes that a single instruction can move quickly
+between memory and registers or between two memory locations.
+@end defmac
+
+@defmac MAX_MOVE_MAX
+The maximum number of bytes that a single instruction can move quickly
+between memory and registers or between two memory locations.  If this
+is undefined, the default is @code{MOVE_MAX}.  Otherwise, it is the
+constant value that is the largest value that @code{MOVE_MAX} can have
+at run-time.
+@end defmac
+
+@defmac SHIFT_COUNT_TRUNCATED
+A C expression that is nonzero if on this machine the number of bits
+actually used for the count of a shift operation is equal to the number
+of bits needed to represent the size of the object being shifted.  When
+this macro is nonzero, the compiler will assume that it is safe to omit
+a sign-extend, zero-extend, and certain bitwise `and' instructions that
+truncates the count of a shift operation.  On machines that have
+instructions that act on bit-fields at variable positions, which may
+include `bit test' instructions, a nonzero @code{SHIFT_COUNT_TRUNCATED}
+also enables deletion of truncations of the values that serve as
+arguments to bit-field instructions.
+
+If both types of instructions truncate the count (for shifts) and
+position (for bit-field operations), or if no variable-position bit-field
+instructions exist, you should define this macro.
+
+However, on some machines, such as the 80386 and the 680x0, truncation
+only applies to shift operations and not the (real or pretended)
+bit-field operations.  Define @code{SHIFT_COUNT_TRUNCATED} to be zero on
+such machines.  Instead, add patterns to the @file{md} file that include
+the implied truncation of the shift instructions.
+
+You need not define this macro if it would always have the value of zero.
+@end defmac
+
+@anchor{TARGET_SHIFT_TRUNCATION_MASK}
+@hook TARGET_SHIFT_TRUNCATION_MASK
+
+@hook TARGET_TRULY_NOOP_TRUNCATION
+
+@hook TARGET_MODE_REP_EXTENDED
+
+@hook TARGET_SETJMP_PRESERVES_NONVOLATILE_REGS_P
+
+@defmac STORE_FLAG_VALUE
+A C expression describing the value returned by a comparison operator
+with an integral mode and stored by a store-flag instruction
+(@samp{cstore@var{mode}4}) when the condition is true.  This description must
+apply to @emph{all} the @samp{cstore@var{mode}4} patterns and all the
+comparison operators whose results have a @code{MODE_INT} mode.
+
+A value of 1 or @minus{}1 means that the instruction implementing the
+comparison operator returns exactly 1 or @minus{}1 when the comparison is true
+and 0 when the comparison is false.  Otherwise, the value indicates
+which bits of the result are guaranteed to be 1 when the comparison is
+true.  This value is interpreted in the mode of the comparison
+operation, which is given by the mode of the first operand in the
+@samp{cstore@var{mode}4} pattern.  Either the low bit or the sign bit of
+@code{STORE_FLAG_VALUE} be on.  Presently, only those bits are used by
+the compiler.
+
+If @code{STORE_FLAG_VALUE} is neither 1 or @minus{}1, the compiler will
+generate code that depends only on the specified bits.  It can also
+replace comparison operators with equivalent operations if they cause
+the required bits to be set, even if the remaining bits are undefined.
+For example, on a machine whose comparison operators return an
+@code{SImode} value and where @code{STORE_FLAG_VALUE} is defined as
+@samp{0x80000000}, saying that just the sign bit is relevant, the
+expression
+
+@smallexample
+(ne:SI (and:SI @var{x} (const_int @var{power-of-2})) (const_int 0))
+@end smallexample
+
+@noindent
+can be converted to
+
+@smallexample
+(ashift:SI @var{x} (const_int @var{n}))
+@end smallexample
+
+@noindent
+where @var{n} is the appropriate shift count to move the bit being
+tested into the sign bit.
+
+There is no way to describe a machine that always sets the low-order bit
+for a true value, but does not guarantee the value of any other bits,
+but we do not know of any machine that has such an instruction.  If you
+are trying to port GCC to such a machine, include an instruction to
+perform a logical-and of the result with 1 in the pattern for the
+comparison operators and let us know at @email{gcc@@gcc.gnu.org}.
+
+Often, a machine will have multiple instructions that obtain a value
+from a comparison (or the condition codes).  Here are rules to guide the
+choice of value for @code{STORE_FLAG_VALUE}, and hence the instructions
+to be used:
+
+@itemize @bullet
+@item
+Use the shortest sequence that yields a valid definition for
+@code{STORE_FLAG_VALUE}.  It is more efficient for the compiler to
+``normalize'' the value (convert it to, e.g., 1 or 0) than for the
+comparison operators to do so because there may be opportunities to
+combine the normalization with other operations.
+
+@item
+For equal-length sequences, use a value of 1 or @minus{}1, with @minus{}1 being
+slightly preferred on machines with expensive jumps and 1 preferred on
+other machines.
+
+@item
+As a second choice, choose a value of @samp{0x80000001} if instructions
+exist that set both the sign and low-order bits but do not define the
+others.
+
+@item
+Otherwise, use a value of @samp{0x80000000}.
+@end itemize
+
+Many machines can produce both the value chosen for
+@code{STORE_FLAG_VALUE} and its negation in the same number of
+instructions.  On those machines, you should also define a pattern for
+those cases, e.g., one matching
+
+@smallexample
+(set @var{A} (neg:@var{m} (ne:@var{m} @var{B} @var{C})))
+@end smallexample
+
+Some machines can also perform @code{and} or @code{plus} operations on
+condition code values with less instructions than the corresponding
+@samp{cstore@var{mode}4} insn followed by @code{and} or @code{plus}.  On those
+machines, define the appropriate patterns.  Use the names @code{incscc}
+and @code{decscc}, respectively, for the patterns which perform
+@code{plus} or @code{minus} operations on condition code values.  See
+@file{rs6000.md} for some examples.  The GNU Superoptimizer can be used to
+find such instruction sequences on other machines.
+
+If this macro is not defined, the default value, 1, is used.  You need
+not define @code{STORE_FLAG_VALUE} if the machine has no store-flag
+instructions, or if the value generated by these instructions is 1.
+@end defmac
+
+@defmac FLOAT_STORE_FLAG_VALUE (@var{mode})
+A C expression that gives a nonzero @code{REAL_VALUE_TYPE} value that is
+returned when comparison operators with floating-point results are true.
+Define this macro on machines that have comparison operations that return
+floating-point values.  If there are no such operations, do not define
+this macro.
+@end defmac
+
+@defmac VECTOR_STORE_FLAG_VALUE (@var{mode})
+A C expression that gives an rtx representing the nonzero true element
+for vector comparisons.  The returned rtx should be valid for the inner
+mode of @var{mode} which is guaranteed to be a vector mode.  Define
+this macro on machines that have vector comparison operations that
+return a vector result.  If there are no such operations, do not define
+this macro.  Typically, this macro is defined as @code{const1_rtx} or
+@code{constm1_rtx}.  This macro may return @code{NULL_RTX} to prevent
+the compiler optimizing such vector comparison operations for the
+given mode.
+@end defmac
+
+@defmac CLZ_DEFINED_VALUE_AT_ZERO (@var{mode}, @var{value})
+@defmacx CTZ_DEFINED_VALUE_AT_ZERO (@var{mode}, @var{value})
+A C expression that indicates whether the architecture defines a value
+for @code{clz} or @code{ctz} with a zero operand.
+A result of @code{0} indicates the value is undefined.
+If the value is defined for only the RTL expression, the macro should
+evaluate to @code{1}; if the value applies also to the corresponding optab
+entry (which is normally the case if it expands directly into
+the corresponding RTL), then the macro should evaluate to @code{2}.
+In the cases where the value is defined, @var{value} should be set to
+this value.
+
+If this macro is not defined, the value of @code{clz} or
+@code{ctz} at zero is assumed to be undefined.
+
+This macro must be defined if the target's expansion for @code{ffs}
+relies on a particular value to get correct results.  Otherwise it
+is not necessary, though it may be used to optimize some corner cases, and
+to provide a default expansion for the @code{ffs} optab.
+
+Note that regardless of this macro the ``definedness'' of @code{clz}
+and @code{ctz} at zero do @emph{not} extend to the builtin functions
+visible to the user.  Thus one may be free to adjust the value at will
+to match the target expansion of these operations without fear of
+breaking the API@.
+@end defmac
+
+@defmac Pmode
+An alias for the machine mode for pointers.  On most machines, define
+this to be the integer mode corresponding to the width of a hardware
+pointer; @code{SImode} on 32-bit machine or @code{DImode} on 64-bit machines.
+On some machines you must define this to be one of the partial integer
+modes, such as @code{PSImode}.
+
+The width of @code{Pmode} must be at least as large as the value of
+@code{POINTER_SIZE}.  If it is not equal, you must define the macro
+@code{POINTERS_EXTEND_UNSIGNED} to specify how pointers are extended
+to @code{Pmode}.
+@end defmac
+
+@defmac FUNCTION_MODE
+An alias for the machine mode used for memory references to functions
+being called, in @code{call} RTL expressions.  On most CISC machines,
+where an instruction can begin at any byte address, this should be
+@code{QImode}.  On most RISC machines, where all instructions have fixed
+size and alignment, this should be a mode with the same size and alignment
+as the machine instruction words - typically @code{SImode} or @code{HImode}.
+@end defmac
+
+@defmac STDC_0_IN_SYSTEM_HEADERS
+In normal operation, the preprocessor expands @code{__STDC__} to the
+constant 1, to signify that GCC conforms to ISO Standard C@.  On some
+hosts, like Solaris, the system compiler uses a different convention,
+where @code{__STDC__} is normally 0, but is 1 if the user specifies
+strict conformance to the C Standard.
+
+Defining @code{STDC_0_IN_SYSTEM_HEADERS} makes GNU CPP follows the host
+convention when processing system header files, but when processing user
+files @code{__STDC__} will always expand to 1.
+@end defmac
+
+@hook TARGET_C_PREINCLUDE
+
+@hook TARGET_CXX_IMPLICIT_EXTERN_C
+
+@defmac SYSTEM_IMPLICIT_EXTERN_C
+Define this macro if the system header files do not support C++@.
+This macro handles system header files by pretending that system
+header files are enclosed in @samp{extern "C" @{@dots{}@}}.
+@end defmac
+
+@findex #pragma
+@findex pragma
+@defmac REGISTER_TARGET_PRAGMAS ()
+Define this macro if you want to implement any target-specific pragmas.
+If defined, it is a C expression which makes a series of calls to
+@code{c_register_pragma} or @code{c_register_pragma_with_expansion}
+for each pragma.  The macro may also do any
+setup required for the pragmas.
+
+The primary reason to define this macro is to provide compatibility with
+other compilers for the same target.  In general, we discourage
+definition of target-specific pragmas for GCC@.
+
+If the pragma can be implemented by attributes then you should consider
+defining the target hook @samp{TARGET_INSERT_ATTRIBUTES} as well.
+
+Preprocessor macros that appear on pragma lines are not expanded.  All
+@samp{#pragma} directives that do not match any registered pragma are
+silently ignored, unless the user specifies @option{-Wunknown-pragmas}.
+@end defmac
+
+@deftypefun void c_register_pragma (const char *@var{space}, const char *@var{name}, void (*@var{callback}) (struct cpp_reader *))
+@deftypefunx void c_register_pragma_with_expansion (const char *@var{space}, const char *@var{name}, void (*@var{callback}) (struct cpp_reader *))
+
+Each call to @code{c_register_pragma} or
+@code{c_register_pragma_with_expansion} establishes one pragma.  The
+@var{callback} routine will be called when the preprocessor encounters a
+pragma of the form
+
+@smallexample
+#pragma [@var{space}] @var{name} @dots{}
+@end smallexample
+
+@var{space} is the case-sensitive namespace of the pragma, or
+@code{NULL} to put the pragma in the global namespace.  The callback
+routine receives @var{pfile} as its first argument, which can be passed
+on to cpplib's functions if necessary.  You can lex tokens after the
+@var{name} by calling @code{pragma_lex}.  Tokens that are not read by the
+callback will be silently ignored.  The end of the line is indicated by
+a token of type @code{CPP_EOF}.  Macro expansion occurs on the
+arguments of pragmas registered with
+@code{c_register_pragma_with_expansion} but not on the arguments of
+pragmas registered with @code{c_register_pragma}.
+
+Note that the use of @code{pragma_lex} is specific to the C and C++
+compilers.  It will not work in the Java or Fortran compilers, or any
+other language compilers for that matter.  Thus if @code{pragma_lex} is going
+to be called from target-specific code, it must only be done so when
+building the C and C++ compilers.  This can be done by defining the
+variables @code{c_target_objs} and @code{cxx_target_objs} in the
+target entry in the @file{config.gcc} file.  These variables should name
+the target-specific, language-specific object file which contains the
+code that uses @code{pragma_lex}.  Note it will also be necessary to add a
+rule to the makefile fragment pointed to by @code{tmake_file} that shows
+how to build this object file.
+@end deftypefun
+
+@defmac HANDLE_PRAGMA_PACK_WITH_EXPANSION
+Define this macro if macros should be expanded in the
+arguments of @samp{#pragma pack}.
+@end defmac
+
+@defmac TARGET_DEFAULT_PACK_STRUCT
+If your target requires a structure packing default other than 0 (meaning
+the machine default), define this macro to the necessary value (in bytes).
+This must be a value that would also be valid to use with
+@samp{#pragma pack()} (that is, a small power of two).
+@end defmac
+
+@defmac DOLLARS_IN_IDENTIFIERS
+Define this macro to control use of the character @samp{$} in
+identifier names for the C family of languages.  0 means @samp{$} is
+not allowed by default; 1 means it is allowed.  1 is the default;
+there is no need to define this macro in that case.
+@end defmac
+
+@defmac INSN_SETS_ARE_DELAYED (@var{insn})
+Define this macro as a C expression that is nonzero if it is safe for the
+delay slot scheduler to place instructions in the delay slot of @var{insn},
+even if they appear to use a resource set or clobbered in @var{insn}.
+@var{insn} is always a @code{jump_insn} or an @code{insn}; GCC knows that
+every @code{call_insn} has this behavior.  On machines where some @code{insn}
+or @code{jump_insn} is really a function call and hence has this behavior,
+you should define this macro.
+
+You need not define this macro if it would always return zero.
+@end defmac
+
+@defmac INSN_REFERENCES_ARE_DELAYED (@var{insn})
+Define this macro as a C expression that is nonzero if it is safe for the
+delay slot scheduler to place instructions in the delay slot of @var{insn},
+even if they appear to set or clobber a resource referenced in @var{insn}.
+@var{insn} is always a @code{jump_insn} or an @code{insn}.  On machines where
+some @code{insn} or @code{jump_insn} is really a function call and its operands
+are registers whose use is actually in the subroutine it calls, you should
+define this macro.  Doing so allows the delay slot scheduler to move
+instructions which copy arguments into the argument registers into the delay
+slot of @var{insn}.
+
+You need not define this macro if it would always return zero.
+@end defmac
+
+@defmac MULTIPLE_SYMBOL_SPACES
+Define this macro as a C expression that is nonzero if, in some cases,
+global symbols from one translation unit may not be bound to undefined
+symbols in another translation unit without user intervention.  For
+instance, under Microsoft Windows symbols must be explicitly imported
+from shared libraries (DLLs).
+
+You need not define this macro if it would always evaluate to zero.
+@end defmac
+
+@hook TARGET_MD_ASM_ADJUST
+
+@defmac MATH_LIBRARY
+Define this macro as a C string constant for the linker argument to link
+in the system math library, minus the initial @samp{"-l"}, or
+@samp{""} if the target does not have a
+separate math library.
+
+You need only define this macro if the default of @samp{"m"} is wrong.
+@end defmac
+
+@defmac LIBRARY_PATH_ENV
+Define this macro as a C string constant for the environment variable that
+specifies where the linker should look for libraries.
+
+You need only define this macro if the default of @samp{"LIBRARY_PATH"}
+is wrong.
+@end defmac
+
+@defmac TARGET_POSIX_IO
+Define this macro if the target supports the following POSIX@ file
+functions, access, mkdir and  file locking with fcntl / F_SETLKW@.
+Defining @code{TARGET_POSIX_IO} will enable the test coverage code
+to use file locking when exiting a program, which avoids race conditions
+if the program has forked. It will also create directories at run-time
+for cross-profiling.
+@end defmac
+
+@defmac MAX_CONDITIONAL_EXECUTE
+
+A C expression for the maximum number of instructions to execute via
+conditional execution instructions instead of a branch.  A value of
+@code{BRANCH_COST}+1 is the default if the machine does not use cc0, and
+1 if it does use cc0.
+@end defmac
+
+@defmac IFCVT_MODIFY_TESTS (@var{ce_info}, @var{true_expr}, @var{false_expr})
+Used if the target needs to perform machine-dependent modifications on the
+conditionals used for turning basic blocks into conditionally executed code.
+@var{ce_info} points to a data structure, @code{struct ce_if_block}, which
+contains information about the currently processed blocks.  @var{true_expr}
+and @var{false_expr} are the tests that are used for converting the
+then-block and the else-block, respectively.  Set either @var{true_expr} or
+@var{false_expr} to a null pointer if the tests cannot be converted.
+@end defmac
+
+@defmac IFCVT_MODIFY_MULTIPLE_TESTS (@var{ce_info}, @var{bb}, @var{true_expr}, @var{false_expr})
+Like @code{IFCVT_MODIFY_TESTS}, but used when converting more complicated
+if-statements into conditions combined by @code{and} and @code{or} operations.
+@var{bb} contains the basic block that contains the test that is currently
+being processed and about to be turned into a condition.
+@end defmac
+
+@defmac IFCVT_MODIFY_INSN (@var{ce_info}, @var{pattern}, @var{insn})
+A C expression to modify the @var{PATTERN} of an @var{INSN} that is to
+be converted to conditional execution format.  @var{ce_info} points to
+a data structure, @code{struct ce_if_block}, which contains information
+about the currently processed blocks.
+@end defmac
+
+@defmac IFCVT_MODIFY_FINAL (@var{ce_info})
+A C expression to perform any final machine dependent modifications in
+converting code to conditional execution.  The involved basic blocks
+can be found in the @code{struct ce_if_block} structure that is pointed
+to by @var{ce_info}.
+@end defmac
+
+@defmac IFCVT_MODIFY_CANCEL (@var{ce_info})
+A C expression to cancel any machine dependent modifications in
+converting code to conditional execution.  The involved basic blocks
+can be found in the @code{struct ce_if_block} structure that is pointed
+to by @var{ce_info}.
+@end defmac
+
+@defmac IFCVT_MACHDEP_INIT (@var{ce_info})
+A C expression to initialize any machine specific data for if-conversion
+of the if-block in the @code{struct ce_if_block} structure that is pointed
+to by @var{ce_info}.
+@end defmac
+
+@hook TARGET_MACHINE_DEPENDENT_REORG
+
+@hook TARGET_INIT_BUILTINS
+
+@hook TARGET_BUILTIN_DECL
+
+@hook TARGET_EXPAND_BUILTIN
+
+@hook TARGET_RESOLVE_OVERLOADED_BUILTIN
+
+@hook TARGET_CHECK_BUILTIN_CALL
+
+@hook TARGET_FOLD_BUILTIN
+
+@hook TARGET_GIMPLE_FOLD_BUILTIN
+
+@hook TARGET_COMPARE_VERSION_PRIORITY
+
+@hook TARGET_GET_FUNCTION_VERSIONS_DISPATCHER
+
+@hook TARGET_GENERATE_VERSION_DISPATCHER_BODY
+
+@hook TARGET_PREDICT_DOLOOP_P
+
+@hook TARGET_HAVE_COUNT_REG_DECR_P
+
+@hook TARGET_DOLOOP_COST_FOR_GENERIC
+
+@hook TARGET_DOLOOP_COST_FOR_ADDRESS
+
+@hook TARGET_CAN_USE_DOLOOP_P
+
+@hook TARGET_INVALID_WITHIN_DOLOOP
+
+@hook TARGET_LEGITIMATE_COMBINED_INSN
+
+@hook TARGET_CAN_FOLLOW_JUMP
+
+@hook TARGET_COMMUTATIVE_P
+
+@hook TARGET_ALLOCATE_INITIAL_VALUE
+
+@hook TARGET_UNSPEC_MAY_TRAP_P
+
+@hook TARGET_SET_CURRENT_FUNCTION
+
+@defmac TARGET_OBJECT_SUFFIX
+Define this macro to be a C string representing the suffix for object
+files on your target machine.  If you do not define this macro, GCC will
+use @samp{.o} as the suffix for object files.
+@end defmac
+
+@defmac TARGET_EXECUTABLE_SUFFIX
+Define this macro to be a C string representing the suffix to be
+automatically added to executable files on your target machine.  If you
+do not define this macro, GCC will use the null string as the suffix for
+executable files.
+@end defmac
+
+@defmac COLLECT_EXPORT_LIST
+If defined, @code{collect2} will scan the individual object files
+specified on its command line and create an export list for the linker.
+Define this macro for systems like AIX, where the linker discards
+object files that are not referenced from @code{main} and uses export
+lists.
+@end defmac
+
+@hook TARGET_CANNOT_MODIFY_JUMPS_P
+
+@hook TARGET_HAVE_CONDITIONAL_EXECUTION
+
+@hook TARGET_GEN_CCMP_FIRST
+
+@hook TARGET_GEN_CCMP_NEXT
+
+@hook TARGET_LOOP_UNROLL_ADJUST
+
+@defmac POWI_MAX_MULTS
+If defined, this macro is interpreted as a signed integer C expression
+that specifies the maximum number of floating point multiplications
+that should be emitted when expanding exponentiation by an integer
+constant inline.  When this value is defined, exponentiation requiring
+more than this number of multiplications is implemented by calling the
+system library's @code{pow}, @code{powf} or @code{powl} routines.
+The default value places no upper bound on the multiplication count.
+@end defmac
+
+@deftypefn Macro void TARGET_EXTRA_INCLUDES (const char *@var{sysroot}, const char *@var{iprefix}, int @var{stdinc})
+This target hook should register any extra include files for the
+target.  The parameter @var{stdinc} indicates if normal include files
+are present.  The parameter @var{sysroot} is the system root directory.
+The parameter @var{iprefix} is the prefix for the gcc directory.
+@end deftypefn
+
+@deftypefn Macro void TARGET_EXTRA_PRE_INCLUDES (const char *@var{sysroot}, const char *@var{iprefix}, int @var{stdinc})
+This target hook should register any extra include files for the
+target before any standard headers.  The parameter @var{stdinc}
+indicates if normal include files are present.  The parameter
+@var{sysroot} is the system root directory.  The parameter
+@var{iprefix} is the prefix for the gcc directory.
+@end deftypefn
+
+@deftypefn Macro void TARGET_OPTF (char *@var{path})
+This target hook should register special include paths for the target.
+The parameter @var{path} is the include to register.  On Darwin
+systems, this is used for Framework includes, which have semantics
+that are different from @option{-I}.
+@end deftypefn
+
+@defmac bool TARGET_USE_LOCAL_THUNK_ALIAS_P (tree @var{fndecl})
+This target macro returns @code{true} if it is safe to use a local alias
+for a virtual function @var{fndecl} when constructing thunks,
+@code{false} otherwise.  By default, the macro returns @code{true} for all
+functions, if a target supports aliases (i.e.@: defines
+@code{ASM_OUTPUT_DEF}), @code{false} otherwise,
+@end defmac
+
+@defmac TARGET_FORMAT_TYPES
+If defined, this macro is the name of a global variable containing
+target-specific format checking information for the @option{-Wformat}
+option.  The default is to have no target-specific format checks.
+@end defmac
+
+@defmac TARGET_N_FORMAT_TYPES
+If defined, this macro is the number of entries in
+@code{TARGET_FORMAT_TYPES}.
+@end defmac
+
+@defmac TARGET_OVERRIDES_FORMAT_ATTRIBUTES
+If defined, this macro is the name of a global variable containing
+target-specific format overrides for the @option{-Wformat} option. The
+default is to have no target-specific format overrides. If defined,
+@code{TARGET_FORMAT_TYPES} must be defined, too.
+@end defmac
+
+@defmac TARGET_OVERRIDES_FORMAT_ATTRIBUTES_COUNT
+If defined, this macro specifies the number of entries in
+@code{TARGET_OVERRIDES_FORMAT_ATTRIBUTES}.
+@end defmac
+
+@defmac TARGET_OVERRIDES_FORMAT_INIT
+If defined, this macro specifies the optional initialization
+routine for target specific customizations of the system printf
+and scanf formatter settings.
+@end defmac
+
+@hook TARGET_INVALID_ARG_FOR_UNPROTOTYPED_FN
+
+@hook TARGET_INVALID_CONVERSION
+
+@hook TARGET_INVALID_UNARY_OP
+
+@hook TARGET_INVALID_BINARY_OP
+
+@hook TARGET_PROMOTED_TYPE
+
+@hook TARGET_CONVERT_TO_TYPE
+
+@hook TARGET_VERIFY_TYPE_CONTEXT
+
+@defmac OBJC_JBLEN
+This macro determines the size of the objective C jump buffer for the
+NeXT runtime. By default, OBJC_JBLEN is defined to an innocuous value.
+@end defmac
+
+@defmac LIBGCC2_UNWIND_ATTRIBUTE
+Define this macro if any target-specific attributes need to be attached
+to the functions in @file{libgcc} that provide low-level support for
+call stack unwinding.  It is used in declarations in @file{unwind-generic.h}
+and the associated definitions of those functions.
+@end defmac
+
+@hook TARGET_UPDATE_STACK_BOUNDARY
+
+@hook TARGET_GET_DRAP_RTX
+
+@hook TARGET_ZERO_CALL_USED_REGS
+
+@hook TARGET_ALLOCATE_STACK_SLOTS_FOR_ARGS
+
+@hook TARGET_CONST_ANCHOR
+
+@hook TARGET_ASAN_SHADOW_OFFSET
+
+@hook TARGET_MEMMODEL_CHECK
+
+@hook TARGET_ATOMIC_TEST_AND_SET_TRUEVAL
+
+@hook TARGET_HAS_IFUNC_P
+
+@hook TARGET_ATOMIC_ALIGN_FOR_MODE
+
+@hook TARGET_ATOMIC_ASSIGN_EXPAND_FENV
+
+@hook TARGET_RECORD_OFFLOAD_SYMBOL
+
+@hook TARGET_OFFLOAD_OPTIONS
+
+@defmac TARGET_SUPPORTS_WIDE_INT
+
+On older ports, large integers are stored in @code{CONST_DOUBLE} rtl
+objects.  Newer ports define @code{TARGET_SUPPORTS_WIDE_INT} to be nonzero
+to indicate that large integers are stored in
+@code{CONST_WIDE_INT} rtl objects.  The @code{CONST_WIDE_INT} allows
+very large integer constants to be represented.  @code{CONST_DOUBLE}
+is limited to twice the size of the host's @code{HOST_WIDE_INT}
+representation.
+
+Converting a port mostly requires looking for the places where
+@code{CONST_DOUBLE}s are used with @code{VOIDmode} and replacing that
+code with code that accesses @code{CONST_WIDE_INT}s.  @samp{"grep -i
+const_double"} at the port level gets you to 95% of the changes that
+need to be made.  There are a few places that require a deeper look.
+
+@itemize @bullet
+@item
+There is no equivalent to @code{hval} and @code{lval} for
+@code{CONST_WIDE_INT}s.  This would be difficult to express in the md
+language since there are a variable number of elements.
+
+Most ports only check that @code{hval} is either 0 or -1 to see if the
+value is small.  As mentioned above, this will no longer be necessary
+since small constants are always @code{CONST_INT}.  Of course there
+are still a few exceptions, the alpha's constraint used by the zap
+instruction certainly requires careful examination by C code.
+However, all the current code does is pass the hval and lval to C
+code, so evolving the c code to look at the @code{CONST_WIDE_INT} is
+not really a large change.
+
+@item
+Because there is no standard template that ports use to materialize
+constants, there is likely to be some futzing that is unique to each
+port in this code.
+
+@item
+The rtx costs may have to be adjusted to properly account for larger
+constants that are represented as @code{CONST_WIDE_INT}.
+@end itemize
+
+All and all it does not take long to convert ports that the
+maintainer is familiar with.
+
+@end defmac
+
+@hook TARGET_HAVE_SPECULATION_SAFE_VALUE
+
+@hook TARGET_SPECULATION_SAFE_VALUE
+
+@hook TARGET_RUN_TARGET_SELFTESTS
+
+@hook TARGET_MEMTAG_CAN_TAG_ADDRESSES
+
+@hook TARGET_MEMTAG_TAG_SIZE
+
+@hook TARGET_MEMTAG_GRANULE_SIZE
+
+@hook TARGET_MEMTAG_INSERT_RANDOM_TAG
+
+@hook TARGET_MEMTAG_ADD_TAG
+
+@hook TARGET_MEMTAG_SET_TAG
+
+@hook TARGET_MEMTAG_EXTRACT_TAG
+
+@hook TARGET_MEMTAG_UNTAGGED_POINTER
diff -uNr gcc-11.2.0.orig/gcc/doc/tm.texi.orig gcc-11.2.0/gcc/doc/tm.texi.orig
--- gcc-11.2.0.orig/gcc/doc/tm.texi.orig	1969-12-31 18:00:00.000000000 -0600
+++ gcc-11.2.0/gcc/doc/tm.texi.orig	2021-07-28 01:55:07.252286856 -0500
@@ -0,0 +1,12335 @@
+@c Copyright (C) 1988-2021 Free Software Foundation, Inc.
+@c This is part of the GCC manual.
+@c For copying conditions, see the file gcc.texi.
+
+@node Target Macros
+@chapter Target Description Macros and Functions
+@cindex machine description macros
+@cindex target description macros
+@cindex macros, target description
+@cindex @file{tm.h} macros
+
+In addition to the file @file{@var{machine}.md}, a machine description
+includes a C header file conventionally given the name
+@file{@var{machine}.h} and a C source file named @file{@var{machine}.c}.
+The header file defines numerous macros that convey the information
+about the target machine that does not fit into the scheme of the
+@file{.md} file.  The file @file{tm.h} should be a link to
+@file{@var{machine}.h}.  The header file @file{config.h} includes
+@file{tm.h} and most compiler source files include @file{config.h}.  The
+source file defines a variable @code{targetm}, which is a structure
+containing pointers to functions and data relating to the target
+machine.  @file{@var{machine}.c} should also contain their definitions,
+if they are not defined elsewhere in GCC, and other functions called
+through the macros defined in the @file{.h} file.
+
+@menu
+* Target Structure::    The @code{targetm} variable.
+* Driver::              Controlling how the driver runs the compilation passes.
+* Run-time Target::     Defining @samp{-m} options like @option{-m68000} and @option{-m68020}.
+* Per-Function Data::   Defining data structures for per-function information.
+* Storage Layout::      Defining sizes and alignments of data.
+* Type Layout::         Defining sizes and properties of basic user data types.
+* Registers::           Naming and describing the hardware registers.
+* Register Classes::    Defining the classes of hardware registers.
+* Stack and Calling::   Defining which way the stack grows and by how much.
+* Varargs::             Defining the varargs macros.
+* Trampolines::         Code set up at run time to enter a nested function.
+* Library Calls::       Controlling how library routines are implicitly called.
+* Addressing Modes::    Defining addressing modes valid for memory operands.
+* Anchored Addresses::  Defining how @option{-fsection-anchors} should work.
+* Condition Code::      Defining how insns update the condition code.
+* Costs::               Defining relative costs of different operations.
+* Scheduling::          Adjusting the behavior of the instruction scheduler.
+* Sections::            Dividing storage into text, data, and other sections.
+* PIC::                 Macros for position independent code.
+* Assembler Format::    Defining how to write insns and pseudo-ops to output.
+* Debugging Info::      Defining the format of debugging output.
+* Floating Point::      Handling floating point for cross-compilers.
+* Mode Switching::      Insertion of mode-switching instructions.
+* Target Attributes::   Defining target-specific uses of @code{__attribute__}.
+* Emulated TLS::        Emulated TLS support.
+* MIPS Coprocessors::   MIPS coprocessor support and how to customize it.
+* PCH Target::          Validity checking for precompiled headers.
+* C++ ABI::             Controlling C++ ABI changes.
+* D Language and ABI::  Controlling D ABI changes.
+* Named Address Spaces:: Adding support for named address spaces
+* Misc::                Everything else.
+@end menu
+
+@node Target Structure
+@section The Global @code{targetm} Variable
+@cindex target hooks
+@cindex target functions
+
+@deftypevar {struct gcc_target} targetm
+The target @file{.c} file must define the global @code{targetm} variable
+which contains pointers to functions and data relating to the target
+machine.  The variable is declared in @file{target.h};
+@file{target-def.h} defines the macro @code{TARGET_INITIALIZER} which is
+used to initialize the variable, and macros for the default initializers
+for elements of the structure.  The @file{.c} file should override those
+macros for which the default definition is inappropriate.  For example:
+@smallexample
+#include "target.h"
+#include "target-def.h"
+
+/* @r{Initialize the GCC target structure.}  */
+
+#undef TARGET_COMP_TYPE_ATTRIBUTES
+#define TARGET_COMP_TYPE_ATTRIBUTES @var{machine}_comp_type_attributes
+
+struct gcc_target targetm = TARGET_INITIALIZER;
+@end smallexample
+@end deftypevar
+
+Where a macro should be defined in the @file{.c} file in this manner to
+form part of the @code{targetm} structure, it is documented below as a
+``Target Hook'' with a prototype.  Many macros will change in future
+from being defined in the @file{.h} file to being part of the
+@code{targetm} structure.
+
+Similarly, there is a @code{targetcm} variable for hooks that are
+specific to front ends for C-family languages, documented as ``C
+Target Hook''.  This is declared in @file{c-family/c-target.h}, the
+initializer @code{TARGETCM_INITIALIZER} in
+@file{c-family/c-target-def.h}.  If targets initialize @code{targetcm}
+themselves, they should set @code{target_has_targetcm=yes} in
+@file{config.gcc}; otherwise a default definition is used.
+
+Similarly, there is a @code{targetm_common} variable for hooks that
+are shared between the compiler driver and the compilers proper,
+documented as ``Common Target Hook''.  This is declared in
+@file{common/common-target.h}, the initializer
+@code{TARGETM_COMMON_INITIALIZER} in
+@file{common/common-target-def.h}.  If targets initialize
+@code{targetm_common} themselves, they should set
+@code{target_has_targetm_common=yes} in @file{config.gcc}; otherwise a
+default definition is used.
+
+Similarly, there is a @code{targetdm} variable for hooks that are
+specific to the D language front end, documented as ``D Target Hook''.
+This is declared in @file{d/d-target.h}, the initializer
+@code{TARGETDM_INITIALIZER} in @file{d/d-target-def.h}.  If targets
+initialize @code{targetdm} themselves, they should set
+@code{target_has_targetdm=yes} in @file{config.gcc}; otherwise a default
+definition is used.
+
+@node Driver
+@section Controlling the Compilation Driver, @file{gcc}
+@cindex driver
+@cindex controlling the compilation driver
+
+@c prevent bad page break with this line
+You can control the compilation driver.
+
+@defmac DRIVER_SELF_SPECS
+A list of specs for the driver itself.  It should be a suitable
+initializer for an array of strings, with no surrounding braces.
+
+The driver applies these specs to its own command line between loading
+default @file{specs} files (but not command-line specified ones) and
+choosing the multilib directory or running any subcommands.  It
+applies them in the order given, so each spec can depend on the
+options added by earlier ones.  It is also possible to remove options
+using @samp{%<@var{option}} in the usual way.
+
+This macro can be useful when a port has several interdependent target
+options.  It provides a way of standardizing the command line so
+that the other specs are easier to write.
+
+Do not define this macro if it does not need to do anything.
+@end defmac
+
+@defmac OPTION_DEFAULT_SPECS
+A list of specs used to support configure-time default options (i.e.@:
+@option{--with} options) in the driver.  It should be a suitable initializer
+for an array of structures, each containing two strings, without the
+outermost pair of surrounding braces.
+
+The first item in the pair is the name of the default.  This must match
+the code in @file{config.gcc} for the target.  The second item is a spec
+to apply if a default with this name was specified.  The string
+@samp{%(VALUE)} in the spec will be replaced by the value of the default
+everywhere it occurs.
+
+The driver will apply these specs to its own command line between loading
+default @file{specs} files and processing @code{DRIVER_SELF_SPECS}, using
+the same mechanism as @code{DRIVER_SELF_SPECS}.
+
+Do not define this macro if it does not need to do anything.
+@end defmac
+
+@defmac CPP_SPEC
+A C string constant that tells the GCC driver program options to
+pass to CPP@.  It can also specify how to translate options you
+give to GCC into options for GCC to pass to the CPP@.
+
+Do not define this macro if it does not need to do anything.
+@end defmac
+
+@defmac CPLUSPLUS_CPP_SPEC
+This macro is just like @code{CPP_SPEC}, but is used for C++, rather
+than C@.  If you do not define this macro, then the value of
+@code{CPP_SPEC} (if any) will be used instead.
+@end defmac
+
+@defmac CC1_SPEC
+A C string constant that tells the GCC driver program options to
+pass to @code{cc1}, @code{cc1plus}, @code{f771}, and the other language
+front ends.
+It can also specify how to translate options you give to GCC into options
+for GCC to pass to front ends.
+
+Do not define this macro if it does not need to do anything.
+@end defmac
+
+@defmac CC1PLUS_SPEC
+A C string constant that tells the GCC driver program options to
+pass to @code{cc1plus}.  It can also specify how to translate options you
+give to GCC into options for GCC to pass to the @code{cc1plus}.
+
+Do not define this macro if it does not need to do anything.
+Note that everything defined in CC1_SPEC is already passed to
+@code{cc1plus} so there is no need to duplicate the contents of
+CC1_SPEC in CC1PLUS_SPEC@.
+@end defmac
+
+@defmac ASM_SPEC
+A C string constant that tells the GCC driver program options to
+pass to the assembler.  It can also specify how to translate options
+you give to GCC into options for GCC to pass to the assembler.
+See the file @file{sun3.h} for an example of this.
+
+Do not define this macro if it does not need to do anything.
+@end defmac
+
+@defmac ASM_FINAL_SPEC
+A C string constant that tells the GCC driver program how to
+run any programs which cleanup after the normal assembler.
+Normally, this is not needed.  See the file @file{mips.h} for
+an example of this.
+
+Do not define this macro if it does not need to do anything.
+@end defmac
+
+@defmac AS_NEEDS_DASH_FOR_PIPED_INPUT
+Define this macro, with no value, if the driver should give the assembler
+an argument consisting of a single dash, @option{-}, to instruct it to
+read from its standard input (which will be a pipe connected to the
+output of the compiler proper).  This argument is given after any
+@option{-o} option specifying the name of the output file.
+
+If you do not define this macro, the assembler is assumed to read its
+standard input if given no non-option arguments.  If your assembler
+cannot read standard input at all, use a @samp{%@{pipe:%e@}} construct;
+see @file{mips.h} for instance.
+@end defmac
+
+@defmac LINK_SPEC
+A C string constant that tells the GCC driver program options to
+pass to the linker.  It can also specify how to translate options you
+give to GCC into options for GCC to pass to the linker.
+
+Do not define this macro if it does not need to do anything.
+@end defmac
+
+@defmac LIB_SPEC
+Another C string constant used much like @code{LINK_SPEC}.  The difference
+between the two is that @code{LIB_SPEC} is used at the end of the
+command given to the linker.
+
+If this macro is not defined, a default is provided that
+loads the standard C library from the usual place.  See @file{gcc.c}.
+@end defmac
+
+@defmac LIBGCC_SPEC
+Another C string constant that tells the GCC driver program
+how and when to place a reference to @file{libgcc.a} into the
+linker command line.  This constant is placed both before and after
+the value of @code{LIB_SPEC}.
+
+If this macro is not defined, the GCC driver provides a default that
+passes the string @option{-lgcc} to the linker.
+@end defmac
+
+@defmac REAL_LIBGCC_SPEC
+By default, if @code{ENABLE_SHARED_LIBGCC} is defined, the
+@code{LIBGCC_SPEC} is not directly used by the driver program but is
+instead modified to refer to different versions of @file{libgcc.a}
+depending on the values of the command line flags @option{-static},
+@option{-shared}, @option{-static-libgcc}, and @option{-shared-libgcc}.  On
+targets where these modifications are inappropriate, define
+@code{REAL_LIBGCC_SPEC} instead.  @code{REAL_LIBGCC_SPEC} tells the
+driver how to place a reference to @file{libgcc} on the link command
+line, but, unlike @code{LIBGCC_SPEC}, it is used unmodified.
+@end defmac
+
+@defmac USE_LD_AS_NEEDED
+A macro that controls the modifications to @code{LIBGCC_SPEC}
+mentioned in @code{REAL_LIBGCC_SPEC}.  If nonzero, a spec will be
+generated that uses @option{--as-needed} or equivalent options and the
+shared @file{libgcc} in place of the
+static exception handler library, when linking without any of
+@code{-static}, @code{-static-libgcc}, or @code{-shared-libgcc}.
+@end defmac
+
+@defmac LINK_EH_SPEC
+If defined, this C string constant is added to @code{LINK_SPEC}.
+When @code{USE_LD_AS_NEEDED} is zero or undefined, it also affects
+the modifications to @code{LIBGCC_SPEC} mentioned in
+@code{REAL_LIBGCC_SPEC}.
+@end defmac
+
+@defmac STARTFILE_SPEC
+Another C string constant used much like @code{LINK_SPEC}.  The
+difference between the two is that @code{STARTFILE_SPEC} is used at
+the very beginning of the command given to the linker.
+
+If this macro is not defined, a default is provided that loads the
+standard C startup file from the usual place.  See @file{gcc.c}.
+@end defmac
+
+@defmac ENDFILE_SPEC
+Another C string constant used much like @code{LINK_SPEC}.  The
+difference between the two is that @code{ENDFILE_SPEC} is used at
+the very end of the command given to the linker.
+
+Do not define this macro if it does not need to do anything.
+@end defmac
+
+@defmac THREAD_MODEL_SPEC
+GCC @code{-v} will print the thread model GCC was configured to use.
+However, this doesn't work on platforms that are multilibbed on thread
+models, such as AIX 4.3.  On such platforms, define
+@code{THREAD_MODEL_SPEC} such that it evaluates to a string without
+blanks that names one of the recognized thread models.  @code{%*}, the
+default value of this macro, will expand to the value of
+@code{thread_file} set in @file{config.gcc}.
+@end defmac
+
+@defmac SYSROOT_SUFFIX_SPEC
+Define this macro to add a suffix to the target sysroot when GCC is
+configured with a sysroot.  This will cause GCC to search for usr/lib,
+et al, within sysroot+suffix.
+@end defmac
+
+@defmac SYSROOT_HEADERS_SUFFIX_SPEC
+Define this macro to add a headers_suffix to the target sysroot when
+GCC is configured with a sysroot.  This will cause GCC to pass the
+updated sysroot+headers_suffix to CPP, causing it to search for
+usr/include, et al, within sysroot+headers_suffix.
+@end defmac
+
+@defmac EXTRA_SPECS
+Define this macro to provide additional specifications to put in the
+@file{specs} file that can be used in various specifications like
+@code{CC1_SPEC}.
+
+The definition should be an initializer for an array of structures,
+containing a string constant, that defines the specification name, and a
+string constant that provides the specification.
+
+Do not define this macro if it does not need to do anything.
+
+@code{EXTRA_SPECS} is useful when an architecture contains several
+related targets, which have various @code{@dots{}_SPECS} which are similar
+to each other, and the maintainer would like one central place to keep
+these definitions.
+
+For example, the PowerPC System V.4 targets use @code{EXTRA_SPECS} to
+define either @code{_CALL_SYSV} when the System V calling sequence is
+used or @code{_CALL_AIX} when the older AIX-based calling sequence is
+used.
+
+The @file{config/rs6000/rs6000.h} target file defines:
+
+@smallexample
+#define EXTRA_SPECS \
+  @{ "cpp_sysv_default", CPP_SYSV_DEFAULT @},
+
+#define CPP_SYS_DEFAULT ""
+@end smallexample
+
+The @file{config/rs6000/sysv.h} target file defines:
+@smallexample
+#undef CPP_SPEC
+#define CPP_SPEC \
+"%@{posix: -D_POSIX_SOURCE @} \
+%@{mcall-sysv: -D_CALL_SYSV @} \
+%@{!mcall-sysv: %(cpp_sysv_default) @} \
+%@{msoft-float: -D_SOFT_FLOAT@} %@{mcpu=403: -D_SOFT_FLOAT@}"
+
+#undef CPP_SYSV_DEFAULT
+#define CPP_SYSV_DEFAULT "-D_CALL_SYSV"
+@end smallexample
+
+while the @file{config/rs6000/eabiaix.h} target file defines
+@code{CPP_SYSV_DEFAULT} as:
+
+@smallexample
+#undef CPP_SYSV_DEFAULT
+#define CPP_SYSV_DEFAULT "-D_CALL_AIX"
+@end smallexample
+@end defmac
+
+@defmac LINK_LIBGCC_SPECIAL_1
+Define this macro if the driver program should find the library
+@file{libgcc.a}.  If you do not define this macro, the driver program will pass
+the argument @option{-lgcc} to tell the linker to do the search.
+@end defmac
+
+@defmac LINK_GCC_C_SEQUENCE_SPEC
+The sequence in which libgcc and libc are specified to the linker.
+By default this is @code{%G %L %G}.
+@end defmac
+
+@defmac POST_LINK_SPEC
+Define this macro to add additional steps to be executed after linker.
+The default value of this macro is empty string.
+@end defmac
+
+@defmac LINK_COMMAND_SPEC
+A C string constant giving the complete command line need to execute the
+linker.  When you do this, you will need to update your port each time a
+change is made to the link command line within @file{gcc.c}.  Therefore,
+define this macro only if you need to completely redefine the command
+line for invoking the linker and there is no other way to accomplish
+the effect you need.  Overriding this macro may be avoidable by overriding
+@code{LINK_GCC_C_SEQUENCE_SPEC} instead.
+@end defmac
+
+@deftypevr {Common Target Hook} bool TARGET_ALWAYS_STRIP_DOTDOT
+True if @file{..} components should always be removed from directory names computed relative to GCC's internal directories, false (default) if such components should be preserved and directory names containing them passed to other tools such as the linker.
+@end deftypevr
+
+@defmac MULTILIB_DEFAULTS
+Define this macro as a C expression for the initializer of an array of
+string to tell the driver program which options are defaults for this
+target and thus do not need to be handled specially when using
+@code{MULTILIB_OPTIONS}.
+
+Do not define this macro if @code{MULTILIB_OPTIONS} is not defined in
+the target makefile fragment or if none of the options listed in
+@code{MULTILIB_OPTIONS} are set by default.
+@xref{Target Fragment}.
+@end defmac
+
+@defmac RELATIVE_PREFIX_NOT_LINKDIR
+Define this macro to tell @command{gcc} that it should only translate
+a @option{-B} prefix into a @option{-L} linker option if the prefix
+indicates an absolute file name.
+@end defmac
+
+@defmac MD_EXEC_PREFIX
+If defined, this macro is an additional prefix to try after
+@code{STANDARD_EXEC_PREFIX}.  @code{MD_EXEC_PREFIX} is not searched
+when the compiler is built as a cross
+compiler.  If you define @code{MD_EXEC_PREFIX}, then be sure to add it
+to the list of directories used to find the assembler in @file{configure.ac}.
+@end defmac
+
+@defmac STANDARD_STARTFILE_PREFIX
+Define this macro as a C string constant if you wish to override the
+standard choice of @code{libdir} as the default prefix to
+try when searching for startup files such as @file{crt0.o}.
+@code{STANDARD_STARTFILE_PREFIX} is not searched when the compiler
+is built as a cross compiler.
+@end defmac
+
+@defmac STANDARD_STARTFILE_PREFIX_1
+Define this macro as a C string constant if you wish to override the
+standard choice of @code{/lib} as a prefix to try after the default prefix
+when searching for startup files such as @file{crt0.o}.
+@code{STANDARD_STARTFILE_PREFIX_1} is not searched when the compiler
+is built as a cross compiler.
+@end defmac
+
+@defmac STANDARD_STARTFILE_PREFIX_2
+Define this macro as a C string constant if you wish to override the
+standard choice of @code{/lib} as yet another prefix to try after the
+default prefix when searching for startup files such as @file{crt0.o}.
+@code{STANDARD_STARTFILE_PREFIX_2} is not searched when the compiler
+is built as a cross compiler.
+@end defmac
+
+@defmac MD_STARTFILE_PREFIX
+If defined, this macro supplies an additional prefix to try after the
+standard prefixes.  @code{MD_EXEC_PREFIX} is not searched when the
+compiler is built as a cross compiler.
+@end defmac
+
+@defmac MD_STARTFILE_PREFIX_1
+If defined, this macro supplies yet another prefix to try after the
+standard prefixes.  It is not searched when the compiler is built as a
+cross compiler.
+@end defmac
+
+@defmac INIT_ENVIRONMENT
+Define this macro as a C string constant if you wish to set environment
+variables for programs called by the driver, such as the assembler and
+loader.  The driver passes the value of this macro to @code{putenv} to
+initialize the necessary environment variables.
+@end defmac
+
+@defmac LOCAL_INCLUDE_DIR
+Define this macro as a C string constant if you wish to override the
+standard choice of @file{/usr/local/include} as the default prefix to
+try when searching for local header files.  @code{LOCAL_INCLUDE_DIR}
+comes before @code{NATIVE_SYSTEM_HEADER_DIR} (set in
+@file{config.gcc}, normally @file{/usr/include}) in the search order.
+
+Cross compilers do not search either @file{/usr/local/include} or its
+replacement.
+@end defmac
+
+@defmac NATIVE_SYSTEM_HEADER_COMPONENT
+The ``component'' corresponding to @code{NATIVE_SYSTEM_HEADER_DIR}.
+See @code{INCLUDE_DEFAULTS}, below, for the description of components.
+If you do not define this macro, no component is used.
+@end defmac
+
+@defmac INCLUDE_DEFAULTS
+Define this macro if you wish to override the entire default search path
+for include files.  For a native compiler, the default search path
+usually consists of @code{GCC_INCLUDE_DIR}, @code{LOCAL_INCLUDE_DIR},
+@code{GPLUSPLUS_INCLUDE_DIR}, and
+@code{NATIVE_SYSTEM_HEADER_DIR}.  In addition, @code{GPLUSPLUS_INCLUDE_DIR}
+and @code{GCC_INCLUDE_DIR} are defined automatically by @file{Makefile},
+and specify private search areas for GCC@.  The directory
+@code{GPLUSPLUS_INCLUDE_DIR} is used only for C++ programs.
+
+The definition should be an initializer for an array of structures.
+Each array element should have four elements: the directory name (a
+string constant), the component name (also a string constant), a flag
+for C++-only directories,
+and a flag showing that the includes in the directory don't need to be
+wrapped in @code{extern @samp{C}} when compiling C++.  Mark the end of
+the array with a null element.
+
+The component name denotes what GNU package the include file is part of,
+if any, in all uppercase letters.  For example, it might be @samp{GCC}
+or @samp{BINUTILS}.  If the package is part of a vendor-supplied
+operating system, code the component name as @samp{0}.
+
+For example, here is the definition used for VAX/VMS:
+
+@smallexample
+#define INCLUDE_DEFAULTS \
+@{                                       \
+  @{ "GNU_GXX_INCLUDE:", "G++", 1, 1@},   \
+  @{ "GNU_CC_INCLUDE:", "GCC", 0, 0@},    \
+  @{ "SYS$SYSROOT:[SYSLIB.]", 0, 0, 0@},  \
+  @{ ".", 0, 0, 0@},                      \
+  @{ 0, 0, 0, 0@}                         \
+@}
+@end smallexample
+@end defmac
+
+Here is the order of prefixes tried for exec files:
+
+@enumerate
+@item
+Any prefixes specified by the user with @option{-B}.
+
+@item
+The environment variable @code{GCC_EXEC_PREFIX} or, if @code{GCC_EXEC_PREFIX}
+is not set and the compiler has not been installed in the configure-time
+@var{prefix}, the location in which the compiler has actually been installed.
+
+@item
+The directories specified by the environment variable @code{COMPILER_PATH}.
+
+@item
+The macro @code{STANDARD_EXEC_PREFIX}, if the compiler has been installed
+in the configured-time @var{prefix}.
+
+@item
+The location @file{/usr/libexec/gcc/}, but only if this is a native compiler.
+
+@item
+The location @file{/usr/lib/gcc/}, but only if this is a native compiler.
+
+@item
+The macro @code{MD_EXEC_PREFIX}, if defined, but only if this is a native
+compiler.
+@end enumerate
+
+Here is the order of prefixes tried for startfiles:
+
+@enumerate
+@item
+Any prefixes specified by the user with @option{-B}.
+
+@item
+The environment variable @code{GCC_EXEC_PREFIX} or its automatically determined
+value based on the installed toolchain location.
+
+@item
+The directories specified by the environment variable @code{LIBRARY_PATH}
+(or port-specific name; native only, cross compilers do not use this).
+
+@item
+The macro @code{STANDARD_EXEC_PREFIX}, but only if the toolchain is installed
+in the configured @var{prefix} or this is a native compiler.
+
+@item
+The location @file{/usr/lib/gcc/}, but only if this is a native compiler.
+
+@item
+The macro @code{MD_EXEC_PREFIX}, if defined, but only if this is a native
+compiler.
+
+@item
+The macro @code{MD_STARTFILE_PREFIX}, if defined, but only if this is a
+native compiler, or we have a target system root.
+
+@item
+The macro @code{MD_STARTFILE_PREFIX_1}, if defined, but only if this is a
+native compiler, or we have a target system root.
+
+@item
+The macro @code{STANDARD_STARTFILE_PREFIX}, with any sysroot modifications.
+If this path is relative it will be prefixed by @code{GCC_EXEC_PREFIX} and
+the machine suffix or @code{STANDARD_EXEC_PREFIX} and the machine suffix.
+
+@item
+The macro @code{STANDARD_STARTFILE_PREFIX_1}, but only if this is a native
+compiler, or we have a target system root. The default for this macro is
+@file{/lib/}.
+
+@item
+The macro @code{STANDARD_STARTFILE_PREFIX_2}, but only if this is a native
+compiler, or we have a target system root. The default for this macro is
+@file{/usr/lib/}.
+@end enumerate
+
+@node Run-time Target
+@section Run-time Target Specification
+@cindex run-time target specification
+@cindex predefined macros
+@cindex target specifications
+
+@c prevent bad page break with this line
+Here are run-time target specifications.
+
+@defmac TARGET_CPU_CPP_BUILTINS ()
+This function-like macro expands to a block of code that defines
+built-in preprocessor macros and assertions for the target CPU, using
+the functions @code{builtin_define}, @code{builtin_define_std} and
+@code{builtin_assert}.  When the front end
+calls this macro it provides a trailing semicolon, and since it has
+finished command line option processing your code can use those
+results freely.
+
+@code{builtin_assert} takes a string in the form you pass to the
+command-line option @option{-A}, such as @code{cpu=mips}, and creates
+the assertion.  @code{builtin_define} takes a string in the form
+accepted by option @option{-D} and unconditionally defines the macro.
+
+@code{builtin_define_std} takes a string representing the name of an
+object-like macro.  If it doesn't lie in the user's namespace,
+@code{builtin_define_std} defines it unconditionally.  Otherwise, it
+defines a version with two leading underscores, and another version
+with two leading and trailing underscores, and defines the original
+only if an ISO standard was not requested on the command line.  For
+example, passing @code{unix} defines @code{__unix}, @code{__unix__}
+and possibly @code{unix}; passing @code{_mips} defines @code{__mips},
+@code{__mips__} and possibly @code{_mips}, and passing @code{_ABI64}
+defines only @code{_ABI64}.
+
+You can also test for the C dialect being compiled.  The variable
+@code{c_language} is set to one of @code{clk_c}, @code{clk_cplusplus}
+or @code{clk_objective_c}.  Note that if we are preprocessing
+assembler, this variable will be @code{clk_c} but the function-like
+macro @code{preprocessing_asm_p()} will return true, so you might want
+to check for that first.  If you need to check for strict ANSI, the
+variable @code{flag_iso} can be used.  The function-like macro
+@code{preprocessing_trad_p()} can be used to check for traditional
+preprocessing.
+@end defmac
+
+@defmac TARGET_OS_CPP_BUILTINS ()
+Similarly to @code{TARGET_CPU_CPP_BUILTINS} but this macro is optional
+and is used for the target operating system instead.
+@end defmac
+
+@defmac TARGET_OBJFMT_CPP_BUILTINS ()
+Similarly to @code{TARGET_CPU_CPP_BUILTINS} but this macro is optional
+and is used for the target object format.  @file{elfos.h} uses this
+macro to define @code{__ELF__}, so you probably do not need to define
+it yourself.
+@end defmac
+
+@deftypevar {extern int} target_flags
+This variable is declared in @file{options.h}, which is included before
+any target-specific headers.
+@end deftypevar
+
+@deftypevr {Common Target Hook} int TARGET_DEFAULT_TARGET_FLAGS
+This variable specifies the initial value of @code{target_flags}.
+Its default setting is 0.
+@end deftypevr
+
+@cindex optional hardware or system features
+@cindex features, optional, in system conventions
+
+@deftypefn {Common Target Hook} bool TARGET_HANDLE_OPTION (struct gcc_options *@var{opts}, struct gcc_options *@var{opts_set}, const struct cl_decoded_option *@var{decoded}, location_t @var{loc})
+This hook is called whenever the user specifies one of the
+target-specific options described by the @file{.opt} definition files
+(@pxref{Options}).  It has the opportunity to do some option-specific
+processing and should return true if the option is valid.  The default
+definition does nothing but return true.
+
+@var{decoded} specifies the option and its arguments.  @var{opts} and
+@var{opts_set} are the @code{gcc_options} structures to be used for
+storing option state, and @var{loc} is the location at which the
+option was passed (@code{UNKNOWN_LOCATION} except for options passed
+via attributes).
+@end deftypefn
+
+@deftypefn {C Target Hook} bool TARGET_HANDLE_C_OPTION (size_t @var{code}, const char *@var{arg}, int @var{value})
+This target hook is called whenever the user specifies one of the
+target-specific C language family options described by the @file{.opt}
+definition files(@pxref{Options}).  It has the opportunity to do some
+option-specific processing and should return true if the option is
+valid.  The arguments are like for @code{TARGET_HANDLE_OPTION}.  The
+default definition does nothing but return false.
+
+In general, you should use @code{TARGET_HANDLE_OPTION} to handle
+options.  However, if processing an option requires routines that are
+only available in the C (and related language) front ends, then you
+should use @code{TARGET_HANDLE_C_OPTION} instead.
+@end deftypefn
+
+@deftypefn {C Target Hook} tree TARGET_OBJC_CONSTRUCT_STRING_OBJECT (tree @var{string})
+Targets may provide a string object type that can be used within and between C, C++ and their respective Objective-C dialects. A string object might, for example, embed encoding and length information. These objects are considered opaque to the compiler and handled as references. An ideal implementation makes the composition of the string object match that of the Objective-C @code{NSString} (@code{NXString} for GNUStep), allowing efficient interworking between C-only and Objective-C code. If a target implements string objects then this hook should return a reference to such an object constructed from the normal `C' string representation provided in @var{string}. At present, the hook is used by Objective-C only, to obtain a common-format string object when the target provides one.
+@end deftypefn
+
+@deftypefn {C Target Hook} void TARGET_OBJC_DECLARE_UNRESOLVED_CLASS_REFERENCE (const char *@var{classname})
+Declare that Objective C class @var{classname} is referenced  by the current TU.
+@end deftypefn
+
+@deftypefn {C Target Hook} void TARGET_OBJC_DECLARE_CLASS_DEFINITION (const char *@var{classname})
+Declare that Objective C class @var{classname} is defined  by the current TU.
+@end deftypefn
+
+@deftypefn {C Target Hook} bool TARGET_STRING_OBJECT_REF_TYPE_P (const_tree @var{stringref})
+If a target implements string objects then this hook should return @code{true} if @var{stringref} is a valid reference to such an object.
+@end deftypefn
+
+@deftypefn {C Target Hook} void TARGET_CHECK_STRING_OBJECT_FORMAT_ARG (tree @var{format_arg}, tree @var{args_list})
+If a target implements string objects then this hook should should  provide a facility to check the function arguments in @var{args_list}  against the format specifiers in @var{format_arg} where the type of  @var{format_arg} is one recognized as a valid string reference type.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_OVERRIDE_OPTIONS_AFTER_CHANGE (void)
+This target function is similar to the hook @code{TARGET_OPTION_OVERRIDE}
+but is called when the optimize level is changed via an attribute or
+pragma or when it is reset at the end of the code affected by the
+attribute or pragma.  It is not called at the beginning of compilation
+when @code{TARGET_OPTION_OVERRIDE} is called so if you want to perform these
+actions then, you should have @code{TARGET_OPTION_OVERRIDE} call
+@code{TARGET_OVERRIDE_OPTIONS_AFTER_CHANGE}.
+@end deftypefn
+
+@defmac C_COMMON_OVERRIDE_OPTIONS
+This is similar to the @code{TARGET_OPTION_OVERRIDE} hook
+but is only used in the C
+language frontends (C, Objective-C, C++, Objective-C++) and so can be
+used to alter option flag variables which only exist in those
+frontends.
+@end defmac
+
+@deftypevr {Common Target Hook} {const struct default_options *} TARGET_OPTION_OPTIMIZATION_TABLE
+Some machines may desire to change what optimizations are performed for
+various optimization levels.   This variable, if defined, describes
+options to enable at particular sets of optimization levels.  These
+options are processed once
+just after the optimization level is determined and before the remainder
+of the command options have been parsed, so may be overridden by other
+options passed explicitly.
+
+This processing is run once at program startup and when the optimization
+options are changed via @code{#pragma GCC optimize} or by using the
+@code{optimize} attribute.
+@end deftypevr
+
+@deftypefn {Common Target Hook} void TARGET_OPTION_INIT_STRUCT (struct gcc_options *@var{opts})
+Set target-dependent initial values of fields in @var{opts}.
+@end deftypefn
+
+@defmac SWITCHABLE_TARGET
+Some targets need to switch between substantially different subtargets
+during compilation.  For example, the MIPS target has one subtarget for
+the traditional MIPS architecture and another for MIPS16.  Source code
+can switch between these two subarchitectures using the @code{mips16}
+and @code{nomips16} attributes.
+
+Such subtargets can differ in things like the set of available
+registers, the set of available instructions, the costs of various
+operations, and so on.  GCC caches a lot of this type of information
+in global variables, and recomputing them for each subtarget takes a
+significant amount of time.  The compiler therefore provides a facility
+for maintaining several versions of the global variables and quickly
+switching between them; see @file{target-globals.h} for details.
+
+Define this macro to 1 if your target needs this facility.  The default
+is 0.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_FLOAT_EXCEPTIONS_ROUNDING_SUPPORTED_P (void)
+Returns true if the target supports IEEE 754 floating-point exceptions and rounding modes, false otherwise.  This is intended to relate to the @code{float} and @code{double} types, but not necessarily @code{long double}. By default, returns true if the @code{adddf3} instruction pattern is available and false otherwise, on the assumption that hardware floating point supports exceptions and rounding modes but software floating point does not.
+@end deftypefn
+
+@node Per-Function Data
+@section Defining data structures for per-function information.
+@cindex per-function data
+@cindex data structures
+
+If the target needs to store information on a per-function basis, GCC
+provides a macro and a couple of variables to allow this.  Note, just
+using statics to store the information is a bad idea, since GCC supports
+nested functions, so you can be halfway through encoding one function
+when another one comes along.
+
+GCC defines a data structure called @code{struct function} which
+contains all of the data specific to an individual function.  This
+structure contains a field called @code{machine} whose type is
+@code{struct machine_function *}, which can be used by targets to point
+to their own specific data.
+
+If a target needs per-function specific data it should define the type
+@code{struct machine_function} and also the macro @code{INIT_EXPANDERS}.
+This macro should be used to initialize the function pointer
+@code{init_machine_status}.  This pointer is explained below.
+
+One typical use of per-function, target specific data is to create an
+RTX to hold the register containing the function's return address.  This
+RTX can then be used to implement the @code{__builtin_return_address}
+function, for level 0.
+
+Note---earlier implementations of GCC used a single data area to hold
+all of the per-function information.  Thus when processing of a nested
+function began the old per-function data had to be pushed onto a
+stack, and when the processing was finished, it had to be popped off the
+stack.  GCC used to provide function pointers called
+@code{save_machine_status} and @code{restore_machine_status} to handle
+the saving and restoring of the target specific information.  Since the
+single data area approach is no longer used, these pointers are no
+longer supported.
+
+@defmac INIT_EXPANDERS
+Macro called to initialize any target specific information.  This macro
+is called once per function, before generation of any RTL has begun.
+The intention of this macro is to allow the initialization of the
+function pointer @code{init_machine_status}.
+@end defmac
+
+@deftypevar {void (*)(struct function *)} init_machine_status
+If this function pointer is non-@code{NULL} it will be called once per
+function, before function compilation starts, in order to allow the
+target to perform any target specific initialization of the
+@code{struct function} structure.  It is intended that this would be
+used to initialize the @code{machine} of that structure.
+
+@code{struct machine_function} structures are expected to be freed by GC@.
+Generally, any memory that they reference must be allocated by using
+GC allocation, including the structure itself.
+@end deftypevar
+
+@node Storage Layout
+@section Storage Layout
+@cindex storage layout
+
+Note that the definitions of the macros in this table which are sizes or
+alignments measured in bits do not need to be constant.  They can be C
+expressions that refer to static variables, such as the @code{target_flags}.
+@xref{Run-time Target}.
+
+@defmac BITS_BIG_ENDIAN
+Define this macro to have the value 1 if the most significant bit in a
+byte has the lowest number; otherwise define it to have the value zero.
+This means that bit-field instructions count from the most significant
+bit.  If the machine has no bit-field instructions, then this must still
+be defined, but it doesn't matter which value it is defined to.  This
+macro need not be a constant.
+
+This macro does not affect the way structure fields are packed into
+bytes or words; that is controlled by @code{BYTES_BIG_ENDIAN}.
+@end defmac
+
+@defmac BYTES_BIG_ENDIAN
+Define this macro to have the value 1 if the most significant byte in a
+word has the lowest number.  This macro need not be a constant.
+@end defmac
+
+@defmac WORDS_BIG_ENDIAN
+Define this macro to have the value 1 if, in a multiword object, the
+most significant word has the lowest number.  This applies to both
+memory locations and registers; see @code{REG_WORDS_BIG_ENDIAN} if the
+order of words in memory is not the same as the order in registers.  This
+macro need not be a constant.
+@end defmac
+
+@defmac REG_WORDS_BIG_ENDIAN
+On some machines, the order of words in a multiword object differs between
+registers in memory.  In such a situation, define this macro to describe
+the order of words in a register.  The macro @code{WORDS_BIG_ENDIAN} controls
+the order of words in memory.
+@end defmac
+
+@defmac FLOAT_WORDS_BIG_ENDIAN
+Define this macro to have the value 1 if @code{DFmode}, @code{XFmode} or
+@code{TFmode} floating point numbers are stored in memory with the word
+containing the sign bit at the lowest address; otherwise define it to
+have the value 0.  This macro need not be a constant.
+
+You need not define this macro if the ordering is the same as for
+multi-word integers.
+@end defmac
+
+@defmac BITS_PER_WORD
+Number of bits in a word.  If you do not define this macro, the default
+is @code{BITS_PER_UNIT * UNITS_PER_WORD}.
+@end defmac
+
+@defmac MAX_BITS_PER_WORD
+Maximum number of bits in a word.  If this is undefined, the default is
+@code{BITS_PER_WORD}.  Otherwise, it is the constant value that is the
+largest value that @code{BITS_PER_WORD} can have at run-time.
+@end defmac
+
+@defmac UNITS_PER_WORD
+Number of storage units in a word; normally the size of a general-purpose
+register, a power of two from 1 or 8.
+@end defmac
+
+@defmac MIN_UNITS_PER_WORD
+Minimum number of units in a word.  If this is undefined, the default is
+@code{UNITS_PER_WORD}.  Otherwise, it is the constant value that is the
+smallest value that @code{UNITS_PER_WORD} can have at run-time.
+@end defmac
+
+@defmac POINTER_SIZE
+Width of a pointer, in bits.  You must specify a value no wider than the
+width of @code{Pmode}.  If it is not equal to the width of @code{Pmode},
+you must define @code{POINTERS_EXTEND_UNSIGNED}.  If you do not specify
+a value the default is @code{BITS_PER_WORD}.
+@end defmac
+
+@defmac POINTERS_EXTEND_UNSIGNED
+A C expression that determines how pointers should be extended from
+@code{ptr_mode} to either @code{Pmode} or @code{word_mode}.  It is
+greater than zero if pointers should be zero-extended, zero if they
+should be sign-extended, and negative if some other sort of conversion
+is needed.  In the last case, the extension is done by the target's
+@code{ptr_extend} instruction.
+
+You need not define this macro if the @code{ptr_mode}, @code{Pmode}
+and @code{word_mode} are all the same width.
+@end defmac
+
+@defmac PROMOTE_MODE (@var{m}, @var{unsignedp}, @var{type})
+A macro to update @var{m} and @var{unsignedp} when an object whose type
+is @var{type} and which has the specified mode and signedness is to be
+stored in a register.  This macro is only called when @var{type} is a
+scalar type.
+
+On most RISC machines, which only have operations that operate on a full
+register, define this macro to set @var{m} to @code{word_mode} if
+@var{m} is an integer mode narrower than @code{BITS_PER_WORD}.  In most
+cases, only integer modes should be widened because wider-precision
+floating-point operations are usually more expensive than their narrower
+counterparts.
+
+For most machines, the macro definition does not change @var{unsignedp}.
+However, some machines, have instructions that preferentially handle
+either signed or unsigned quantities of certain modes.  For example, on
+the DEC Alpha, 32-bit loads from memory and 32-bit add instructions
+sign-extend the result to 64 bits.  On such machines, set
+@var{unsignedp} according to which kind of extension is more efficient.
+
+Do not define this macro if it would never modify @var{m}.
+@end defmac
+
+@deftypefn {Target Hook} {enum flt_eval_method} TARGET_C_EXCESS_PRECISION (enum excess_precision_type @var{type})
+Return a value, with the same meaning as the C99 macro @code{FLT_EVAL_METHOD} that describes which excess precision should be applied.  @var{type} is either @code{EXCESS_PRECISION_TYPE_IMPLICIT}, @code{EXCESS_PRECISION_TYPE_FAST}, or @code{EXCESS_PRECISION_TYPE_STANDARD}.  For @code{EXCESS_PRECISION_TYPE_IMPLICIT}, the target should return which precision and range operations will be implictly evaluated in regardless of the excess precision explicitly added.  For @code{EXCESS_PRECISION_TYPE_STANDARD} and @code{EXCESS_PRECISION_TYPE_FAST}, the target should return the explicit excess precision that should be added depending on the value set for @option{-fexcess-precision=@r{[}standard@r{|}fast@r{]}}. Note that unpredictable explicit excess precision does not make sense, so a target should never return @code{FLT_EVAL_METHOD_UNPREDICTABLE} when @var{type} is @code{EXCESS_PRECISION_TYPE_STANDARD} or @code{EXCESS_PRECISION_TYPE_FAST}.
+@end deftypefn
+
+@deftypefn {Target Hook} machine_mode TARGET_PROMOTE_FUNCTION_MODE (const_tree @var{type}, machine_mode @var{mode}, int *@var{punsignedp}, const_tree @var{funtype}, int @var{for_return})
+Like @code{PROMOTE_MODE}, but it is applied to outgoing function arguments or
+function return values.  The target hook should return the new mode
+and possibly change @code{*@var{punsignedp}} if the promotion should
+change signedness.  This function is called only for scalar @emph{or
+pointer} types.
+
+@var{for_return} allows to distinguish the promotion of arguments and
+return values.  If it is @code{1}, a return value is being promoted and
+@code{TARGET_FUNCTION_VALUE} must perform the same promotions done here.
+If it is @code{2}, the returned mode should be that of the register in
+which an incoming parameter is copied, or the outgoing result is computed;
+then the hook should return the same mode as @code{promote_mode}, though
+the signedness may be different.
+
+@var{type} can be NULL when promoting function arguments of libcalls.
+
+The default is to not promote arguments and return values.  You can
+also define the hook to @code{default_promote_function_mode_always_promote}
+if you would like to apply the same rules given by @code{PROMOTE_MODE}.
+@end deftypefn
+
+@defmac PARM_BOUNDARY
+Normal alignment required for function parameters on the stack, in
+bits.  All stack parameters receive at least this much alignment
+regardless of data type.  On most machines, this is the same as the
+size of an integer.
+@end defmac
+
+@defmac STACK_BOUNDARY
+Define this macro to the minimum alignment enforced by hardware for the
+stack pointer on this machine.  The definition is a C expression for the
+desired alignment (measured in bits).  This value is used as a default
+if @code{PREFERRED_STACK_BOUNDARY} is not defined.  On most machines,
+this should be the same as @code{PARM_BOUNDARY}.
+@end defmac
+
+@defmac PREFERRED_STACK_BOUNDARY
+Define this macro if you wish to preserve a certain alignment for the
+stack pointer, greater than what the hardware enforces.  The definition
+is a C expression for the desired alignment (measured in bits).  This
+macro must evaluate to a value equal to or larger than
+@code{STACK_BOUNDARY}.
+@end defmac
+
+@defmac INCOMING_STACK_BOUNDARY
+Define this macro if the incoming stack boundary may be different
+from @code{PREFERRED_STACK_BOUNDARY}.  This macro must evaluate
+to a value equal to or larger than @code{STACK_BOUNDARY}.
+@end defmac
+
+@defmac FUNCTION_BOUNDARY
+Alignment required for a function entry point, in bits.
+@end defmac
+
+@defmac BIGGEST_ALIGNMENT
+Biggest alignment that any data type can require on this machine, in
+bits.  Note that this is not the biggest alignment that is supported,
+just the biggest alignment that, when violated, may cause a fault.
+@end defmac
+
+@deftypevr {Target Hook} HOST_WIDE_INT TARGET_ABSOLUTE_BIGGEST_ALIGNMENT
+If defined, this target hook specifies the absolute biggest alignment
+that a type or variable can have on this machine, otherwise,
+@code{BIGGEST_ALIGNMENT} is used.
+@end deftypevr
+
+@defmac MALLOC_ABI_ALIGNMENT
+Alignment, in bits, a C conformant malloc implementation has to
+provide.  If not defined, the default value is @code{BITS_PER_WORD}.
+@end defmac
+
+@defmac ATTRIBUTE_ALIGNED_VALUE
+Alignment used by the @code{__attribute__ ((aligned))} construct.  If
+not defined, the default value is @code{BIGGEST_ALIGNMENT}.
+@end defmac
+
+@defmac MINIMUM_ATOMIC_ALIGNMENT
+If defined, the smallest alignment, in bits, that can be given to an
+object that can be referenced in one operation, without disturbing any
+nearby object.  Normally, this is @code{BITS_PER_UNIT}, but may be larger
+on machines that don't have byte or half-word store operations.
+@end defmac
+
+@defmac BIGGEST_FIELD_ALIGNMENT
+Biggest alignment that any structure or union field can require on this
+machine, in bits.  If defined, this overrides @code{BIGGEST_ALIGNMENT} for
+structure and union fields only, unless the field alignment has been set
+by the @code{__attribute__ ((aligned (@var{n})))} construct.
+@end defmac
+
+@defmac ADJUST_FIELD_ALIGN (@var{field}, @var{type}, @var{computed})
+An expression for the alignment of a structure field @var{field} of
+type @var{type} if the alignment computed in the usual way (including
+applying of @code{BIGGEST_ALIGNMENT} and @code{BIGGEST_FIELD_ALIGNMENT} to the
+alignment) is @var{computed}.  It overrides alignment only if the
+field alignment has not been set by the
+@code{__attribute__ ((aligned (@var{n})))} construct.  Note that @var{field}
+may be @code{NULL_TREE} in case we just query for the minimum alignment
+of a field of type @var{type} in structure context.
+@end defmac
+
+@defmac MAX_STACK_ALIGNMENT
+Biggest stack alignment guaranteed by the backend.  Use this macro
+to specify the maximum alignment of a variable on stack.
+
+If not defined, the default value is @code{STACK_BOUNDARY}.
+
+@c FIXME: The default should be @code{PREFERRED_STACK_BOUNDARY}.
+@c But the fix for PR 32893 indicates that we can only guarantee
+@c maximum stack alignment on stack up to @code{STACK_BOUNDARY}, not
+@c @code{PREFERRED_STACK_BOUNDARY}, if stack alignment isn't supported.
+@end defmac
+
+@defmac MAX_OFILE_ALIGNMENT
+Biggest alignment supported by the object file format of this machine.
+Use this macro to limit the alignment which can be specified using the
+@code{__attribute__ ((aligned (@var{n})))} construct for functions and
+objects with static storage duration.  The alignment of automatic
+objects may exceed the object file format maximum up to the maximum
+supported by GCC.  If not defined, the default value is
+@code{BIGGEST_ALIGNMENT}.
+
+On systems that use ELF, the default (in @file{config/elfos.h}) is
+the largest supported 32-bit ELF section alignment representable on
+a 32-bit host e.g.@: @samp{(((uint64_t) 1 << 28) * 8)}.
+On 32-bit ELF the largest supported section alignment in bits is
+@samp{(0x80000000 * 8)}, but this is not representable on 32-bit hosts.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_LOWER_LOCAL_DECL_ALIGNMENT (tree @var{decl})
+Define this hook to lower alignment of local, parm or result
+decl @samp{(@var{decl})}.
+@end deftypefn
+
+@deftypefn {Target Hook} HOST_WIDE_INT TARGET_STATIC_RTX_ALIGNMENT (machine_mode @var{mode})
+This hook returns the preferred alignment in bits for a
+statically-allocated rtx, such as a constant pool entry.  @var{mode}
+is the mode of the rtx.  The default implementation returns
+@samp{GET_MODE_ALIGNMENT (@var{mode})}.
+@end deftypefn
+
+@defmac DATA_ALIGNMENT (@var{type}, @var{basic-align})
+If defined, a C expression to compute the alignment for a variable in
+the static store.  @var{type} is the data type, and @var{basic-align} is
+the alignment that the object would ordinarily have.  The value of this
+macro is used instead of that alignment to align the object.
+
+If this macro is not defined, then @var{basic-align} is used.
+
+@findex strcpy
+One use of this macro is to increase alignment of medium-size data to
+make it all fit in fewer cache lines.  Another is to cause character
+arrays to be word-aligned so that @code{strcpy} calls that copy
+constants to character arrays can be done inline.
+@end defmac
+
+@defmac DATA_ABI_ALIGNMENT (@var{type}, @var{basic-align})
+Similar to @code{DATA_ALIGNMENT}, but for the cases where the ABI mandates
+some alignment increase, instead of optimization only purposes.  E.g.@
+AMD x86-64 psABI says that variables with array type larger than 15 bytes
+must be aligned to 16 byte boundaries.
+
+If this macro is not defined, then @var{basic-align} is used.
+@end defmac
+
+@deftypefn {Target Hook} HOST_WIDE_INT TARGET_CONSTANT_ALIGNMENT (const_tree @var{constant}, HOST_WIDE_INT @var{basic_align})
+This hook returns the alignment in bits of a constant that is being
+placed in memory.  @var{constant} is the constant and @var{basic_align}
+is the alignment that the object would ordinarily have.
+
+The default definition just returns @var{basic_align}.
+
+The typical use of this hook is to increase alignment for string
+constants to be word aligned so that @code{strcpy} calls that copy
+constants can be done inline.  The function
+@code{constant_alignment_word_strings} provides such a definition.
+@end deftypefn
+
+@defmac LOCAL_ALIGNMENT (@var{type}, @var{basic-align})
+If defined, a C expression to compute the alignment for a variable in
+the local store.  @var{type} is the data type, and @var{basic-align} is
+the alignment that the object would ordinarily have.  The value of this
+macro is used instead of that alignment to align the object.
+
+If this macro is not defined, then @var{basic-align} is used.
+
+One use of this macro is to increase alignment of medium-size data to
+make it all fit in fewer cache lines.
+
+If the value of this macro has a type, it should be an unsigned type.
+@end defmac
+
+@deftypefn {Target Hook} HOST_WIDE_INT TARGET_VECTOR_ALIGNMENT (const_tree @var{type})
+This hook can be used to define the alignment for a vector of type
+@var{type}, in order to comply with a platform ABI.  The default is to
+require natural alignment for vector types.  The alignment returned by
+this hook must be a power-of-two multiple of the default alignment of
+the vector element type.
+@end deftypefn
+
+@defmac STACK_SLOT_ALIGNMENT (@var{type}, @var{mode}, @var{basic-align})
+If defined, a C expression to compute the alignment for stack slot.
+@var{type} is the data type, @var{mode} is the widest mode available,
+and @var{basic-align} is the alignment that the slot would ordinarily
+have.  The value of this macro is used instead of that alignment to
+align the slot.
+
+If this macro is not defined, then @var{basic-align} is used when
+@var{type} is @code{NULL}.  Otherwise, @code{LOCAL_ALIGNMENT} will
+be used.
+
+This macro is to set alignment of stack slot to the maximum alignment
+of all possible modes which the slot may have.
+
+If the value of this macro has a type, it should be an unsigned type.
+@end defmac
+
+@defmac LOCAL_DECL_ALIGNMENT (@var{decl})
+If defined, a C expression to compute the alignment for a local
+variable @var{decl}.
+
+If this macro is not defined, then
+@code{LOCAL_ALIGNMENT (TREE_TYPE (@var{decl}), DECL_ALIGN (@var{decl}))}
+is used.
+
+One use of this macro is to increase alignment of medium-size data to
+make it all fit in fewer cache lines.
+
+If the value of this macro has a type, it should be an unsigned type.
+@end defmac
+
+@defmac MINIMUM_ALIGNMENT (@var{exp}, @var{mode}, @var{align})
+If defined, a C expression to compute the minimum required alignment
+for dynamic stack realignment purposes for @var{exp} (a type or decl),
+@var{mode}, assuming normal alignment @var{align}.
+
+If this macro is not defined, then @var{align} will be used.
+@end defmac
+
+@defmac EMPTY_FIELD_BOUNDARY
+Alignment in bits to be given to a structure bit-field that follows an
+empty field such as @code{int : 0;}.
+
+If @code{PCC_BITFIELD_TYPE_MATTERS} is true, it overrides this macro.
+@end defmac
+
+@defmac STRUCTURE_SIZE_BOUNDARY
+Number of bits which any structure or union's size must be a multiple of.
+Each structure or union's size is rounded up to a multiple of this.
+
+If you do not define this macro, the default is the same as
+@code{BITS_PER_UNIT}.
+@end defmac
+
+@defmac STRICT_ALIGNMENT
+Define this macro to be the value 1 if instructions will fail to work
+if given data not on the nominal alignment.  If instructions will merely
+go slower in that case, define this macro as 0.
+@end defmac
+
+@defmac PCC_BITFIELD_TYPE_MATTERS
+Define this if you wish to imitate the way many other C compilers handle
+alignment of bit-fields and the structures that contain them.
+
+The behavior is that the type written for a named bit-field (@code{int},
+@code{short}, or other integer type) imposes an alignment for the entire
+structure, as if the structure really did contain an ordinary field of
+that type.  In addition, the bit-field is placed within the structure so
+that it would fit within such a field, not crossing a boundary for it.
+
+Thus, on most machines, a named bit-field whose type is written as
+@code{int} would not cross a four-byte boundary, and would force
+four-byte alignment for the whole structure.  (The alignment used may
+not be four bytes; it is controlled by the other alignment parameters.)
+
+An unnamed bit-field will not affect the alignment of the containing
+structure.
+
+If the macro is defined, its definition should be a C expression;
+a nonzero value for the expression enables this behavior.
+
+Note that if this macro is not defined, or its value is zero, some
+bit-fields may cross more than one alignment boundary.  The compiler can
+support such references if there are @samp{insv}, @samp{extv}, and
+@samp{extzv} insns that can directly reference memory.
+
+The other known way of making bit-fields work is to define
+@code{STRUCTURE_SIZE_BOUNDARY} as large as @code{BIGGEST_ALIGNMENT}.
+Then every structure can be accessed with fullwords.
+
+Unless the machine has bit-field instructions or you define
+@code{STRUCTURE_SIZE_BOUNDARY} that way, you must define
+@code{PCC_BITFIELD_TYPE_MATTERS} to have a nonzero value.
+
+If your aim is to make GCC use the same conventions for laying out
+bit-fields as are used by another compiler, here is how to investigate
+what the other compiler does.  Compile and run this program:
+
+@smallexample
+struct foo1
+@{
+  char x;
+  char :0;
+  char y;
+@};
+
+struct foo2
+@{
+  char x;
+  int :0;
+  char y;
+@};
+
+main ()
+@{
+  printf ("Size of foo1 is %d\n",
+          sizeof (struct foo1));
+  printf ("Size of foo2 is %d\n",
+          sizeof (struct foo2));
+  exit (0);
+@}
+@end smallexample
+
+If this prints 2 and 5, then the compiler's behavior is what you would
+get from @code{PCC_BITFIELD_TYPE_MATTERS}.
+@end defmac
+
+@defmac BITFIELD_NBYTES_LIMITED
+Like @code{PCC_BITFIELD_TYPE_MATTERS} except that its effect is limited
+to aligning a bit-field within the structure.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_ALIGN_ANON_BITFIELD (void)
+When @code{PCC_BITFIELD_TYPE_MATTERS} is true this hook will determine
+whether unnamed bitfields affect the alignment of the containing
+structure.  The hook should return true if the structure should inherit
+the alignment requirements of an unnamed bitfield's type.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_NARROW_VOLATILE_BITFIELD (void)
+This target hook should return @code{true} if accesses to volatile bitfields
+should use the narrowest mode possible.  It should return @code{false} if
+these accesses should use the bitfield container type.
+
+The default is @code{false}.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_MEMBER_TYPE_FORCES_BLK (const_tree @var{field}, machine_mode @var{mode})
+Return true if a structure, union or array containing @var{field} should
+be accessed using @code{BLKMODE}.
+
+If @var{field} is the only field in the structure, @var{mode} is its
+mode, otherwise @var{mode} is VOIDmode.  @var{mode} is provided in the
+case where structures of one field would require the structure's mode to
+retain the field's mode.
+
+Normally, this is not needed.
+@end deftypefn
+
+@defmac ROUND_TYPE_ALIGN (@var{type}, @var{computed}, @var{specified})
+Define this macro as an expression for the alignment of a type (given
+by @var{type} as a tree node) if the alignment computed in the usual
+way is @var{computed} and the alignment explicitly specified was
+@var{specified}.
+
+The default is to use @var{specified} if it is larger; otherwise, use
+the smaller of @var{computed} and @code{BIGGEST_ALIGNMENT}
+@end defmac
+
+@defmac MAX_FIXED_MODE_SIZE
+An integer expression for the size in bits of the largest integer
+machine mode that should actually be used.  All integer machine modes of
+this size or smaller can be used for structures and unions with the
+appropriate sizes.  If this macro is undefined, @code{GET_MODE_BITSIZE
+(DImode)} is assumed.
+@end defmac
+
+@defmac STACK_SAVEAREA_MODE (@var{save_level})
+If defined, an expression of type @code{machine_mode} that
+specifies the mode of the save area operand of a
+@code{save_stack_@var{level}} named pattern (@pxref{Standard Names}).
+@var{save_level} is one of @code{SAVE_BLOCK}, @code{SAVE_FUNCTION}, or
+@code{SAVE_NONLOCAL} and selects which of the three named patterns is
+having its mode specified.
+
+You need not define this macro if it always returns @code{Pmode}.  You
+would most commonly define this macro if the
+@code{save_stack_@var{level}} patterns need to support both a 32- and a
+64-bit mode.
+@end defmac
+
+@defmac STACK_SIZE_MODE
+If defined, an expression of type @code{machine_mode} that
+specifies the mode of the size increment operand of an
+@code{allocate_stack} named pattern (@pxref{Standard Names}).
+
+You need not define this macro if it always returns @code{word_mode}.
+You would most commonly define this macro if the @code{allocate_stack}
+pattern needs to support both a 32- and a 64-bit mode.
+@end defmac
+
+@deftypefn {Target Hook} scalar_int_mode TARGET_LIBGCC_CMP_RETURN_MODE (void)
+This target hook should return the mode to be used for the return value
+of compare instructions expanded to libgcc calls.  If not defined
+@code{word_mode} is returned which is the right choice for a majority of
+targets.
+@end deftypefn
+
+@deftypefn {Target Hook} scalar_int_mode TARGET_LIBGCC_SHIFT_COUNT_MODE (void)
+This target hook should return the mode to be used for the shift count operand
+of shift instructions expanded to libgcc calls.  If not defined
+@code{word_mode} is returned which is the right choice for a majority of
+targets.
+@end deftypefn
+
+@deftypefn {Target Hook} scalar_int_mode TARGET_UNWIND_WORD_MODE (void)
+Return machine mode to be used for @code{_Unwind_Word} type.
+The default is to use @code{word_mode}.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_MS_BITFIELD_LAYOUT_P (const_tree @var{record_type})
+This target hook returns @code{true} if bit-fields in the given
+@var{record_type} are to be laid out following the rules of Microsoft
+Visual C/C++, namely: (i) a bit-field won't share the same storage
+unit with the previous bit-field if their underlying types have
+different sizes, and the bit-field will be aligned to the highest
+alignment of the underlying types of itself and of the previous
+bit-field; (ii) a zero-sized bit-field will affect the alignment of
+the whole enclosing structure, even if it is unnamed; except that
+(iii) a zero-sized bit-field will be disregarded unless it follows
+another bit-field of nonzero size.  If this hook returns @code{true},
+other macros that control bit-field layout are ignored.
+
+When a bit-field is inserted into a packed record, the whole size
+of the underlying type is used by one or more same-size adjacent
+bit-fields (that is, if its long:3, 32 bits is used in the record,
+and any additional adjacent long bit-fields are packed into the same
+chunk of 32 bits.  However, if the size changes, a new field of that
+size is allocated).  In an unpacked record, this is the same as using
+alignment, but not equivalent when packing.
+
+If both MS bit-fields and @samp{__attribute__((packed))} are used,
+the latter will take precedence.  If @samp{__attribute__((packed))} is
+used on a single field when MS bit-fields are in use, it will take
+precedence for that field, but the alignment of the rest of the structure
+may affect its placement.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_DECIMAL_FLOAT_SUPPORTED_P (void)
+Returns true if the target supports decimal floating point.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_FIXED_POINT_SUPPORTED_P (void)
+Returns true if the target supports fixed-point arithmetic.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_EXPAND_TO_RTL_HOOK (void)
+This hook is called just before expansion into rtl, allowing the target
+to perform additional initializations or analysis before the expansion.
+For example, the rs6000 port uses it to allocate a scratch stack slot
+for use in copying SDmode values between memory and floating point
+registers whenever the function being expanded has any SDmode
+usage.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_INSTANTIATE_DECLS (void)
+This hook allows the backend to perform additional instantiations on rtl
+that are not actually in any insns yet, but will be later.
+@end deftypefn
+
+@deftypefn {Target Hook} {const char *} TARGET_MANGLE_TYPE (const_tree @var{type})
+If your target defines any fundamental types, or any types your target
+uses should be mangled differently from the default, define this hook
+to return the appropriate encoding for these types as part of a C++
+mangled name.  The @var{type} argument is the tree structure representing
+the type to be mangled.  The hook may be applied to trees which are
+not target-specific fundamental types; it should return @code{NULL}
+for all such types, as well as arguments it does not recognize.  If the
+return value is not @code{NULL}, it must point to a statically-allocated
+string constant.
+
+Target-specific fundamental types might be new fundamental types or
+qualified versions of ordinary fundamental types.  Encode new
+fundamental types as @samp{@w{u @var{n} @var{name}}}, where @var{name}
+is the name used for the type in source code, and @var{n} is the
+length of @var{name} in decimal.  Encode qualified versions of
+ordinary types as @samp{@w{U @var{n} @var{name} @var{code}}}, where
+@var{name} is the name used for the type qualifier in source code,
+@var{n} is the length of @var{name} as above, and @var{code} is the
+code used to represent the unqualified version of this type.  (See
+@code{write_builtin_type} in @file{cp/mangle.c} for the list of
+codes.)  In both cases the spaces are for clarity; do not include any
+spaces in your string.
+
+This hook is applied to types prior to typedef resolution.  If the mangled
+name for a particular type depends only on that type's main variant, you
+can perform typedef resolution yourself using @code{TYPE_MAIN_VARIANT}
+before mangling.
+
+The default version of this hook always returns @code{NULL}, which is
+appropriate for a target that does not define any new fundamental
+types.
+@end deftypefn
+
+@node Type Layout
+@section Layout of Source Language Data Types
+
+These macros define the sizes and other characteristics of the standard
+basic data types used in programs being compiled.  Unlike the macros in
+the previous section, these apply to specific features of C and related
+languages, rather than to fundamental aspects of storage layout.
+
+@defmac INT_TYPE_SIZE
+A C expression for the size in bits of the type @code{int} on the
+target machine.  If you don't define this, the default is one word.
+@end defmac
+
+@defmac SHORT_TYPE_SIZE
+A C expression for the size in bits of the type @code{short} on the
+target machine.  If you don't define this, the default is half a word.
+(If this would be less than one storage unit, it is rounded up to one
+unit.)
+@end defmac
+
+@defmac LONG_TYPE_SIZE
+A C expression for the size in bits of the type @code{long} on the
+target machine.  If you don't define this, the default is one word.
+@end defmac
+
+@defmac ADA_LONG_TYPE_SIZE
+On some machines, the size used for the Ada equivalent of the type
+@code{long} by a native Ada compiler differs from that used by C@.  In
+that situation, define this macro to be a C expression to be used for
+the size of that type.  If you don't define this, the default is the
+value of @code{LONG_TYPE_SIZE}.
+@end defmac
+
+@defmac LONG_LONG_TYPE_SIZE
+A C expression for the size in bits of the type @code{long long} on the
+target machine.  If you don't define this, the default is two
+words.  If you want to support GNU Ada on your machine, the value of this
+macro must be at least 64.
+@end defmac
+
+@defmac CHAR_TYPE_SIZE
+A C expression for the size in bits of the type @code{char} on the
+target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT}.
+@end defmac
+
+@defmac BOOL_TYPE_SIZE
+A C expression for the size in bits of the C++ type @code{bool} and
+C99 type @code{_Bool} on the target machine.  If you don't define
+this, and you probably shouldn't, the default is @code{CHAR_TYPE_SIZE}.
+@end defmac
+
+@defmac FLOAT_TYPE_SIZE
+A C expression for the size in bits of the type @code{float} on the
+target machine.  If you don't define this, the default is one word.
+@end defmac
+
+@defmac DOUBLE_TYPE_SIZE
+A C expression for the size in bits of the type @code{double} on the
+target machine.  If you don't define this, the default is two
+words.
+@end defmac
+
+@defmac LONG_DOUBLE_TYPE_SIZE
+A C expression for the size in bits of the type @code{long double} on
+the target machine.  If you don't define this, the default is two
+words.
+@end defmac
+
+@defmac SHORT_FRACT_TYPE_SIZE
+A C expression for the size in bits of the type @code{short _Fract} on
+the target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT}.
+@end defmac
+
+@defmac FRACT_TYPE_SIZE
+A C expression for the size in bits of the type @code{_Fract} on
+the target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT * 2}.
+@end defmac
+
+@defmac LONG_FRACT_TYPE_SIZE
+A C expression for the size in bits of the type @code{long _Fract} on
+the target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT * 4}.
+@end defmac
+
+@defmac LONG_LONG_FRACT_TYPE_SIZE
+A C expression for the size in bits of the type @code{long long _Fract} on
+the target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT * 8}.
+@end defmac
+
+@defmac SHORT_ACCUM_TYPE_SIZE
+A C expression for the size in bits of the type @code{short _Accum} on
+the target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT * 2}.
+@end defmac
+
+@defmac ACCUM_TYPE_SIZE
+A C expression for the size in bits of the type @code{_Accum} on
+the target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT * 4}.
+@end defmac
+
+@defmac LONG_ACCUM_TYPE_SIZE
+A C expression for the size in bits of the type @code{long _Accum} on
+the target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT * 8}.
+@end defmac
+
+@defmac LONG_LONG_ACCUM_TYPE_SIZE
+A C expression for the size in bits of the type @code{long long _Accum} on
+the target machine.  If you don't define this, the default is
+@code{BITS_PER_UNIT * 16}.
+@end defmac
+
+@defmac LIBGCC2_GNU_PREFIX
+This macro corresponds to the @code{TARGET_LIBFUNC_GNU_PREFIX} target
+hook and should be defined if that hook is overriden to be true.  It
+causes function names in libgcc to be changed to use a @code{__gnu_}
+prefix for their name rather than the default @code{__}.  A port which
+uses this macro should also arrange to use @file{t-gnu-prefix} in
+the libgcc @file{config.host}.
+@end defmac
+
+@defmac WIDEST_HARDWARE_FP_SIZE
+A C expression for the size in bits of the widest floating-point format
+supported by the hardware.  If you define this macro, you must specify a
+value less than or equal to the value of @code{LONG_DOUBLE_TYPE_SIZE}.
+If you do not define this macro, the value of @code{LONG_DOUBLE_TYPE_SIZE}
+is the default.
+@end defmac
+
+@defmac DEFAULT_SIGNED_CHAR
+An expression whose value is 1 or 0, according to whether the type
+@code{char} should be signed or unsigned by default.  The user can
+always override this default with the options @option{-fsigned-char}
+and @option{-funsigned-char}.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_DEFAULT_SHORT_ENUMS (void)
+This target hook should return true if the compiler should give an
+@code{enum} type only as many bytes as it takes to represent the range
+of possible values of that type.  It should return false if all
+@code{enum} types should be allocated like @code{int}.
+
+The default is to return false.
+@end deftypefn
+
+@defmac SIZE_TYPE
+A C expression for a string describing the name of the data type to use
+for size values.  The typedef name @code{size_t} is defined using the
+contents of the string.
+
+The string can contain more than one keyword.  If so, separate them with
+spaces, and write first any length keyword, then @code{unsigned} if
+appropriate, and finally @code{int}.  The string must exactly match one
+of the data type names defined in the function
+@code{c_common_nodes_and_builtins} in the file @file{c-family/c-common.c}.
+You may not omit @code{int} or change the order---that would cause the
+compiler to crash on startup.
+
+If you don't define this macro, the default is @code{"long unsigned
+int"}.
+@end defmac
+
+@defmac SIZETYPE
+GCC defines internal types (@code{sizetype}, @code{ssizetype},
+@code{bitsizetype} and @code{sbitsizetype}) for expressions
+dealing with size.  This macro is a C expression for a string describing
+the name of the data type from which the precision of @code{sizetype}
+is extracted.
+
+The string has the same restrictions as @code{SIZE_TYPE} string.
+
+If you don't define this macro, the default is @code{SIZE_TYPE}.
+@end defmac
+
+@defmac PTRDIFF_TYPE
+A C expression for a string describing the name of the data type to use
+for the result of subtracting two pointers.  The typedef name
+@code{ptrdiff_t} is defined using the contents of the string.  See
+@code{SIZE_TYPE} above for more information.
+
+If you don't define this macro, the default is @code{"long int"}.
+@end defmac
+
+@defmac WCHAR_TYPE
+A C expression for a string describing the name of the data type to use
+for wide characters.  The typedef name @code{wchar_t} is defined using
+the contents of the string.  See @code{SIZE_TYPE} above for more
+information.
+
+If you don't define this macro, the default is @code{"int"}.
+@end defmac
+
+@defmac WCHAR_TYPE_SIZE
+A C expression for the size in bits of the data type for wide
+characters.  This is used in @code{cpp}, which cannot make use of
+@code{WCHAR_TYPE}.
+@end defmac
+
+@defmac WINT_TYPE
+A C expression for a string describing the name of the data type to
+use for wide characters passed to @code{printf} and returned from
+@code{getwc}.  The typedef name @code{wint_t} is defined using the
+contents of the string.  See @code{SIZE_TYPE} above for more
+information.
+
+If you don't define this macro, the default is @code{"unsigned int"}.
+@end defmac
+
+@defmac INTMAX_TYPE
+A C expression for a string describing the name of the data type that
+can represent any value of any standard or extended signed integer type.
+The typedef name @code{intmax_t} is defined using the contents of the
+string.  See @code{SIZE_TYPE} above for more information.
+
+If you don't define this macro, the default is the first of
+@code{"int"}, @code{"long int"}, or @code{"long long int"} that has as
+much precision as @code{long long int}.
+@end defmac
+
+@defmac UINTMAX_TYPE
+A C expression for a string describing the name of the data type that
+can represent any value of any standard or extended unsigned integer
+type.  The typedef name @code{uintmax_t} is defined using the contents
+of the string.  See @code{SIZE_TYPE} above for more information.
+
+If you don't define this macro, the default is the first of
+@code{"unsigned int"}, @code{"long unsigned int"}, or @code{"long long
+unsigned int"} that has as much precision as @code{long long unsigned
+int}.
+@end defmac
+
+@defmac SIG_ATOMIC_TYPE
+@defmacx INT8_TYPE
+@defmacx INT16_TYPE
+@defmacx INT32_TYPE
+@defmacx INT64_TYPE
+@defmacx UINT8_TYPE
+@defmacx UINT16_TYPE
+@defmacx UINT32_TYPE
+@defmacx UINT64_TYPE
+@defmacx INT_LEAST8_TYPE
+@defmacx INT_LEAST16_TYPE
+@defmacx INT_LEAST32_TYPE
+@defmacx INT_LEAST64_TYPE
+@defmacx UINT_LEAST8_TYPE
+@defmacx UINT_LEAST16_TYPE
+@defmacx UINT_LEAST32_TYPE
+@defmacx UINT_LEAST64_TYPE
+@defmacx INT_FAST8_TYPE
+@defmacx INT_FAST16_TYPE
+@defmacx INT_FAST32_TYPE
+@defmacx INT_FAST64_TYPE
+@defmacx UINT_FAST8_TYPE
+@defmacx UINT_FAST16_TYPE
+@defmacx UINT_FAST32_TYPE
+@defmacx UINT_FAST64_TYPE
+@defmacx INTPTR_TYPE
+@defmacx UINTPTR_TYPE
+C expressions for the standard types @code{sig_atomic_t},
+@code{int8_t}, @code{int16_t}, @code{int32_t}, @code{int64_t},
+@code{uint8_t}, @code{uint16_t}, @code{uint32_t}, @code{uint64_t},
+@code{int_least8_t}, @code{int_least16_t}, @code{int_least32_t},
+@code{int_least64_t}, @code{uint_least8_t}, @code{uint_least16_t},
+@code{uint_least32_t}, @code{uint_least64_t}, @code{int_fast8_t},
+@code{int_fast16_t}, @code{int_fast32_t}, @code{int_fast64_t},
+@code{uint_fast8_t}, @code{uint_fast16_t}, @code{uint_fast32_t},
+@code{uint_fast64_t}, @code{intptr_t}, and @code{uintptr_t}.  See
+@code{SIZE_TYPE} above for more information.
+
+If any of these macros evaluates to a null pointer, the corresponding
+type is not supported; if GCC is configured to provide
+@code{<stdint.h>} in such a case, the header provided may not conform
+to C99, depending on the type in question.  The defaults for all of
+these macros are null pointers.
+@end defmac
+
+@defmac TARGET_PTRMEMFUNC_VBIT_LOCATION
+The C++ compiler represents a pointer-to-member-function with a struct
+that looks like:
+
+@smallexample
+  struct @{
+    union @{
+      void (*fn)();
+      ptrdiff_t vtable_index;
+    @};
+    ptrdiff_t delta;
+  @};
+@end smallexample
+
+@noindent
+The C++ compiler must use one bit to indicate whether the function that
+will be called through a pointer-to-member-function is virtual.
+Normally, we assume that the low-order bit of a function pointer must
+always be zero.  Then, by ensuring that the vtable_index is odd, we can
+distinguish which variant of the union is in use.  But, on some
+platforms function pointers can be odd, and so this doesn't work.  In
+that case, we use the low-order bit of the @code{delta} field, and shift
+the remainder of the @code{delta} field to the left.
+
+GCC will automatically make the right selection about where to store
+this bit using the @code{FUNCTION_BOUNDARY} setting for your platform.
+However, some platforms such as ARM/Thumb have @code{FUNCTION_BOUNDARY}
+set such that functions always start at even addresses, but the lowest
+bit of pointers to functions indicate whether the function at that
+address is in ARM or Thumb mode.  If this is the case of your
+architecture, you should define this macro to
+@code{ptrmemfunc_vbit_in_delta}.
+
+In general, you should not have to define this macro.  On architectures
+in which function addresses are always even, according to
+@code{FUNCTION_BOUNDARY}, GCC will automatically define this macro to
+@code{ptrmemfunc_vbit_in_pfn}.
+@end defmac
+
+@defmac TARGET_VTABLE_USES_DESCRIPTORS
+Normally, the C++ compiler uses function pointers in vtables.  This
+macro allows the target to change to use ``function descriptors''
+instead.  Function descriptors are found on targets for whom a
+function pointer is actually a small data structure.  Normally the
+data structure consists of the actual code address plus a data
+pointer to which the function's data is relative.
+
+If vtables are used, the value of this macro should be the number
+of words that the function descriptor occupies.
+@end defmac
+
+@defmac TARGET_VTABLE_ENTRY_ALIGN
+By default, the vtable entries are void pointers, the so the alignment
+is the same as pointer alignment.  The value of this macro specifies
+the alignment of the vtable entry in bits.  It should be defined only
+when special alignment is necessary. */
+@end defmac
+
+@defmac TARGET_VTABLE_DATA_ENTRY_DISTANCE
+There are a few non-descriptor entries in the vtable at offsets below
+zero.  If these entries must be padded (say, to preserve the alignment
+specified by @code{TARGET_VTABLE_ENTRY_ALIGN}), set this to the number
+of words in each data entry.
+@end defmac
+
+@node Registers
+@section Register Usage
+@cindex register usage
+
+This section explains how to describe what registers the target machine
+has, and how (in general) they can be used.
+
+The description of which registers a specific instruction can use is
+done with register classes; see @ref{Register Classes}.  For information
+on using registers to access a stack frame, see @ref{Frame Registers}.
+For passing values in registers, see @ref{Register Arguments}.
+For returning values in registers, see @ref{Scalar Return}.
+
+@menu
+* Register Basics::             Number and kinds of registers.
+* Allocation Order::            Order in which registers are allocated.
+* Values in Registers::         What kinds of values each reg can hold.
+* Leaf Functions::              Renumbering registers for leaf functions.
+* Stack Registers::             Handling a register stack such as 80387.
+@end menu
+
+@node Register Basics
+@subsection Basic Characteristics of Registers
+
+@c prevent bad page break with this line
+Registers have various characteristics.
+
+@defmac FIRST_PSEUDO_REGISTER
+Number of hardware registers known to the compiler.  They receive
+numbers 0 through @code{FIRST_PSEUDO_REGISTER-1}; thus, the first
+pseudo register's number really is assigned the number
+@code{FIRST_PSEUDO_REGISTER}.
+@end defmac
+
+@defmac FIXED_REGISTERS
+@cindex fixed register
+An initializer that says which registers are used for fixed purposes
+all throughout the compiled code and are therefore not available for
+general allocation.  These would include the stack pointer, the frame
+pointer (except on machines where that can be used as a general
+register when no frame pointer is needed), the program counter on
+machines where that is considered one of the addressable registers,
+and any other numbered register with a standard use.
+
+This information is expressed as a sequence of numbers, separated by
+commas and surrounded by braces.  The @var{n}th number is 1 if
+register @var{n} is fixed, 0 otherwise.
+
+The table initialized from this macro, and the table initialized by
+the following one, may be overridden at run time either automatically,
+by the actions of the macro @code{CONDITIONAL_REGISTER_USAGE}, or by
+the user with the command options @option{-ffixed-@var{reg}},
+@option{-fcall-used-@var{reg}} and @option{-fcall-saved-@var{reg}}.
+@end defmac
+
+@defmac CALL_USED_REGISTERS
+@cindex call-used register
+@cindex call-clobbered register
+@cindex call-saved register
+Like @code{FIXED_REGISTERS} but has 1 for each register that is
+clobbered (in general) by function calls as well as for fixed
+registers.  This macro therefore identifies the registers that are not
+available for general allocation of values that must live across
+function calls.
+
+If a register has 0 in @code{CALL_USED_REGISTERS}, the compiler
+automatically saves it on function entry and restores it on function
+exit, if the register is used within the function.
+
+Exactly one of @code{CALL_USED_REGISTERS} and @code{CALL_REALLY_USED_REGISTERS}
+must be defined.  Modern ports should define @code{CALL_REALLY_USED_REGISTERS}.
+@end defmac
+
+@defmac CALL_REALLY_USED_REGISTERS
+@cindex call-used register
+@cindex call-clobbered register
+@cindex call-saved register
+Like @code{CALL_USED_REGISTERS} except this macro doesn't require
+that the entire set of @code{FIXED_REGISTERS} be included.
+(@code{CALL_USED_REGISTERS} must be a superset of @code{FIXED_REGISTERS}).
+
+Exactly one of @code{CALL_USED_REGISTERS} and @code{CALL_REALLY_USED_REGISTERS}
+must be defined.  Modern ports should define @code{CALL_REALLY_USED_REGISTERS}.
+@end defmac
+
+@cindex call-used register
+@cindex call-clobbered register
+@cindex call-saved register
+@deftypefn {Target Hook} {const predefined_function_abi &} TARGET_FNTYPE_ABI (const_tree @var{type})
+Return the ABI used by a function with type @var{type}; see the
+definition of @code{predefined_function_abi} for details of the ABI
+descriptor.  Targets only need to define this hook if they support
+interoperability between several ABIs in the same translation unit.
+@end deftypefn
+
+@deftypefn {Target Hook} {const predefined_function_abi &} TARGET_INSN_CALLEE_ABI (const rtx_insn *@var{insn})
+This hook returns a description of the ABI used by the target of
+call instruction @var{insn}; see the definition of
+@code{predefined_function_abi} for details of the ABI descriptor.
+Only the global function @code{insn_callee_abi} should call this hook
+directly.
+
+Targets only need to define this hook if they support
+interoperability between several ABIs in the same translation unit.
+@end deftypefn
+
+@cindex call-used register
+@cindex call-clobbered register
+@cindex call-saved register
+@deftypefn {Target Hook} bool TARGET_HARD_REGNO_CALL_PART_CLOBBERED (unsigned int @var{abi_id}, unsigned int @var{regno}, machine_mode @var{mode})
+ABIs usually specify that calls must preserve the full contents
+of a particular register, or that calls can alter any part of a
+particular register.  This information is captured by the target macro
+@code{CALL_REALLY_USED_REGISTERS}.  However, some ABIs specify that calls
+must preserve certain bits of a particular register but can alter others.
+This hook should return true if this applies to at least one of the
+registers in @samp{(reg:@var{mode} @var{regno})}, and if as a result the
+call would alter part of the @var{mode} value.  For example, if a call
+preserves the low 32 bits of a 64-bit hard register @var{regno} but can
+clobber the upper 32 bits, this hook should return true for a 64-bit mode
+but false for a 32-bit mode.
+
+The value of @var{abi_id} comes from the @code{predefined_function_abi}
+structure that describes the ABI of the call; see the definition of the
+structure for more details.  If (as is usual) the target uses the same ABI
+for all functions in a translation unit, @var{abi_id} is always 0.
+
+The default implementation returns false, which is correct
+for targets that don't have partly call-clobbered registers.
+@end deftypefn
+
+@deftypefn {Target Hook} {const char *} TARGET_GET_MULTILIB_ABI_NAME (void)
+This hook returns name of multilib ABI name.
+@end deftypefn
+
+@findex fixed_regs
+@findex call_used_regs
+@findex global_regs
+@findex reg_names
+@findex reg_class_contents
+@deftypefn {Target Hook} void TARGET_CONDITIONAL_REGISTER_USAGE (void)
+This hook may conditionally modify five variables
+@code{fixed_regs}, @code{call_used_regs}, @code{global_regs},
+@code{reg_names}, and @code{reg_class_contents}, to take into account
+any dependence of these register sets on target flags.  The first three
+of these are of type @code{char []} (interpreted as boolean vectors).
+@code{global_regs} is a @code{const char *[]}, and
+@code{reg_class_contents} is a @code{HARD_REG_SET}.  Before the macro is
+called, @code{fixed_regs}, @code{call_used_regs},
+@code{reg_class_contents}, and @code{reg_names} have been initialized
+from @code{FIXED_REGISTERS}, @code{CALL_USED_REGISTERS},
+@code{REG_CLASS_CONTENTS}, and @code{REGISTER_NAMES}, respectively.
+@code{global_regs} has been cleared, and any @option{-ffixed-@var{reg}},
+@option{-fcall-used-@var{reg}} and @option{-fcall-saved-@var{reg}}
+command options have been applied.
+
+@cindex disabling certain registers
+@cindex controlling register usage
+If the usage of an entire class of registers depends on the target
+flags, you may indicate this to GCC by using this macro to modify
+@code{fixed_regs} and @code{call_used_regs} to 1 for each of the
+registers in the classes which should not be used by GCC@.  Also make
+@code{define_register_constraint}s return @code{NO_REGS} for constraints
+that shouldn't be used.
+
+(However, if this class is not included in @code{GENERAL_REGS} and all
+of the insn patterns whose constraints permit this class are
+controlled by target switches, then GCC will automatically avoid using
+these registers when the target switches are opposed to them.)
+@end deftypefn
+
+@defmac INCOMING_REGNO (@var{out})
+Define this macro if the target machine has register windows.  This C
+expression returns the register number as seen by the called function
+corresponding to the register number @var{out} as seen by the calling
+function.  Return @var{out} if register number @var{out} is not an
+outbound register.
+@end defmac
+
+@defmac OUTGOING_REGNO (@var{in})
+Define this macro if the target machine has register windows.  This C
+expression returns the register number as seen by the calling function
+corresponding to the register number @var{in} as seen by the called
+function.  Return @var{in} if register number @var{in} is not an inbound
+register.
+@end defmac
+
+@defmac LOCAL_REGNO (@var{regno})
+Define this macro if the target machine has register windows.  This C
+expression returns true if the register is call-saved but is in the
+register window.  Unlike most call-saved registers, such registers
+need not be explicitly restored on function exit or during non-local
+gotos.
+@end defmac
+
+@defmac PC_REGNUM
+If the program counter has a register number, define this as that
+register number.  Otherwise, do not define it.
+@end defmac
+
+@node Allocation Order
+@subsection Order of Allocation of Registers
+@cindex order of register allocation
+@cindex register allocation order
+
+@c prevent bad page break with this line
+Registers are allocated in order.
+
+@defmac REG_ALLOC_ORDER
+If defined, an initializer for a vector of integers, containing the
+numbers of hard registers in the order in which GCC should prefer
+to use them (from most preferred to least).
+
+If this macro is not defined, registers are used lowest numbered first
+(all else being equal).
+
+One use of this macro is on machines where the highest numbered
+registers must always be saved and the save-multiple-registers
+instruction supports only sequences of consecutive registers.  On such
+machines, define @code{REG_ALLOC_ORDER} to be an initializer that lists
+the highest numbered allocable register first.
+@end defmac
+
+@defmac ADJUST_REG_ALLOC_ORDER
+A C statement (sans semicolon) to choose the order in which to allocate
+hard registers for pseudo-registers local to a basic block.
+
+Store the desired register order in the array @code{reg_alloc_order}.
+Element 0 should be the register to allocate first; element 1, the next
+register; and so on.
+
+The macro body should not assume anything about the contents of
+@code{reg_alloc_order} before execution of the macro.
+
+On most machines, it is not necessary to define this macro.
+@end defmac
+
+@defmac HONOR_REG_ALLOC_ORDER
+Normally, IRA tries to estimate the costs for saving a register in the
+prologue and restoring it in the epilogue.  This discourages it from
+using call-saved registers.  If a machine wants to ensure that IRA
+allocates registers in the order given by REG_ALLOC_ORDER even if some
+call-saved registers appear earlier than call-used ones, then define this
+macro as a C expression to nonzero. Default is 0.
+@end defmac
+
+@defmac IRA_HARD_REGNO_ADD_COST_MULTIPLIER (@var{regno})
+In some case register allocation order is not enough for the
+Integrated Register Allocator (@acronym{IRA}) to generate a good code.
+If this macro is defined, it should return a floating point value
+based on @var{regno}.  The cost of using @var{regno} for a pseudo will
+be increased by approximately the pseudo's usage frequency times the
+value returned by this macro.  Not defining this macro is equivalent
+to having it always return @code{0.0}.
+
+On most machines, it is not necessary to define this macro.
+@end defmac
+
+@node Values in Registers
+@subsection How Values Fit in Registers
+
+This section discusses the macros that describe which kinds of values
+(specifically, which machine modes) each register can hold, and how many
+consecutive registers are needed for a given mode.
+
+@deftypefn {Target Hook} {unsigned int} TARGET_HARD_REGNO_NREGS (unsigned int @var{regno}, machine_mode @var{mode})
+This hook returns the number of consecutive hard registers, starting
+at register number @var{regno}, required to hold a value of mode
+@var{mode}.  This hook must never return zero, even if a register
+cannot hold the requested mode - indicate that with
+@code{TARGET_HARD_REGNO_MODE_OK} and/or
+@code{TARGET_CAN_CHANGE_MODE_CLASS} instead.
+
+The default definition returns the number of words in @var{mode}.
+@end deftypefn
+
+@defmac HARD_REGNO_NREGS_HAS_PADDING (@var{regno}, @var{mode})
+A C expression that is nonzero if a value of mode @var{mode}, stored
+in memory, ends with padding that causes it to take up more space than
+in registers starting at register number @var{regno} (as determined by
+multiplying GCC's notion of the size of the register when containing
+this mode by the number of registers returned by
+@code{TARGET_HARD_REGNO_NREGS}).  By default this is zero.
+
+For example, if a floating-point value is stored in three 32-bit
+registers but takes up 128 bits in memory, then this would be
+nonzero.
+
+This macros only needs to be defined if there are cases where
+@code{subreg_get_info}
+would otherwise wrongly determine that a @code{subreg} can be
+represented by an offset to the register number, when in fact such a
+@code{subreg} would contain some of the padding not stored in
+registers and so not be representable.
+@end defmac
+
+@defmac HARD_REGNO_NREGS_WITH_PADDING (@var{regno}, @var{mode})
+For values of @var{regno} and @var{mode} for which
+@code{HARD_REGNO_NREGS_HAS_PADDING} returns nonzero, a C expression
+returning the greater number of registers required to hold the value
+including any padding.  In the example above, the value would be four.
+@end defmac
+
+@defmac REGMODE_NATURAL_SIZE (@var{mode})
+Define this macro if the natural size of registers that hold values
+of mode @var{mode} is not the word size.  It is a C expression that
+should give the natural size in bytes for the specified mode.  It is
+used by the register allocator to try to optimize its results.  This
+happens for example on SPARC 64-bit where the natural size of
+floating-point registers is still 32-bit.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_HARD_REGNO_MODE_OK (unsigned int @var{regno}, machine_mode @var{mode})
+This hook returns true if it is permissible to store a value
+of mode @var{mode} in hard register number @var{regno} (or in several
+registers starting with that one).  The default definition returns true
+unconditionally.
+
+You need not include code to check for the numbers of fixed registers,
+because the allocation mechanism considers them to be always occupied.
+
+@cindex register pairs
+On some machines, double-precision values must be kept in even/odd
+register pairs.  You can implement that by defining this hook to reject
+odd register numbers for such modes.
+
+The minimum requirement for a mode to be OK in a register is that the
+@samp{mov@var{mode}} instruction pattern support moves between the
+register and other hard register in the same class and that moving a
+value into the register and back out not alter it.
+
+Since the same instruction used to move @code{word_mode} will work for
+all narrower integer modes, it is not necessary on any machine for
+this hook to distinguish between these modes, provided you define
+patterns @samp{movhi}, etc., to take advantage of this.  This is
+useful because of the interaction between @code{TARGET_HARD_REGNO_MODE_OK}
+and @code{TARGET_MODES_TIEABLE_P}; it is very desirable for all integer
+modes to be tieable.
+
+Many machines have special registers for floating point arithmetic.
+Often people assume that floating point machine modes are allowed only
+in floating point registers.  This is not true.  Any registers that
+can hold integers can safely @emph{hold} a floating point machine
+mode, whether or not floating arithmetic can be done on it in those
+registers.  Integer move instructions can be used to move the values.
+
+On some machines, though, the converse is true: fixed-point machine
+modes may not go in floating registers.  This is true if the floating
+registers normalize any value stored in them, because storing a
+non-floating value there would garble it.  In this case,
+@code{TARGET_HARD_REGNO_MODE_OK} should reject fixed-point machine modes in
+floating registers.  But if the floating registers do not automatically
+normalize, if you can store any bit pattern in one and retrieve it
+unchanged without a trap, then any machine mode may go in a floating
+register, so you can define this hook to say so.
+
+The primary significance of special floating registers is rather that
+they are the registers acceptable in floating point arithmetic
+instructions.  However, this is of no concern to
+@code{TARGET_HARD_REGNO_MODE_OK}.  You handle it by writing the proper
+constraints for those instructions.
+
+On some machines, the floating registers are especially slow to access,
+so that it is better to store a value in a stack frame than in such a
+register if floating point arithmetic is not being done.  As long as the
+floating registers are not in class @code{GENERAL_REGS}, they will not
+be used unless some pattern's constraint asks for one.
+@end deftypefn
+
+@defmac HARD_REGNO_RENAME_OK (@var{from}, @var{to})
+A C expression that is nonzero if it is OK to rename a hard register
+@var{from} to another hard register @var{to}.
+
+One common use of this macro is to prevent renaming of a register to
+another register that is not saved by a prologue in an interrupt
+handler.
+
+The default is always nonzero.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_MODES_TIEABLE_P (machine_mode @var{mode1}, machine_mode @var{mode2})
+This hook returns true if a value of mode @var{mode1} is accessible
+in mode @var{mode2} without copying.
+
+If @code{TARGET_HARD_REGNO_MODE_OK (@var{r}, @var{mode1})} and
+@code{TARGET_HARD_REGNO_MODE_OK (@var{r}, @var{mode2})} are always
+the same for any @var{r}, then
+@code{TARGET_MODES_TIEABLE_P (@var{mode1}, @var{mode2})}
+should be true.  If they differ for any @var{r}, you should define
+this hook to return false unless some other mechanism ensures the
+accessibility of the value in a narrower mode.
+
+You should define this hook to return true in as many cases as
+possible since doing so will allow GCC to perform better register
+allocation.  The default definition returns true unconditionally.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_HARD_REGNO_SCRATCH_OK (unsigned int @var{regno})
+This target hook should return @code{true} if it is OK to use a hard register
+@var{regno} as scratch reg in peephole2.
+
+One common use of this macro is to prevent using of a register that
+is not saved by a prologue in an interrupt handler.
+
+The default version of this hook always returns @code{true}.
+@end deftypefn
+
+@defmac AVOID_CCMODE_COPIES
+Define this macro if the compiler should avoid copies to/from @code{CCmode}
+registers.  You should only define this macro if support for copying to/from
+@code{CCmode} is incomplete.
+@end defmac
+
+@node Leaf Functions
+@subsection Handling Leaf Functions
+
+@cindex leaf functions
+@cindex functions, leaf
+On some machines, a leaf function (i.e., one which makes no calls) can run
+more efficiently if it does not make its own register window.  Often this
+means it is required to receive its arguments in the registers where they
+are passed by the caller, instead of the registers where they would
+normally arrive.
+
+The special treatment for leaf functions generally applies only when
+other conditions are met; for example, often they may use only those
+registers for its own variables and temporaries.  We use the term ``leaf
+function'' to mean a function that is suitable for this special
+handling, so that functions with no calls are not necessarily ``leaf
+functions''.
+
+GCC assigns register numbers before it knows whether the function is
+suitable for leaf function treatment.  So it needs to renumber the
+registers in order to output a leaf function.  The following macros
+accomplish this.
+
+@defmac LEAF_REGISTERS
+Name of a char vector, indexed by hard register number, which
+contains 1 for a register that is allowable in a candidate for leaf
+function treatment.
+
+If leaf function treatment involves renumbering the registers, then the
+registers marked here should be the ones before renumbering---those that
+GCC would ordinarily allocate.  The registers which will actually be
+used in the assembler code, after renumbering, should not be marked with 1
+in this vector.
+
+Define this macro only if the target machine offers a way to optimize
+the treatment of leaf functions.
+@end defmac
+
+@defmac LEAF_REG_REMAP (@var{regno})
+A C expression whose value is the register number to which @var{regno}
+should be renumbered, when a function is treated as a leaf function.
+
+If @var{regno} is a register number which should not appear in a leaf
+function before renumbering, then the expression should yield @minus{}1, which
+will cause the compiler to abort.
+
+Define this macro only if the target machine offers a way to optimize the
+treatment of leaf functions, and registers need to be renumbered to do
+this.
+@end defmac
+
+@findex current_function_is_leaf
+@findex current_function_uses_only_leaf_regs
+@code{TARGET_ASM_FUNCTION_PROLOGUE} and
+@code{TARGET_ASM_FUNCTION_EPILOGUE} must usually treat leaf functions
+specially.  They can test the C variable @code{current_function_is_leaf}
+which is nonzero for leaf functions.  @code{current_function_is_leaf} is
+set prior to local register allocation and is valid for the remaining
+compiler passes.  They can also test the C variable
+@code{current_function_uses_only_leaf_regs} which is nonzero for leaf
+functions which only use leaf registers.
+@code{current_function_uses_only_leaf_regs} is valid after all passes
+that modify the instructions have been run and is only useful if
+@code{LEAF_REGISTERS} is defined.
+@c changed this to fix overfull.  ALSO:  why the "it" at the beginning
+@c of the next paragraph?!  --mew 2feb93
+
+@node Stack Registers
+@subsection Registers That Form a Stack
+
+There are special features to handle computers where some of the
+``registers'' form a stack.  Stack registers are normally written by
+pushing onto the stack, and are numbered relative to the top of the
+stack.
+
+Currently, GCC can only handle one group of stack-like registers, and
+they must be consecutively numbered.  Furthermore, the existing
+support for stack-like registers is specific to the 80387 floating
+point coprocessor.  If you have a new architecture that uses
+stack-like registers, you will need to do substantial work on
+@file{reg-stack.c} and write your machine description to cooperate
+with it, as well as defining these macros.
+
+@defmac STACK_REGS
+Define this if the machine has any stack-like registers.
+@end defmac
+
+@defmac STACK_REG_COVER_CLASS
+This is a cover class containing the stack registers.  Define this if
+the machine has any stack-like registers.
+@end defmac
+
+@defmac FIRST_STACK_REG
+The number of the first stack-like register.  This one is the top
+of the stack.
+@end defmac
+
+@defmac LAST_STACK_REG
+The number of the last stack-like register.  This one is the bottom of
+the stack.
+@end defmac
+
+@node Register Classes
+@section Register Classes
+@cindex register class definitions
+@cindex class definitions, register
+
+On many machines, the numbered registers are not all equivalent.
+For example, certain registers may not be allowed for indexed addressing;
+certain registers may not be allowed in some instructions.  These machine
+restrictions are described to the compiler using @dfn{register classes}.
+
+You define a number of register classes, giving each one a name and saying
+which of the registers belong to it.  Then you can specify register classes
+that are allowed as operands to particular instruction patterns.
+
+@findex ALL_REGS
+@findex NO_REGS
+In general, each register will belong to several classes.  In fact, one
+class must be named @code{ALL_REGS} and contain all the registers.  Another
+class must be named @code{NO_REGS} and contain no registers.  Often the
+union of two classes will be another class; however, this is not required.
+
+@findex GENERAL_REGS
+One of the classes must be named @code{GENERAL_REGS}.  There is nothing
+terribly special about the name, but the operand constraint letters
+@samp{r} and @samp{g} specify this class.  If @code{GENERAL_REGS} is
+the same as @code{ALL_REGS}, just define it as a macro which expands
+to @code{ALL_REGS}.
+
+Order the classes so that if class @var{x} is contained in class @var{y}
+then @var{x} has a lower class number than @var{y}.
+
+The way classes other than @code{GENERAL_REGS} are specified in operand
+constraints is through machine-dependent operand constraint letters.
+You can define such letters to correspond to various classes, then use
+them in operand constraints.
+
+You must define the narrowest register classes for allocatable
+registers, so that each class either has no subclasses, or that for
+some mode, the move cost between registers within the class is
+cheaper than moving a register in the class to or from memory
+(@pxref{Costs}).
+
+You should define a class for the union of two classes whenever some
+instruction allows both classes.  For example, if an instruction allows
+either a floating point (coprocessor) register or a general register for a
+certain operand, you should define a class @code{FLOAT_OR_GENERAL_REGS}
+which includes both of them.  Otherwise you will get suboptimal code,
+or even internal compiler errors when reload cannot find a register in the
+class computed via @code{reg_class_subunion}.
+
+You must also specify certain redundant information about the register
+classes: for each class, which classes contain it and which ones are
+contained in it; for each pair of classes, the largest class contained
+in their union.
+
+When a value occupying several consecutive registers is expected in a
+certain class, all the registers used must belong to that class.
+Therefore, register classes cannot be used to enforce a requirement for
+a register pair to start with an even-numbered register.  The way to
+specify this requirement is with @code{TARGET_HARD_REGNO_MODE_OK}.
+
+Register classes used for input-operands of bitwise-and or shift
+instructions have a special requirement: each such class must have, for
+each fixed-point machine mode, a subclass whose registers can transfer that
+mode to or from memory.  For example, on some machines, the operations for
+single-byte values (@code{QImode}) are limited to certain registers.  When
+this is so, each register class that is used in a bitwise-and or shift
+instruction must have a subclass consisting of registers from which
+single-byte values can be loaded or stored.  This is so that
+@code{PREFERRED_RELOAD_CLASS} can always have a possible value to return.
+
+@deftp {Data type} {enum reg_class}
+An enumerated type that must be defined with all the register class names
+as enumerated values.  @code{NO_REGS} must be first.  @code{ALL_REGS}
+must be the last register class, followed by one more enumerated value,
+@code{LIM_REG_CLASSES}, which is not a register class but rather
+tells how many classes there are.
+
+Each register class has a number, which is the value of casting
+the class name to type @code{int}.  The number serves as an index
+in many of the tables described below.
+@end deftp
+
+@defmac N_REG_CLASSES
+The number of distinct register classes, defined as follows:
+
+@smallexample
+#define N_REG_CLASSES (int) LIM_REG_CLASSES
+@end smallexample
+@end defmac
+
+@defmac REG_CLASS_NAMES
+An initializer containing the names of the register classes as C string
+constants.  These names are used in writing some of the debugging dumps.
+@end defmac
+
+@defmac REG_CLASS_CONTENTS
+An initializer containing the contents of the register classes, as integers
+which are bit masks.  The @var{n}th integer specifies the contents of class
+@var{n}.  The way the integer @var{mask} is interpreted is that
+register @var{r} is in the class if @code{@var{mask} & (1 << @var{r})} is 1.
+
+When the machine has more than 32 registers, an integer does not suffice.
+Then the integers are replaced by sub-initializers, braced groupings containing
+several integers.  Each sub-initializer must be suitable as an initializer
+for the type @code{HARD_REG_SET} which is defined in @file{hard-reg-set.h}.
+In this situation, the first integer in each sub-initializer corresponds to
+registers 0 through 31, the second integer to registers 32 through 63, and
+so on.
+@end defmac
+
+@defmac REGNO_REG_CLASS (@var{regno})
+A C expression whose value is a register class containing hard register
+@var{regno}.  In general there is more than one such class; choose a class
+which is @dfn{minimal}, meaning that no smaller class also contains the
+register.
+@end defmac
+
+@defmac BASE_REG_CLASS
+A macro whose definition is the name of the class to which a valid
+base register must belong.  A base register is one used in an address
+which is the register value plus a displacement.
+@end defmac
+
+@defmac MODE_BASE_REG_CLASS (@var{mode})
+This is a variation of the @code{BASE_REG_CLASS} macro which allows
+the selection of a base register in a mode dependent manner.  If
+@var{mode} is VOIDmode then it should return the same value as
+@code{BASE_REG_CLASS}.
+@end defmac
+
+@defmac MODE_BASE_REG_REG_CLASS (@var{mode})
+A C expression whose value is the register class to which a valid
+base register must belong in order to be used in a base plus index
+register address.  You should define this macro if base plus index
+addresses have different requirements than other base register uses.
+@end defmac
+
+@defmac MODE_CODE_BASE_REG_CLASS (@var{mode}, @var{address_space}, @var{outer_code}, @var{index_code})
+A C expression whose value is the register class to which a valid
+base register for a memory reference in mode @var{mode} to address
+space @var{address_space} must belong.  @var{outer_code} and @var{index_code}
+define the context in which the base register occurs.  @var{outer_code} is
+the code of the immediately enclosing expression (@code{MEM} for the top level
+of an address, @code{ADDRESS} for something that occurs in an
+@code{address_operand}).  @var{index_code} is the code of the corresponding
+index expression if @var{outer_code} is @code{PLUS}; @code{SCRATCH} otherwise.
+@end defmac
+
+@defmac INDEX_REG_CLASS
+A macro whose definition is the name of the class to which a valid
+index register must belong.  An index register is one used in an
+address where its value is either multiplied by a scale factor or
+added to another register (as well as added to a displacement).
+@end defmac
+
+@defmac REGNO_OK_FOR_BASE_P (@var{num})
+A C expression which is nonzero if register number @var{num} is
+suitable for use as a base register in operand addresses.
+@end defmac
+
+@defmac REGNO_MODE_OK_FOR_BASE_P (@var{num}, @var{mode})
+A C expression that is just like @code{REGNO_OK_FOR_BASE_P}, except that
+that expression may examine the mode of the memory reference in
+@var{mode}.  You should define this macro if the mode of the memory
+reference affects whether a register may be used as a base register.  If
+you define this macro, the compiler will use it instead of
+@code{REGNO_OK_FOR_BASE_P}.  The mode may be @code{VOIDmode} for
+addresses that appear outside a @code{MEM}, i.e., as an
+@code{address_operand}.
+@end defmac
+
+@defmac REGNO_MODE_OK_FOR_REG_BASE_P (@var{num}, @var{mode})
+A C expression which is nonzero if register number @var{num} is suitable for
+use as a base register in base plus index operand addresses, accessing
+memory in mode @var{mode}.  It may be either a suitable hard register or a
+pseudo register that has been allocated such a hard register.  You should
+define this macro if base plus index addresses have different requirements
+than other base register uses.
+
+Use of this macro is deprecated; please use the more general
+@code{REGNO_MODE_CODE_OK_FOR_BASE_P}.
+@end defmac
+
+@defmac REGNO_MODE_CODE_OK_FOR_BASE_P (@var{num}, @var{mode}, @var{address_space}, @var{outer_code}, @var{index_code})
+A C expression which is nonzero if register number @var{num} is
+suitable for use as a base register in operand addresses, accessing
+memory in mode @var{mode} in address space @var{address_space}.
+This is similar to @code{REGNO_MODE_OK_FOR_BASE_P}, except
+that that expression may examine the context in which the register
+appears in the memory reference.  @var{outer_code} is the code of the
+immediately enclosing expression (@code{MEM} if at the top level of the
+address, @code{ADDRESS} for something that occurs in an
+@code{address_operand}).  @var{index_code} is the code of the
+corresponding index expression if @var{outer_code} is @code{PLUS};
+@code{SCRATCH} otherwise.  The mode may be @code{VOIDmode} for addresses
+that appear outside a @code{MEM}, i.e., as an @code{address_operand}.
+@end defmac
+
+@defmac REGNO_OK_FOR_INDEX_P (@var{num})
+A C expression which is nonzero if register number @var{num} is
+suitable for use as an index register in operand addresses.  It may be
+either a suitable hard register or a pseudo register that has been
+allocated such a hard register.
+
+The difference between an index register and a base register is that
+the index register may be scaled.  If an address involves the sum of
+two registers, neither one of them scaled, then either one may be
+labeled the ``base'' and the other the ``index''; but whichever
+labeling is used must fit the machine's constraints of which registers
+may serve in each capacity.  The compiler will try both labelings,
+looking for one that is valid, and will reload one or both registers
+only if neither labeling works.
+@end defmac
+
+@deftypefn {Target Hook} reg_class_t TARGET_PREFERRED_RENAME_CLASS (reg_class_t @var{rclass})
+A target hook that places additional preference on the register class to use when it is necessary to rename a register in class @var{rclass} to another class, or perhaps @var{NO_REGS}, if no preferred register class is found or hook @code{preferred_rename_class} is not implemented. Sometimes returning a more restrictive class makes better code.  For example, on ARM, thumb-2 instructions using @code{LO_REGS} may be smaller than instructions using @code{GENERIC_REGS}.  By returning @code{LO_REGS} from @code{preferred_rename_class}, code size can be reduced.
+@end deftypefn
+
+@deftypefn {Target Hook} reg_class_t TARGET_PREFERRED_RELOAD_CLASS (rtx @var{x}, reg_class_t @var{rclass})
+A target hook that places additional restrictions on the register class
+to use when it is necessary to copy value @var{x} into a register in class
+@var{rclass}.  The value is a register class; perhaps @var{rclass}, or perhaps
+another, smaller class.
+
+The default version of this hook always returns value of @code{rclass} argument.
+
+Sometimes returning a more restrictive class makes better code.  For
+example, on the 68000, when @var{x} is an integer constant that is in range
+for a @samp{moveq} instruction, the value of this macro is always
+@code{DATA_REGS} as long as @var{rclass} includes the data registers.
+Requiring a data register guarantees that a @samp{moveq} will be used.
+
+One case where @code{TARGET_PREFERRED_RELOAD_CLASS} must not return
+@var{rclass} is if @var{x} is a legitimate constant which cannot be
+loaded into some register class.  By returning @code{NO_REGS} you can
+force @var{x} into a memory location.  For example, rs6000 can load
+immediate values into general-purpose registers, but does not have an
+instruction for loading an immediate value into a floating-point
+register, so @code{TARGET_PREFERRED_RELOAD_CLASS} returns @code{NO_REGS} when
+@var{x} is a floating-point constant.  If the constant can't be loaded
+into any kind of register, code generation will be better if
+@code{TARGET_LEGITIMATE_CONSTANT_P} makes the constant illegitimate instead
+of using @code{TARGET_PREFERRED_RELOAD_CLASS}.
+
+If an insn has pseudos in it after register allocation, reload will go
+through the alternatives and call repeatedly @code{TARGET_PREFERRED_RELOAD_CLASS}
+to find the best one.  Returning @code{NO_REGS}, in this case, makes
+reload add a @code{!} in front of the constraint: the x86 back-end uses
+this feature to discourage usage of 387 registers when math is done in
+the SSE registers (and vice versa).
+@end deftypefn
+
+@defmac PREFERRED_RELOAD_CLASS (@var{x}, @var{class})
+A C expression that places additional restrictions on the register class
+to use when it is necessary to copy value @var{x} into a register in class
+@var{class}.  The value is a register class; perhaps @var{class}, or perhaps
+another, smaller class.  On many machines, the following definition is
+safe:
+
+@smallexample
+#define PREFERRED_RELOAD_CLASS(X,CLASS) CLASS
+@end smallexample
+
+Sometimes returning a more restrictive class makes better code.  For
+example, on the 68000, when @var{x} is an integer constant that is in range
+for a @samp{moveq} instruction, the value of this macro is always
+@code{DATA_REGS} as long as @var{class} includes the data registers.
+Requiring a data register guarantees that a @samp{moveq} will be used.
+
+One case where @code{PREFERRED_RELOAD_CLASS} must not return
+@var{class} is if @var{x} is a legitimate constant which cannot be
+loaded into some register class.  By returning @code{NO_REGS} you can
+force @var{x} into a memory location.  For example, rs6000 can load
+immediate values into general-purpose registers, but does not have an
+instruction for loading an immediate value into a floating-point
+register, so @code{PREFERRED_RELOAD_CLASS} returns @code{NO_REGS} when
+@var{x} is a floating-point constant.  If the constant cannot be loaded
+into any kind of register, code generation will be better if
+@code{TARGET_LEGITIMATE_CONSTANT_P} makes the constant illegitimate instead
+of using @code{TARGET_PREFERRED_RELOAD_CLASS}.
+
+If an insn has pseudos in it after register allocation, reload will go
+through the alternatives and call repeatedly @code{PREFERRED_RELOAD_CLASS}
+to find the best one.  Returning @code{NO_REGS}, in this case, makes
+reload add a @code{!} in front of the constraint: the x86 back-end uses
+this feature to discourage usage of 387 registers when math is done in
+the SSE registers (and vice versa).
+@end defmac
+
+@deftypefn {Target Hook} reg_class_t TARGET_PREFERRED_OUTPUT_RELOAD_CLASS (rtx @var{x}, reg_class_t @var{rclass})
+Like @code{TARGET_PREFERRED_RELOAD_CLASS}, but for output reloads instead of
+input reloads.
+
+The default version of this hook always returns value of @code{rclass}
+argument.
+
+You can also use @code{TARGET_PREFERRED_OUTPUT_RELOAD_CLASS} to discourage
+reload from using some alternatives, like @code{TARGET_PREFERRED_RELOAD_CLASS}.
+@end deftypefn
+
+@defmac LIMIT_RELOAD_CLASS (@var{mode}, @var{class})
+A C expression that places additional restrictions on the register class
+to use when it is necessary to be able to hold a value of mode
+@var{mode} in a reload register for which class @var{class} would
+ordinarily be used.
+
+Unlike @code{PREFERRED_RELOAD_CLASS}, this macro should be used when
+there are certain modes that simply cannot go in certain reload classes.
+
+The value is a register class; perhaps @var{class}, or perhaps another,
+smaller class.
+
+Don't define this macro unless the target machine has limitations which
+require the macro to do something nontrivial.
+@end defmac
+
+@deftypefn {Target Hook} reg_class_t TARGET_SECONDARY_RELOAD (bool @var{in_p}, rtx @var{x}, reg_class_t @var{reload_class}, machine_mode @var{reload_mode}, secondary_reload_info *@var{sri})
+Many machines have some registers that cannot be copied directly to or
+from memory or even from other types of registers.  An example is the
+@samp{MQ} register, which on most machines, can only be copied to or
+from general registers, but not memory.  Below, we shall be using the
+term 'intermediate register' when a move operation cannot be performed
+directly, but has to be done by copying the source into the intermediate
+register first, and then copying the intermediate register to the
+destination.  An intermediate register always has the same mode as
+source and destination.  Since it holds the actual value being copied,
+reload might apply optimizations to re-use an intermediate register
+and eliding the copy from the source when it can determine that the
+intermediate register still holds the required value.
+
+Another kind of secondary reload is required on some machines which
+allow copying all registers to and from memory, but require a scratch
+register for stores to some memory locations (e.g., those with symbolic
+address on the RT, and those with certain symbolic address on the SPARC
+when compiling PIC)@.  Scratch registers need not have the same mode
+as the value being copied, and usually hold a different value than
+that being copied.  Special patterns in the md file are needed to
+describe how the copy is performed with the help of the scratch register;
+these patterns also describe the number, register class(es) and mode(s)
+of the scratch register(s).
+
+In some cases, both an intermediate and a scratch register are required.
+
+For input reloads, this target hook is called with nonzero @var{in_p},
+and @var{x} is an rtx that needs to be copied to a register of class
+@var{reload_class} in @var{reload_mode}.  For output reloads, this target
+hook is called with zero @var{in_p}, and a register of class @var{reload_class}
+needs to be copied to rtx @var{x} in @var{reload_mode}.
+
+If copying a register of @var{reload_class} from/to @var{x} requires
+an intermediate register, the hook @code{secondary_reload} should
+return the register class required for this intermediate register.
+If no intermediate register is required, it should return NO_REGS.
+If more than one intermediate register is required, describe the one
+that is closest in the copy chain to the reload register.
+
+If scratch registers are needed, you also have to describe how to
+perform the copy from/to the reload register to/from this
+closest intermediate register.  Or if no intermediate register is
+required, but still a scratch register is needed, describe the
+copy  from/to the reload register to/from the reload operand @var{x}.
+
+You do this by setting @code{sri->icode} to the instruction code of a pattern
+in the md file which performs the move.  Operands 0 and 1 are the output
+and input of this copy, respectively.  Operands from operand 2 onward are
+for scratch operands.  These scratch operands must have a mode, and a
+single-register-class
+@c [later: or memory]
+output constraint.
+
+When an intermediate register is used, the @code{secondary_reload}
+hook will be called again to determine how to copy the intermediate
+register to/from the reload operand @var{x}, so your hook must also
+have code to handle the register class of the intermediate operand.
+
+@c [For later: maybe we'll allow multi-alternative reload patterns -
+@c   the port maintainer could name a mov<mode> pattern that has clobbers -
+@c   and match the constraints of input and output to determine the required
+@c   alternative.  A restriction would be that constraints used to match
+@c   against reloads registers would have to be written as register class
+@c   constraints, or we need a new target macro / hook that tells us if an
+@c   arbitrary constraint can match an unknown register of a given class.
+@c   Such a macro / hook would also be useful in other places.]
+
+
+@var{x} might be a pseudo-register or a @code{subreg} of a
+pseudo-register, which could either be in a hard register or in memory.
+Use @code{true_regnum} to find out; it will return @minus{}1 if the pseudo is
+in memory and the hard register number if it is in a register.
+
+Scratch operands in memory (constraint @code{"=m"} / @code{"=&m"}) are
+currently not supported.  For the time being, you will have to continue
+to use @code{TARGET_SECONDARY_MEMORY_NEEDED} for that purpose.
+
+@code{copy_cost} also uses this target hook to find out how values are
+copied.  If you want it to include some extra cost for the need to allocate
+(a) scratch register(s), set @code{sri->extra_cost} to the additional cost.
+Or if two dependent moves are supposed to have a lower cost than the sum
+of the individual moves due to expected fortuitous scheduling and/or special
+forwarding logic, you can set @code{sri->extra_cost} to a negative amount.
+@end deftypefn
+
+@defmac SECONDARY_RELOAD_CLASS (@var{class}, @var{mode}, @var{x})
+@defmacx SECONDARY_INPUT_RELOAD_CLASS (@var{class}, @var{mode}, @var{x})
+@defmacx SECONDARY_OUTPUT_RELOAD_CLASS (@var{class}, @var{mode}, @var{x})
+These macros are obsolete, new ports should use the target hook
+@code{TARGET_SECONDARY_RELOAD} instead.
+
+These are obsolete macros, replaced by the @code{TARGET_SECONDARY_RELOAD}
+target hook.  Older ports still define these macros to indicate to the
+reload phase that it may
+need to allocate at least one register for a reload in addition to the
+register to contain the data.  Specifically, if copying @var{x} to a
+register @var{class} in @var{mode} requires an intermediate register,
+you were supposed to define @code{SECONDARY_INPUT_RELOAD_CLASS} to return the
+largest register class all of whose registers can be used as
+intermediate registers or scratch registers.
+
+If copying a register @var{class} in @var{mode} to @var{x} requires an
+intermediate or scratch register, @code{SECONDARY_OUTPUT_RELOAD_CLASS}
+was supposed to be defined be defined to return the largest register
+class required.  If the
+requirements for input and output reloads were the same, the macro
+@code{SECONDARY_RELOAD_CLASS} should have been used instead of defining both
+macros identically.
+
+The values returned by these macros are often @code{GENERAL_REGS}.
+Return @code{NO_REGS} if no spare register is needed; i.e., if @var{x}
+can be directly copied to or from a register of @var{class} in
+@var{mode} without requiring a scratch register.  Do not define this
+macro if it would always return @code{NO_REGS}.
+
+If a scratch register is required (either with or without an
+intermediate register), you were supposed to define patterns for
+@samp{reload_in@var{m}} or @samp{reload_out@var{m}}, as required
+(@pxref{Standard Names}.  These patterns, which were normally
+implemented with a @code{define_expand}, should be similar to the
+@samp{mov@var{m}} patterns, except that operand 2 is the scratch
+register.
+
+These patterns need constraints for the reload register and scratch
+register that
+contain a single register class.  If the original reload register (whose
+class is @var{class}) can meet the constraint given in the pattern, the
+value returned by these macros is used for the class of the scratch
+register.  Otherwise, two additional reload registers are required.
+Their classes are obtained from the constraints in the insn pattern.
+
+@var{x} might be a pseudo-register or a @code{subreg} of a
+pseudo-register, which could either be in a hard register or in memory.
+Use @code{true_regnum} to find out; it will return @minus{}1 if the pseudo is
+in memory and the hard register number if it is in a register.
+
+These macros should not be used in the case where a particular class of
+registers can only be copied to memory and not to another class of
+registers.  In that case, secondary reload registers are not needed and
+would not be helpful.  Instead, a stack location must be used to perform
+the copy and the @code{mov@var{m}} pattern should use memory as an
+intermediate storage.  This case often occurs between floating-point and
+general registers.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_SECONDARY_MEMORY_NEEDED (machine_mode @var{mode}, reg_class_t @var{class1}, reg_class_t @var{class2})
+Certain machines have the property that some registers cannot be copied
+to some other registers without using memory.  Define this hook on
+those machines to return true if objects of mode @var{m} in registers
+of @var{class1} can only be copied to registers of class @var{class2} by
+ storing a register of @var{class1} into memory and loading that memory
+location into a register of @var{class2}.  The default definition returns
+false for all inputs.
+@end deftypefn
+
+@defmac SECONDARY_MEMORY_NEEDED_RTX (@var{mode})
+Normally when @code{TARGET_SECONDARY_MEMORY_NEEDED} is defined, the compiler
+allocates a stack slot for a memory location needed for register copies.
+If this macro is defined, the compiler instead uses the memory location
+defined by this macro.
+
+Do not define this macro if you do not define
+@code{TARGET_SECONDARY_MEMORY_NEEDED}.
+@end defmac
+
+@deftypefn {Target Hook} machine_mode TARGET_SECONDARY_MEMORY_NEEDED_MODE (machine_mode @var{mode})
+If @code{TARGET_SECONDARY_MEMORY_NEEDED} tells the compiler to use memory
+when moving between two particular registers of mode @var{mode},
+this hook specifies the mode that the memory should have.
+
+The default depends on @code{TARGET_LRA_P}.  Without LRA, the default
+is to use a word-sized mode for integral modes that are smaller than a
+a word.  This is right thing to do on most machines because it ensures
+that all bits of the register are copied and prevents accesses to the
+registers in a narrower mode, which some machines prohibit for
+floating-point registers.
+
+However, this default behavior is not correct on some machines, such as
+the DEC Alpha, that store short integers in floating-point registers
+differently than in integer registers.  On those machines, the default
+widening will not work correctly and you must define this hook to
+suppress that widening in some cases.  See the file @file{alpha.c} for
+details.
+
+With LRA, the default is to use @var{mode} unmodified.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SELECT_EARLY_REMAT_MODES (sbitmap @var{modes})
+On some targets, certain modes cannot be held in registers around a
+standard ABI call and are relatively expensive to spill to the stack.
+The early rematerialization pass can help in such cases by aggressively
+recomputing values after calls, so that they don't need to be spilled.
+
+This hook returns the set of such modes by setting the associated bits
+in @var{modes}.  The default implementation selects no modes, which has
+the effect of disabling the early rematerialization pass.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_CLASS_LIKELY_SPILLED_P (reg_class_t @var{rclass})
+A target hook which returns @code{true} if pseudos that have been assigned
+to registers of class @var{rclass} would likely be spilled because
+registers of @var{rclass} are needed for spill registers.
+
+The default version of this target hook returns @code{true} if @var{rclass}
+has exactly one register and @code{false} otherwise.  On most machines, this
+default should be used.  For generally register-starved machines, such as
+i386, or machines with right register constraints, such as SH, this hook
+can be used to avoid excessive spilling.
+
+This hook is also used by some of the global intra-procedural code
+transformations to throtle code motion, to avoid increasing register
+pressure.
+@end deftypefn
+
+@deftypefn {Target Hook} {unsigned char} TARGET_CLASS_MAX_NREGS (reg_class_t @var{rclass}, machine_mode @var{mode})
+A target hook returns the maximum number of consecutive registers
+of class @var{rclass} needed to hold a value of mode @var{mode}.
+
+This is closely related to the macro @code{TARGET_HARD_REGNO_NREGS}.
+In fact, the value returned by @code{TARGET_CLASS_MAX_NREGS (@var{rclass},
+@var{mode})} target hook should be the maximum value of
+@code{TARGET_HARD_REGNO_NREGS (@var{regno}, @var{mode})} for all @var{regno}
+values in the class @var{rclass}.
+
+This target hook helps control the handling of multiple-word values
+in the reload pass.
+
+The default version of this target hook returns the size of @var{mode}
+in words.
+@end deftypefn
+
+@defmac CLASS_MAX_NREGS (@var{class}, @var{mode})
+A C expression for the maximum number of consecutive registers
+of class @var{class} needed to hold a value of mode @var{mode}.
+
+This is closely related to the macro @code{TARGET_HARD_REGNO_NREGS}.  In fact,
+the value of the macro @code{CLASS_MAX_NREGS (@var{class}, @var{mode})}
+should be the maximum value of @code{TARGET_HARD_REGNO_NREGS (@var{regno},
+@var{mode})} for all @var{regno} values in the class @var{class}.
+
+This macro helps control the handling of multiple-word values
+in the reload pass.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_CAN_CHANGE_MODE_CLASS (machine_mode @var{from}, machine_mode @var{to}, reg_class_t @var{rclass})
+This hook returns true if it is possible to bitcast values held in
+registers of class @var{rclass} from mode @var{from} to mode @var{to}
+and if doing so preserves the low-order bits that are common to both modes.
+The result is only meaningful if @var{rclass} has registers that can hold
+both @code{from} and @code{to}.  The default implementation returns true.
+
+As an example of when such bitcasting is invalid, loading 32-bit integer or
+floating-point objects into floating-point registers on Alpha extends them
+to 64 bits.  Therefore loading a 64-bit object and then storing it as a
+32-bit object does not store the low-order 32 bits, as would be the case
+for a normal register.  Therefore, @file{alpha.h} defines
+@code{TARGET_CAN_CHANGE_MODE_CLASS} to return:
+
+@smallexample
+(GET_MODE_SIZE (from) == GET_MODE_SIZE (to)
+ || !reg_classes_intersect_p (FLOAT_REGS, rclass))
+@end smallexample
+
+Even if storing from a register in mode @var{to} would be valid,
+if both @var{from} and @code{raw_reg_mode} for @var{rclass} are wider
+than @code{word_mode}, then we must prevent @var{to} narrowing the
+mode.  This happens when the middle-end assumes that it can load
+or store pieces of an @var{N}-word pseudo, and that the pseudo will
+eventually be allocated to @var{N} @code{word_mode} hard registers.
+Failure to prevent this kind of mode change will result in the
+entire @code{raw_reg_mode} being modified instead of the partial
+value that the middle-end intended.
+@end deftypefn
+
+@deftypefn {Target Hook} reg_class_t TARGET_IRA_CHANGE_PSEUDO_ALLOCNO_CLASS (int, @var{reg_class_t}, @var{reg_class_t})
+A target hook which can change allocno class for given pseudo from
+  allocno and best class calculated by IRA.
+  
+  The default version of this target hook always returns given class.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_LRA_P (void)
+A target hook which returns true if we use LRA instead of reload pass.    The default version of this target hook returns true.  New ports  should use LRA, and existing ports are encouraged to convert.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_REGISTER_PRIORITY (int)
+A target hook which returns the register priority number to which the  register @var{hard_regno} belongs to.  The bigger the number, the  more preferable the hard register usage (when all other conditions are  the same).  This hook can be used to prefer some hard register over  others in LRA.  For example, some x86-64 register usage needs  additional prefix which makes instructions longer.  The hook can  return lower priority number for such registers make them less favorable  and as result making the generated code smaller.    The default version of this target hook returns always zero.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_REGISTER_USAGE_LEVELING_P (void)
+A target hook which returns true if we need register usage leveling.  That means if a few hard registers are equally good for the  assignment, we choose the least used hard register.  The register  usage leveling may be profitable for some targets.  Don't use the  usage leveling for targets with conditional execution or targets  with big register files as it hurts if-conversion and cross-jumping  optimizations.    The default version of this target hook returns always false.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_DIFFERENT_ADDR_DISPLACEMENT_P (void)
+A target hook which returns true if an address with the same structure  can have different maximal legitimate displacement.  For example, the  displacement can depend on memory mode or on operand combinations in  the insn.    The default version of this target hook returns always false.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_CANNOT_SUBSTITUTE_MEM_EQUIV_P (rtx @var{subst})
+A target hook which returns @code{true} if @var{subst} can't
+substitute safely pseudos with equivalent memory values during
+register allocation.
+The default version of this target hook returns @code{false}.
+On most machines, this default should be used.  For generally
+machines with non orthogonal register usage for addressing, such
+as SH, this hook can be used to avoid excessive spilling.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_LEGITIMIZE_ADDRESS_DISPLACEMENT (rtx *@var{offset1}, rtx *@var{offset2}, poly_int64 @var{orig_offset}, machine_mode @var{mode})
+This hook tries to split address offset @var{orig_offset} into
+two parts: one that should be added to the base address to create
+a local anchor point, and an additional offset that can be applied
+to the anchor to address a value of mode @var{mode}.  The idea is that
+the local anchor could be shared by other accesses to nearby locations.
+
+The hook returns true if it succeeds, storing the offset of the
+anchor from the base in @var{offset1} and the offset of the final address
+from the anchor in @var{offset2}.  The default implementation returns false.
+@end deftypefn
+
+@deftypefn {Target Hook} reg_class_t TARGET_SPILL_CLASS (reg_class_t, @var{machine_mode})
+This hook defines a class of registers which could be used for spilling  pseudos of the given mode and class, or @code{NO_REGS} if only memory  should be used.  Not defining this hook is equivalent to returning  @code{NO_REGS} for all inputs.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_ADDITIONAL_ALLOCNO_CLASS_P (reg_class_t)
+This hook should return @code{true} if given class of registers should  be an allocno class in any way.  Usually RA uses only one register  class from all classes containing the same register set.  In some  complicated cases, you need to have two or more such classes as  allocno ones for RA correct work.  Not defining this hook is  equivalent to returning @code{false} for all inputs.
+@end deftypefn
+
+@deftypefn {Target Hook} scalar_int_mode TARGET_CSTORE_MODE (enum insn_code @var{icode})
+This hook defines the machine mode to use for the boolean result of  conditional store patterns.  The ICODE argument is the instruction code  for the cstore being performed.  Not definiting this hook is the same  as accepting the mode encoded into operand 0 of the cstore expander  patterns.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_COMPUTE_PRESSURE_CLASSES (enum reg_class *@var{pressure_classes})
+A target hook which lets a backend compute the set of pressure classes to  be used by those optimization passes which take register pressure into  account, as opposed to letting IRA compute them.  It returns the number of  register classes stored in the array @var{pressure_classes}.
+@end deftypefn
+
+@node Stack and Calling
+@section Stack Layout and Calling Conventions
+@cindex calling conventions
+
+@c prevent bad page break with this line
+This describes the stack layout and calling conventions.
+
+@menu
+* Frame Layout::
+* Exception Handling::
+* Stack Checking::
+* Frame Registers::
+* Elimination::
+* Stack Arguments::
+* Register Arguments::
+* Scalar Return::
+* Aggregate Return::
+* Caller Saves::
+* Function Entry::
+* Profiling::
+* Tail Calls::
+* Shrink-wrapping separate components::
+* Stack Smashing Protection::
+* Miscellaneous Register Hooks::
+@end menu
+
+@node Frame Layout
+@subsection Basic Stack Layout
+@cindex stack frame layout
+@cindex frame layout
+
+@c prevent bad page break with this line
+Here is the basic stack layout.
+
+@defmac STACK_GROWS_DOWNWARD
+Define this macro to be true if pushing a word onto the stack moves the stack
+pointer to a smaller address, and false otherwise.
+@end defmac
+
+@defmac STACK_PUSH_CODE
+This macro defines the operation used when something is pushed
+on the stack.  In RTL, a push operation will be
+@code{(set (mem (STACK_PUSH_CODE (reg sp))) @dots{})}
+
+The choices are @code{PRE_DEC}, @code{POST_DEC}, @code{PRE_INC},
+and @code{POST_INC}.  Which of these is correct depends on
+the stack direction and on whether the stack pointer points
+to the last item on the stack or whether it points to the
+space for the next item on the stack.
+
+The default is @code{PRE_DEC} when @code{STACK_GROWS_DOWNWARD} is
+true, which is almost always right, and @code{PRE_INC} otherwise,
+which is often wrong.
+@end defmac
+
+@defmac FRAME_GROWS_DOWNWARD
+Define this macro to nonzero value if the addresses of local variable slots
+are at negative offsets from the frame pointer.
+@end defmac
+
+@defmac ARGS_GROW_DOWNWARD
+Define this macro if successive arguments to a function occupy decreasing
+addresses on the stack.
+@end defmac
+
+@deftypefn {Target Hook} HOST_WIDE_INT TARGET_STARTING_FRAME_OFFSET (void)
+This hook returns the offset from the frame pointer to the first local
+variable slot to be allocated.  If @code{FRAME_GROWS_DOWNWARD}, it is the
+offset to @emph{end} of the first slot allocated, otherwise it is the
+offset to @emph{beginning} of the first slot allocated.  The default
+implementation returns 0.
+@end deftypefn
+
+@defmac STACK_ALIGNMENT_NEEDED
+Define to zero to disable final alignment of the stack during reload.
+The nonzero default for this macro is suitable for most ports.
+
+On ports where @code{TARGET_STARTING_FRAME_OFFSET} is nonzero or where there
+is a register save block following the local block that doesn't require
+alignment to @code{STACK_BOUNDARY}, it may be beneficial to disable
+stack alignment and do it in the backend.
+@end defmac
+
+@defmac STACK_POINTER_OFFSET
+Offset from the stack pointer register to the first location at which
+outgoing arguments are placed.  If not specified, the default value of
+zero is used.  This is the proper value for most machines.
+
+If @code{ARGS_GROW_DOWNWARD}, this is the offset to the location above
+the first location at which outgoing arguments are placed.
+@end defmac
+
+@defmac FIRST_PARM_OFFSET (@var{fundecl})
+Offset from the argument pointer register to the first argument's
+address.  On some machines it may depend on the data type of the
+function.
+
+If @code{ARGS_GROW_DOWNWARD}, this is the offset to the location above
+the first argument's address.
+@end defmac
+
+@defmac STACK_DYNAMIC_OFFSET (@var{fundecl})
+Offset from the stack pointer register to an item dynamically allocated
+on the stack, e.g., by @code{alloca}.
+
+The default value for this macro is @code{STACK_POINTER_OFFSET} plus the
+length of the outgoing arguments.  The default is correct for most
+machines.  See @file{function.c} for details.
+@end defmac
+
+@defmac INITIAL_FRAME_ADDRESS_RTX
+A C expression whose value is RTL representing the address of the initial
+stack frame. This address is passed to @code{RETURN_ADDR_RTX} and
+@code{DYNAMIC_CHAIN_ADDRESS}.  If you don't define this macro, a reasonable
+default value will be used.  Define this macro in order to make frame pointer
+elimination work in the presence of @code{__builtin_frame_address (count)} and
+@code{__builtin_return_address (count)} for @code{count} not equal to zero.
+@end defmac
+
+@defmac DYNAMIC_CHAIN_ADDRESS (@var{frameaddr})
+A C expression whose value is RTL representing the address in a stack
+frame where the pointer to the caller's frame is stored.  Assume that
+@var{frameaddr} is an RTL expression for the address of the stack frame
+itself.
+
+If you don't define this macro, the default is to return the value
+of @var{frameaddr}---that is, the stack frame address is also the
+address of the stack word that points to the previous frame.
+@end defmac
+
+@defmac SETUP_FRAME_ADDRESSES
+A C expression that produces the machine-specific code to
+setup the stack so that arbitrary frames can be accessed.  For example,
+on the SPARC, we must flush all of the register windows to the stack
+before we can access arbitrary stack frames.  You will seldom need to
+define this macro.  The default is to do nothing.
+@end defmac
+
+@deftypefn {Target Hook} rtx TARGET_BUILTIN_SETJMP_FRAME_VALUE (void)
+This target hook should return an rtx that is used to store
+the address of the current frame into the built in @code{setjmp} buffer.
+The default value, @code{virtual_stack_vars_rtx}, is correct for most
+machines.  One reason you may need to define this target hook is if
+@code{hard_frame_pointer_rtx} is the appropriate value on your machine.
+@end deftypefn
+
+@defmac FRAME_ADDR_RTX (@var{frameaddr})
+A C expression whose value is RTL representing the value of the frame
+address for the current frame.  @var{frameaddr} is the frame pointer
+of the current frame.  This is used for __builtin_frame_address.
+You need only define this macro if the frame address is not the same
+as the frame pointer.  Most machines do not need to define it.
+@end defmac
+
+@defmac RETURN_ADDR_RTX (@var{count}, @var{frameaddr})
+A C expression whose value is RTL representing the value of the return
+address for the frame @var{count} steps up from the current frame, after
+the prologue.  @var{frameaddr} is the frame pointer of the @var{count}
+frame, or the frame pointer of the @var{count} @minus{} 1 frame if
+@code{RETURN_ADDR_IN_PREVIOUS_FRAME} is nonzero.
+
+The value of the expression must always be the correct address when
+@var{count} is zero, but may be @code{NULL_RTX} if there is no way to
+determine the return address of other frames.
+@end defmac
+
+@defmac RETURN_ADDR_IN_PREVIOUS_FRAME
+Define this macro to nonzero value if the return address of a particular
+stack frame is accessed from the frame pointer of the previous stack
+frame.  The zero default for this macro is suitable for most ports.
+@end defmac
+
+@defmac INCOMING_RETURN_ADDR_RTX
+A C expression whose value is RTL representing the location of the
+incoming return address at the beginning of any function, before the
+prologue.  This RTL is either a @code{REG}, indicating that the return
+value is saved in @samp{REG}, or a @code{MEM} representing a location in
+the stack.
+
+You only need to define this macro if you want to support call frame
+debugging information like that provided by DWARF 2.
+
+If this RTL is a @code{REG}, you should also define
+@code{DWARF_FRAME_RETURN_COLUMN} to @code{DWARF_FRAME_REGNUM (REGNO)}.
+@end defmac
+
+@defmac DWARF_ALT_FRAME_RETURN_COLUMN
+A C expression whose value is an integer giving a DWARF 2 column
+number that may be used as an alternative return column.  The column
+must not correspond to any gcc hard register (that is, it must not
+be in the range of @code{DWARF_FRAME_REGNUM}).
+
+This macro can be useful if @code{DWARF_FRAME_RETURN_COLUMN} is set to a
+general register, but an alternative column needs to be used for signal
+frames.  Some targets have also used different frame return columns
+over time.
+@end defmac
+
+@defmac DWARF_ZERO_REG
+A C expression whose value is an integer giving a DWARF 2 register
+number that is considered to always have the value zero.  This should
+only be defined if the target has an architected zero register, and
+someone decided it was a good idea to use that register number to
+terminate the stack backtrace.  New ports should avoid this.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_DWARF_HANDLE_FRAME_UNSPEC (const char *@var{label}, rtx @var{pattern}, int @var{index})
+This target hook allows the backend to emit frame-related insns that
+contain UNSPECs or UNSPEC_VOLATILEs.  The DWARF 2 call frame debugging
+info engine will invoke it on insns of the form
+@smallexample
+(set (reg) (unspec [@dots{}] UNSPEC_INDEX))
+@end smallexample
+and
+@smallexample
+(set (reg) (unspec_volatile [@dots{}] UNSPECV_INDEX)).
+@end smallexample
+to let the backend emit the call frame instructions.  @var{label} is
+the CFI label attached to the insn, @var{pattern} is the pattern of
+the insn and @var{index} is @code{UNSPEC_INDEX} or @code{UNSPECV_INDEX}.
+@end deftypefn
+
+@deftypefn {Target Hook} {unsigned int} TARGET_DWARF_POLY_INDETERMINATE_VALUE (unsigned int @var{i}, unsigned int *@var{factor}, int *@var{offset})
+Express the value of @code{poly_int} indeterminate @var{i} as a DWARF
+expression, with @var{i} counting from 1.  Return the number of a DWARF
+register @var{R} and set @samp{*@var{factor}} and @samp{*@var{offset}} such
+that the value of the indeterminate is:
+@smallexample
+value_of(@var{R}) / @var{factor} - @var{offset}
+@end smallexample
+
+A target only needs to define this hook if it sets
+@samp{NUM_POLY_INT_COEFFS} to a value greater than 1.
+@end deftypefn
+
+@defmac INCOMING_FRAME_SP_OFFSET
+A C expression whose value is an integer giving the offset, in bytes,
+from the value of the stack pointer register to the top of the stack
+frame at the beginning of any function, before the prologue.  The top of
+the frame is defined to be the value of the stack pointer in the
+previous frame, just before the call instruction.
+
+You only need to define this macro if you want to support call frame
+debugging information like that provided by DWARF 2.
+@end defmac
+
+@defmac DEFAULT_INCOMING_FRAME_SP_OFFSET
+Like @code{INCOMING_FRAME_SP_OFFSET}, but must be the same for all
+functions of the same ABI, and when using GAS @code{.cfi_*} directives
+must also agree with the default CFI GAS emits.  Define this macro
+only if @code{INCOMING_FRAME_SP_OFFSET} can have different values
+between different functions of the same ABI or when
+@code{INCOMING_FRAME_SP_OFFSET} does not agree with GAS default CFI.
+@end defmac
+
+@defmac ARG_POINTER_CFA_OFFSET (@var{fundecl})
+A C expression whose value is an integer giving the offset, in bytes,
+from the argument pointer to the canonical frame address (cfa).  The
+final value should coincide with that calculated by
+@code{INCOMING_FRAME_SP_OFFSET}.  Which is unfortunately not usable
+during virtual register instantiation.
+
+The default value for this macro is
+@code{FIRST_PARM_OFFSET (fundecl) + crtl->args.pretend_args_size},
+which is correct for most machines; in general, the arguments are found
+immediately before the stack frame.  Note that this is not the case on
+some targets that save registers into the caller's frame, such as SPARC
+and rs6000, and so such targets need to define this macro.
+
+You only need to define this macro if the default is incorrect, and you
+want to support call frame debugging information like that provided by
+DWARF 2.
+@end defmac
+
+@defmac FRAME_POINTER_CFA_OFFSET (@var{fundecl})
+If defined, a C expression whose value is an integer giving the offset
+in bytes from the frame pointer to the canonical frame address (cfa).
+The final value should coincide with that calculated by
+@code{INCOMING_FRAME_SP_OFFSET}.
+
+Normally the CFA is calculated as an offset from the argument pointer,
+via @code{ARG_POINTER_CFA_OFFSET}, but if the argument pointer is
+variable due to the ABI, this may not be possible.  If this macro is
+defined, it implies that the virtual register instantiation should be
+based on the frame pointer instead of the argument pointer.  Only one
+of @code{FRAME_POINTER_CFA_OFFSET} and @code{ARG_POINTER_CFA_OFFSET}
+should be defined.
+@end defmac
+
+@defmac CFA_FRAME_BASE_OFFSET (@var{fundecl})
+If defined, a C expression whose value is an integer giving the offset
+in bytes from the canonical frame address (cfa) to the frame base used
+in DWARF 2 debug information.  The default is zero.  A different value
+may reduce the size of debug information on some ports.
+@end defmac
+
+@node Exception Handling
+@subsection Exception Handling Support
+@cindex exception handling
+
+@defmac EH_RETURN_DATA_REGNO (@var{N})
+A C expression whose value is the @var{N}th register number used for
+data by exception handlers, or @code{INVALID_REGNUM} if fewer than
+@var{N} registers are usable.
+
+The exception handling library routines communicate with the exception
+handlers via a set of agreed upon registers.  Ideally these registers
+should be call-clobbered; it is possible to use call-saved registers,
+but may negatively impact code size.  The target must support at least
+2 data registers, but should define 4 if there are enough free registers.
+
+You must define this macro if you want to support call frame exception
+handling like that provided by DWARF 2.
+@end defmac
+
+@defmac EH_RETURN_STACKADJ_RTX
+A C expression whose value is RTL representing a location in which
+to store a stack adjustment to be applied before function return.
+This is used to unwind the stack to an exception handler's call frame.
+It will be assigned zero on code paths that return normally.
+
+Typically this is a call-clobbered hard register that is otherwise
+untouched by the epilogue, but could also be a stack slot.
+
+Do not define this macro if the stack pointer is saved and restored
+by the regular prolog and epilog code in the call frame itself; in
+this case, the exception handling library routines will update the
+stack location to be restored in place.  Otherwise, you must define
+this macro if you want to support call frame exception handling like
+that provided by DWARF 2.
+@end defmac
+
+@defmac EH_RETURN_HANDLER_RTX
+A C expression whose value is RTL representing a location in which
+to store the address of an exception handler to which we should
+return.  It will not be assigned on code paths that return normally.
+
+Typically this is the location in the call frame at which the normal
+return address is stored.  For targets that return by popping an
+address off the stack, this might be a memory address just below
+the @emph{target} call frame rather than inside the current call
+frame.  If defined, @code{EH_RETURN_STACKADJ_RTX} will have already
+been assigned, so it may be used to calculate the location of the
+target call frame.
+
+Some targets have more complex requirements than storing to an
+address calculable during initial code generation.  In that case
+the @code{eh_return} instruction pattern should be used instead.
+
+If you want to support call frame exception handling, you must
+define either this macro or the @code{eh_return} instruction pattern.
+@end defmac
+
+@defmac RETURN_ADDR_OFFSET
+If defined, an integer-valued C expression for which rtl will be generated
+to add it to the exception handler address before it is searched in the
+exception handling tables, and to subtract it again from the address before
+using it to return to the exception handler.
+@end defmac
+
+@defmac ASM_PREFERRED_EH_DATA_FORMAT (@var{code}, @var{global})
+This macro chooses the encoding of pointers embedded in the exception
+handling sections.  If at all possible, this should be defined such
+that the exception handling section will not require dynamic relocations,
+and so may be read-only.
+
+@var{code} is 0 for data, 1 for code labels, 2 for function pointers.
+@var{global} is true if the symbol may be affected by dynamic relocations.
+The macro should return a combination of the @code{DW_EH_PE_*} defines
+as found in @file{dwarf2.h}.
+
+If this macro is not defined, pointers will not be encoded but
+represented directly.
+@end defmac
+
+@defmac ASM_MAYBE_OUTPUT_ENCODED_ADDR_RTX (@var{file}, @var{encoding}, @var{size}, @var{addr}, @var{done})
+This macro allows the target to emit whatever special magic is required
+to represent the encoding chosen by @code{ASM_PREFERRED_EH_DATA_FORMAT}.
+Generic code takes care of pc-relative and indirect encodings; this must
+be defined if the target uses text-relative or data-relative encodings.
+
+This is a C statement that branches to @var{done} if the format was
+handled.  @var{encoding} is the format chosen, @var{size} is the number
+of bytes that the format occupies, @var{addr} is the @code{SYMBOL_REF}
+to be emitted.
+@end defmac
+
+@defmac MD_FALLBACK_FRAME_STATE_FOR (@var{context}, @var{fs})
+This macro allows the target to add CPU and operating system specific
+code to the call-frame unwinder for use when there is no unwind data
+available.  The most common reason to implement this macro is to unwind
+through signal frames.
+
+This macro is called from @code{uw_frame_state_for} in
+@file{unwind-dw2.c}, @file{unwind-dw2-xtensa.c} and
+@file{unwind-ia64.c}.  @var{context} is an @code{_Unwind_Context};
+@var{fs} is an @code{_Unwind_FrameState}.  Examine @code{context->ra}
+for the address of the code being executed and @code{context->cfa} for
+the stack pointer value.  If the frame can be decoded, the register
+save addresses should be updated in @var{fs} and the macro should
+evaluate to @code{_URC_NO_REASON}.  If the frame cannot be decoded,
+the macro should evaluate to @code{_URC_END_OF_STACK}.
+
+For proper signal handling in Java this macro is accompanied by
+@code{MAKE_THROW_FRAME}, defined in @file{libjava/include/*-signal.h} headers.
+@end defmac
+
+@defmac MD_HANDLE_UNWABI (@var{context}, @var{fs})
+This macro allows the target to add operating system specific code to the
+call-frame unwinder to handle the IA-64 @code{.unwabi} unwinding directive,
+usually used for signal or interrupt frames.
+
+This macro is called from @code{uw_update_context} in libgcc's
+@file{unwind-ia64.c}.  @var{context} is an @code{_Unwind_Context};
+@var{fs} is an @code{_Unwind_FrameState}.  Examine @code{fs->unwabi}
+for the abi and context in the @code{.unwabi} directive.  If the
+@code{.unwabi} directive can be handled, the register save addresses should
+be updated in @var{fs}.
+@end defmac
+
+@defmac TARGET_USES_WEAK_UNWIND_INFO
+A C expression that evaluates to true if the target requires unwind
+info to be given comdat linkage.  Define it to be @code{1} if comdat
+linkage is necessary.  The default is @code{0}.
+@end defmac
+
+@node Stack Checking
+@subsection Specifying How Stack Checking is Done
+
+GCC will check that stack references are within the boundaries of the
+stack, if the option @option{-fstack-check} is specified, in one of
+three ways:
+
+@enumerate
+@item
+If the value of the @code{STACK_CHECK_BUILTIN} macro is nonzero, GCC
+will assume that you have arranged for full stack checking to be done
+at appropriate places in the configuration files.  GCC will not do
+other special processing.
+
+@item
+If @code{STACK_CHECK_BUILTIN} is zero and the value of the
+@code{STACK_CHECK_STATIC_BUILTIN} macro is nonzero, GCC will assume
+that you have arranged for static stack checking (checking of the
+static stack frame of functions) to be done at appropriate places
+in the configuration files.  GCC will only emit code to do dynamic
+stack checking (checking on dynamic stack allocations) using the third
+approach below.
+
+@item
+If neither of the above are true, GCC will generate code to periodically
+``probe'' the stack pointer using the values of the macros defined below.
+@end enumerate
+
+If neither STACK_CHECK_BUILTIN nor STACK_CHECK_STATIC_BUILTIN is defined,
+GCC will change its allocation strategy for large objects if the option
+@option{-fstack-check} is specified: they will always be allocated
+dynamically if their size exceeds @code{STACK_CHECK_MAX_VAR_SIZE} bytes.
+
+@defmac STACK_CHECK_BUILTIN
+A nonzero value if stack checking is done by the configuration files in a
+machine-dependent manner.  You should define this macro if stack checking
+is required by the ABI of your machine or if you would like to do stack
+checking in some more efficient way than the generic approach.  The default
+value of this macro is zero.
+@end defmac
+
+@defmac STACK_CHECK_STATIC_BUILTIN
+A nonzero value if static stack checking is done by the configuration files
+in a machine-dependent manner.  You should define this macro if you would
+like to do static stack checking in some more efficient way than the generic
+approach.  The default value of this macro is zero.
+@end defmac
+
+@defmac STACK_CHECK_PROBE_INTERVAL_EXP
+An integer specifying the interval at which GCC must generate stack probe
+instructions, defined as 2 raised to this integer.  You will normally
+define this macro so that the interval be no larger than the size of
+the ``guard pages'' at the end of a stack area.  The default value
+of 12 (4096-byte interval) is suitable for most systems.
+@end defmac
+
+@defmac STACK_CHECK_MOVING_SP
+An integer which is nonzero if GCC should move the stack pointer page by page
+when doing probes.  This can be necessary on systems where the stack pointer
+contains the bottom address of the memory area accessible to the executing
+thread at any point in time.  In this situation an alternate signal stack
+is required in order to be able to recover from a stack overflow.  The
+default value of this macro is zero.
+@end defmac
+
+@defmac STACK_CHECK_PROTECT
+The number of bytes of stack needed to recover from a stack overflow, for
+languages where such a recovery is supported.  The default value of 4KB/8KB
+with the @code{setjmp}/@code{longjmp}-based exception handling mechanism and
+8KB/12KB with other exception handling mechanisms should be adequate for most
+architectures and operating systems.
+@end defmac
+
+The following macros are relevant only if neither STACK_CHECK_BUILTIN
+nor STACK_CHECK_STATIC_BUILTIN is defined; you can omit them altogether
+in the opposite case.
+
+@defmac STACK_CHECK_MAX_FRAME_SIZE
+The maximum size of a stack frame, in bytes.  GCC will generate probe
+instructions in non-leaf functions to ensure at least this many bytes of
+stack are available.  If a stack frame is larger than this size, stack
+checking will not be reliable and GCC will issue a warning.  The
+default is chosen so that GCC only generates one instruction on most
+systems.  You should normally not change the default value of this macro.
+@end defmac
+
+@defmac STACK_CHECK_FIXED_FRAME_SIZE
+GCC uses this value to generate the above warning message.  It
+represents the amount of fixed frame used by a function, not including
+space for any callee-saved registers, temporaries and user variables.
+You need only specify an upper bound for this amount and will normally
+use the default of four words.
+@end defmac
+
+@defmac STACK_CHECK_MAX_VAR_SIZE
+The maximum size, in bytes, of an object that GCC will place in the
+fixed area of the stack frame when the user specifies
+@option{-fstack-check}.
+GCC computed the default from the values of the above macros and you will
+normally not need to override that default.
+@end defmac
+
+@deftypefn {Target Hook} HOST_WIDE_INT TARGET_STACK_CLASH_PROTECTION_ALLOCA_PROBE_RANGE (void)
+Some targets have an ABI defined interval for which no probing needs to be done.
+When a probe does need to be done this same interval is used as the probe distance up when doing stack clash protection for alloca.
+On such targets this value can be set to override the default probing up interval.
+Define this variable to return nonzero if such a probe range is required or zero otherwise.  Defining this hook also requires your functions which make use of alloca to have at least 8 byesof outgoing arguments.  If this is not the case the stack will be corrupted.
+You need not define this macro if it would always have the value zero.
+@end deftypefn
+
+@need 2000
+@node Frame Registers
+@subsection Registers That Address the Stack Frame
+
+@c prevent bad page break with this line
+This discusses registers that address the stack frame.
+
+@defmac STACK_POINTER_REGNUM
+The register number of the stack pointer register, which must also be a
+fixed register according to @code{FIXED_REGISTERS}.  On most machines,
+the hardware determines which register this is.
+@end defmac
+
+@defmac FRAME_POINTER_REGNUM
+The register number of the frame pointer register, which is used to
+access automatic variables in the stack frame.  On some machines, the
+hardware determines which register this is.  On other machines, you can
+choose any register you wish for this purpose.
+@end defmac
+
+@defmac HARD_FRAME_POINTER_REGNUM
+On some machines the offset between the frame pointer and starting
+offset of the automatic variables is not known until after register
+allocation has been done (for example, because the saved registers are
+between these two locations).  On those machines, define
+@code{FRAME_POINTER_REGNUM} the number of a special, fixed register to
+be used internally until the offset is known, and define
+@code{HARD_FRAME_POINTER_REGNUM} to be the actual hard register number
+used for the frame pointer.
+
+You should define this macro only in the very rare circumstances when it
+is not possible to calculate the offset between the frame pointer and
+the automatic variables until after register allocation has been
+completed.  When this macro is defined, you must also indicate in your
+definition of @code{ELIMINABLE_REGS} how to eliminate
+@code{FRAME_POINTER_REGNUM} into either @code{HARD_FRAME_POINTER_REGNUM}
+or @code{STACK_POINTER_REGNUM}.
+
+Do not define this macro if it would be the same as
+@code{FRAME_POINTER_REGNUM}.
+@end defmac
+
+@defmac ARG_POINTER_REGNUM
+The register number of the arg pointer register, which is used to access
+the function's argument list.  On some machines, this is the same as the
+frame pointer register.  On some machines, the hardware determines which
+register this is.  On other machines, you can choose any register you
+wish for this purpose.  If this is not the same register as the frame
+pointer register, then you must mark it as a fixed register according to
+@code{FIXED_REGISTERS}, or arrange to be able to eliminate it
+(@pxref{Elimination}).
+@end defmac
+
+@defmac HARD_FRAME_POINTER_IS_FRAME_POINTER
+Define this to a preprocessor constant that is nonzero if
+@code{hard_frame_pointer_rtx} and @code{frame_pointer_rtx} should be
+the same.  The default definition is @samp{(HARD_FRAME_POINTER_REGNUM
+== FRAME_POINTER_REGNUM)}; you only need to define this macro if that
+definition is not suitable for use in preprocessor conditionals.
+@end defmac
+
+@defmac HARD_FRAME_POINTER_IS_ARG_POINTER
+Define this to a preprocessor constant that is nonzero if
+@code{hard_frame_pointer_rtx} and @code{arg_pointer_rtx} should be the
+same.  The default definition is @samp{(HARD_FRAME_POINTER_REGNUM ==
+ARG_POINTER_REGNUM)}; you only need to define this macro if that
+definition is not suitable for use in preprocessor conditionals.
+@end defmac
+
+@defmac RETURN_ADDRESS_POINTER_REGNUM
+The register number of the return address pointer register, which is used to
+access the current function's return address from the stack.  On some
+machines, the return address is not at a fixed offset from the frame
+pointer or stack pointer or argument pointer.  This register can be defined
+to point to the return address on the stack, and then be converted by
+@code{ELIMINABLE_REGS} into either the frame pointer or stack pointer.
+
+Do not define this macro unless there is no other way to get the return
+address from the stack.
+@end defmac
+
+@defmac STATIC_CHAIN_REGNUM
+@defmacx STATIC_CHAIN_INCOMING_REGNUM
+Register numbers used for passing a function's static chain pointer.  If
+register windows are used, the register number as seen by the called
+function is @code{STATIC_CHAIN_INCOMING_REGNUM}, while the register
+number as seen by the calling function is @code{STATIC_CHAIN_REGNUM}.  If
+these registers are the same, @code{STATIC_CHAIN_INCOMING_REGNUM} need
+not be defined.
+
+The static chain register need not be a fixed register.
+
+If the static chain is passed in memory, these macros should not be
+defined; instead, the @code{TARGET_STATIC_CHAIN} hook should be used.
+@end defmac
+
+@deftypefn {Target Hook} rtx TARGET_STATIC_CHAIN (const_tree @var{fndecl_or_type}, bool @var{incoming_p})
+This hook replaces the use of @code{STATIC_CHAIN_REGNUM} et al for
+targets that may use different static chain locations for different
+nested functions.  This may be required if the target has function
+attributes that affect the calling conventions of the function and
+those calling conventions use different static chain locations.
+
+The default version of this hook uses @code{STATIC_CHAIN_REGNUM} et al.
+
+If the static chain is passed in memory, this hook should be used to
+provide rtx giving @code{mem} expressions that denote where they are stored.
+Often the @code{mem} expression as seen by the caller will be at an offset
+from the stack pointer and the @code{mem} expression as seen by the callee
+will be at an offset from the frame pointer.
+@findex stack_pointer_rtx
+@findex frame_pointer_rtx
+@findex arg_pointer_rtx
+The variables @code{stack_pointer_rtx}, @code{frame_pointer_rtx}, and
+@code{arg_pointer_rtx} will have been initialized and should be used
+to refer to those items.
+@end deftypefn
+
+@defmac DWARF_FRAME_REGISTERS
+This macro specifies the maximum number of hard registers that can be
+saved in a call frame.  This is used to size data structures used in
+DWARF2 exception handling.
+
+Prior to GCC 3.0, this macro was needed in order to establish a stable
+exception handling ABI in the face of adding new hard registers for ISA
+extensions.  In GCC 3.0 and later, the EH ABI is insulated from changes
+in the number of hard registers.  Nevertheless, this macro can still be
+used to reduce the runtime memory requirements of the exception handling
+routines, which can be substantial if the ISA contains a lot of
+registers that are not call-saved.
+
+If this macro is not defined, it defaults to
+@code{FIRST_PSEUDO_REGISTER}.
+@end defmac
+
+@defmac PRE_GCC3_DWARF_FRAME_REGISTERS
+
+This macro is similar to @code{DWARF_FRAME_REGISTERS}, but is provided
+for backward compatibility in pre GCC 3.0 compiled code.
+
+If this macro is not defined, it defaults to
+@code{DWARF_FRAME_REGISTERS}.
+@end defmac
+
+@defmac DWARF_REG_TO_UNWIND_COLUMN (@var{regno})
+
+Define this macro if the target's representation for dwarf registers
+is different than the internal representation for unwind column.
+Given a dwarf register, this macro should return the internal unwind
+column number to use instead.
+@end defmac
+
+@defmac DWARF_FRAME_REGNUM (@var{regno})
+
+Define this macro if the target's representation for dwarf registers
+used in .eh_frame or .debug_frame is different from that used in other
+debug info sections.  Given a GCC hard register number, this macro
+should return the .eh_frame register number.  The default is
+@code{DBX_REGISTER_NUMBER (@var{regno})}.
+
+@end defmac
+
+@defmac DWARF2_FRAME_REG_OUT (@var{regno}, @var{for_eh})
+
+Define this macro to map register numbers held in the call frame info
+that GCC has collected using @code{DWARF_FRAME_REGNUM} to those that
+should be output in .debug_frame (@code{@var{for_eh}} is zero) and
+.eh_frame (@code{@var{for_eh}} is nonzero).  The default is to
+return @code{@var{regno}}.
+
+@end defmac
+
+@defmac REG_VALUE_IN_UNWIND_CONTEXT
+
+Define this macro if the target stores register values as
+@code{_Unwind_Word} type in unwind context.  It should be defined if
+target register size is larger than the size of @code{void *}.  The
+default is to store register values as @code{void *} type.
+
+@end defmac
+
+@defmac ASSUME_EXTENDED_UNWIND_CONTEXT
+
+Define this macro to be 1 if the target always uses extended unwind
+context with version, args_size and by_value fields.  If it is undefined,
+it will be defined to 1 when @code{REG_VALUE_IN_UNWIND_CONTEXT} is
+defined and 0 otherwise.
+
+@end defmac
+
+@defmac DWARF_LAZY_REGISTER_VALUE (@var{regno}, @var{value})
+Define this macro if the target has pseudo DWARF registers whose
+values need to be computed lazily on demand by the unwinder (such as when
+referenced in a CFA expression).  The macro returns true if @var{regno}
+is such a register and stores its value in @samp{*@var{value}} if so.
+@end defmac
+
+@node Elimination
+@subsection Eliminating Frame Pointer and Arg Pointer
+
+@c prevent bad page break with this line
+This is about eliminating the frame pointer and arg pointer.
+
+@deftypefn {Target Hook} bool TARGET_FRAME_POINTER_REQUIRED (void)
+This target hook should return @code{true} if a function must have and use
+a frame pointer.  This target hook is called in the reload pass.  If its return
+value is @code{true} the function will have a frame pointer.
+
+This target hook can in principle examine the current function and decide
+according to the facts, but on most machines the constant @code{false} or the
+constant @code{true} suffices.  Use @code{false} when the machine allows code
+to be generated with no frame pointer, and doing so saves some time or space.
+Use @code{true} when there is no possible advantage to avoiding a frame
+pointer.
+
+In certain cases, the compiler does not know how to produce valid code
+without a frame pointer.  The compiler recognizes those cases and
+automatically gives the function a frame pointer regardless of what
+@code{targetm.frame_pointer_required} returns.  You don't need to worry about
+them.
+
+In a function that does not require a frame pointer, the frame pointer
+register can be allocated for ordinary usage, unless you mark it as a
+fixed register.  See @code{FIXED_REGISTERS} for more information.
+
+Default return value is @code{false}.
+@end deftypefn
+
+@defmac ELIMINABLE_REGS
+This macro specifies a table of register pairs used to eliminate
+unneeded registers that point into the stack frame.
+
+The definition of this macro is a list of structure initializations, each
+of which specifies an original and replacement register.
+
+On some machines, the position of the argument pointer is not known until
+the compilation is completed.  In such a case, a separate hard register
+must be used for the argument pointer.  This register can be eliminated by
+replacing it with either the frame pointer or the argument pointer,
+depending on whether or not the frame pointer has been eliminated.
+
+In this case, you might specify:
+@smallexample
+#define ELIMINABLE_REGS  \
+@{@{ARG_POINTER_REGNUM, STACK_POINTER_REGNUM@}, \
+ @{ARG_POINTER_REGNUM, FRAME_POINTER_REGNUM@}, \
+ @{FRAME_POINTER_REGNUM, STACK_POINTER_REGNUM@}@}
+@end smallexample
+
+Note that the elimination of the argument pointer with the stack pointer is
+specified first since that is the preferred elimination.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_CAN_ELIMINATE (const int @var{from_reg}, const int @var{to_reg})
+This target hook should return @code{true} if the compiler is allowed to
+try to replace register number @var{from_reg} with register number
+@var{to_reg}.  This target hook will usually be @code{true}, since most of the
+cases preventing register elimination are things that the compiler already
+knows about.
+
+Default return value is @code{true}.
+@end deftypefn
+
+@defmac INITIAL_ELIMINATION_OFFSET (@var{from-reg}, @var{to-reg}, @var{offset-var})
+This macro returns the initial difference between the specified pair
+of registers.  The value would be computed from information
+such as the result of @code{get_frame_size ()} and the tables of
+registers @code{df_regs_ever_live_p} and @code{call_used_regs}.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_COMPUTE_FRAME_LAYOUT (void)
+This target hook is called once each time the frame layout needs to be
+recalculated.  The calculations can be cached by the target and can then
+be used by @code{INITIAL_ELIMINATION_OFFSET} instead of re-computing the
+layout on every invocation of that hook.  This is particularly useful
+for targets that have an expensive frame layout function.  Implementing
+this callback is optional.
+@end deftypefn
+
+@node Stack Arguments
+@subsection Passing Function Arguments on the Stack
+@cindex arguments on stack
+@cindex stack arguments
+
+The macros in this section control how arguments are passed
+on the stack.  See the following section for other macros that
+control passing certain arguments in registers.
+
+@deftypefn {Target Hook} bool TARGET_PROMOTE_PROTOTYPES (const_tree @var{fntype})
+This target hook returns @code{true} if an argument declared in a
+prototype as an integral type smaller than @code{int} should actually be
+passed as an @code{int}.  In addition to avoiding errors in certain
+cases of mismatch, it also makes for better code on certain machines.
+The default is to not promote prototypes.
+@end deftypefn
+
+@defmac PUSH_ARGS
+A C expression.  If nonzero, push insns will be used to pass
+outgoing arguments.
+If the target machine does not have a push instruction, set it to zero.
+That directs GCC to use an alternate strategy: to
+allocate the entire argument block and then store the arguments into
+it.  When @code{PUSH_ARGS} is nonzero, @code{PUSH_ROUNDING} must be defined too.
+@end defmac
+
+@defmac PUSH_ARGS_REVERSED
+A C expression.  If nonzero, function arguments will be evaluated from
+last to first, rather than from first to last.  If this macro is not
+defined, it defaults to @code{PUSH_ARGS} on targets where the stack
+and args grow in opposite directions, and 0 otherwise.
+@end defmac
+
+@defmac PUSH_ROUNDING (@var{npushed})
+A C expression that is the number of bytes actually pushed onto the
+stack when an instruction attempts to push @var{npushed} bytes.
+
+On some machines, the definition
+
+@smallexample
+#define PUSH_ROUNDING(BYTES) (BYTES)
+@end smallexample
+
+@noindent
+will suffice.  But on other machines, instructions that appear
+to push one byte actually push two bytes in an attempt to maintain
+alignment.  Then the definition should be
+
+@smallexample
+#define PUSH_ROUNDING(BYTES) (((BYTES) + 1) & ~1)
+@end smallexample
+
+If the value of this macro has a type, it should be an unsigned type.
+@end defmac
+
+@findex outgoing_args_size
+@findex crtl->outgoing_args_size
+@defmac ACCUMULATE_OUTGOING_ARGS
+A C expression.  If nonzero, the maximum amount of space required for outgoing arguments
+will be computed and placed into
+@code{crtl->outgoing_args_size}.  No space will be pushed
+onto the stack for each call; instead, the function prologue should
+increase the stack frame size by this amount.
+
+Setting both @code{PUSH_ARGS} and @code{ACCUMULATE_OUTGOING_ARGS}
+is not proper.
+@end defmac
+
+@defmac REG_PARM_STACK_SPACE (@var{fndecl})
+Define this macro if functions should assume that stack space has been
+allocated for arguments even when their values are passed in
+registers.
+
+The value of this macro is the size, in bytes, of the area reserved for
+arguments passed in registers for the function represented by @var{fndecl},
+which can be zero if GCC is calling a library function.
+The argument @var{fndecl} can be the FUNCTION_DECL, or the type itself
+of the function.
+
+This space can be allocated by the caller, or be a part of the
+machine-dependent stack frame: @code{OUTGOING_REG_PARM_STACK_SPACE} says
+which.
+@end defmac
+@c above is overfull.  not sure what to do.  --mew 5feb93  did
+@c something, not sure if it looks good.  --mew 10feb93
+
+@defmac INCOMING_REG_PARM_STACK_SPACE (@var{fndecl})
+Like @code{REG_PARM_STACK_SPACE}, but for incoming register arguments.
+Define this macro if space guaranteed when compiling a function body
+is different to space required when making a call, a situation that
+can arise with K&R style function definitions.
+@end defmac
+
+@defmac OUTGOING_REG_PARM_STACK_SPACE (@var{fntype})
+Define this to a nonzero value if it is the responsibility of the
+caller to allocate the area reserved for arguments passed in registers
+when calling a function of @var{fntype}.  @var{fntype} may be NULL
+if the function called is a library function.
+
+If @code{ACCUMULATE_OUTGOING_ARGS} is defined, this macro controls
+whether the space for these arguments counts in the value of
+@code{crtl->outgoing_args_size}.
+@end defmac
+
+@defmac STACK_PARMS_IN_REG_PARM_AREA
+Define this macro if @code{REG_PARM_STACK_SPACE} is defined, but the
+stack parameters don't skip the area specified by it.
+@c i changed this, makes more sens and it should have taken care of the
+@c overfull.. not as specific, tho.  --mew 5feb93
+
+Normally, when a parameter is not passed in registers, it is placed on the
+stack beyond the @code{REG_PARM_STACK_SPACE} area.  Defining this macro
+suppresses this behavior and causes the parameter to be passed on the
+stack in its natural location.
+@end defmac
+
+@deftypefn {Target Hook} poly_int64 TARGET_RETURN_POPS_ARGS (tree @var{fundecl}, tree @var{funtype}, poly_int64 @var{size})
+This target hook returns the number of bytes of its own arguments that
+a function pops on returning, or 0 if the function pops no arguments
+and the caller must therefore pop them all after the function returns.
+
+@var{fundecl} is a C variable whose value is a tree node that describes
+the function in question.  Normally it is a node of type
+@code{FUNCTION_DECL} that describes the declaration of the function.
+From this you can obtain the @code{DECL_ATTRIBUTES} of the function.
+
+@var{funtype} is a C variable whose value is a tree node that
+describes the function in question.  Normally it is a node of type
+@code{FUNCTION_TYPE} that describes the data type of the function.
+From this it is possible to obtain the data types of the value and
+arguments (if known).
+
+When a call to a library function is being considered, @var{fundecl}
+will contain an identifier node for the library function.  Thus, if
+you need to distinguish among various library functions, you can do so
+by their names.  Note that ``library function'' in this context means
+a function used to perform arithmetic, whose name is known specially
+in the compiler and was not mentioned in the C code being compiled.
+
+@var{size} is the number of bytes of arguments passed on the
+stack.  If a variable number of bytes is passed, it is zero, and
+argument popping will always be the responsibility of the calling function.
+
+On the VAX, all functions always pop their arguments, so the definition
+of this macro is @var{size}.  On the 68000, using the standard
+calling convention, no functions pop their arguments, so the value of
+the macro is always 0 in this case.  But an alternative calling
+convention is available in which functions that take a fixed number of
+arguments pop them but other functions (such as @code{printf}) pop
+nothing (the caller pops all).  When this convention is in use,
+@var{funtype} is examined to determine whether a function takes a fixed
+number of arguments.
+@end deftypefn
+
+@defmac CALL_POPS_ARGS (@var{cum})
+A C expression that should indicate the number of bytes a call sequence
+pops off the stack.  It is added to the value of @code{RETURN_POPS_ARGS}
+when compiling a function call.
+
+@var{cum} is the variable in which all arguments to the called function
+have been accumulated.
+
+On certain architectures, such as the SH5, a call trampoline is used
+that pops certain registers off the stack, depending on the arguments
+that have been passed to the function.  Since this is a property of the
+call site, not of the called function, @code{RETURN_POPS_ARGS} is not
+appropriate.
+@end defmac
+
+@node Register Arguments
+@subsection Passing Arguments in Registers
+@cindex arguments in registers
+@cindex registers arguments
+
+This section describes the macros which let you control how various
+types of arguments are passed in registers or how they are arranged in
+the stack.
+
+@deftypefn {Target Hook} rtx TARGET_FUNCTION_ARG (cumulative_args_t @var{ca}, const function_arg_info @var{&arg})
+Return an RTX indicating whether function argument @var{arg} is passed
+in a register and if so, which register.  Argument @var{ca} summarizes all
+the previous arguments.
+
+The return value is usually either a @code{reg} RTX for the hard
+register in which to pass the argument, or zero to pass the argument
+on the stack.
+
+The return value can be a @code{const_int} which means argument is
+passed in a target specific slot with specified number.  Target hooks
+should be used to store or load argument in such case.  See
+@code{TARGET_STORE_BOUNDS_FOR_ARG} and @code{TARGET_LOAD_BOUNDS_FOR_ARG}
+for more information.
+
+The value of the expression can also be a @code{parallel} RTX@.  This is
+used when an argument is passed in multiple locations.  The mode of the
+@code{parallel} should be the mode of the entire argument.  The
+@code{parallel} holds any number of @code{expr_list} pairs; each one
+describes where part of the argument is passed.  In each
+@code{expr_list} the first operand must be a @code{reg} RTX for the hard
+register in which to pass this part of the argument, and the mode of the
+register RTX indicates how large this part of the argument is.  The
+second operand of the @code{expr_list} is a @code{const_int} which gives
+the offset in bytes into the entire argument of where this part starts.
+As a special exception the first @code{expr_list} in the @code{parallel}
+RTX may have a first operand of zero.  This indicates that the entire
+argument is also stored on the stack.
+
+The last time this hook is called, it is called with @code{MODE ==
+VOIDmode}, and its result is passed to the @code{call} or @code{call_value}
+pattern as operands 2 and 3 respectively.
+
+@cindex @file{stdarg.h} and register arguments
+The usual way to make the ISO library @file{stdarg.h} work on a
+machine where some arguments are usually passed in registers, is to
+cause nameless arguments to be passed on the stack instead.  This is
+done by making @code{TARGET_FUNCTION_ARG} return 0 whenever
+@var{named} is @code{false}.
+
+@cindex @code{TARGET_MUST_PASS_IN_STACK}, and @code{TARGET_FUNCTION_ARG}
+@cindex @code{REG_PARM_STACK_SPACE}, and @code{TARGET_FUNCTION_ARG}
+You may use the hook @code{targetm.calls.must_pass_in_stack}
+in the definition of this macro to determine if this argument is of a
+type that must be passed in the stack.  If @code{REG_PARM_STACK_SPACE}
+is not defined and @code{TARGET_FUNCTION_ARG} returns nonzero for such an
+argument, the compiler will abort.  If @code{REG_PARM_STACK_SPACE} is
+defined, the argument will be computed in the stack and then loaded into
+a register.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_MUST_PASS_IN_STACK (const function_arg_info @var{&arg})
+This target hook should return @code{true} if we should not pass @var{arg}
+solely in registers.  The file @file{expr.h} defines a
+definition that is usually appropriate, refer to @file{expr.h} for additional
+documentation.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_FUNCTION_INCOMING_ARG (cumulative_args_t @var{ca}, const function_arg_info @var{&arg})
+Define this hook if the caller and callee on the target have different
+views of where arguments are passed.  Also define this hook if there are
+functions that are never directly called, but are invoked by the hardware
+and which have nonstandard calling conventions.
+
+In this case @code{TARGET_FUNCTION_ARG} computes the register in
+which the caller passes the value, and
+@code{TARGET_FUNCTION_INCOMING_ARG} should be defined in a similar
+fashion to tell the function being called where the arguments will
+arrive.
+
+@code{TARGET_FUNCTION_INCOMING_ARG} can also return arbitrary address
+computation using hard register, which can be forced into a register,
+so that it can be used to pass special arguments.
+
+If @code{TARGET_FUNCTION_INCOMING_ARG} is not defined,
+@code{TARGET_FUNCTION_ARG} serves both purposes.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_USE_PSEUDO_PIC_REG (void)
+This hook should return 1 in case pseudo register should be created
+for pic_offset_table_rtx during function expand.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_INIT_PIC_REG (void)
+Perform a target dependent initialization of pic_offset_table_rtx.
+This hook is called at the start of register allocation.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_ARG_PARTIAL_BYTES (cumulative_args_t @var{cum}, const function_arg_info @var{&arg})
+This target hook returns the number of bytes at the beginning of an
+argument that must be put in registers.  The value must be zero for
+arguments that are passed entirely in registers or that are entirely
+pushed on the stack.
+
+On some machines, certain arguments must be passed partially in
+registers and partially in memory.  On these machines, typically the
+first few words of arguments are passed in registers, and the rest
+on the stack.  If a multi-word argument (a @code{double} or a
+structure) crosses that boundary, its first few words must be passed
+in registers and the rest must be pushed.  This macro tells the
+compiler when this occurs, and how many bytes should go in registers.
+
+@code{TARGET_FUNCTION_ARG} for these arguments should return the first
+register to be used by the caller for this argument; likewise
+@code{TARGET_FUNCTION_INCOMING_ARG}, for the called function.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_PASS_BY_REFERENCE (cumulative_args_t @var{cum}, const function_arg_info @var{&arg})
+This target hook should return @code{true} if argument @var{arg} at the
+position indicated by @var{cum} should be passed by reference.  This
+predicate is queried after target independent reasons for being
+passed by reference, such as @code{TREE_ADDRESSABLE (@var{arg}.type)}.
+
+If the hook returns true, a copy of that argument is made in memory and a
+pointer to the argument is passed instead of the argument itself.
+The pointer is passed in whatever way is appropriate for passing a pointer
+to that type.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_CALLEE_COPIES (cumulative_args_t @var{cum}, const function_arg_info @var{&arg})
+The function argument described by the parameters to this hook is
+known to be passed by reference.  The hook should return true if the
+function argument should be copied by the callee instead of copied
+by the caller.
+
+For any argument for which the hook returns true, if it can be
+determined that the argument is not modified, then a copy need
+not be generated.
+
+The default version of this hook always returns false.
+@end deftypefn
+
+@defmac CUMULATIVE_ARGS
+A C type for declaring a variable that is used as the first argument
+of @code{TARGET_FUNCTION_ARG} and other related values.  For some
+target machines, the type @code{int} suffices and can hold the number
+of bytes of argument so far.
+
+There is no need to record in @code{CUMULATIVE_ARGS} anything about the
+arguments that have been passed on the stack.  The compiler has other
+variables to keep track of that.  For target machines on which all
+arguments are passed on the stack, there is no need to store anything in
+@code{CUMULATIVE_ARGS}; however, the data structure must exist and
+should not be empty, so use @code{int}.
+@end defmac
+
+@defmac OVERRIDE_ABI_FORMAT (@var{fndecl})
+If defined, this macro is called before generating any code for a
+function, but after the @var{cfun} descriptor for the function has been
+created.  The back end may use this macro to update @var{cfun} to
+reflect an ABI other than that which would normally be used by default.
+If the compiler is generating code for a compiler-generated function,
+@var{fndecl} may be @code{NULL}.
+@end defmac
+
+@defmac INIT_CUMULATIVE_ARGS (@var{cum}, @var{fntype}, @var{libname}, @var{fndecl}, @var{n_named_args})
+A C statement (sans semicolon) for initializing the variable
+@var{cum} for the state at the beginning of the argument list.  The
+variable has type @code{CUMULATIVE_ARGS}.  The value of @var{fntype}
+is the tree node for the data type of the function which will receive
+the args, or 0 if the args are to a compiler support library function.
+For direct calls that are not libcalls, @var{fndecl} contain the
+declaration node of the function.  @var{fndecl} is also set when
+@code{INIT_CUMULATIVE_ARGS} is used to find arguments for the function
+being compiled.  @var{n_named_args} is set to the number of named
+arguments, including a structure return address if it is passed as a
+parameter, when making a call.  When processing incoming arguments,
+@var{n_named_args} is set to @minus{}1.
+
+When processing a call to a compiler support library function,
+@var{libname} identifies which one.  It is a @code{symbol_ref} rtx which
+contains the name of the function, as a string.  @var{libname} is 0 when
+an ordinary C function call is being processed.  Thus, each time this
+macro is called, either @var{libname} or @var{fntype} is nonzero, but
+never both of them at once.
+@end defmac
+
+@defmac INIT_CUMULATIVE_LIBCALL_ARGS (@var{cum}, @var{mode}, @var{libname})
+Like @code{INIT_CUMULATIVE_ARGS} but only used for outgoing libcalls,
+it gets a @code{MODE} argument instead of @var{fntype}, that would be
+@code{NULL}.  @var{indirect} would always be zero, too.  If this macro
+is not defined, @code{INIT_CUMULATIVE_ARGS (cum, NULL_RTX, libname,
+0)} is used instead.
+@end defmac
+
+@defmac INIT_CUMULATIVE_INCOMING_ARGS (@var{cum}, @var{fntype}, @var{libname})
+Like @code{INIT_CUMULATIVE_ARGS} but overrides it for the purposes of
+finding the arguments for the function being compiled.  If this macro is
+undefined, @code{INIT_CUMULATIVE_ARGS} is used instead.
+
+The value passed for @var{libname} is always 0, since library routines
+with special calling conventions are never compiled with GCC@.  The
+argument @var{libname} exists for symmetry with
+@code{INIT_CUMULATIVE_ARGS}.
+@c could use "this macro" in place of @code{INIT_CUMULATIVE_ARGS}, maybe.
+@c --mew 5feb93   i switched the order of the sentences.  --mew 10feb93
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_FUNCTION_ARG_ADVANCE (cumulative_args_t @var{ca}, const function_arg_info @var{&arg})
+This hook updates the summarizer variable pointed to by @var{ca} to
+advance past argument @var{arg} in the argument list.  Once this is done,
+the variable @var{cum} is suitable for analyzing the @emph{following}
+argument with @code{TARGET_FUNCTION_ARG}, etc.
+
+This hook need not do anything if the argument in question was passed
+on the stack.  The compiler knows how to track the amount of stack space
+used for arguments without any special help.
+@end deftypefn
+
+@deftypefn {Target Hook} HOST_WIDE_INT TARGET_FUNCTION_ARG_OFFSET (machine_mode @var{mode}, const_tree @var{type})
+This hook returns the number of bytes to add to the offset of an
+argument of type @var{type} and mode @var{mode} when passed in memory.
+This is needed for the SPU, which passes @code{char} and @code{short}
+arguments in the preferred slot that is in the middle of the quad word
+instead of starting at the top.  The default implementation returns 0.
+@end deftypefn
+
+@deftypefn {Target Hook} pad_direction TARGET_FUNCTION_ARG_PADDING (machine_mode @var{mode}, const_tree @var{type})
+This hook determines whether, and in which direction, to pad out
+an argument of mode @var{mode} and type @var{type}.  It returns
+@code{PAD_UPWARD} to insert padding above the argument, @code{PAD_DOWNWARD}
+to insert padding below the argument, or @code{PAD_NONE} to inhibit padding.
+
+The @emph{amount} of padding is not controlled by this hook, but by
+@code{TARGET_FUNCTION_ARG_ROUND_BOUNDARY}.  It is always just enough
+to reach the next multiple of that boundary.
+
+This hook has a default definition that is right for most systems.
+For little-endian machines, the default is to pad upward.  For
+big-endian machines, the default is to pad downward for an argument of
+constant size shorter than an @code{int}, and upward otherwise.
+@end deftypefn
+
+@defmac PAD_VARARGS_DOWN
+If defined, a C expression which determines whether the default
+implementation of va_arg will attempt to pad down before reading the
+next argument, if that argument is smaller than its aligned space as
+controlled by @code{PARM_BOUNDARY}.  If this macro is not defined, all such
+arguments are padded down if @code{BYTES_BIG_ENDIAN} is true.
+@end defmac
+
+@defmac BLOCK_REG_PADDING (@var{mode}, @var{type}, @var{first})
+Specify padding for the last element of a block move between registers and
+memory.  @var{first} is nonzero if this is the only element.  Defining this
+macro allows better control of register function parameters on big-endian
+machines, without using @code{PARALLEL} rtl.  In particular,
+@code{MUST_PASS_IN_STACK} need not test padding and mode of types in
+registers, as there is no longer a "wrong" part of a register;  For example,
+a three byte aggregate may be passed in the high part of a register if so
+required.
+@end defmac
+
+@deftypefn {Target Hook} {unsigned int} TARGET_FUNCTION_ARG_BOUNDARY (machine_mode @var{mode}, const_tree @var{type})
+This hook returns the alignment boundary, in bits, of an argument
+with the specified mode and type.  The default hook returns
+@code{PARM_BOUNDARY} for all arguments.
+@end deftypefn
+
+@deftypefn {Target Hook} {unsigned int} TARGET_FUNCTION_ARG_ROUND_BOUNDARY (machine_mode @var{mode}, const_tree @var{type})
+Normally, the size of an argument is rounded up to @code{PARM_BOUNDARY},
+which is the default value for this hook.  You can define this hook to
+return a different value if an argument size must be rounded to a larger
+value.
+@end deftypefn
+
+@defmac FUNCTION_ARG_REGNO_P (@var{regno})
+A C expression that is nonzero if @var{regno} is the number of a hard
+register in which function arguments are sometimes passed.  This does
+@emph{not} include implicit arguments such as the static chain and
+the structure-value address.  On many machines, no registers can be
+used for this purpose since all function arguments are pushed on the
+stack.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_SPLIT_COMPLEX_ARG (const_tree @var{type})
+This hook should return true if parameter of type @var{type} are passed
+as two scalar parameters.  By default, GCC will attempt to pack complex
+arguments into the target's word size.  Some ABIs require complex arguments
+to be split and treated as their individual components.  For example, on
+AIX64, complex floats should be passed in a pair of floating point
+registers, even though a complex float would fit in one 64-bit floating
+point register.
+
+The default value of this hook is @code{NULL}, which is treated as always
+false.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_BUILD_BUILTIN_VA_LIST (void)
+This hook returns a type node for @code{va_list} for the target.
+The default version of the hook returns @code{void*}.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_ENUM_VA_LIST_P (int @var{idx}, const char **@var{pname}, tree *@var{ptree})
+This target hook is used in function @code{c_common_nodes_and_builtins}
+to iterate through the target specific builtin types for va_list. The
+variable @var{idx} is used as iterator. @var{pname} has to be a pointer
+to a @code{const char *} and @var{ptree} a pointer to a @code{tree} typed
+variable.
+The arguments @var{pname} and @var{ptree} are used to store the result of
+this macro and are set to the name of the va_list builtin type and its
+internal type.
+If the return value of this macro is zero, then there is no more element.
+Otherwise the @var{IDX} should be increased for the next call of this
+macro to iterate through all types.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_FN_ABI_VA_LIST (tree @var{fndecl})
+This hook returns the va_list type of the calling convention specified by
+@var{fndecl}.
+The default version of this hook returns @code{va_list_type_node}.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_CANONICAL_VA_LIST_TYPE (tree @var{type})
+This hook returns the va_list type of the calling convention specified by the
+type of @var{type}. If @var{type} is not a valid va_list type, it returns
+@code{NULL_TREE}.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_GIMPLIFY_VA_ARG_EXPR (tree @var{valist}, tree @var{type}, gimple_seq *@var{pre_p}, gimple_seq *@var{post_p})
+This hook performs target-specific gimplification of
+@code{VA_ARG_EXPR}.  The first two parameters correspond to the
+arguments to @code{va_arg}; the latter two are as in
+@code{gimplify.c:gimplify_expr}.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_VALID_POINTER_MODE (scalar_int_mode @var{mode})
+Define this to return nonzero if the port can handle pointers
+with machine mode @var{mode}.  The default version of this
+hook returns true for both @code{ptr_mode} and @code{Pmode}.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_REF_MAY_ALIAS_ERRNO (ao_ref *@var{ref})
+Define this to return nonzero if the memory reference @var{ref}  may alias with the system C library errno location.  The default  version of this hook assumes the system C library errno location  is either a declaration of type int or accessed by dereferencing  a pointer to int.
+@end deftypefn
+
+@deftypefn {Target Hook} machine_mode TARGET_TRANSLATE_MODE_ATTRIBUTE (machine_mode @var{mode})
+Define this hook if during mode attribute processing, the port should
+translate machine_mode @var{mode} to another mode.  For example, rs6000's
+@code{KFmode}, when it is the same as @code{TFmode}.
+
+The default version of the hook returns that mode that was passed in.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_SCALAR_MODE_SUPPORTED_P (scalar_mode @var{mode})
+Define this to return nonzero if the port is prepared to handle
+insns involving scalar mode @var{mode}.  For a scalar mode to be
+considered supported, all the basic arithmetic and comparisons
+must work.
+
+The default version of this hook returns true for any mode
+required to handle the basic C types (as defined by the port).
+Included here are the double-word arithmetic supported by the
+code in @file{optabs.c}.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_VECTOR_MODE_SUPPORTED_P (machine_mode @var{mode})
+Define this to return nonzero if the port is prepared to handle
+insns involving vector mode @var{mode}.  At the very least, it
+must have move patterns for this mode.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_COMPATIBLE_VECTOR_TYPES_P (const_tree @var{type1}, const_tree @var{type2})
+Return true if there is no target-specific reason for treating
+vector types @var{type1} and @var{type2} as distinct types.  The caller
+has already checked for target-independent reasons, meaning that the
+types are known to have the same mode, to have the same number of elements,
+and to have what the caller considers to be compatible element types.
+
+The main reason for defining this hook is to reject pairs of types
+that are handled differently by the target's calling convention.
+For example, when a new @var{N}-bit vector architecture is added
+to a target, the target may want to handle normal @var{N}-bit
+@code{VECTOR_TYPE} arguments and return values in the same way as
+before, to maintain backwards compatibility.  However, it may also
+provide new, architecture-specific @code{VECTOR_TYPE}s that are passed
+and returned in a more efficient way.  It is then important to maintain
+a distinction between the ``normal'' @code{VECTOR_TYPE}s and the new
+architecture-specific ones.
+
+The default implementation returns true, which is correct for most targets.
+@end deftypefn
+
+@deftypefn {Target Hook} opt_machine_mode TARGET_ARRAY_MODE (machine_mode @var{mode}, unsigned HOST_WIDE_INT @var{nelems})
+Return the mode that GCC should use for an array that has
+@var{nelems} elements, with each element having mode @var{mode}.
+Return no mode if the target has no special requirements.  In the
+latter case, GCC looks for an integer mode of the appropriate size
+if available and uses BLKmode otherwise.  Usually the search for the
+integer mode is limited to @code{MAX_FIXED_MODE_SIZE}, but the
+@code{TARGET_ARRAY_MODE_SUPPORTED_P} hook allows a larger mode to be
+used in specific cases.
+
+The main use of this hook is to specify that an array of vectors should
+also have a vector mode.  The default implementation returns no mode.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_ARRAY_MODE_SUPPORTED_P (machine_mode @var{mode}, unsigned HOST_WIDE_INT @var{nelems})
+Return true if GCC should try to use a scalar mode to store an array
+of @var{nelems} elements, given that each element has mode @var{mode}.
+Returning true here overrides the usual @code{MAX_FIXED_MODE} limit
+and allows GCC to use any defined integer mode.
+
+One use of this hook is to support vector load and store operations
+that operate on several homogeneous vectors.  For example, ARM NEON
+has operations like:
+
+@smallexample
+int8x8x3_t vld3_s8 (const int8_t *)
+@end smallexample
+
+where the return type is defined as:
+
+@smallexample
+typedef struct int8x8x3_t
+@{
+  int8x8_t val[3];
+@} int8x8x3_t;
+@end smallexample
+
+If this hook allows @code{val} to have a scalar mode, then
+@code{int8x8x3_t} can have the same mode.  GCC can then store
+@code{int8x8x3_t}s in registers rather than forcing them onto the stack.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_LIBGCC_FLOATING_MODE_SUPPORTED_P (scalar_float_mode @var{mode})
+Define this to return nonzero if libgcc provides support for the 
+floating-point mode @var{mode}, which is known to pass 
+@code{TARGET_SCALAR_MODE_SUPPORTED_P}.  The default version of this 
+hook returns true for all of @code{SFmode}, @code{DFmode}, 
+@code{XFmode} and @code{TFmode}, if such modes exist.
+@end deftypefn
+
+@deftypefn {Target Hook} opt_scalar_float_mode TARGET_FLOATN_MODE (int @var{n}, bool @var{extended})
+Define this to return the machine mode to use for the type 
+@code{_Float@var{n}}, if @var{extended} is false, or the type 
+@code{_Float@var{n}x}, if @var{extended} is true.  If such a type is not
+supported, return @code{opt_scalar_float_mode ()}.  The default version of
+this hook returns @code{SFmode} for @code{_Float32}, @code{DFmode} for
+@code{_Float64} and @code{_Float32x} and @code{TFmode} for 
+@code{_Float128}, if those modes exist and satisfy the requirements for 
+those types and pass @code{TARGET_SCALAR_MODE_SUPPORTED_P} and 
+@code{TARGET_LIBGCC_FLOATING_MODE_SUPPORTED_P}; for @code{_Float64x}, it 
+returns the first of @code{XFmode} and @code{TFmode} that exists and 
+satisfies the same requirements; for other types, it returns 
+@code{opt_scalar_float_mode ()}.  The hook is only called for values
+of @var{n} and @var{extended} that are valid according to
+ISO/IEC TS 18661-3:2015; that is, @var{n} is one of 32, 64, 128, or,
+if @var{extended} is false, 16 or greater than 128 and a multiple of 32.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_FLOATN_BUILTIN_P (int @var{func})
+Define this to return true if the @code{_Float@var{n}} and
+@code{_Float@var{n}x} built-in functions should implicitly enable the
+built-in function without the @code{__builtin_} prefix in addition to the
+normal built-in function with the @code{__builtin_} prefix.  The default is
+to only enable built-in functions without the @code{__builtin_} prefix for
+the GNU C langauge.  In strict ANSI/ISO mode, the built-in function without
+the @code{__builtin_} prefix is not enabled.  The argument @code{FUNC} is the
+@code{enum built_in_function} id of the function to be enabled.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_SMALL_REGISTER_CLASSES_FOR_MODE_P (machine_mode @var{mode})
+Define this to return nonzero for machine modes for which the port has
+small register classes.  If this target hook returns nonzero for a given
+@var{mode}, the compiler will try to minimize the lifetime of registers
+in @var{mode}.  The hook may be called with @code{VOIDmode} as argument.
+In this case, the hook is expected to return nonzero if it returns nonzero
+for any mode.
+
+On some machines, it is risky to let hard registers live across arbitrary
+insns.  Typically, these machines have instructions that require values
+to be in specific registers (like an accumulator), and reload will fail
+if the required hard register is used for another purpose across such an
+insn.
+
+Passes before reload do not know which hard registers will be used
+in an instruction, but the machine modes of the registers set or used in
+the instruction are already known.  And for some machines, register
+classes are small for, say, integer registers but not for floating point
+registers.  For example, the AMD x86-64 architecture requires specific
+registers for the legacy x86 integer instructions, but there are many
+SSE registers for floating point operations.  On such targets, a good
+strategy may be to return nonzero from this hook for @code{INTEGRAL_MODE_P}
+machine modes but zero for the SSE register classes.
+
+The default version of this hook returns false for any mode.  It is always
+safe to redefine this hook to return with a nonzero value.  But if you
+unnecessarily define it, you will reduce the amount of optimizations
+that can be performed in some cases.  If you do not define this hook
+to return a nonzero value when it is required, the compiler will run out
+of spill registers and print a fatal error message.
+@end deftypefn
+
+@node Scalar Return
+@subsection How Scalar Function Values Are Returned
+@cindex return values in registers
+@cindex values, returned by functions
+@cindex scalars, returned as values
+
+This section discusses the macros that control returning scalars as
+values---values that can fit in registers.
+
+@deftypefn {Target Hook} rtx TARGET_FUNCTION_VALUE (const_tree @var{ret_type}, const_tree @var{fn_decl_or_type}, bool @var{outgoing})
+
+Define this to return an RTX representing the place where a function
+returns or receives a value of data type @var{ret_type}, a tree node
+representing a data type.  @var{fn_decl_or_type} is a tree node
+representing @code{FUNCTION_DECL} or @code{FUNCTION_TYPE} of a
+function being called.  If @var{outgoing} is false, the hook should
+compute the register in which the caller will see the return value.
+Otherwise, the hook should return an RTX representing the place where
+a function returns a value.
+
+On many machines, only @code{TYPE_MODE (@var{ret_type})} is relevant.
+(Actually, on most machines, scalar values are returned in the same
+place regardless of mode.)  The value of the expression is usually a
+@code{reg} RTX for the hard register where the return value is stored.
+The value can also be a @code{parallel} RTX, if the return value is in
+multiple places.  See @code{TARGET_FUNCTION_ARG} for an explanation of the
+@code{parallel} form.   Note that the callee will populate every
+location specified in the @code{parallel}, but if the first element of
+the @code{parallel} contains the whole return value, callers will use
+that element as the canonical location and ignore the others.  The m68k
+port uses this type of @code{parallel} to return pointers in both
+@samp{%a0} (the canonical location) and @samp{%d0}.
+
+If @code{TARGET_PROMOTE_FUNCTION_RETURN} returns true, you must apply
+the same promotion rules specified in @code{PROMOTE_MODE} if
+@var{valtype} is a scalar type.
+
+If the precise function being called is known, @var{func} is a tree
+node (@code{FUNCTION_DECL}) for it; otherwise, @var{func} is a null
+pointer.  This makes it possible to use a different value-returning
+convention for specific functions when all their calls are
+known.
+
+Some target machines have ``register windows'' so that the register in
+which a function returns its value is not the same as the one in which
+the caller sees the value.  For such machines, you should return
+different RTX depending on @var{outgoing}.
+
+@code{TARGET_FUNCTION_VALUE} is not used for return values with
+aggregate data types, because these are returned in another way.  See
+@code{TARGET_STRUCT_VALUE_RTX} and related macros, below.
+@end deftypefn
+
+@defmac FUNCTION_VALUE (@var{valtype}, @var{func})
+This macro has been deprecated.  Use @code{TARGET_FUNCTION_VALUE} for
+a new target instead.
+@end defmac
+
+@defmac LIBCALL_VALUE (@var{mode})
+A C expression to create an RTX representing the place where a library
+function returns a value of mode @var{mode}.
+
+Note that ``library function'' in this context means a compiler
+support routine, used to perform arithmetic, whose name is known
+specially by the compiler and was not mentioned in the C code being
+compiled.
+@end defmac
+
+@deftypefn {Target Hook} rtx TARGET_LIBCALL_VALUE (machine_mode @var{mode}, const_rtx @var{fun})
+Define this hook if the back-end needs to know the name of the libcall
+function in order to determine where the result should be returned.
+
+The mode of the result is given by @var{mode} and the name of the called
+library function is given by @var{fun}.  The hook should return an RTX
+representing the place where the library function result will be returned.
+
+If this hook is not defined, then LIBCALL_VALUE will be used.
+@end deftypefn
+
+@defmac FUNCTION_VALUE_REGNO_P (@var{regno})
+A C expression that is nonzero if @var{regno} is the number of a hard
+register in which the values of called function may come back.
+
+A register whose use for returning values is limited to serving as the
+second of a pair (for a value of type @code{double}, say) need not be
+recognized by this macro.  So for most machines, this definition
+suffices:
+
+@smallexample
+#define FUNCTION_VALUE_REGNO_P(N) ((N) == 0)
+@end smallexample
+
+If the machine has register windows, so that the caller and the called
+function use different registers for the return value, this macro
+should recognize only the caller's register numbers.
+
+This macro has been deprecated.  Use @code{TARGET_FUNCTION_VALUE_REGNO_P}
+for a new target instead.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_FUNCTION_VALUE_REGNO_P (const unsigned int @var{regno})
+A target hook that return @code{true} if @var{regno} is the number of a hard
+register in which the values of called function may come back.
+
+A register whose use for returning values is limited to serving as the
+second of a pair (for a value of type @code{double}, say) need not be
+recognized by this target hook.
+
+If the machine has register windows, so that the caller and the called
+function use different registers for the return value, this target hook
+should recognize only the caller's register numbers.
+
+If this hook is not defined, then FUNCTION_VALUE_REGNO_P will be used.
+@end deftypefn
+
+@defmac APPLY_RESULT_SIZE
+Define this macro if @samp{untyped_call} and @samp{untyped_return}
+need more space than is implied by @code{FUNCTION_VALUE_REGNO_P} for
+saving and restoring an arbitrary return value.
+@end defmac
+
+@deftypevr {Target Hook} bool TARGET_OMIT_STRUCT_RETURN_REG
+Normally, when a function returns a structure by memory, the address
+is passed as an invisible pointer argument, but the compiler also
+arranges to return the address from the function like it would a normal
+pointer return value.  Define this to true if that behavior is
+undesirable on your target.
+@end deftypevr
+
+@deftypefn {Target Hook} bool TARGET_RETURN_IN_MSB (const_tree @var{type})
+This hook should return true if values of type @var{type} are returned
+at the most significant end of a register (in other words, if they are
+padded at the least significant end).  You can assume that @var{type}
+is returned in a register; the caller is required to check this.
+
+Note that the register provided by @code{TARGET_FUNCTION_VALUE} must
+be able to hold the complete return value.  For example, if a 1-, 2-
+or 3-byte structure is returned at the most significant end of a
+4-byte register, @code{TARGET_FUNCTION_VALUE} should provide an
+@code{SImode} rtx.
+@end deftypefn
+
+@node Aggregate Return
+@subsection How Large Values Are Returned
+@cindex aggregates as return values
+@cindex large return values
+@cindex returning aggregate values
+@cindex structure value address
+
+When a function value's mode is @code{BLKmode} (and in some other
+cases), the value is not returned according to
+@code{TARGET_FUNCTION_VALUE} (@pxref{Scalar Return}).  Instead, the
+caller passes the address of a block of memory in which the value
+should be stored.  This address is called the @dfn{structure value
+address}.
+
+This section describes how to control returning structure values in
+memory.
+
+@deftypefn {Target Hook} bool TARGET_RETURN_IN_MEMORY (const_tree @var{type}, const_tree @var{fntype})
+This target hook should return a nonzero value to say to return the
+function value in memory, just as large structures are always returned.
+Here @var{type} will be the data type of the value, and @var{fntype}
+will be the type of the function doing the returning, or @code{NULL} for
+libcalls.
+
+Note that values of mode @code{BLKmode} must be explicitly handled
+by this function.  Also, the option @option{-fpcc-struct-return}
+takes effect regardless of this macro.  On most systems, it is
+possible to leave the hook undefined; this causes a default
+definition to be used, whose value is the constant 1 for @code{BLKmode}
+values, and 0 otherwise.
+
+Do not use this hook to indicate that structures and unions should always
+be returned in memory.  You should instead use @code{DEFAULT_PCC_STRUCT_RETURN}
+to indicate this.
+@end deftypefn
+
+@defmac DEFAULT_PCC_STRUCT_RETURN
+Define this macro to be 1 if all structure and union return values must be
+in memory.  Since this results in slower code, this should be defined
+only if needed for compatibility with other compilers or with an ABI@.
+If you define this macro to be 0, then the conventions used for structure
+and union return values are decided by the @code{TARGET_RETURN_IN_MEMORY}
+target hook.
+
+If not defined, this defaults to the value 1.
+@end defmac
+
+@deftypefn {Target Hook} rtx TARGET_STRUCT_VALUE_RTX (tree @var{fndecl}, int @var{incoming})
+This target hook should return the location of the structure value
+address (normally a @code{mem} or @code{reg}), or 0 if the address is
+passed as an ``invisible'' first argument.  Note that @var{fndecl} may
+be @code{NULL}, for libcalls.  You do not need to define this target
+hook if the address is always passed as an ``invisible'' first
+argument.
+
+On some architectures the place where the structure value address
+is found by the called function is not the same place that the
+caller put it.  This can be due to register windows, or it could
+be because the function prologue moves it to a different place.
+@var{incoming} is @code{1} or @code{2} when the location is needed in
+the context of the called function, and @code{0} in the context of
+the caller.
+
+If @var{incoming} is nonzero and the address is to be found on the
+stack, return a @code{mem} which refers to the frame pointer. If
+@var{incoming} is @code{2}, the result is being used to fetch the
+structure value address at the beginning of a function.  If you need
+to emit adjusting code, you should do it at this point.
+@end deftypefn
+
+@defmac PCC_STATIC_STRUCT_RETURN
+Define this macro if the usual system convention on the target machine
+for returning structures and unions is for the called function to return
+the address of a static variable containing the value.
+
+Do not define this if the usual system convention is for the caller to
+pass an address to the subroutine.
+
+This macro has effect in @option{-fpcc-struct-return} mode, but it does
+nothing when you use @option{-freg-struct-return} mode.
+@end defmac
+
+@deftypefn {Target Hook} fixed_size_mode TARGET_GET_RAW_RESULT_MODE (int @var{regno})
+This target hook returns the mode to be used when accessing raw return registers in @code{__builtin_return}.  Define this macro if the value in @var{reg_raw_mode} is not correct.
+@end deftypefn
+
+@deftypefn {Target Hook} fixed_size_mode TARGET_GET_RAW_ARG_MODE (int @var{regno})
+This target hook returns the mode to be used when accessing raw argument registers in @code{__builtin_apply_args}.  Define this macro if the value in @var{reg_raw_mode} is not correct.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_EMPTY_RECORD_P (const_tree @var{type})
+This target hook returns true if the type is an empty record.  The default
+is to return @code{false}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_WARN_PARAMETER_PASSING_ABI (cumulative_args_t @var{ca}, tree @var{type})
+This target hook warns about the change in empty class parameter passing
+ABI.
+@end deftypefn
+
+@node Caller Saves
+@subsection Caller-Saves Register Allocation
+
+If you enable it, GCC can save registers around function calls.  This
+makes it possible to use call-clobbered registers to hold variables that
+must live across calls.
+
+@defmac HARD_REGNO_CALLER_SAVE_MODE (@var{regno}, @var{nregs})
+A C expression specifying which mode is required for saving @var{nregs}
+of a pseudo-register in call-clobbered hard register @var{regno}.  If
+@var{regno} is unsuitable for caller save, @code{VOIDmode} should be
+returned.  For most machines this macro need not be defined since GCC
+will select the smallest suitable mode.
+@end defmac
+
+@node Function Entry
+@subsection Function Entry and Exit
+@cindex function entry and exit
+@cindex prologue
+@cindex epilogue
+
+This section describes the macros that output function entry
+(@dfn{prologue}) and exit (@dfn{epilogue}) code.
+
+@deftypefn {Target Hook} void TARGET_ASM_PRINT_PATCHABLE_FUNCTION_ENTRY (FILE *@var{file}, unsigned HOST_WIDE_INT @var{patch_area_size}, bool @var{record_p})
+Generate a patchable area at the function start, consisting of
+@var{patch_area_size} NOP instructions.  If the target supports named
+sections and if @var{record_p} is true, insert a pointer to the current
+location in the table of patchable functions.  The default implementation
+of the hook places the table of pointers in the special section named
+@code{__patchable_function_entries}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_FUNCTION_PROLOGUE (FILE *@var{file})
+If defined, a function that outputs the assembler code for entry to a
+function.  The prologue is responsible for setting up the stack frame,
+initializing the frame pointer register, saving registers that must be
+saved, and allocating @var{size} additional bytes of storage for the
+local variables.  @var{file} is a stdio stream to which the assembler
+code should be output.
+
+The label for the beginning of the function need not be output by this
+macro.  That has already been done when the macro is run.
+
+@findex regs_ever_live
+To determine which registers to save, the macro can refer to the array
+@code{regs_ever_live}: element @var{r} is nonzero if hard register
+@var{r} is used anywhere within the function.  This implies the function
+prologue should save register @var{r}, provided it is not one of the
+call-used registers.  (@code{TARGET_ASM_FUNCTION_EPILOGUE} must likewise use
+@code{regs_ever_live}.)
+
+On machines that have ``register windows'', the function entry code does
+not save on the stack the registers that are in the windows, even if
+they are supposed to be preserved by function calls; instead it takes
+appropriate steps to ``push'' the register stack, if any non-call-used
+registers are used in the function.
+
+@findex frame_pointer_needed
+On machines where functions may or may not have frame-pointers, the
+function entry code must vary accordingly; it must set up the frame
+pointer if one is wanted, and not otherwise.  To determine whether a
+frame pointer is in wanted, the macro can refer to the variable
+@code{frame_pointer_needed}.  The variable's value will be 1 at run
+time in a function that needs a frame pointer.  @xref{Elimination}.
+
+The function entry code is responsible for allocating any stack space
+required for the function.  This stack space consists of the regions
+listed below.  In most cases, these regions are allocated in the
+order listed, with the last listed region closest to the top of the
+stack (the lowest address if @code{STACK_GROWS_DOWNWARD} is defined, and
+the highest address if it is not defined).  You can use a different order
+for a machine if doing so is more convenient or required for
+compatibility reasons.  Except in cases where required by standard
+or by a debugger, there is no reason why the stack layout used by GCC
+need agree with that used by other compilers for a machine.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_FUNCTION_END_PROLOGUE (FILE *@var{file})
+If defined, a function that outputs assembler code at the end of a
+prologue.  This should be used when the function prologue is being
+emitted as RTL, and you have some extra assembler that needs to be
+emitted.  @xref{prologue instruction pattern}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_FUNCTION_BEGIN_EPILOGUE (FILE *@var{file})
+If defined, a function that outputs assembler code at the start of an
+epilogue.  This should be used when the function epilogue is being
+emitted as RTL, and you have some extra assembler that needs to be
+emitted.  @xref{epilogue instruction pattern}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_FUNCTION_EPILOGUE (FILE *@var{file})
+If defined, a function that outputs the assembler code for exit from a
+function.  The epilogue is responsible for restoring the saved
+registers and stack pointer to their values when the function was
+called, and returning control to the caller.  This macro takes the
+same argument as the macro @code{TARGET_ASM_FUNCTION_PROLOGUE}, and the
+registers to restore are determined from @code{regs_ever_live} and
+@code{CALL_USED_REGISTERS} in the same way.
+
+On some machines, there is a single instruction that does all the work
+of returning from the function.  On these machines, give that
+instruction the name @samp{return} and do not define the macro
+@code{TARGET_ASM_FUNCTION_EPILOGUE} at all.
+
+Do not define a pattern named @samp{return} if you want the
+@code{TARGET_ASM_FUNCTION_EPILOGUE} to be used.  If you want the target
+switches to control whether return instructions or epilogues are used,
+define a @samp{return} pattern with a validity condition that tests the
+target switches appropriately.  If the @samp{return} pattern's validity
+condition is false, epilogues will be used.
+
+On machines where functions may or may not have frame-pointers, the
+function exit code must vary accordingly.  Sometimes the code for these
+two cases is completely different.  To determine whether a frame pointer
+is wanted, the macro can refer to the variable
+@code{frame_pointer_needed}.  The variable's value will be 1 when compiling
+a function that needs a frame pointer.
+
+Normally, @code{TARGET_ASM_FUNCTION_PROLOGUE} and
+@code{TARGET_ASM_FUNCTION_EPILOGUE} must treat leaf functions specially.
+The C variable @code{current_function_is_leaf} is nonzero for such a
+function.  @xref{Leaf Functions}.
+
+On some machines, some functions pop their arguments on exit while
+others leave that for the caller to do.  For example, the 68020 when
+given @option{-mrtd} pops arguments in functions that take a fixed
+number of arguments.
+
+@findex pops_args
+@findex crtl->args.pops_args
+Your definition of the macro @code{RETURN_POPS_ARGS} decides which
+functions pop their own arguments.  @code{TARGET_ASM_FUNCTION_EPILOGUE}
+needs to know what was decided.  The number of bytes of the current
+function's arguments that this function should pop is available in
+@code{crtl->args.pops_args}.  @xref{Scalar Return}.
+@end deftypefn
+
+@itemize @bullet
+@item
+@findex pretend_args_size
+@findex crtl->args.pretend_args_size
+A region of @code{crtl->args.pretend_args_size} bytes of
+uninitialized space just underneath the first argument arriving on the
+stack.  (This may not be at the very start of the allocated stack region
+if the calling sequence has pushed anything else since pushing the stack
+arguments.  But usually, on such machines, nothing else has been pushed
+yet, because the function prologue itself does all the pushing.)  This
+region is used on machines where an argument may be passed partly in
+registers and partly in memory, and, in some cases to support the
+features in @code{<stdarg.h>}.
+
+@item
+An area of memory used to save certain registers used by the function.
+The size of this area, which may also include space for such things as
+the return address and pointers to previous stack frames, is
+machine-specific and usually depends on which registers have been used
+in the function.  Machines with register windows often do not require
+a save area.
+
+@item
+A region of at least @var{size} bytes, possibly rounded up to an allocation
+boundary, to contain the local variables of the function.  On some machines,
+this region and the save area may occur in the opposite order, with the
+save area closer to the top of the stack.
+
+@item
+@cindex @code{ACCUMULATE_OUTGOING_ARGS} and stack frames
+Optionally, when @code{ACCUMULATE_OUTGOING_ARGS} is defined, a region of
+@code{crtl->outgoing_args_size} bytes to be used for outgoing
+argument lists of the function.  @xref{Stack Arguments}.
+@end itemize
+
+@defmac EXIT_IGNORE_STACK
+Define this macro as a C expression that is nonzero if the return
+instruction or the function epilogue ignores the value of the stack
+pointer; in other words, if it is safe to delete an instruction to
+adjust the stack pointer before a return from the function.  The
+default is 0.
+
+Note that this macro's value is relevant only for functions for which
+frame pointers are maintained.  It is never safe to delete a final
+stack adjustment in a function that has no frame pointer, and the
+compiler knows this regardless of @code{EXIT_IGNORE_STACK}.
+@end defmac
+
+@defmac EPILOGUE_USES (@var{regno})
+Define this macro as a C expression that is nonzero for registers that are
+used by the epilogue or the @samp{return} pattern.  The stack and frame
+pointer registers are already assumed to be used as needed.
+@end defmac
+
+@defmac EH_USES (@var{regno})
+Define this macro as a C expression that is nonzero for registers that are
+used by the exception handling mechanism, and so should be considered live
+on entry to an exception edge.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_ASM_OUTPUT_MI_THUNK (FILE *@var{file}, tree @var{thunk_fndecl}, HOST_WIDE_INT @var{delta}, HOST_WIDE_INT @var{vcall_offset}, tree @var{function})
+A function that outputs the assembler code for a thunk
+function, used to implement C++ virtual function calls with multiple
+inheritance.  The thunk acts as a wrapper around a virtual function,
+adjusting the implicit object parameter before handing control off to
+the real function.
+
+First, emit code to add the integer @var{delta} to the location that
+contains the incoming first argument.  Assume that this argument
+contains a pointer, and is the one used to pass the @code{this} pointer
+in C++.  This is the incoming argument @emph{before} the function prologue,
+e.g.@: @samp{%o0} on a sparc.  The addition must preserve the values of
+all other incoming arguments.
+
+Then, if @var{vcall_offset} is nonzero, an additional adjustment should be
+made after adding @code{delta}.  In particular, if @var{p} is the
+adjusted pointer, the following adjustment should be made:
+
+@smallexample
+p += (*((ptrdiff_t **)p))[vcall_offset/sizeof(ptrdiff_t)]
+@end smallexample
+
+After the additions, emit code to jump to @var{function}, which is a
+@code{FUNCTION_DECL}.  This is a direct pure jump, not a call, and does
+not touch the return address.  Hence returning from @var{FUNCTION} will
+return to whoever called the current @samp{thunk}.
+
+The effect must be as if @var{function} had been called directly with
+the adjusted first argument.  This macro is responsible for emitting all
+of the code for a thunk function; @code{TARGET_ASM_FUNCTION_PROLOGUE}
+and @code{TARGET_ASM_FUNCTION_EPILOGUE} are not invoked.
+
+The @var{thunk_fndecl} is redundant.  (@var{delta} and @var{function}
+have already been extracted from it.)  It might possibly be useful on
+some targets, but probably not.
+
+If you do not define this macro, the target-independent code in the C++
+front end will generate a less efficient heavyweight thunk that calls
+@var{function} instead of jumping to it.  The generic approach does
+not support varargs.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_ASM_CAN_OUTPUT_MI_THUNK (const_tree @var{thunk_fndecl}, HOST_WIDE_INT @var{delta}, HOST_WIDE_INT @var{vcall_offset}, const_tree @var{function})
+A function that returns true if TARGET_ASM_OUTPUT_MI_THUNK would be able
+to output the assembler code for the thunk function specified by the
+arguments it is passed, and false otherwise.  In the latter case, the
+generic approach will be used by the C++ front end, with the limitations
+previously exposed.
+@end deftypefn
+
+@node Profiling
+@subsection Generating Code for Profiling
+@cindex profiling, code generation
+
+These macros will help you generate code for profiling.
+
+@defmac FUNCTION_PROFILER (@var{file}, @var{labelno})
+A C statement or compound statement to output to @var{file} some
+assembler code to call the profiling subroutine @code{mcount}.
+
+@findex mcount
+The details of how @code{mcount} expects to be called are determined by
+your operating system environment, not by GCC@.  To figure them out,
+compile a small program for profiling using the system's installed C
+compiler and look at the assembler code that results.
+
+Older implementations of @code{mcount} expect the address of a counter
+variable to be loaded into some register.  The name of this variable is
+@samp{LP} followed by the number @var{labelno}, so you would generate
+the name using @samp{LP%d} in a @code{fprintf}.
+@end defmac
+
+@defmac PROFILE_HOOK
+A C statement or compound statement to output to @var{file} some assembly
+code to call the profiling subroutine @code{mcount} even the target does
+not support profiling.
+@end defmac
+
+@defmac NO_PROFILE_COUNTERS
+Define this macro to be an expression with a nonzero value if the
+@code{mcount} subroutine on your system does not need a counter variable
+allocated for each function.  This is true for almost all modern
+implementations.  If you define this macro, you must not use the
+@var{labelno} argument to @code{FUNCTION_PROFILER}.
+@end defmac
+
+@defmac PROFILE_BEFORE_PROLOGUE
+Define this macro if the code for function profiling should come before
+the function prologue.  Normally, the profiling code comes after.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_KEEP_LEAF_WHEN_PROFILED (void)
+This target hook returns true if the target wants the leaf flag for the current function to stay true even if it calls mcount.  This might make sense for targets using the leaf flag only to determine whether a stack frame needs to be generated or not and for which the call to mcount is generated before the function prologue.
+@end deftypefn
+
+@node Tail Calls
+@subsection Permitting tail calls
+@cindex tail calls
+
+@deftypefn {Target Hook} bool TARGET_FUNCTION_OK_FOR_SIBCALL (tree @var{decl}, tree @var{exp})
+True if it is OK to do sibling call optimization for the specified
+call expression @var{exp}.  @var{decl} will be the called function,
+or @code{NULL} if this is an indirect call.
+
+It is not uncommon for limitations of calling conventions to prevent
+tail calls to functions outside the current unit of translation, or
+during PIC compilation.  The hook is used to enforce these restrictions,
+as the @code{sibcall} md pattern cannot fail, or fall over to a
+``normal'' call.  The criteria for successful sibling call optimization
+may vary greatly between different architectures.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_EXTRA_LIVE_ON_ENTRY (bitmap @var{regs})
+Add any hard registers to @var{regs} that are live on entry to the
+function.  This hook only needs to be defined to provide registers that
+cannot be found by examination of FUNCTION_ARG_REGNO_P, the callee saved
+registers, STATIC_CHAIN_INCOMING_REGNUM, STATIC_CHAIN_REGNUM,
+TARGET_STRUCT_VALUE_RTX, FRAME_POINTER_REGNUM, EH_USES,
+FRAME_POINTER_REGNUM, ARG_POINTER_REGNUM, and the PIC_OFFSET_TABLE_REGNUM.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SET_UP_BY_PROLOGUE (struct hard_reg_set_container *@var{})
+This hook should add additional registers that are computed by the prologue to the hard regset for shrink-wrapping optimization purposes.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_WARN_FUNC_RETURN (tree)
+True if a function's return statements should be checked for matching the function's return type.  This includes checking for falling off the end of a non-void function.  Return false if no such check should be made.
+@end deftypefn
+
+@node Shrink-wrapping separate components
+@subsection Shrink-wrapping separate components
+@cindex shrink-wrapping separate components
+
+The prologue may perform a variety of target dependent tasks such as
+saving callee-saved registers, saving the return address, aligning the
+stack, creating a stack frame, initializing the PIC register, setting
+up the static chain, etc.
+
+On some targets some of these tasks may be independent of others and
+thus may be shrink-wrapped separately.  These independent tasks are
+referred to as components and are handled generically by the target
+independent parts of GCC.
+
+Using the following hooks those prologue or epilogue components can be
+shrink-wrapped separately, so that the initialization (and possibly
+teardown) those components do is not done as frequently on execution
+paths where this would unnecessary.
+
+What exactly those components are is up to the target code; the generic
+code treats them abstractly, as a bit in an @code{sbitmap}.  These
+@code{sbitmap}s are allocated by the @code{shrink_wrap.get_separate_components}
+and @code{shrink_wrap.components_for_bb} hooks, and deallocated by the
+generic code.
+
+@deftypefn {Target Hook} sbitmap TARGET_SHRINK_WRAP_GET_SEPARATE_COMPONENTS (void)
+This hook should return an @code{sbitmap} with the bits set for those
+components that can be separately shrink-wrapped in the current function.
+Return @code{NULL} if the current function should not get any separate
+shrink-wrapping.
+Don't define this hook if it would always return @code{NULL}.
+If it is defined, the other hooks in this group have to be defined as well.
+@end deftypefn
+
+@deftypefn {Target Hook} sbitmap TARGET_SHRINK_WRAP_COMPONENTS_FOR_BB (basic_block)
+This hook should return an @code{sbitmap} with the bits set for those
+components where either the prologue component has to be executed before
+the @code{basic_block}, or the epilogue component after it, or both.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SHRINK_WRAP_DISQUALIFY_COMPONENTS (sbitmap @var{components}, edge @var{e}, sbitmap @var{edge_components}, bool @var{is_prologue})
+This hook should clear the bits in the @var{components} bitmap for those
+components in @var{edge_components} that the target cannot handle on edge
+@var{e}, where @var{is_prologue} says if this is for a prologue or an
+epilogue instead.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SHRINK_WRAP_EMIT_PROLOGUE_COMPONENTS (sbitmap)
+Emit prologue insns for the components indicated by the parameter.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SHRINK_WRAP_EMIT_EPILOGUE_COMPONENTS (sbitmap)
+Emit epilogue insns for the components indicated by the parameter.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SHRINK_WRAP_SET_HANDLED_COMPONENTS (sbitmap)
+Mark the components in the parameter as handled, so that the
+@code{prologue} and @code{epilogue} named patterns know to ignore those
+components.  The target code should not hang on to the @code{sbitmap}, it
+will be deleted after this call.
+@end deftypefn
+
+@node Stack Smashing Protection
+@subsection Stack smashing protection
+@cindex stack smashing protection
+
+@deftypefn {Target Hook} tree TARGET_STACK_PROTECT_GUARD (void)
+This hook returns a @code{DECL} node for the external variable to use
+for the stack protection guard.  This variable is initialized by the
+runtime to some random value and is used to initialize the guard value
+that is placed at the top of the local stack frame.  The type of this
+variable must be @code{ptr_type_node}.
+
+The default version of this hook creates a variable called
+@samp{__stack_chk_guard}, which is normally defined in @file{libgcc2.c}.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_STACK_PROTECT_FAIL (void)
+This hook returns a @code{CALL_EXPR} that alerts the runtime that the
+stack protect guard variable has been modified.  This expression should
+involve a call to a @code{noreturn} function.
+
+The default version of this hook invokes a function called
+@samp{__stack_chk_fail}, taking no arguments.  This function is
+normally defined in @file{libgcc2.c}.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_STACK_PROTECT_RUNTIME_ENABLED_P (void)
+Returns true if the target wants GCC's default stack protect runtime support, otherwise return false.  The default implementation always returns true.
+@end deftypefn
+
+@deftypefn {Common Target Hook} bool TARGET_SUPPORTS_SPLIT_STACK (bool @var{report}, struct gcc_options *@var{opts})
+Whether this target supports splitting the stack when the options described in @var{opts} have been passed.  This is called after options have been parsed, so the target may reject splitting the stack in some configurations.  The default version of this hook returns false.  If @var{report} is true, this function may issue a warning or error; if @var{report} is false, it must simply return a value
+@end deftypefn
+
+@deftypefn {Common Target Hook} {vec<const char *>} TARGET_GET_VALID_OPTION_VALUES (int @var{option_code}, const char *@var{prefix})
+The hook is used for options that have a non-trivial list of possible option values.  OPTION_CODE is option code of opt_code enum type.  PREFIX is used for bash completion and allows an implementation to return more specific completion based on the prefix.  All string values should be allocated from heap memory and consumers should release them.  The result will be pruned to cases with PREFIX if not NULL.
+@end deftypefn
+
+@node Miscellaneous Register Hooks
+@subsection Miscellaneous register hooks
+@cindex miscellaneous register hooks
+
+@deftypevr {Target Hook} bool TARGET_CALL_FUSAGE_CONTAINS_NON_CALLEE_CLOBBERS
+Set to true if each call that binds to a local definition explicitly
+clobbers or sets all non-fixed registers modified by performing the call.
+That is, by the call pattern itself, or by code that might be inserted by the
+linker (e.g.@: stubs, veneers, branch islands), but not including those
+modifiable by the callee.  The affected registers may be mentioned explicitly
+in the call pattern, or included as clobbers in CALL_INSN_FUNCTION_USAGE.
+The default version of this hook is set to false.  The purpose of this hook
+is to enable the fipa-ra optimization.
+@end deftypevr
+
+@node Varargs
+@section Implementing the Varargs Macros
+@cindex varargs implementation
+
+GCC comes with an implementation of @code{<varargs.h>} and
+@code{<stdarg.h>} that work without change on machines that pass arguments
+on the stack.  Other machines require their own implementations of
+varargs, and the two machine independent header files must have
+conditionals to include it.
+
+ISO @code{<stdarg.h>} differs from traditional @code{<varargs.h>} mainly in
+the calling convention for @code{va_start}.  The traditional
+implementation takes just one argument, which is the variable in which
+to store the argument pointer.  The ISO implementation of
+@code{va_start} takes an additional second argument.  The user is
+supposed to write the last named argument of the function here.
+
+However, @code{va_start} should not use this argument.  The way to find
+the end of the named arguments is with the built-in functions described
+below.
+
+@defmac __builtin_saveregs ()
+Use this built-in function to save the argument registers in memory so
+that the varargs mechanism can access them.  Both ISO and traditional
+versions of @code{va_start} must use @code{__builtin_saveregs}, unless
+you use @code{TARGET_SETUP_INCOMING_VARARGS} (see below) instead.
+
+On some machines, @code{__builtin_saveregs} is open-coded under the
+control of the target hook @code{TARGET_EXPAND_BUILTIN_SAVEREGS}.  On
+other machines, it calls a routine written in assembler language,
+found in @file{libgcc2.c}.
+
+Code generated for the call to @code{__builtin_saveregs} appears at the
+beginning of the function, as opposed to where the call to
+@code{__builtin_saveregs} is written, regardless of what the code is.
+This is because the registers must be saved before the function starts
+to use them for its own purposes.
+@c i rewrote the first sentence above to fix an overfull hbox. --mew
+@c 10feb93
+@end defmac
+
+@defmac __builtin_next_arg (@var{lastarg})
+This builtin returns the address of the first anonymous stack
+argument, as type @code{void *}.  If @code{ARGS_GROW_DOWNWARD}, it
+returns the address of the location above the first anonymous stack
+argument.  Use it in @code{va_start} to initialize the pointer for
+fetching arguments from the stack.  Also use it in @code{va_start} to
+verify that the second parameter @var{lastarg} is the last named argument
+of the current function.
+@end defmac
+
+@defmac __builtin_classify_type (@var{object})
+Since each machine has its own conventions for which data types are
+passed in which kind of register, your implementation of @code{va_arg}
+has to embody these conventions.  The easiest way to categorize the
+specified data type is to use @code{__builtin_classify_type} together
+with @code{sizeof} and @code{__alignof__}.
+
+@code{__builtin_classify_type} ignores the value of @var{object},
+considering only its data type.  It returns an integer describing what
+kind of type that is---integer, floating, pointer, structure, and so on.
+
+The file @file{typeclass.h} defines an enumeration that you can use to
+interpret the values of @code{__builtin_classify_type}.
+@end defmac
+
+These machine description macros help implement varargs:
+
+@deftypefn {Target Hook} rtx TARGET_EXPAND_BUILTIN_SAVEREGS (void)
+If defined, this hook produces the machine-specific code for a call to
+@code{__builtin_saveregs}.  This code will be moved to the very
+beginning of the function, before any parameter access are made.  The
+return value of this function should be an RTX that contains the value
+to use as the return of @code{__builtin_saveregs}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SETUP_INCOMING_VARARGS (cumulative_args_t @var{args_so_far}, const function_arg_info @var{&arg}, int *@var{pretend_args_size}, int @var{second_time})
+This target hook offers an alternative to using
+@code{__builtin_saveregs} and defining the hook
+@code{TARGET_EXPAND_BUILTIN_SAVEREGS}.  Use it to store the anonymous
+register arguments into the stack so that all the arguments appear to
+have been passed consecutively on the stack.  Once this is done, you can
+use the standard implementation of varargs that works for machines that
+pass all their arguments on the stack.
+
+The argument @var{args_so_far} points to the @code{CUMULATIVE_ARGS} data
+structure, containing the values that are obtained after processing the
+named arguments.  The argument @var{arg} describes the last of these named
+arguments.
+
+The target hook should do two things: first, push onto the stack all the
+argument registers @emph{not} used for the named arguments, and second,
+store the size of the data thus pushed into the @code{int}-valued
+variable pointed to by @var{pretend_args_size}.  The value that you
+store here will serve as additional offset for setting up the stack
+frame.
+
+Because you must generate code to push the anonymous arguments at
+compile time without knowing their data types,
+@code{TARGET_SETUP_INCOMING_VARARGS} is only useful on machines that
+have just a single category of argument register and use it uniformly
+for all data types.
+
+If the argument @var{second_time} is nonzero, it means that the
+arguments of the function are being analyzed for the second time.  This
+happens for an inline function, which is not actually compiled until the
+end of the source file.  The hook @code{TARGET_SETUP_INCOMING_VARARGS} should
+not generate any instructions in this case.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_STRICT_ARGUMENT_NAMING (cumulative_args_t @var{ca})
+Define this hook to return @code{true} if the location where a function
+argument is passed depends on whether or not it is a named argument.
+
+This hook controls how the @var{named} argument to @code{TARGET_FUNCTION_ARG}
+is set for varargs and stdarg functions.  If this hook returns
+@code{true}, the @var{named} argument is always true for named
+arguments, and false for unnamed arguments.  If it returns @code{false},
+but @code{TARGET_PRETEND_OUTGOING_VARARGS_NAMED} returns @code{true},
+then all arguments are treated as named.  Otherwise, all named arguments
+except the last are treated as named.
+
+You need not define this hook if it always returns @code{false}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_CALL_ARGS (rtx, @var{tree})
+While generating RTL for a function call, this target hook is invoked once
+for each argument passed to the function, either a register returned by
+@code{TARGET_FUNCTION_ARG} or a memory location.  It is called just
+before the point where argument registers are stored.  The type of the
+function to be called is also passed as the second argument; it is
+@code{NULL_TREE} for libcalls.  The @code{TARGET_END_CALL_ARGS} hook is
+invoked just after the code to copy the return reg has been emitted.
+This functionality can be used to perform special setup of call argument
+registers if a target needs it.
+For functions without arguments, the hook is called once with @code{pc_rtx}
+passed instead of an argument register.
+Most ports do not need to implement anything for this hook.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_END_CALL_ARGS (void)
+This target hook is invoked while generating RTL for a function call,
+just after the point where the return reg is copied into a pseudo.  It
+signals that all the call argument and return registers for the just
+emitted call are now no longer in use.
+Most ports do not need to implement anything for this hook.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_PRETEND_OUTGOING_VARARGS_NAMED (cumulative_args_t @var{ca})
+If you need to conditionally change ABIs so that one works with
+@code{TARGET_SETUP_INCOMING_VARARGS}, but the other works like neither
+@code{TARGET_SETUP_INCOMING_VARARGS} nor @code{TARGET_STRICT_ARGUMENT_NAMING} was
+defined, then define this hook to return @code{true} if
+@code{TARGET_SETUP_INCOMING_VARARGS} is used, @code{false} otherwise.
+Otherwise, you should not define this hook.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_LOAD_BOUNDS_FOR_ARG (rtx @var{slot}, rtx @var{arg}, rtx @var{slot_no})
+This hook is used by expand pass to emit insn to load bounds of
+@var{arg} passed in @var{slot}.  Expand pass uses this hook in case
+bounds of @var{arg} are not passed in register.  If @var{slot} is a
+memory, then bounds are loaded as for regular pointer loaded from
+memory.  If @var{slot} is not a memory then @var{slot_no} is an integer
+constant holding number of the target dependent special slot which
+should be used to obtain bounds.  Hook returns RTX holding loaded bounds.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_STORE_BOUNDS_FOR_ARG (rtx @var{arg}, rtx @var{slot}, rtx @var{bounds}, rtx @var{slot_no})
+This hook is used by expand pass to emit insns to store @var{bounds} of
+@var{arg} passed in @var{slot}.  Expand pass uses this hook in case
+@var{bounds} of @var{arg} are not passed in register.  If @var{slot} is a
+memory, then @var{bounds} are stored as for regular pointer stored in
+memory.  If @var{slot} is not a memory then @var{slot_no} is an integer
+constant holding number of the target dependent special slot which
+should be used to store @var{bounds}.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_LOAD_RETURNED_BOUNDS (rtx @var{slot})
+This hook is used by expand pass to emit insn to load bounds
+returned by function call in @var{slot}.  Hook returns RTX holding
+loaded bounds.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_STORE_RETURNED_BOUNDS (rtx @var{slot}, rtx @var{bounds})
+This hook is used by expand pass to emit insn to store @var{bounds}
+returned by function call into @var{slot}.
+@end deftypefn
+
+@node Trampolines
+@section Support for Nested Functions
+@cindex support for nested functions
+@cindex trampolines for nested functions
+@cindex descriptors for nested functions
+@cindex nested functions, support for
+
+Taking the address of a nested function requires special compiler
+handling to ensure that the static chain register is loaded when
+the function is invoked via an indirect call.
+
+GCC has traditionally supported nested functions by creating an
+executable @dfn{trampoline} at run time when the address of a nested
+function is taken.  This is a small piece of code which normally
+resides on the stack, in the stack frame of the containing function.
+The trampoline loads the static chain register and then jumps to the
+real address of the nested function.
+
+The use of trampolines requires an executable stack, which is a
+security risk.  To avoid this problem, GCC also supports another
+strategy: using descriptors for nested functions.  Under this model,
+taking the address of a nested function results in a pointer to a
+non-executable function descriptor object.  Initializing the static chain
+from the descriptor is handled at indirect call sites.
+
+On some targets, including HPPA and IA-64, function descriptors may be
+mandated by the ABI or be otherwise handled in a target-specific way
+by the back end in its code generation strategy for indirect calls.
+GCC also provides its own generic descriptor implementation to support the
+@option{-fno-trampolines} option.  In this case runtime detection of
+function descriptors at indirect call sites relies on descriptor
+pointers being tagged with a bit that is never set in bare function
+addresses.  Since GCC's generic function descriptors are
+not ABI-compliant, this option is typically used only on a
+per-language basis (notably by Ada) or when it can otherwise be
+applied to the whole program.
+
+Define the following hook if your backend either implements ABI-specified
+descriptor support, or can use GCC's generic descriptor implementation
+for nested functions.
+
+@deftypevr {Target Hook} int TARGET_CUSTOM_FUNCTION_DESCRIPTORS
+If the target can use GCC's generic descriptor mechanism for nested
+functions, define this hook to a power of 2 representing an unused bit
+in function pointers which can be used to differentiate descriptors at
+run time.  This value gives the number of bytes by which descriptor
+pointers are misaligned compared to function pointers.  For example, on
+targets that require functions to be aligned to a 4-byte boundary, a
+value of either 1 or 2 is appropriate unless the architecture already
+reserves the bit for another purpose, such as on ARM.
+
+Define this hook to 0 if the target implements ABI support for
+function descriptors in its standard calling sequence, like for example
+HPPA or IA-64.
+
+Using descriptors for nested functions
+eliminates the need for trampolines that reside on the stack and require
+it to be made executable.
+@end deftypevr
+
+The following macros tell GCC how to generate code to allocate and
+initialize an executable trampoline.  You can also use this interface
+if your back end needs to create ABI-specified non-executable descriptors; in
+this case the "trampoline" created is the descriptor containing data only.
+
+The instructions in an executable trampoline must do two things: load
+a constant address into the static chain register, and jump to the real
+address of the nested function.  On CISC machines such as the m68k,
+this requires two instructions, a move immediate and a jump.  Then the
+two addresses exist in the trampoline as word-long immediate operands.
+On RISC machines, it is often necessary to load each address into a
+register in two parts.  Then pieces of each address form separate
+immediate operands.
+
+The code generated to initialize the trampoline must store the variable
+parts---the static chain value and the function address---into the
+immediate operands of the instructions.  On a CISC machine, this is
+simply a matter of copying each address to a memory reference at the
+proper offset from the start of the trampoline.  On a RISC machine, it
+may be necessary to take out pieces of the address and store them
+separately.
+
+@deftypefn {Target Hook} void TARGET_ASM_TRAMPOLINE_TEMPLATE (FILE *@var{f})
+This hook is called by @code{assemble_trampoline_template} to output,
+on the stream @var{f}, assembler code for a block of data that contains
+the constant parts of a trampoline.  This code should not include a
+label---the label is taken care of automatically.
+
+If you do not define this hook, it means no template is needed
+for the target.  Do not define this hook on systems where the block move
+code to copy the trampoline into place would be larger than the code
+to generate it on the spot.
+@end deftypefn
+
+@defmac TRAMPOLINE_SECTION
+Return the section into which the trampoline template is to be placed
+(@pxref{Sections}).  The default value is @code{readonly_data_section}.
+@end defmac
+
+@defmac TRAMPOLINE_SIZE
+A C expression for the size in bytes of the trampoline, as an integer.
+@end defmac
+
+@defmac TRAMPOLINE_ALIGNMENT
+Alignment required for trampolines, in bits.
+
+If you don't define this macro, the value of @code{FUNCTION_ALIGNMENT}
+is used for aligning trampolines.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_TRAMPOLINE_INIT (rtx @var{m_tramp}, tree @var{fndecl}, rtx @var{static_chain})
+This hook is called to initialize a trampoline.
+@var{m_tramp} is an RTX for the memory block for the trampoline; @var{fndecl}
+is the @code{FUNCTION_DECL} for the nested function; @var{static_chain} is an
+RTX for the static chain value that should be passed to the function
+when it is called.
+
+If the target defines @code{TARGET_ASM_TRAMPOLINE_TEMPLATE}, then the
+first thing this hook should do is emit a block move into @var{m_tramp}
+from the memory block returned by @code{assemble_trampoline_template}.
+Note that the block move need only cover the constant parts of the
+trampoline.  If the target isolates the variable parts of the trampoline
+to the end, not all @code{TRAMPOLINE_SIZE} bytes need be copied.
+
+If the target requires any other actions, such as flushing caches
+(possibly calling function maybe_emit_call_builtin___clear_cache) or
+enabling stack execution, these actions should be performed after
+initializing the trampoline proper.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_EMIT_CALL_BUILTIN___CLEAR_CACHE (rtx @var{begin}, rtx @var{end})
+On targets that do not define a @code{clear_cache} insn expander,
+but that define the @code{CLEAR_CACHE_INSN} macro,
+maybe_emit_call_builtin___clear_cache relies on this target hook
+to clear an address range in the instruction cache.
+
+The default implementation calls the @code{__clear_cache} builtin,
+taking the assembler name from the builtin declaration.  Overriding
+definitions may call alternate functions, with alternate calling
+conventions, or emit alternate RTX to perform the job.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_TRAMPOLINE_ADJUST_ADDRESS (rtx @var{addr})
+This hook should perform any machine-specific adjustment in
+the address of the trampoline.  Its argument contains the address of the
+memory block that was passed to @code{TARGET_TRAMPOLINE_INIT}.  In case
+the address to be used for a function call should be different from the
+address at which the template was stored, the different address should
+be returned; otherwise @var{addr} should be returned unchanged.
+If this hook is not defined, @var{addr} will be used for function calls.
+@end deftypefn
+
+Implementing trampolines is difficult on many machines because they have
+separate instruction and data caches.  Writing into a stack location
+fails to clear the memory in the instruction cache, so when the program
+jumps to that location, it executes the old contents.
+
+Here are two possible solutions.  One is to clear the relevant parts of
+the instruction cache whenever a trampoline is set up.  The other is to
+make all trampolines identical, by having them jump to a standard
+subroutine.  The former technique makes trampoline execution faster; the
+latter makes initialization faster.
+
+To clear the instruction cache when a trampoline is initialized, define
+the following macro.
+
+@defmac CLEAR_INSN_CACHE (@var{beg}, @var{end})
+If defined, expands to a C expression clearing the @emph{instruction
+cache} in the specified interval.  The definition of this macro would
+typically be a series of @code{asm} statements.  Both @var{beg} and
+@var{end} are pointer expressions.
+@end defmac
+
+To use a standard subroutine, define the following macro.  In addition,
+you must make sure that the instructions in a trampoline fill an entire
+cache line with identical instructions, or else ensure that the
+beginning of the trampoline code is always aligned at the same point in
+its cache line.  Look in @file{m68k.h} as a guide.
+
+@defmac TRANSFER_FROM_TRAMPOLINE
+Define this macro if trampolines need a special subroutine to do their
+work.  The macro should expand to a series of @code{asm} statements
+which will be compiled with GCC@.  They go in a library function named
+@code{__transfer_from_trampoline}.
+
+If you need to avoid executing the ordinary prologue code of a compiled
+C function when you jump to the subroutine, you can do so by placing a
+special label of your own in the assembler code.  Use one @code{asm}
+statement to generate an assembler label, and another to make the label
+global.  Then trampolines can use that label to jump directly to your
+special assembler code.
+@end defmac
+
+@node Library Calls
+@section Implicit Calls to Library Routines
+@cindex library subroutine names
+@cindex @file{libgcc.a}
+
+@c prevent bad page break with this line
+Here is an explanation of implicit calls to library routines.
+
+@defmac DECLARE_LIBRARY_RENAMES
+This macro, if defined, should expand to a piece of C code that will get
+expanded when compiling functions for libgcc.a.  It can be used to
+provide alternate names for GCC's internal library functions if there
+are ABI-mandated names that the compiler should provide.
+@end defmac
+
+@findex set_optab_libfunc
+@findex init_one_libfunc
+@deftypefn {Target Hook} void TARGET_INIT_LIBFUNCS (void)
+This hook should declare additional library routines or rename
+existing ones, using the functions @code{set_optab_libfunc} and
+@code{init_one_libfunc} defined in @file{optabs.c}.
+@code{init_optabs} calls this macro after initializing all the normal
+library routines.
+
+The default is to do nothing.  Most ports don't need to define this hook.
+@end deftypefn
+
+@deftypevr {Target Hook} bool TARGET_LIBFUNC_GNU_PREFIX
+If false (the default), internal library routines start with two
+underscores.  If set to true, these routines start with @code{__gnu_}
+instead.  E.g., @code{__muldi3} changes to @code{__gnu_muldi3}.  This
+currently only affects functions defined in @file{libgcc2.c}.  If this
+is set to true, the @file{tm.h} file must also
+@code{#define LIBGCC2_GNU_PREFIX}.
+@end deftypevr
+
+@defmac FLOAT_LIB_COMPARE_RETURNS_BOOL (@var{mode}, @var{comparison})
+This macro should return @code{true} if the library routine that
+implements the floating point comparison operator @var{comparison} in
+mode @var{mode} will return a boolean, and @var{false} if it will
+return a tristate.
+
+GCC's own floating point libraries return tristates from the
+comparison operators, so the default returns false always.  Most ports
+don't need to define this macro.
+@end defmac
+
+@defmac TARGET_LIB_INT_CMP_BIASED
+This macro should evaluate to @code{true} if the integer comparison
+functions (like @code{__cmpdi2}) return 0 to indicate that the first
+operand is smaller than the second, 1 to indicate that they are equal,
+and 2 to indicate that the first operand is greater than the second.
+If this macro evaluates to @code{false} the comparison functions return
+@minus{}1, 0, and 1 instead of 0, 1, and 2.  If the target uses the routines
+in @file{libgcc.a}, you do not need to define this macro.
+@end defmac
+
+@defmac TARGET_HAS_NO_HW_DIVIDE
+This macro should be defined if the target has no hardware divide
+instructions.  If this macro is defined, GCC will use an algorithm which
+make use of simple logical and arithmetic operations for 64-bit
+division.  If the macro is not defined, GCC will use an algorithm which
+make use of a 64-bit by 32-bit divide primitive.
+@end defmac
+
+@cindex @code{EDOM}, implicit usage
+@findex matherr
+@defmac TARGET_EDOM
+The value of @code{EDOM} on the target machine, as a C integer constant
+expression.  If you don't define this macro, GCC does not attempt to
+deposit the value of @code{EDOM} into @code{errno} directly.  Look in
+@file{/usr/include/errno.h} to find the value of @code{EDOM} on your
+system.
+
+If you do not define @code{TARGET_EDOM}, then compiled code reports
+domain errors by calling the library function and letting it report the
+error.  If mathematical functions on your system use @code{matherr} when
+there is an error, then you should leave @code{TARGET_EDOM} undefined so
+that @code{matherr} is used normally.
+@end defmac
+
+@cindex @code{errno}, implicit usage
+@defmac GEN_ERRNO_RTX
+Define this macro as a C expression to create an rtl expression that
+refers to the global ``variable'' @code{errno}.  (On certain systems,
+@code{errno} may not actually be a variable.)  If you don't define this
+macro, a reasonable default is used.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_LIBC_HAS_FUNCTION (enum function_class @var{fn_class}, tree @var{type})
+This hook determines whether a function from a class of functions
+@var{fn_class} is present in the target C library.  If @var{type} is NULL,
+the caller asks for support for all standard (float, double, long double)
+types.  If @var{type} is non-NULL, the caller asks for support for a
+specific type.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_LIBC_HAS_FAST_FUNCTION (int @var{fcode})
+This hook determines whether a function from a class of functions
+@code{(enum function_class)}@var{fcode} has a fast implementation.
+@end deftypefn
+
+@defmac NEXT_OBJC_RUNTIME
+Set this macro to 1 to use the "NeXT" Objective-C message sending conventions
+by default.  This calling convention involves passing the object, the selector
+and the method arguments all at once to the method-lookup library function.
+This is the usual setting when targeting Darwin/Mac OS X systems, which have
+the NeXT runtime installed.
+
+If the macro is set to 0, the "GNU" Objective-C message sending convention
+will be used by default.  This convention passes just the object and the
+selector to the method-lookup function, which returns a pointer to the method.
+
+In either case, it remains possible to select code-generation for the alternate
+scheme, by means of compiler command line switches.
+@end defmac
+
+@node Addressing Modes
+@section Addressing Modes
+@cindex addressing modes
+
+@c prevent bad page break with this line
+This is about addressing modes.
+
+@defmac HAVE_PRE_INCREMENT
+@defmacx HAVE_PRE_DECREMENT
+@defmacx HAVE_POST_INCREMENT
+@defmacx HAVE_POST_DECREMENT
+A C expression that is nonzero if the machine supports pre-increment,
+pre-decrement, post-increment, or post-decrement addressing respectively.
+@end defmac
+
+@defmac HAVE_PRE_MODIFY_DISP
+@defmacx HAVE_POST_MODIFY_DISP
+A C expression that is nonzero if the machine supports pre- or
+post-address side-effect generation involving constants other than
+the size of the memory operand.
+@end defmac
+
+@defmac HAVE_PRE_MODIFY_REG
+@defmacx HAVE_POST_MODIFY_REG
+A C expression that is nonzero if the machine supports pre- or
+post-address side-effect generation involving a register displacement.
+@end defmac
+
+@defmac CONSTANT_ADDRESS_P (@var{x})
+A C expression that is 1 if the RTX @var{x} is a constant which
+is a valid address.  On most machines the default definition of
+@code{(CONSTANT_P (@var{x}) && GET_CODE (@var{x}) != CONST_DOUBLE)}
+is acceptable, but a few machines are more restrictive as to which
+constant addresses are supported.
+@end defmac
+
+@defmac CONSTANT_P (@var{x})
+@code{CONSTANT_P}, which is defined by target-independent code,
+accepts integer-values expressions whose values are not explicitly
+known, such as @code{symbol_ref}, @code{label_ref}, and @code{high}
+expressions and @code{const} arithmetic expressions, in addition to
+@code{const_int} and @code{const_double} expressions.
+@end defmac
+
+@defmac MAX_REGS_PER_ADDRESS
+A number, the maximum number of registers that can appear in a valid
+memory address.  Note that it is up to you to specify a value equal to
+the maximum number that @code{TARGET_LEGITIMATE_ADDRESS_P} would ever
+accept.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_LEGITIMATE_ADDRESS_P (machine_mode @var{mode}, rtx @var{x}, bool @var{strict})
+A function that returns whether @var{x} (an RTX) is a legitimate memory
+address on the target machine for a memory operand of mode @var{mode}.
+
+Legitimate addresses are defined in two variants: a strict variant and a
+non-strict one.  The @var{strict} parameter chooses which variant is
+desired by the caller.
+
+The strict variant is used in the reload pass.  It must be defined so
+that any pseudo-register that has not been allocated a hard register is
+considered a memory reference.  This is because in contexts where some
+kind of register is required, a pseudo-register with no hard register
+must be rejected.  For non-hard registers, the strict variant should look
+up the @code{reg_renumber} array; it should then proceed using the hard
+register number in the array, or treat the pseudo as a memory reference
+if the array holds @code{-1}.
+
+The non-strict variant is used in other passes.  It must be defined to
+accept all pseudo-registers in every context where some kind of
+register is required.
+
+Normally, constant addresses which are the sum of a @code{symbol_ref}
+and an integer are stored inside a @code{const} RTX to mark them as
+constant.  Therefore, there is no need to recognize such sums
+specifically as legitimate addresses.  Normally you would simply
+recognize any @code{const} as legitimate.
+
+Usually @code{PRINT_OPERAND_ADDRESS} is not prepared to handle constant
+sums that are not marked with  @code{const}.  It assumes that a naked
+@code{plus} indicates indexing.  If so, then you @emph{must} reject such
+naked constant sums as illegitimate addresses, so that none of them will
+be given to @code{PRINT_OPERAND_ADDRESS}.
+
+@cindex @code{TARGET_ENCODE_SECTION_INFO} and address validation
+On some machines, whether a symbolic address is legitimate depends on
+the section that the address refers to.  On these machines, define the
+target hook @code{TARGET_ENCODE_SECTION_INFO} to store the information
+into the @code{symbol_ref}, and then check for it here.  When you see a
+@code{const}, you will have to look inside it to find the
+@code{symbol_ref} in order to determine the section.  @xref{Assembler
+Format}.
+
+@cindex @code{GO_IF_LEGITIMATE_ADDRESS}
+Some ports are still using a deprecated legacy substitute for
+this hook, the @code{GO_IF_LEGITIMATE_ADDRESS} macro.  This macro
+has this syntax:
+
+@example
+#define GO_IF_LEGITIMATE_ADDRESS (@var{mode}, @var{x}, @var{label})
+@end example
+
+@noindent
+and should @code{goto @var{label}} if the address @var{x} is a valid
+address on the target machine for a memory operand of mode @var{mode}.
+
+@findex REG_OK_STRICT
+Compiler source files that want to use the strict variant of this
+macro define the macro @code{REG_OK_STRICT}.  You should use an
+@code{#ifdef REG_OK_STRICT} conditional to define the strict variant in
+that case and the non-strict variant otherwise.
+
+Using the hook is usually simpler because it limits the number of
+files that are recompiled when changes are made.
+@end deftypefn
+
+@defmac TARGET_MEM_CONSTRAINT
+A single character to be used instead of the default @code{'m'}
+character for general memory addresses.  This defines the constraint
+letter which matches the memory addresses accepted by
+@code{TARGET_LEGITIMATE_ADDRESS_P}.  Define this macro if you want to
+support new address formats in your back end without changing the
+semantics of the @code{'m'} constraint.  This is necessary in order to
+preserve functionality of inline assembly constructs using the
+@code{'m'} constraint.
+@end defmac
+
+@defmac FIND_BASE_TERM (@var{x})
+A C expression to determine the base term of address @var{x},
+or to provide a simplified version of @var{x} from which @file{alias.c}
+can easily find the base term.  This macro is used in only two places:
+@code{find_base_value} and @code{find_base_term} in @file{alias.c}.
+
+It is always safe for this macro to not be defined.  It exists so
+that alias analysis can understand machine-dependent addresses.
+
+The typical use of this macro is to handle addresses containing
+a label_ref or symbol_ref within an UNSPEC@.
+@end defmac
+
+@deftypefn {Target Hook} rtx TARGET_LEGITIMIZE_ADDRESS (rtx @var{x}, rtx @var{oldx}, machine_mode @var{mode})
+This hook is given an invalid memory address @var{x} for an
+operand of mode @var{mode} and should try to return a valid memory
+address.
+
+@findex break_out_memory_refs
+@var{x} will always be the result of a call to @code{break_out_memory_refs},
+and @var{oldx} will be the operand that was given to that function to produce
+@var{x}.
+
+The code of the hook should not alter the substructure of
+@var{x}.  If it transforms @var{x} into a more legitimate form, it
+should return the new @var{x}.
+
+It is not necessary for this hook to come up with a legitimate address,
+with the exception of native TLS addresses (@pxref{Emulated TLS}).
+The compiler has standard ways of doing so in all cases.  In fact, if
+the target supports only emulated TLS, it
+is safe to omit this hook or make it return @var{x} if it cannot find
+a valid way to legitimize the address.  But often a machine-dependent
+strategy can generate better code.
+@end deftypefn
+
+@defmac LEGITIMIZE_RELOAD_ADDRESS (@var{x}, @var{mode}, @var{opnum}, @var{type}, @var{ind_levels}, @var{win})
+A C compound statement that attempts to replace @var{x}, which is an address
+that needs reloading, with a valid memory address for an operand of mode
+@var{mode}.  @var{win} will be a C statement label elsewhere in the code.
+It is not necessary to define this macro, but it might be useful for
+performance reasons.
+
+For example, on the i386, it is sometimes possible to use a single
+reload register instead of two by reloading a sum of two pseudo
+registers into a register.  On the other hand, for number of RISC
+processors offsets are limited so that often an intermediate address
+needs to be generated in order to address a stack slot.  By defining
+@code{LEGITIMIZE_RELOAD_ADDRESS} appropriately, the intermediate addresses
+generated for adjacent some stack slots can be made identical, and thus
+be shared.
+
+@emph{Note}: This macro should be used with caution.  It is necessary
+to know something of how reload works in order to effectively use this,
+and it is quite easy to produce macros that build in too much knowledge
+of reload internals.
+
+@emph{Note}: This macro must be able to reload an address created by a
+previous invocation of this macro.  If it fails to handle such addresses
+then the compiler may generate incorrect code or abort.
+
+@findex push_reload
+The macro definition should use @code{push_reload} to indicate parts that
+need reloading; @var{opnum}, @var{type} and @var{ind_levels} are usually
+suitable to be passed unaltered to @code{push_reload}.
+
+The code generated by this macro must not alter the substructure of
+@var{x}.  If it transforms @var{x} into a more legitimate form, it
+should assign @var{x} (which will always be a C variable) a new value.
+This also applies to parts that you change indirectly by calling
+@code{push_reload}.
+
+@findex strict_memory_address_p
+The macro definition may use @code{strict_memory_address_p} to test if
+the address has become legitimate.
+
+@findex copy_rtx
+If you want to change only a part of @var{x}, one standard way of doing
+this is to use @code{copy_rtx}.  Note, however, that it unshares only a
+single level of rtl.  Thus, if the part to be changed is not at the
+top level, you'll need to replace first the top level.
+It is not necessary for this macro to come up with a legitimate
+address;  but often a machine-dependent strategy can generate better code.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_MODE_DEPENDENT_ADDRESS_P (const_rtx @var{addr}, addr_space_t @var{addrspace})
+This hook returns @code{true} if memory address @var{addr} in address
+space @var{addrspace} can have
+different meanings depending on the machine mode of the memory
+reference it is used for or if the address is valid for some modes
+but not others.
+
+Autoincrement and autodecrement addresses typically have mode-dependent
+effects because the amount of the increment or decrement is the size
+of the operand being addressed.  Some machines have other mode-dependent
+addresses.  Many RISC machines have no mode-dependent addresses.
+
+You may assume that @var{addr} is a valid address for the machine.
+
+The default version of this hook returns @code{false}.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_LEGITIMATE_CONSTANT_P (machine_mode @var{mode}, rtx @var{x})
+This hook returns true if @var{x} is a legitimate constant for a
+@var{mode}-mode immediate operand on the target machine.  You can assume that
+@var{x} satisfies @code{CONSTANT_P}, so you need not check this.
+
+The default definition returns true.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_PRECOMPUTE_TLS_P (machine_mode @var{mode}, rtx @var{x})
+This hook returns true if @var{x} is a TLS operand on the target
+machine that should be pre-computed when used as the argument in a call.
+You can assume that @var{x} satisfies @code{CONSTANT_P}, so you need not 
+check this.
+
+The default definition returns false.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_DELEGITIMIZE_ADDRESS (rtx @var{x})
+This hook is used to undo the possibly obfuscating effects of the
+@code{LEGITIMIZE_ADDRESS} and @code{LEGITIMIZE_RELOAD_ADDRESS} target
+macros.  Some backend implementations of these macros wrap symbol
+references inside an @code{UNSPEC} rtx to represent PIC or similar
+addressing modes.  This target hook allows GCC's optimizers to understand
+the semantics of these opaque @code{UNSPEC}s by converting them back
+into their original form.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_CONST_NOT_OK_FOR_DEBUG_P (rtx @var{x})
+This hook should return true if @var{x} should not be emitted into
+debug sections.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_CANNOT_FORCE_CONST_MEM (machine_mode @var{mode}, rtx @var{x})
+This hook should return true if @var{x} is of a form that cannot (or
+should not) be spilled to the constant pool.  @var{mode} is the mode
+of @var{x}.
+
+The default version of this hook returns false.
+
+The primary reason to define this hook is to prevent reload from
+deciding that a non-legitimate constant would be better reloaded
+from the constant pool instead of spilling and reloading a register
+holding the constant.  This restriction is often true of addresses
+of TLS symbols for various targets.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_USE_BLOCKS_FOR_CONSTANT_P (machine_mode @var{mode}, const_rtx @var{x})
+This hook should return true if pool entries for constant @var{x} can
+be placed in an @code{object_block} structure.  @var{mode} is the mode
+of @var{x}.
+
+The default version returns false for all constants.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_USE_BLOCKS_FOR_DECL_P (const_tree @var{decl})
+This hook should return true if pool entries for @var{decl} should
+be placed in an @code{object_block} structure.
+
+The default version returns true for all decls.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_BUILTIN_RECIPROCAL (tree @var{fndecl})
+This hook should return the DECL of a function that implements the
+reciprocal of the machine-specific builtin function @var{fndecl}, or
+@code{NULL_TREE} if such a function is not available.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_VECTORIZE_BUILTIN_MASK_FOR_LOAD (void)
+This hook should return the DECL of a function @var{f} that given an
+address @var{addr} as an argument returns a mask @var{m} that can be
+used to extract from two vectors the relevant data that resides in
+@var{addr} in case @var{addr} is not properly aligned.
+
+The autovectorizer, when vectorizing a load operation from an address
+@var{addr} that may be unaligned, will generate two vector loads from
+the two aligned addresses around @var{addr}. It then generates a
+@code{REALIGN_LOAD} operation to extract the relevant data from the
+two loaded vectors. The first two arguments to @code{REALIGN_LOAD},
+@var{v1} and @var{v2}, are the two vectors, each of size @var{VS}, and
+the third argument, @var{OFF}, defines how the data will be extracted
+from these two vectors: if @var{OFF} is 0, then the returned vector is
+@var{v2}; otherwise, the returned vector is composed from the last
+@var{VS}-@var{OFF} elements of @var{v1} concatenated to the first
+@var{OFF} elements of @var{v2}.
+
+If this hook is defined, the autovectorizer will generate a call
+to @var{f} (using the DECL tree that this hook returns) and will
+use the return value of @var{f} as the argument @var{OFF} to
+@code{REALIGN_LOAD}. Therefore, the mask @var{m} returned by @var{f}
+should comply with the semantics expected by @code{REALIGN_LOAD}
+described above.
+If this hook is not defined, then @var{addr} will be used as
+the argument @var{OFF} to @code{REALIGN_LOAD}, in which case the low
+log2(@var{VS}) @minus{} 1 bits of @var{addr} will be considered.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_VECTORIZE_BUILTIN_VECTORIZATION_COST (enum vect_cost_for_stmt @var{type_of_cost}, tree @var{vectype}, int @var{misalign})
+Returns cost of different scalar or vector statements for vectorization cost model.
+For vector memory operations the cost may depend on type (@var{vectype}) and
+misalignment value (@var{misalign}).
+@end deftypefn
+
+@deftypefn {Target Hook} poly_uint64 TARGET_VECTORIZE_PREFERRED_VECTOR_ALIGNMENT (const_tree @var{type})
+This hook returns the preferred alignment in bits for accesses to
+vectors of type @var{type} in vectorized code.  This might be less than
+or greater than the ABI-defined value returned by
+@code{TARGET_VECTOR_ALIGNMENT}.  It can be equal to the alignment of
+a single element, in which case the vectorizer will not try to optimize
+for alignment.
+
+The default hook returns @code{TYPE_ALIGN (@var{type})}, which is
+correct for most targets.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_VECTORIZE_VECTOR_ALIGNMENT_REACHABLE (const_tree @var{type}, bool @var{is_packed})
+Return true if vector alignment is reachable (by peeling N iterations) for the given scalar type @var{type}.  @var{is_packed} is false if the scalar access using @var{type} is known to be naturally aligned.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_VECTORIZE_VEC_PERM_CONST (machine_mode @var{mode}, rtx @var{output}, rtx @var{in0}, rtx @var{in1}, const vec_perm_indices @var{&sel})
+This hook is used to test whether the target can permute up to two
+vectors of mode @var{mode} using the permutation vector @code{sel}, and
+also to emit such a permutation.  In the former case @var{in0}, @var{in1}
+and @var{out} are all null.  In the latter case @var{in0} and @var{in1} are
+the source vectors and @var{out} is the destination vector; all three are
+registers of mode @var{mode}.  @var{in1} is the same as @var{in0} if
+@var{sel} describes a permutation on one vector instead of two.
+
+Return true if the operation is possible, emitting instructions for it
+if rtxes are provided.
+
+@cindex @code{vec_perm@var{m}} instruction pattern
+If the hook returns false for a mode with multibyte elements, GCC will
+try the equivalent byte operation.  If that also fails, it will try forcing
+the selector into a register and using the @var{vec_perm@var{mode}}
+instruction pattern.  There is no need for the hook to handle these two
+implementation approaches itself.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_VECTORIZE_BUILTIN_VECTORIZED_FUNCTION (unsigned @var{code}, tree @var{vec_type_out}, tree @var{vec_type_in})
+This hook should return the decl of a function that implements the
+vectorized variant of the function with the @code{combined_fn} code
+@var{code} or @code{NULL_TREE} if such a function is not available.
+The return type of the vectorized function shall be of vector type
+@var{vec_type_out} and the argument types should be @var{vec_type_in}.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_VECTORIZE_BUILTIN_MD_VECTORIZED_FUNCTION (tree @var{fndecl}, tree @var{vec_type_out}, tree @var{vec_type_in})
+This hook should return the decl of a function that implements the
+vectorized variant of target built-in function @code{fndecl}.  The
+return type of the vectorized function shall be of vector type
+@var{vec_type_out} and the argument types should be @var{vec_type_in}.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_VECTORIZE_SUPPORT_VECTOR_MISALIGNMENT (machine_mode @var{mode}, const_tree @var{type}, int @var{misalignment}, bool @var{is_packed})
+This hook should return true if the target supports misaligned vector
+store/load of a specific factor denoted in the @var{misalignment}
+parameter.  The vector store/load should be of machine mode @var{mode} and
+the elements in the vectors should be of type @var{type}.  @var{is_packed}
+parameter is true if the memory access is defined in a packed struct.
+@end deftypefn
+
+@deftypefn {Target Hook} machine_mode TARGET_VECTORIZE_PREFERRED_SIMD_MODE (scalar_mode @var{mode})
+This hook should return the preferred mode for vectorizing scalar
+mode @var{mode}.  The default is
+equal to @code{word_mode}, because the vectorizer can do some
+transformations even in absence of specialized @acronym{SIMD} hardware.
+@end deftypefn
+
+@deftypefn {Target Hook} machine_mode TARGET_VECTORIZE_SPLIT_REDUCTION (machine_mode)
+This hook should return the preferred mode to split the final reduction
+step on @var{mode} to.  The reduction is then carried out reducing upper
+against lower halves of vectors recursively until the specified mode is
+reached.  The default is @var{mode} which means no splitting.
+@end deftypefn
+
+@deftypefn {Target Hook} {unsigned int} TARGET_VECTORIZE_AUTOVECTORIZE_VECTOR_MODES (vector_modes *@var{modes}, bool @var{all})
+If using the mode returned by @code{TARGET_VECTORIZE_PREFERRED_SIMD_MODE}
+is not the only approach worth considering, this hook should add one mode to
+@var{modes} for each useful alternative approach.  These modes are then
+passed to @code{TARGET_VECTORIZE_RELATED_MODE} to obtain the vector mode
+for a given element mode.
+
+The modes returned in @var{modes} should use the smallest element mode
+possible for the vectorization approach that they represent, preferring
+integer modes over floating-poing modes in the event of a tie.  The first
+mode should be the @code{TARGET_VECTORIZE_PREFERRED_SIMD_MODE} for its
+element mode.
+
+If @var{all} is true, add suitable vector modes even when they are generally
+not expected to be worthwhile.
+
+The hook returns a bitmask of flags that control how the modes in
+@var{modes} are used.  The flags are:
+@table @code
+@item VECT_COMPARE_COSTS
+Tells the loop vectorizer to try all the provided modes and pick the one
+with the lowest cost.  By default the vectorizer will choose the first
+mode that works.
+@end table
+
+The hook does not need to do anything if the vector returned by
+@code{TARGET_VECTORIZE_PREFERRED_SIMD_MODE} is the only one relevant
+for autovectorization.  The default implementation adds no modes and
+returns 0.
+@end deftypefn
+
+@deftypefn {Target Hook} opt_machine_mode TARGET_VECTORIZE_RELATED_MODE (machine_mode @var{vector_mode}, scalar_mode @var{element_mode}, poly_uint64 @var{nunits})
+If a piece of code is using vector mode @var{vector_mode} and also wants
+to operate on elements of mode @var{element_mode}, return the vector mode
+it should use for those elements.  If @var{nunits} is nonzero, ensure that
+the mode has exactly @var{nunits} elements, otherwise pick whichever vector
+size pairs the most naturally with @var{vector_mode}.  Return an empty
+@code{opt_machine_mode} if there is no supported vector mode with the
+required properties.
+
+There is no prescribed way of handling the case in which @var{nunits}
+is zero.  One common choice is to pick a vector mode with the same size
+as @var{vector_mode}; this is the natural choice if the target has a
+fixed vector size.  Another option is to choose a vector mode with the
+same number of elements as @var{vector_mode}; this is the natural choice
+if the target has a fixed number of elements.  Alternatively, the hook
+might choose a middle ground, such as trying to keep the number of
+elements as similar as possible while applying maximum and minimum
+vector sizes.
+
+The default implementation uses @code{mode_for_vector} to find the
+requested mode, returning a mode with the same size as @var{vector_mode}
+when @var{nunits} is zero.  This is the correct behavior for most targets.
+@end deftypefn
+
+@deftypefn {Target Hook} opt_machine_mode TARGET_VECTORIZE_GET_MASK_MODE (machine_mode @var{mode})
+Return the mode to use for a vector mask that holds one boolean
+result for each element of vector mode @var{mode}.  The returned mask mode
+can be a vector of integers (class @code{MODE_VECTOR_INT}), a vector of
+booleans (class @code{MODE_VECTOR_BOOL}) or a scalar integer (class
+@code{MODE_INT}).  Return an empty @code{opt_machine_mode} if no such
+mask mode exists.
+
+The default implementation returns a @code{MODE_VECTOR_INT} with the
+same size and number of elements as @var{mode}, if such a mode exists.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_VECTORIZE_EMPTY_MASK_IS_EXPENSIVE (unsigned @var{ifn})
+This hook returns true if masked internal function @var{ifn} (really of
+type @code{internal_fn}) should be considered expensive when the mask is
+all zeros.  GCC can then try to branch around the instruction instead.
+@end deftypefn
+
+@deftypefn {Target Hook} {void *} TARGET_VECTORIZE_INIT_COST (class loop *@var{loop_info})
+This hook should initialize target-specific data structures in preparation for modeling the costs of vectorizing a loop or basic block.  The default allocates three unsigned integers for accumulating costs for the prologue, body, and epilogue of the loop or basic block.  If @var{loop_info} is non-NULL, it identifies the loop being vectorized; otherwise a single block is being vectorized.
+@end deftypefn
+
+@deftypefn {Target Hook} unsigned TARGET_VECTORIZE_ADD_STMT_COST (class vec_info *@var{}, void *@var{data}, int @var{count}, enum vect_cost_for_stmt @var{kind}, class _stmt_vec_info *@var{stmt_info}, tree @var{vectype}, int @var{misalign}, enum vect_cost_model_location @var{where})
+This hook should update the target-specific @var{data} in response to adding @var{count} copies of the given @var{kind} of statement to a loop or basic block.  The default adds the builtin vectorizer cost for the copies of the statement to the accumulator specified by @var{where}, (the prologue, body, or epilogue) and returns the amount added.  The return value should be viewed as a tentative cost that may later be revised.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_VECTORIZE_FINISH_COST (void *@var{data}, unsigned *@var{prologue_cost}, unsigned *@var{body_cost}, unsigned *@var{epilogue_cost})
+This hook should complete calculations of the cost of vectorizing a loop or basic block based on @var{data}, and return the prologue, body, and epilogue costs as unsigned integers.  The default returns the value of the three accumulators.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_VECTORIZE_DESTROY_COST_DATA (void *@var{data})
+This hook should release @var{data} and any related data structures allocated by TARGET_VECTORIZE_INIT_COST.  The default releases the accumulator.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_VECTORIZE_BUILTIN_GATHER (const_tree @var{mem_vectype}, const_tree @var{index_type}, int @var{scale})
+Target builtin that implements vector gather operation.  @var{mem_vectype}
+is the vector type of the load and @var{index_type} is scalar type of
+the index, scaled by @var{scale}.
+The default is @code{NULL_TREE} which means to not vectorize gather
+loads.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_VECTORIZE_BUILTIN_SCATTER (const_tree @var{vectype}, const_tree @var{index_type}, int @var{scale})
+Target builtin that implements vector scatter operation.  @var{vectype}
+is the vector type of the store and @var{index_type} is scalar type of
+the index, scaled by @var{scale}.
+The default is @code{NULL_TREE} which means to not vectorize scatter
+stores.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_SIMD_CLONE_COMPUTE_VECSIZE_AND_SIMDLEN (struct cgraph_node *@var{}, struct cgraph_simd_clone *@var{}, @var{tree}, @var{int})
+This hook should set @var{vecsize_mangle}, @var{vecsize_int}, @var{vecsize_float}
+fields in @var{simd_clone} structure pointed by @var{clone_info} argument and also
+@var{simdlen} field if it was previously 0.
+The hook should return 0 if SIMD clones shouldn't be emitted,
+or number of @var{vecsize_mangle} variants that should be emitted.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SIMD_CLONE_ADJUST (struct cgraph_node *@var{})
+This hook should add implicit @code{attribute(target("..."))} attribute
+to SIMD clone @var{node} if needed.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_SIMD_CLONE_USABLE (struct cgraph_node *@var{})
+This hook should return -1 if SIMD clone @var{node} shouldn't be used
+in vectorized loops in current function, or non-negative number if it is
+usable.  In that case, the smaller the number is, the more desirable it is
+to use it.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_SIMT_VF (void)
+Return number of threads in SIMT thread group on the target.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_OMP_DEVICE_KIND_ARCH_ISA (enum omp_device_kind_arch_isa @var{trait}, const char *@var{name})
+Return 1 if @var{trait} @var{name} is present in the OpenMP context's
+device trait set, return 0 if not present in any OpenMP context in the
+whole translation unit, or -1 if not present in the current OpenMP context
+but might be present in another OpenMP context in the same TU.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_GOACC_VALIDATE_DIMS (tree @var{decl}, int *@var{dims}, int @var{fn_level}, unsigned @var{used})
+This hook should check the launch dimensions provided for an OpenACC
+compute region, or routine.  Defaulted values are represented as -1
+and non-constant values as 0.  The @var{fn_level} is negative for the
+function corresponding to the compute region.  For a routine it is the
+outermost level at which partitioned execution may be spawned.  The hook
+should verify non-default values.  If DECL is NULL, global defaults
+are being validated and unspecified defaults should be filled in.
+Diagnostics should be issued as appropriate.  Return
+true, if changes have been made.  You must override this hook to
+provide dimensions larger than 1.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_GOACC_DIM_LIMIT (int @var{axis})
+This hook should return the maximum size of a particular dimension,
+or zero if unbounded.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_GOACC_FORK_JOIN (gcall *@var{call}, const int *@var{dims}, bool @var{is_fork})
+This hook can be used to convert IFN_GOACC_FORK and IFN_GOACC_JOIN
+function calls to target-specific gimple, or indicate whether they
+should be retained.  It is executed during the oacc_device_lower pass.
+It should return true, if the call should be retained.  It should
+return false, if it is to be deleted (either because target-specific
+gimple has been inserted before it, or there is no need for it).
+The default hook returns false, if there are no RTL expanders for them.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_GOACC_REDUCTION (gcall *@var{call})
+This hook is used by the oacc_transform pass to expand calls to the
+@var{GOACC_REDUCTION} internal function, into a sequence of gimple
+instructions.  @var{call} is gimple statement containing the call to
+the function.  This hook removes statement @var{call} after the
+expanded sequence has been inserted.  This hook is also responsible
+for allocating any storage for reductions when necessary.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_PREFERRED_ELSE_VALUE (unsigned @var{ifn}, tree @var{type}, unsigned @var{nops}, tree *@var{ops})
+This hook returns the target's preferred final argument for a call
+to conditional internal function @var{ifn} (really of type
+@code{internal_fn}).  @var{type} specifies the return type of the
+function and @var{ops} are the operands to the conditional operation,
+of which there are @var{nops}.
+
+For example, if @var{ifn} is @code{IFN_COND_ADD}, the hook returns
+a value of type @var{type} that should be used when @samp{@var{ops}[0]}
+and @samp{@var{ops}[1]} are conditionally added together.
+
+This hook is only relevant if the target supports conditional patterns
+like @code{cond_add@var{m}}.  The default implementation returns a zero
+constant of type @var{type}.
+@end deftypefn
+
+@node Anchored Addresses
+@section Anchored Addresses
+@cindex anchored addresses
+@cindex @option{-fsection-anchors}
+
+GCC usually addresses every static object as a separate entity.
+For example, if we have:
+
+@smallexample
+static int a, b, c;
+int foo (void) @{ return a + b + c; @}
+@end smallexample
+
+the code for @code{foo} will usually calculate three separate symbolic
+addresses: those of @code{a}, @code{b} and @code{c}.  On some targets,
+it would be better to calculate just one symbolic address and access
+the three variables relative to it.  The equivalent pseudocode would
+be something like:
+
+@smallexample
+int foo (void)
+@{
+  register int *xr = &x;
+  return xr[&a - &x] + xr[&b - &x] + xr[&c - &x];
+@}
+@end smallexample
+
+(which isn't valid C).  We refer to shared addresses like @code{x} as
+``section anchors''.  Their use is controlled by @option{-fsection-anchors}.
+
+The hooks below describe the target properties that GCC needs to know
+in order to make effective use of section anchors.  It won't use
+section anchors at all unless either @code{TARGET_MIN_ANCHOR_OFFSET}
+or @code{TARGET_MAX_ANCHOR_OFFSET} is set to a nonzero value.
+
+@deftypevr {Target Hook} HOST_WIDE_INT TARGET_MIN_ANCHOR_OFFSET
+The minimum offset that should be applied to a section anchor.
+On most targets, it should be the smallest offset that can be
+applied to a base register while still giving a legitimate address
+for every mode.  The default value is 0.
+@end deftypevr
+
+@deftypevr {Target Hook} HOST_WIDE_INT TARGET_MAX_ANCHOR_OFFSET
+Like @code{TARGET_MIN_ANCHOR_OFFSET}, but the maximum (inclusive)
+offset that should be applied to section anchors.  The default
+value is 0.
+@end deftypevr
+
+@deftypefn {Target Hook} void TARGET_ASM_OUTPUT_ANCHOR (rtx @var{x})
+Write the assembly code to define section anchor @var{x}, which is a
+@code{SYMBOL_REF} for which @samp{SYMBOL_REF_ANCHOR_P (@var{x})} is true.
+The hook is called with the assembly output position set to the beginning
+of @code{SYMBOL_REF_BLOCK (@var{x})}.
+
+If @code{ASM_OUTPUT_DEF} is available, the hook's default definition uses
+it to define the symbol as @samp{. + SYMBOL_REF_BLOCK_OFFSET (@var{x})}.
+If @code{ASM_OUTPUT_DEF} is not available, the hook's default definition
+is @code{NULL}, which disables the use of section anchors altogether.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_USE_ANCHORS_FOR_SYMBOL_P (const_rtx @var{x})
+Return true if GCC should attempt to use anchors to access @code{SYMBOL_REF}
+@var{x}.  You can assume @samp{SYMBOL_REF_HAS_BLOCK_INFO_P (@var{x})} and
+@samp{!SYMBOL_REF_ANCHOR_P (@var{x})}.
+
+The default version is correct for most targets, but you might need to
+intercept this hook to handle things like target-specific attributes
+or target-specific sections.
+@end deftypefn
+
+@node Condition Code
+@section Condition Code Status
+@cindex condition code status
+
+The macros in this section can be split in two families, according to the
+two ways of representing condition codes in GCC.
+
+The first representation is the so called @code{(cc0)} representation
+(@pxref{Jump Patterns}), where all instructions can have an implicit
+clobber of the condition codes.  The second is the condition code
+register representation, which provides better schedulability for
+architectures that do have a condition code register, but on which
+most instructions do not affect it.  The latter category includes
+most RISC machines.
+
+The implicit clobbering poses a strong restriction on the placement of
+the definition and use of the condition code.  In the past the definition
+and use were always adjacent.  However, recent changes to support trapping
+arithmatic may result in the definition and user being in different blocks.
+Thus, there may be a @code{NOTE_INSN_BASIC_BLOCK} between them.  Additionally,
+the definition may be the source of exception handling edges.
+
+These restrictions can prevent important
+optimizations on some machines.  For example, on the IBM RS/6000, there
+is a delay for taken branches unless the condition code register is set
+three instructions earlier than the conditional branch.  The instruction
+scheduler cannot perform this optimization if it is not permitted to
+separate the definition and use of the condition code register.
+
+For this reason, it is possible and suggested to use a register to
+represent the condition code for new ports.  If there is a specific
+condition code register in the machine, use a hard register.  If the
+condition code or comparison result can be placed in any general register,
+or if there are multiple condition registers, use a pseudo register.
+Registers used to store the condition code value will usually have a mode
+that is in class @code{MODE_CC}.
+
+Alternatively, you can use @code{BImode} if the comparison operator is
+specified already in the compare instruction.  In this case, you are not
+interested in most macros in this section.
+
+@menu
+* CC0 Condition Codes::      Old style representation of condition codes.
+* MODE_CC Condition Codes::  Modern representation of condition codes.
+@end menu
+
+@node CC0 Condition Codes
+@subsection Representation of condition codes using @code{(cc0)}
+@findex cc0
+
+@findex cc_status
+The file @file{conditions.h} defines a variable @code{cc_status} to
+describe how the condition code was computed (in case the interpretation of
+the condition code depends on the instruction that it was set by).  This
+variable contains the RTL expressions on which the condition code is
+currently based, and several standard flags.
+
+Sometimes additional machine-specific flags must be defined in the machine
+description header file.  It can also add additional machine-specific
+information by defining @code{CC_STATUS_MDEP}.
+
+@defmac CC_STATUS_MDEP
+C code for a data type which is used for declaring the @code{mdep}
+component of @code{cc_status}.  It defaults to @code{int}.
+
+This macro is not used on machines that do not use @code{cc0}.
+@end defmac
+
+@defmac CC_STATUS_MDEP_INIT
+A C expression to initialize the @code{mdep} field to ``empty''.
+The default definition does nothing, since most machines don't use
+the field anyway.  If you want to use the field, you should probably
+define this macro to initialize it.
+
+This macro is not used on machines that do not use @code{cc0}.
+@end defmac
+
+@defmac NOTICE_UPDATE_CC (@var{exp}, @var{insn})
+A C compound statement to set the components of @code{cc_status}
+appropriately for an insn @var{insn} whose body is @var{exp}.  It is
+this macro's responsibility to recognize insns that set the condition
+code as a byproduct of other activity as well as those that explicitly
+set @code{(cc0)}.
+
+This macro is not used on machines that do not use @code{cc0}.
+
+If there are insns that do not set the condition code but do alter
+other machine registers, this macro must check to see whether they
+invalidate the expressions that the condition code is recorded as
+reflecting.  For example, on the 68000, insns that store in address
+registers do not set the condition code, which means that usually
+@code{NOTICE_UPDATE_CC} can leave @code{cc_status} unaltered for such
+insns.  But suppose that the previous insn set the condition code
+based on location @samp{a4@@(102)} and the current insn stores a new
+value in @samp{a4}.  Although the condition code is not changed by
+this, it will no longer be true that it reflects the contents of
+@samp{a4@@(102)}.  Therefore, @code{NOTICE_UPDATE_CC} must alter
+@code{cc_status} in this case to say that nothing is known about the
+condition code value.
+
+The definition of @code{NOTICE_UPDATE_CC} must be prepared to deal
+with the results of peephole optimization: insns whose patterns are
+@code{parallel} RTXs containing various @code{reg}, @code{mem} or
+constants which are just the operands.  The RTL structure of these
+insns is not sufficient to indicate what the insns actually do.  What
+@code{NOTICE_UPDATE_CC} should do when it sees one is just to run
+@code{CC_STATUS_INIT}.
+
+A possible definition of @code{NOTICE_UPDATE_CC} is to call a function
+that looks at an attribute (@pxref{Insn Attributes}) named, for example,
+@samp{cc}.  This avoids having detailed information about patterns in
+two places, the @file{md} file and in @code{NOTICE_UPDATE_CC}.
+@end defmac
+
+@node MODE_CC Condition Codes
+@subsection Representation of condition codes using registers
+@findex CCmode
+@findex MODE_CC
+
+@defmac SELECT_CC_MODE (@var{op}, @var{x}, @var{y})
+On many machines, the condition code may be produced by other instructions
+than compares, for example the branch can use directly the condition
+code set by a subtract instruction.  However, on some machines
+when the condition code is set this way some bits (such as the overflow
+bit) are not set in the same way as a test instruction, so that a different
+branch instruction must be used for some conditional branches.  When
+this happens, use the machine mode of the condition code register to
+record different formats of the condition code register.  Modes can
+also be used to record which compare instruction (e.g.@: a signed or an
+unsigned comparison) produced the condition codes.
+
+If other modes than @code{CCmode} are required, add them to
+@file{@var{machine}-modes.def} and define @code{SELECT_CC_MODE} to choose
+a mode given an operand of a compare.  This is needed because the modes
+have to be chosen not only during RTL generation but also, for example,
+by instruction combination.  The result of @code{SELECT_CC_MODE} should
+be consistent with the mode used in the patterns; for example to support
+the case of the add on the SPARC discussed above, we have the pattern
+
+@smallexample
+(define_insn ""
+  [(set (reg:CCNZ 0)
+        (compare:CCNZ
+          (plus:SI (match_operand:SI 0 "register_operand" "%r")
+                   (match_operand:SI 1 "arith_operand" "rI"))
+          (const_int 0)))]
+  ""
+  "@dots{}")
+@end smallexample
+
+@noindent
+together with a @code{SELECT_CC_MODE} that returns @code{CCNZmode}
+for comparisons whose argument is a @code{plus}:
+
+@smallexample
+#define SELECT_CC_MODE(OP,X,Y) \
+  (GET_MODE_CLASS (GET_MODE (X)) == MODE_FLOAT           \
+   ? ((OP == LT || OP == LE || OP == GT || OP == GE)     \
+      ? CCFPEmode : CCFPmode)                            \
+   : ((GET_CODE (X) == PLUS || GET_CODE (X) == MINUS     \
+       || GET_CODE (X) == NEG || GET_CODE (x) == ASHIFT) \
+      ? CCNZmode : CCmode))
+@end smallexample
+
+Another reason to use modes is to retain information on which operands
+were used by the comparison; see @code{REVERSIBLE_CC_MODE} later in
+this section.
+
+You should define this macro if and only if you define extra CC modes
+in @file{@var{machine}-modes.def}.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_CANONICALIZE_COMPARISON (int *@var{code}, rtx *@var{op0}, rtx *@var{op1}, bool @var{op0_preserve_value})
+On some machines not all possible comparisons are defined, but you can
+convert an invalid comparison into a valid one.  For example, the Alpha
+does not have a @code{GT} comparison, but you can use an @code{LT}
+comparison instead and swap the order of the operands.
+
+On such machines, implement this hook to do any required conversions.
+@var{code} is the initial comparison code and @var{op0} and @var{op1}
+are the left and right operands of the comparison, respectively.  If
+@var{op0_preserve_value} is @code{true} the implementation is not
+allowed to change the value of @var{op0} since the value might be used
+in RTXs which aren't comparisons.  E.g. the implementation is not
+allowed to swap operands in that case.
+
+GCC will not assume that the comparison resulting from this macro is
+valid but will see if the resulting insn matches a pattern in the
+@file{md} file.
+
+You need not to implement this hook if it would never change the
+comparison code or operands.
+@end deftypefn
+
+@defmac REVERSIBLE_CC_MODE (@var{mode})
+A C expression whose value is one if it is always safe to reverse a
+comparison whose mode is @var{mode}.  If @code{SELECT_CC_MODE}
+can ever return @var{mode} for a floating-point inequality comparison,
+then @code{REVERSIBLE_CC_MODE (@var{mode})} must be zero.
+
+You need not define this macro if it would always returns zero or if the
+floating-point format is anything other than @code{IEEE_FLOAT_FORMAT}.
+For example, here is the definition used on the SPARC, where floating-point
+inequality comparisons are given either @code{CCFPEmode} or @code{CCFPmode}:
+
+@smallexample
+#define REVERSIBLE_CC_MODE(MODE) \
+   ((MODE) != CCFPEmode && (MODE) != CCFPmode)
+@end smallexample
+@end defmac
+
+@defmac REVERSE_CONDITION (@var{code}, @var{mode})
+A C expression whose value is reversed condition code of the @var{code} for
+comparison done in CC_MODE @var{mode}.  The macro is used only in case
+@code{REVERSIBLE_CC_MODE (@var{mode})} is nonzero.  Define this macro in case
+machine has some non-standard way how to reverse certain conditionals.  For
+instance in case all floating point conditions are non-trapping, compiler may
+freely convert unordered compares to ordered ones.  Then definition may look
+like:
+
+@smallexample
+#define REVERSE_CONDITION(CODE, MODE) \
+   ((MODE) != CCFPmode ? reverse_condition (CODE) \
+    : reverse_condition_maybe_unordered (CODE))
+@end smallexample
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_FIXED_CONDITION_CODE_REGS (unsigned int *@var{p1}, unsigned int *@var{p2})
+On targets which do not use @code{(cc0)}, and which use a hard
+register rather than a pseudo-register to hold condition codes, the
+regular CSE passes are often not able to identify cases in which the
+hard register is set to a common value.  Use this hook to enable a
+small pass which optimizes such cases.  This hook should return true
+to enable this pass, and it should set the integers to which its
+arguments point to the hard register numbers used for condition codes.
+When there is only one such register, as is true on most systems, the
+integer pointed to by @var{p2} should be set to
+@code{INVALID_REGNUM}.
+
+The default version of this hook returns false.
+@end deftypefn
+
+@deftypefn {Target Hook} machine_mode TARGET_CC_MODES_COMPATIBLE (machine_mode @var{m1}, machine_mode @var{m2})
+On targets which use multiple condition code modes in class
+@code{MODE_CC}, it is sometimes the case that a comparison can be
+validly done in more than one mode.  On such a system, define this
+target hook to take two mode arguments and to return a mode in which
+both comparisons may be validly done.  If there is no such mode,
+return @code{VOIDmode}.
+
+The default version of this hook checks whether the modes are the
+same.  If they are, it returns that mode.  If they are different, it
+returns @code{VOIDmode}.
+@end deftypefn
+
+@deftypevr {Target Hook} {unsigned int} TARGET_FLAGS_REGNUM
+If the target has a dedicated flags register, and it needs to use the
+post-reload comparison elimination pass, or the delay slot filler pass,
+then this value should be set appropriately.
+@end deftypevr
+
+@node Costs
+@section Describing Relative Costs of Operations
+@cindex costs of instructions
+@cindex relative costs
+@cindex speed of instructions
+
+These macros let you describe the relative speed of various operations
+on the target machine.
+
+@defmac REGISTER_MOVE_COST (@var{mode}, @var{from}, @var{to})
+A C expression for the cost of moving data of mode @var{mode} from a
+register in class @var{from} to one in class @var{to}.  The classes are
+expressed using the enumeration values such as @code{GENERAL_REGS}.  A
+value of 2 is the default; other values are interpreted relative to
+that.
+
+It is not required that the cost always equal 2 when @var{from} is the
+same as @var{to}; on some machines it is expensive to move between
+registers if they are not general registers.
+
+If reload sees an insn consisting of a single @code{set} between two
+hard registers, and if @code{REGISTER_MOVE_COST} applied to their
+classes returns a value of 2, reload does not check to ensure that the
+constraints of the insn are met.  Setting a cost of other than 2 will
+allow reload to verify that the constraints are met.  You should do this
+if the @samp{mov@var{m}} pattern's constraints do not allow such copying.
+
+These macros are obsolete, new ports should use the target hook
+@code{TARGET_REGISTER_MOVE_COST} instead.
+@end defmac
+
+@deftypefn {Target Hook} int TARGET_REGISTER_MOVE_COST (machine_mode @var{mode}, reg_class_t @var{from}, reg_class_t @var{to})
+This target hook should return the cost of moving data of mode @var{mode}
+from a register in class @var{from} to one in class @var{to}.  The classes
+are expressed using the enumeration values such as @code{GENERAL_REGS}.
+A value of 2 is the default; other values are interpreted relative to
+that.
+
+It is not required that the cost always equal 2 when @var{from} is the
+same as @var{to}; on some machines it is expensive to move between
+registers if they are not general registers.
+
+If reload sees an insn consisting of a single @code{set} between two
+hard registers, and if @code{TARGET_REGISTER_MOVE_COST} applied to their
+classes returns a value of 2, reload does not check to ensure that the
+constraints of the insn are met.  Setting a cost of other than 2 will
+allow reload to verify that the constraints are met.  You should do this
+if the @samp{mov@var{m}} pattern's constraints do not allow such copying.
+
+The default version of this function returns 2.
+@end deftypefn
+
+@defmac MEMORY_MOVE_COST (@var{mode}, @var{class}, @var{in})
+A C expression for the cost of moving data of mode @var{mode} between a
+register of class @var{class} and memory; @var{in} is zero if the value
+is to be written to memory, nonzero if it is to be read in.  This cost
+is relative to those in @code{REGISTER_MOVE_COST}.  If moving between
+registers and memory is more expensive than between two registers, you
+should define this macro to express the relative cost.
+
+If you do not define this macro, GCC uses a default cost of 4 plus
+the cost of copying via a secondary reload register, if one is
+needed.  If your machine requires a secondary reload register to copy
+between memory and a register of @var{class} but the reload mechanism is
+more complex than copying via an intermediate, define this macro to
+reflect the actual cost of the move.
+
+GCC defines the function @code{memory_move_secondary_cost} if
+secondary reloads are needed.  It computes the costs due to copying via
+a secondary register.  If your machine copies from memory using a
+secondary register in the conventional way but the default base value of
+4 is not correct for your machine, define this macro to add some other
+value to the result of that function.  The arguments to that function
+are the same as to this macro.
+
+These macros are obsolete, new ports should use the target hook
+@code{TARGET_MEMORY_MOVE_COST} instead.
+@end defmac
+
+@deftypefn {Target Hook} int TARGET_MEMORY_MOVE_COST (machine_mode @var{mode}, reg_class_t @var{rclass}, bool @var{in})
+This target hook should return the cost of moving data of mode @var{mode}
+between a register of class @var{rclass} and memory; @var{in} is @code{false}
+if the value is to be written to memory, @code{true} if it is to be read in.
+This cost is relative to those in @code{TARGET_REGISTER_MOVE_COST}.
+If moving between registers and memory is more expensive than between two
+registers, you should add this target hook to express the relative cost.
+
+If you do not add this target hook, GCC uses a default cost of 4 plus
+the cost of copying via a secondary reload register, if one is
+needed.  If your machine requires a secondary reload register to copy
+between memory and a register of @var{rclass} but the reload mechanism is
+more complex than copying via an intermediate, use this target hook to
+reflect the actual cost of the move.
+
+GCC defines the function @code{memory_move_secondary_cost} if
+secondary reloads are needed.  It computes the costs due to copying via
+a secondary register.  If your machine copies from memory using a
+secondary register in the conventional way but the default base value of
+4 is not correct for your machine, use this target hook to add some other
+value to the result of that function.  The arguments to that function
+are the same as to this target hook.
+@end deftypefn
+
+@defmac BRANCH_COST (@var{speed_p}, @var{predictable_p})
+A C expression for the cost of a branch instruction.  A value of 1 is
+the default; other values are interpreted relative to that. Parameter
+@var{speed_p} is true when the branch in question should be optimized
+for speed.  When it is false, @code{BRANCH_COST} should return a value
+optimal for code size rather than performance.  @var{predictable_p} is
+true for well-predicted branches. On many architectures the
+@code{BRANCH_COST} can be reduced then.
+@end defmac
+
+Here are additional macros which do not specify precise relative costs,
+but only that certain actions are more expensive than GCC would
+ordinarily expect.
+
+@defmac SLOW_BYTE_ACCESS
+Define this macro as a C expression which is nonzero if accessing less
+than a word of memory (i.e.@: a @code{char} or a @code{short}) is no
+faster than accessing a word of memory, i.e., if such access
+require more than one instruction or if there is no difference in cost
+between byte and (aligned) word loads.
+
+When this macro is not defined, the compiler will access a field by
+finding the smallest containing object; when it is defined, a fullword
+load will be used if alignment permits.  Unless bytes accesses are
+faster than word accesses, using word accesses is preferable since it
+may eliminate subsequent memory access if subsequent accesses occur to
+other fields in the same word of the structure, but to different bytes.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_SLOW_UNALIGNED_ACCESS (machine_mode @var{mode}, unsigned int @var{align})
+This hook returns true if memory accesses described by the
+@var{mode} and @var{alignment} parameters have a cost many times greater
+than aligned accesses, for example if they are emulated in a trap handler.
+This hook is invoked only for unaligned accesses, i.e.@: when
+@code{@var{alignment} < GET_MODE_ALIGNMENT (@var{mode})}.
+
+When this hook returns true, the compiler will act as if
+@code{STRICT_ALIGNMENT} were true when generating code for block
+moves.  This can cause significantly more instructions to be produced.
+Therefore, do not make this hook return true if unaligned accesses only
+add a cycle or two to the time for a memory access.
+
+The hook must return true whenever @code{STRICT_ALIGNMENT} is true.
+The default implementation returns @code{STRICT_ALIGNMENT}.
+@end deftypefn
+
+@defmac MOVE_RATIO (@var{speed})
+The threshold of number of scalar memory-to-memory move insns, @emph{below}
+which a sequence of insns should be generated instead of a
+string move insn or a library call.  Increasing the value will always
+make code faster, but eventually incurs high cost in increased code size.
+
+Note that on machines where the corresponding move insn is a
+@code{define_expand} that emits a sequence of insns, this macro counts
+the number of such sequences.
+
+The parameter @var{speed} is true if the code is currently being
+optimized for speed rather than size.
+
+If you don't define this, a reasonable default is used.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_USE_BY_PIECES_INFRASTRUCTURE_P (unsigned HOST_WIDE_INT @var{size}, unsigned int @var{alignment}, enum by_pieces_operation @var{op}, bool @var{speed_p})
+GCC will attempt several strategies when asked to copy between
+two areas of memory, or to set, clear or store to memory, for example
+when copying a @code{struct}. The @code{by_pieces} infrastructure
+implements such memory operations as a sequence of load, store or move
+insns.  Alternate strategies are to expand the
+@code{cpymem} or @code{setmem} optabs, to emit a library call, or to emit
+unit-by-unit, loop-based operations.
+
+This target hook should return true if, for a memory operation with a
+given @var{size} and @var{alignment}, using the @code{by_pieces}
+infrastructure is expected to result in better code generation.
+Both @var{size} and @var{alignment} are measured in terms of storage
+units.
+
+The parameter @var{op} is one of: @code{CLEAR_BY_PIECES},
+@code{MOVE_BY_PIECES}, @code{SET_BY_PIECES}, @code{STORE_BY_PIECES} or
+@code{COMPARE_BY_PIECES}.  These describe the type of memory operation
+under consideration.
+
+The parameter @var{speed_p} is true if the code is currently being
+optimized for speed rather than size.
+
+Returning true for higher values of @var{size} can improve code generation
+for speed if the target does not provide an implementation of the
+@code{cpymem} or @code{setmem} standard names, if the @code{cpymem} or
+@code{setmem} implementation would be more expensive than a sequence of
+insns, or if the overhead of a library call would dominate that of
+the body of the memory operation.
+
+Returning true for higher values of @code{size} may also cause an increase
+in code size, for example where the number of insns emitted to perform a
+move would be greater than that of a library call.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_COMPARE_BY_PIECES_BRANCH_RATIO (machine_mode @var{mode})
+When expanding a block comparison in MODE, gcc can try to reduce the
+number of branches at the expense of more memory operations.  This hook
+allows the target to override the default choice.  It should return the
+factor by which branches should be reduced over the plain expansion with
+one comparison per @var{mode}-sized piece.  A port can also prevent a
+particular mode from being used for block comparisons by returning a
+negative number from this hook.
+@end deftypefn
+
+@defmac MOVE_MAX_PIECES
+A C expression used by @code{move_by_pieces} to determine the largest unit
+a load or store used to copy memory is.  Defaults to @code{MOVE_MAX}.
+@end defmac
+
+@defmac STORE_MAX_PIECES
+A C expression used by @code{store_by_pieces} to determine the largest unit
+a store used to memory is.  Defaults to @code{MOVE_MAX_PIECES}, or two times
+the size of @code{HOST_WIDE_INT}, whichever is smaller.
+@end defmac
+
+@defmac COMPARE_MAX_PIECES
+A C expression used by @code{compare_by_pieces} to determine the largest unit
+a load or store used to compare memory is.  Defaults to
+@code{MOVE_MAX_PIECES}.
+@end defmac
+
+@defmac CLEAR_RATIO (@var{speed})
+The threshold of number of scalar move insns, @emph{below} which a sequence
+of insns should be generated to clear memory instead of a string clear insn
+or a library call.  Increasing the value will always make code faster, but
+eventually incurs high cost in increased code size.
+
+The parameter @var{speed} is true if the code is currently being
+optimized for speed rather than size.
+
+If you don't define this, a reasonable default is used.
+@end defmac
+
+@defmac SET_RATIO (@var{speed})
+The threshold of number of scalar move insns, @emph{below} which a sequence
+of insns should be generated to set memory to a constant value, instead of
+a block set insn or a library call.
+Increasing the value will always make code faster, but
+eventually incurs high cost in increased code size.
+
+The parameter @var{speed} is true if the code is currently being
+optimized for speed rather than size.
+
+If you don't define this, it defaults to the value of @code{MOVE_RATIO}.
+@end defmac
+
+@defmac USE_LOAD_POST_INCREMENT (@var{mode})
+A C expression used to determine whether a load postincrement is a good
+thing to use for a given mode.  Defaults to the value of
+@code{HAVE_POST_INCREMENT}.
+@end defmac
+
+@defmac USE_LOAD_POST_DECREMENT (@var{mode})
+A C expression used to determine whether a load postdecrement is a good
+thing to use for a given mode.  Defaults to the value of
+@code{HAVE_POST_DECREMENT}.
+@end defmac
+
+@defmac USE_LOAD_PRE_INCREMENT (@var{mode})
+A C expression used to determine whether a load preincrement is a good
+thing to use for a given mode.  Defaults to the value of
+@code{HAVE_PRE_INCREMENT}.
+@end defmac
+
+@defmac USE_LOAD_PRE_DECREMENT (@var{mode})
+A C expression used to determine whether a load predecrement is a good
+thing to use for a given mode.  Defaults to the value of
+@code{HAVE_PRE_DECREMENT}.
+@end defmac
+
+@defmac USE_STORE_POST_INCREMENT (@var{mode})
+A C expression used to determine whether a store postincrement is a good
+thing to use for a given mode.  Defaults to the value of
+@code{HAVE_POST_INCREMENT}.
+@end defmac
+
+@defmac USE_STORE_POST_DECREMENT (@var{mode})
+A C expression used to determine whether a store postdecrement is a good
+thing to use for a given mode.  Defaults to the value of
+@code{HAVE_POST_DECREMENT}.
+@end defmac
+
+@defmac USE_STORE_PRE_INCREMENT (@var{mode})
+This macro is used to determine whether a store preincrement is a good
+thing to use for a given mode.  Defaults to the value of
+@code{HAVE_PRE_INCREMENT}.
+@end defmac
+
+@defmac USE_STORE_PRE_DECREMENT (@var{mode})
+This macro is used to determine whether a store predecrement is a good
+thing to use for a given mode.  Defaults to the value of
+@code{HAVE_PRE_DECREMENT}.
+@end defmac
+
+@defmac NO_FUNCTION_CSE
+Define this macro to be true if it is as good or better to call a constant
+function address than to call an address kept in a register.
+@end defmac
+
+@defmac LOGICAL_OP_NON_SHORT_CIRCUIT
+Define this macro if a non-short-circuit operation produced by
+@samp{fold_range_test ()} is optimal.  This macro defaults to true if
+@code{BRANCH_COST} is greater than or equal to the value 2.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_OPTAB_SUPPORTED_P (int @var{op}, machine_mode @var{mode1}, machine_mode @var{mode2}, optimization_type @var{opt_type})
+Return true if the optimizers should use optab @var{op} with
+modes @var{mode1} and @var{mode2} for optimization type @var{opt_type}.
+The optab is known to have an associated @file{.md} instruction
+whose C condition is true.  @var{mode2} is only meaningful for conversion
+optabs; for direct optabs it is a copy of @var{mode1}.
+
+For example, when called with @var{op} equal to @code{rint_optab} and
+@var{mode1} equal to @code{DFmode}, the hook should say whether the
+optimizers should use optab @code{rintdf2}.
+
+The default hook returns true for all inputs.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_RTX_COSTS (rtx @var{x}, machine_mode @var{mode}, int @var{outer_code}, int @var{opno}, int *@var{total}, bool @var{speed})
+This target hook describes the relative costs of RTL expressions.
+
+The cost may depend on the precise form of the expression, which is
+available for examination in @var{x}, and the fact that @var{x} appears
+as operand @var{opno} of an expression with rtx code @var{outer_code}.
+That is, the hook can assume that there is some rtx @var{y} such
+that @samp{GET_CODE (@var{y}) == @var{outer_code}} and such that
+either (a) @samp{XEXP (@var{y}, @var{opno}) == @var{x}} or
+(b) @samp{XVEC (@var{y}, @var{opno})} contains @var{x}.
+
+@var{mode} is @var{x}'s machine mode, or for cases like @code{const_int} that
+do not have a mode, the mode in which @var{x} is used.
+
+In implementing this hook, you can use the construct
+@code{COSTS_N_INSNS (@var{n})} to specify a cost equal to @var{n} fast
+instructions.
+
+On entry to the hook, @code{*@var{total}} contains a default estimate
+for the cost of the expression.  The hook should modify this value as
+necessary.  Traditionally, the default costs are @code{COSTS_N_INSNS (5)}
+for multiplications, @code{COSTS_N_INSNS (7)} for division and modulus
+operations, and @code{COSTS_N_INSNS (1)} for all other operations.
+
+When optimizing for code size, i.e.@: when @code{speed} is
+false, this target hook should be used to estimate the relative
+size cost of an expression, again relative to @code{COSTS_N_INSNS}.
+
+The hook returns true when all subexpressions of @var{x} have been
+processed, and false when @code{rtx_cost} should recurse.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_ADDRESS_COST (rtx @var{address}, machine_mode @var{mode}, addr_space_t @var{as}, bool @var{speed})
+This hook computes the cost of an addressing mode that contains
+@var{address}.  If not defined, the cost is computed from
+the @var{address} expression and the @code{TARGET_RTX_COST} hook.
+
+For most CISC machines, the default cost is a good approximation of the
+true cost of the addressing mode.  However, on RISC machines, all
+instructions normally have the same length and execution time.  Hence
+all addresses will have equal costs.
+
+In cases where more than one form of an address is known, the form with
+the lowest cost will be used.  If multiple forms have the same, lowest,
+cost, the one that is the most complex will be used.
+
+For example, suppose an address that is equal to the sum of a register
+and a constant is used twice in the same basic block.  When this macro
+is not defined, the address will be computed in a register and memory
+references will be indirect through that register.  On machines where
+the cost of the addressing mode containing the sum is no higher than
+that of a simple indirect reference, this will produce an additional
+instruction and possibly require an additional register.  Proper
+specification of this macro eliminates this overhead for such machines.
+
+This hook is never called with an invalid address.
+
+On machines where an address involving more than one register is as
+cheap as an address computation involving only one register, defining
+@code{TARGET_ADDRESS_COST} to reflect this can cause two registers to
+be live over a region of code where only one would have been if
+@code{TARGET_ADDRESS_COST} were not defined in that manner.  This effect
+should be considered in the definition of this macro.  Equivalent costs
+should probably only be given to addresses with different numbers of
+registers on machines with lots of registers.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_INSN_COST (rtx_insn *@var{insn}, bool @var{speed})
+This target hook describes the relative costs of RTL instructions.
+
+In implementing this hook, you can use the construct
+@code{COSTS_N_INSNS (@var{n})} to specify a cost equal to @var{n} fast
+instructions.
+
+When optimizing for code size, i.e.@: when @code{speed} is
+false, this target hook should be used to estimate the relative
+size cost of an expression, again relative to @code{COSTS_N_INSNS}.
+@end deftypefn
+
+@deftypefn {Target Hook} {unsigned int} TARGET_MAX_NOCE_IFCVT_SEQ_COST (edge @var{e})
+This hook returns a value in the same units as @code{TARGET_RTX_COSTS},
+giving the maximum acceptable cost for a sequence generated by the RTL
+if-conversion pass when conditional execution is not available.
+The RTL if-conversion pass attempts to convert conditional operations
+that would require a branch to a series of unconditional operations and
+@code{mov@var{mode}cc} insns.  This hook returns the maximum cost of the
+unconditional instructions and the @code{mov@var{mode}cc} insns.
+RTL if-conversion is cancelled if the cost of the converted sequence
+is greater than the value returned by this hook.
+
+@code{e} is the edge between the basic block containing the conditional
+branch to the basic block which would be executed if the condition
+were true.
+
+The default implementation of this hook uses the
+@code{max-rtl-if-conversion-[un]predictable} parameters if they are set,
+and uses a multiple of @code{BRANCH_COST} otherwise.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_NOCE_CONVERSION_PROFITABLE_P (rtx_insn *@var{seq}, struct noce_if_info *@var{if_info})
+This hook returns true if the instruction sequence @code{seq} is a good
+candidate as a replacement for the if-convertible sequence described in
+@code{if_info}.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_NEW_ADDRESS_PROFITABLE_P (rtx @var{memref}, rtx_insn * @var{insn}, rtx @var{new_addr})
+Return @code{true} if it is profitable to replace the address in
+@var{memref} with @var{new_addr}.  This allows targets to prevent the
+scheduler from undoing address optimizations.  The instruction containing the
+memref is @var{insn}.  The default implementation returns @code{true}.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_NO_SPECULATION_IN_DELAY_SLOTS_P (void)
+This predicate controls the use of the eager delay slot filler to disallow
+speculatively executed instructions being placed in delay slots.  Targets
+such as certain MIPS architectures possess both branches with and without
+delay slots.  As the eager delay slot filler can decrease performance,
+disabling it is beneficial when ordinary branches are available.  Use of
+delay slot branches filled using the basic filler is often still desirable
+as the delay slot can hide a pipeline bubble.
+@end deftypefn
+
+@deftypefn {Target Hook} HOST_WIDE_INT TARGET_ESTIMATED_POLY_VALUE (poly_int64 @var{val}, poly_value_estimate_kind @var{kind})
+Return an estimate of the runtime value of @var{val}, for use in
+things like cost calculations or profiling frequencies.  @var{kind} is used
+to ask for the minimum, maximum, and likely estimates of the value through
+the @code{POLY_VALUE_MIN}, @code{POLY_VALUE_MAX} and
+@code{POLY_VALUE_LIKELY} values.  The default
+implementation returns the lowest possible value of @var{val}.
+@end deftypefn
+
+@node Scheduling
+@section Adjusting the Instruction Scheduler
+
+The instruction scheduler may need a fair amount of machine-specific
+adjustment in order to produce good code.  GCC provides several target
+hooks for this purpose.  It is usually enough to define just a few of
+them: try the first ones in this list first.
+
+@deftypefn {Target Hook} int TARGET_SCHED_ISSUE_RATE (void)
+This hook returns the maximum number of instructions that can ever
+issue at the same time on the target machine.  The default is one.
+Although the insn scheduler can define itself the possibility of issue
+an insn on the same cycle, the value can serve as an additional
+constraint to issue insns on the same simulated processor cycle (see
+hooks @samp{TARGET_SCHED_REORDER} and @samp{TARGET_SCHED_REORDER2}).
+This value must be constant over the entire compilation.  If you need
+it to vary depending on what the instructions are, you must use
+@samp{TARGET_SCHED_VARIABLE_ISSUE}.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_SCHED_VARIABLE_ISSUE (FILE *@var{file}, int @var{verbose}, rtx_insn *@var{insn}, int @var{more})
+This hook is executed by the scheduler after it has scheduled an insn
+from the ready list.  It should return the number of insns which can
+still be issued in the current cycle.  The default is
+@samp{@w{@var{more} - 1}} for insns other than @code{CLOBBER} and
+@code{USE}, which normally are not counted against the issue rate.
+You should define this hook if some insns take more machine resources
+than others, so that fewer insns can follow them in the same cycle.
+@var{file} is either a null pointer, or a stdio stream to write any
+debug output to.  @var{verbose} is the verbose level provided by
+@option{-fsched-verbose-@var{n}}.  @var{insn} is the instruction that
+was scheduled.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_SCHED_ADJUST_COST (rtx_insn *@var{insn}, int @var{dep_type1}, rtx_insn *@var{dep_insn}, int @var{cost}, unsigned int @var{dw})
+This function corrects the value of @var{cost} based on the
+relationship between @var{insn} and @var{dep_insn} through a
+dependence of type dep_type, and strength @var{dw}.  It should return the new
+value.  The default is to make no adjustment to @var{cost}.  This can be
+used for example to specify to the scheduler using the traditional pipeline
+description that an output- or anti-dependence does not incur the same cost
+as a data-dependence.  If the scheduler using the automaton based pipeline
+description, the cost of anti-dependence is zero and the cost of
+output-dependence is maximum of one and the difference of latency
+times of the first and the second insns.  If these values are not
+acceptable, you could use the hook to modify them too.  See also
+@pxref{Processor pipeline description}.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_SCHED_ADJUST_PRIORITY (rtx_insn *@var{insn}, int @var{priority})
+This hook adjusts the integer scheduling priority @var{priority} of
+@var{insn}.  It should return the new priority.  Increase the priority to
+execute @var{insn} earlier, reduce the priority to execute @var{insn}
+later.  Do not define this hook if you do not need to adjust the
+scheduling priorities of insns.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_SCHED_REORDER (FILE *@var{file}, int @var{verbose}, rtx_insn **@var{ready}, int *@var{n_readyp}, int @var{clock})
+This hook is executed by the scheduler after it has scheduled the ready
+list, to allow the machine description to reorder it (for example to
+combine two small instructions together on @samp{VLIW} machines).
+@var{file} is either a null pointer, or a stdio stream to write any
+debug output to.  @var{verbose} is the verbose level provided by
+@option{-fsched-verbose-@var{n}}.  @var{ready} is a pointer to the ready
+list of instructions that are ready to be scheduled.  @var{n_readyp} is
+a pointer to the number of elements in the ready list.  The scheduler
+reads the ready list in reverse order, starting with
+@var{ready}[@var{*n_readyp} @minus{} 1] and going to @var{ready}[0].  @var{clock}
+is the timer tick of the scheduler.  You may modify the ready list and
+the number of ready insns.  The return value is the number of insns that
+can issue this cycle; normally this is just @code{issue_rate}.  See also
+@samp{TARGET_SCHED_REORDER2}.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_SCHED_REORDER2 (FILE *@var{file}, int @var{verbose}, rtx_insn **@var{ready}, int *@var{n_readyp}, int @var{clock})
+Like @samp{TARGET_SCHED_REORDER}, but called at a different time.  That
+function is called whenever the scheduler starts a new cycle.  This one
+is called once per iteration over a cycle, immediately after
+@samp{TARGET_SCHED_VARIABLE_ISSUE}; it can reorder the ready list and
+return the number of insns to be scheduled in the same cycle.  Defining
+this hook can be useful if there are frequent situations where
+scheduling one insn causes other insns to become ready in the same
+cycle.  These other insns can then be taken into account properly.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_SCHED_MACRO_FUSION_P (void)
+This hook is used to check whether target platform supports macro fusion.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_SCHED_MACRO_FUSION_PAIR_P (rtx_insn *@var{prev}, rtx_insn *@var{curr})
+This hook is used to check whether two insns should be macro fused for
+a target microarchitecture. If this hook returns true for the given insn pair
+(@var{prev} and @var{curr}), the scheduler will put them into a sched
+group, and they will not be scheduled apart.  The two insns will be either
+two SET insns or a compare and a conditional jump and this hook should
+validate any dependencies needed to fuse the two insns together.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_DEPENDENCIES_EVALUATION_HOOK (rtx_insn *@var{head}, rtx_insn *@var{tail})
+This hook is called after evaluation forward dependencies of insns in
+chain given by two parameter values (@var{head} and @var{tail}
+correspondingly) but before insns scheduling of the insn chain.  For
+example, it can be used for better insn classification if it requires
+analysis of dependencies.  This hook can use backward and forward
+dependencies of the insn scheduler because they are already
+calculated.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_INIT (FILE *@var{file}, int @var{verbose}, int @var{max_ready})
+This hook is executed by the scheduler at the beginning of each block of
+instructions that are to be scheduled.  @var{file} is either a null
+pointer, or a stdio stream to write any debug output to.  @var{verbose}
+is the verbose level provided by @option{-fsched-verbose-@var{n}}.
+@var{max_ready} is the maximum number of insns in the current scheduling
+region that can be live at the same time.  This can be used to allocate
+scratch space if it is needed, e.g.@: by @samp{TARGET_SCHED_REORDER}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_FINISH (FILE *@var{file}, int @var{verbose})
+This hook is executed by the scheduler at the end of each block of
+instructions that are to be scheduled.  It can be used to perform
+cleanup of any actions done by the other scheduling hooks.  @var{file}
+is either a null pointer, or a stdio stream to write any debug output
+to.  @var{verbose} is the verbose level provided by
+@option{-fsched-verbose-@var{n}}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_INIT_GLOBAL (FILE *@var{file}, int @var{verbose}, int @var{old_max_uid})
+This hook is executed by the scheduler after function level initializations.
+@var{file} is either a null pointer, or a stdio stream to write any debug output to.
+@var{verbose} is the verbose level provided by @option{-fsched-verbose-@var{n}}.
+@var{old_max_uid} is the maximum insn uid when scheduling begins.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_FINISH_GLOBAL (FILE *@var{file}, int @var{verbose})
+This is the cleanup hook corresponding to @code{TARGET_SCHED_INIT_GLOBAL}.
+@var{file} is either a null pointer, or a stdio stream to write any debug output to.
+@var{verbose} is the verbose level provided by @option{-fsched-verbose-@var{n}}.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_SCHED_DFA_PRE_CYCLE_INSN (void)
+The hook returns an RTL insn.  The automaton state used in the
+pipeline hazard recognizer is changed as if the insn were scheduled
+when the new simulated processor cycle starts.  Usage of the hook may
+simplify the automaton pipeline description for some @acronym{VLIW}
+processors.  If the hook is defined, it is used only for the automaton
+based pipeline description.  The default is not to change the state
+when the new simulated processor cycle starts.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_INIT_DFA_PRE_CYCLE_INSN (void)
+The hook can be used to initialize data used by the previous hook.
+@end deftypefn
+
+@deftypefn {Target Hook} {rtx_insn *} TARGET_SCHED_DFA_POST_CYCLE_INSN (void)
+The hook is analogous to @samp{TARGET_SCHED_DFA_PRE_CYCLE_INSN} but used
+to changed the state as if the insn were scheduled when the new
+simulated processor cycle finishes.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_INIT_DFA_POST_CYCLE_INSN (void)
+The hook is analogous to @samp{TARGET_SCHED_INIT_DFA_PRE_CYCLE_INSN} but
+used to initialize data used by the previous hook.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_DFA_PRE_ADVANCE_CYCLE (void)
+The hook to notify target that the current simulated cycle is about to finish.
+The hook is analogous to @samp{TARGET_SCHED_DFA_PRE_CYCLE_INSN} but used
+to change the state in more complicated situations - e.g., when advancing
+state on a single insn is not enough.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_DFA_POST_ADVANCE_CYCLE (void)
+The hook to notify target that new simulated cycle has just started.
+The hook is analogous to @samp{TARGET_SCHED_DFA_POST_CYCLE_INSN} but used
+to change the state in more complicated situations - e.g., when advancing
+state on a single insn is not enough.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD (void)
+This hook controls better choosing an insn from the ready insn queue
+for the @acronym{DFA}-based insn scheduler.  Usually the scheduler
+chooses the first insn from the queue.  If the hook returns a positive
+value, an additional scheduler code tries all permutations of
+@samp{TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD ()}
+subsequent ready insns to choose an insn whose issue will result in
+maximal number of issued insns on the same cycle.  For the
+@acronym{VLIW} processor, the code could actually solve the problem of
+packing simple insns into the @acronym{VLIW} insn.  Of course, if the
+rules of @acronym{VLIW} packing are described in the automaton.
+
+This code also could be used for superscalar @acronym{RISC}
+processors.  Let us consider a superscalar @acronym{RISC} processor
+with 3 pipelines.  Some insns can be executed in pipelines @var{A} or
+@var{B}, some insns can be executed only in pipelines @var{B} or
+@var{C}, and one insn can be executed in pipeline @var{B}.  The
+processor may issue the 1st insn into @var{A} and the 2nd one into
+@var{B}.  In this case, the 3rd insn will wait for freeing @var{B}
+until the next cycle.  If the scheduler issues the 3rd insn the first,
+the processor could issue all 3 insns per cycle.
+
+Actually this code demonstrates advantages of the automaton based
+pipeline hazard recognizer.  We try quickly and easy many insn
+schedules to choose the best one.
+
+The default is no multipass scheduling.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD (rtx_insn *@var{insn}, int @var{ready_index})
+
+This hook controls what insns from the ready insn queue will be
+considered for the multipass insn scheduling.  If the hook returns
+zero for @var{insn}, the insn will be considered in multipass scheduling.
+Positive return values will remove @var{insn} from consideration on
+the current round of multipass scheduling.
+Negative return values will remove @var{insn} from consideration for given
+number of cycles.
+Backends should be careful about returning non-zero for highest priority
+instruction at position 0 in the ready list.  @var{ready_index} is passed
+to allow backends make correct judgements.
+
+The default is that any ready insns can be chosen to be issued.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_FIRST_CYCLE_MULTIPASS_BEGIN (void *@var{data}, signed char *@var{ready_try}, int @var{n_ready}, bool @var{first_cycle_insn_p})
+This hook prepares the target backend for a new round of multipass
+scheduling.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_FIRST_CYCLE_MULTIPASS_ISSUE (void *@var{data}, signed char *@var{ready_try}, int @var{n_ready}, rtx_insn *@var{insn}, const void *@var{prev_data})
+This hook is called when multipass scheduling evaluates instruction INSN.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_FIRST_CYCLE_MULTIPASS_BACKTRACK (const void *@var{data}, signed char *@var{ready_try}, int @var{n_ready})
+This is called when multipass scheduling backtracks from evaluation of
+an instruction.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_FIRST_CYCLE_MULTIPASS_END (const void *@var{data})
+This hook notifies the target about the result of the concluded current
+round of multipass scheduling.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_FIRST_CYCLE_MULTIPASS_INIT (void *@var{data})
+This hook initializes target-specific data used in multipass scheduling.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_FIRST_CYCLE_MULTIPASS_FINI (void *@var{data})
+This hook finalizes target-specific data used in multipass scheduling.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_SCHED_DFA_NEW_CYCLE (FILE *@var{dump}, int @var{verbose}, rtx_insn *@var{insn}, int @var{last_clock}, int @var{clock}, int *@var{sort_p})
+This hook is called by the insn scheduler before issuing @var{insn}
+on cycle @var{clock}.  If the hook returns nonzero,
+@var{insn} is not issued on this processor cycle.  Instead,
+the processor cycle is advanced.  If *@var{sort_p}
+is zero, the insn ready queue is not sorted on the new cycle
+start as usually.  @var{dump} and @var{verbose} specify the file and
+verbosity level to use for debugging output.
+@var{last_clock} and @var{clock} are, respectively, the
+processor cycle on which the previous insn has been issued,
+and the current processor cycle.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_SCHED_IS_COSTLY_DEPENDENCE (struct _dep *@var{_dep}, int @var{cost}, int @var{distance})
+This hook is used to define which dependences are considered costly by
+the target, so costly that it is not advisable to schedule the insns that
+are involved in the dependence too close to one another.  The parameters
+to this hook are as follows:  The first parameter @var{_dep} is the dependence
+being evaluated.  The second parameter @var{cost} is the cost of the
+dependence as estimated by the scheduler, and the third
+parameter @var{distance} is the distance in cycles between the two insns.
+The hook returns @code{true} if considering the distance between the two
+insns the dependence between them is considered costly by the target,
+and @code{false} otherwise.
+
+Defining this hook can be useful in multiple-issue out-of-order machines,
+where (a) it's practically hopeless to predict the actual data/resource
+delays, however: (b) there's a better chance to predict the actual grouping
+that will be formed, and (c) correctly emulating the grouping can be very
+important.  In such targets one may want to allow issuing dependent insns
+closer to one another---i.e., closer than the dependence distance;  however,
+not in cases of ``costly dependences'', which this hooks allows to define.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_H_I_D_EXTENDED (void)
+This hook is called by the insn scheduler after emitting a new instruction to
+the instruction stream.  The hook notifies a target backend to extend its
+per instruction data structures.
+@end deftypefn
+
+@deftypefn {Target Hook} {void *} TARGET_SCHED_ALLOC_SCHED_CONTEXT (void)
+Return a pointer to a store large enough to hold target scheduling context.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_INIT_SCHED_CONTEXT (void *@var{tc}, bool @var{clean_p})
+Initialize store pointed to by @var{tc} to hold target scheduling context.
+It @var{clean_p} is true then initialize @var{tc} as if scheduler is at the
+beginning of the block.  Otherwise, copy the current context into @var{tc}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_SET_SCHED_CONTEXT (void *@var{tc})
+Copy target scheduling context pointed to by @var{tc} to the current context.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_CLEAR_SCHED_CONTEXT (void *@var{tc})
+Deallocate internal data in target scheduling context pointed to by @var{tc}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_FREE_SCHED_CONTEXT (void *@var{tc})
+Deallocate a store for target scheduling context pointed to by @var{tc}.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_SCHED_SPECULATE_INSN (rtx_insn *@var{insn}, unsigned int @var{dep_status}, rtx *@var{new_pat})
+This hook is called by the insn scheduler when @var{insn} has only
+speculative dependencies and therefore can be scheduled speculatively.
+The hook is used to check if the pattern of @var{insn} has a speculative
+version and, in case of successful check, to generate that speculative
+pattern.  The hook should return 1, if the instruction has a speculative form,
+or @minus{}1, if it doesn't.  @var{request} describes the type of requested
+speculation.  If the return value equals 1 then @var{new_pat} is assigned
+the generated speculative pattern.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_SCHED_NEEDS_BLOCK_P (unsigned int @var{dep_status})
+This hook is called by the insn scheduler during generation of recovery code
+for @var{insn}.  It should return @code{true}, if the corresponding check
+instruction should branch to recovery code, or @code{false} otherwise.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_SCHED_GEN_SPEC_CHECK (rtx_insn *@var{insn}, rtx_insn *@var{label}, unsigned int @var{ds})
+This hook is called by the insn scheduler to generate a pattern for recovery
+check instruction.  If @var{mutate_p} is zero, then @var{insn} is a
+speculative instruction for which the check should be generated.
+@var{label} is either a label of a basic block, where recovery code should
+be emitted, or a null pointer, when requested check doesn't branch to
+recovery code (a simple check).  If @var{mutate_p} is nonzero, then
+a pattern for a branchy check corresponding to a simple check denoted by
+@var{insn} should be generated.  In this case @var{label} can't be null.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_SET_SCHED_FLAGS (struct spec_info_def *@var{spec_info})
+This hook is used by the insn scheduler to find out what features should be
+enabled/used.
+The structure *@var{spec_info} should be filled in by the target.
+The structure describes speculation types that can be used in the scheduler.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_SCHED_CAN_SPECULATE_INSN (rtx_insn *@var{insn})
+Some instructions should never be speculated by the schedulers, usually
+ because the instruction is too expensive to get this wrong.  Often such
+ instructions have long latency, and often they are not fully modeled in the
+ pipeline descriptions.  This hook should return @code{false} if @var{insn}
+ should not be speculated.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_SCHED_SMS_RES_MII (struct ddg *@var{g})
+This hook is called by the swing modulo scheduler to calculate a
+resource-based lower bound which is based on the resources available in
+the machine and the resources required by each instruction.  The target
+backend can use @var{g} to calculate such bound.  A very simple lower
+bound will be used in case this hook is not implemented: the total number
+of instructions divided by the issue rate.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_SCHED_DISPATCH (rtx_insn *@var{insn}, int @var{x})
+This hook is called by Haifa Scheduler.  It returns true if dispatch scheduling
+is supported in hardware and the condition specified in the parameter is true.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_DISPATCH_DO (rtx_insn *@var{insn}, int @var{x})
+This hook is called by Haifa Scheduler.  It performs the operation specified
+in its second parameter.
+@end deftypefn
+
+@deftypevr {Target Hook} bool TARGET_SCHED_EXPOSED_PIPELINE
+True if the processor has an exposed pipeline, which means that not just
+the order of instructions is important for correctness when scheduling, but
+also the latencies of operations.
+@end deftypevr
+
+@deftypefn {Target Hook} int TARGET_SCHED_REASSOCIATION_WIDTH (unsigned int @var{opc}, machine_mode @var{mode})
+This hook is called by tree reassociator to determine a level of
+parallelism required in output calculations chain.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SCHED_FUSION_PRIORITY (rtx_insn *@var{insn}, int @var{max_pri}, int *@var{fusion_pri}, int *@var{pri})
+This hook is called by scheduling fusion pass.  It calculates fusion
+priorities for each instruction passed in by parameter.  The priorities
+are returned via pointer parameters.
+
+@var{insn} is the instruction whose priorities need to be calculated.
+@var{max_pri} is the maximum priority can be returned in any cases.
+@var{fusion_pri} is the pointer parameter through which @var{insn}'s
+fusion priority should be calculated and returned.
+@var{pri} is the pointer parameter through which @var{insn}'s priority
+should be calculated and returned.
+
+Same @var{fusion_pri} should be returned for instructions which should
+be scheduled together.  Different @var{pri} should be returned for
+instructions with same @var{fusion_pri}.  @var{fusion_pri} is the major
+sort key, @var{pri} is the minor sort key.  All instructions will be
+scheduled according to the two priorities.  All priorities calculated
+should be between 0 (exclusive) and @var{max_pri} (inclusive).  To avoid
+false dependencies, @var{fusion_pri} of instructions which need to be
+scheduled together should be smaller than @var{fusion_pri} of irrelevant
+instructions.
+
+Given below example:
+
+@smallexample
+    ldr r10, [r1, 4]
+    add r4, r4, r10
+    ldr r15, [r2, 8]
+    sub r5, r5, r15
+    ldr r11, [r1, 0]
+    add r4, r4, r11
+    ldr r16, [r2, 12]
+    sub r5, r5, r16
+@end smallexample
+
+On targets like ARM/AArch64, the two pairs of consecutive loads should be
+merged.  Since peephole2 pass can't help in this case unless consecutive
+loads are actually next to each other in instruction flow.  That's where
+this scheduling fusion pass works.  This hook calculates priority for each
+instruction based on its fustion type, like:
+
+@smallexample
+    ldr r10, [r1, 4]  ; fusion_pri=99,  pri=96
+    add r4, r4, r10   ; fusion_pri=100, pri=100
+    ldr r15, [r2, 8]  ; fusion_pri=98,  pri=92
+    sub r5, r5, r15   ; fusion_pri=100, pri=100
+    ldr r11, [r1, 0]  ; fusion_pri=99,  pri=100
+    add r4, r4, r11   ; fusion_pri=100, pri=100
+    ldr r16, [r2, 12] ; fusion_pri=98,  pri=88
+    sub r5, r5, r16   ; fusion_pri=100, pri=100
+@end smallexample
+
+Scheduling fusion pass then sorts all ready to issue instructions according
+to the priorities.  As a result, instructions of same fusion type will be
+pushed together in instruction flow, like:
+
+@smallexample
+    ldr r11, [r1, 0]
+    ldr r10, [r1, 4]
+    ldr r15, [r2, 8]
+    ldr r16, [r2, 12]
+    add r4, r4, r10
+    sub r5, r5, r15
+    add r4, r4, r11
+    sub r5, r5, r16
+@end smallexample
+
+Now peephole2 pass can simply merge the two pairs of loads.
+
+Since scheduling fusion pass relies on peephole2 to do real fusion
+work, it is only enabled by default when peephole2 is in effect.
+
+This is firstly introduced on ARM/AArch64 targets, please refer to
+the hook implementation for how different fusion types are supported.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_EXPAND_DIVMOD_LIBFUNC (rtx @var{libfunc}, machine_mode @var{mode}, rtx @var{op0}, rtx @var{op1}, rtx *@var{quot}, rtx *@var{rem})
+Define this hook for enabling divmod transform if the port does not have
+hardware divmod insn but defines target-specific divmod libfuncs.
+@end deftypefn
+
+@node Sections
+@section Dividing the Output into Sections (Texts, Data, @dots{})
+@c the above section title is WAY too long.  maybe cut the part between
+@c the (...)?  --mew 10feb93
+
+An object file is divided into sections containing different types of
+data.  In the most common case, there are three sections: the @dfn{text
+section}, which holds instructions and read-only data; the @dfn{data
+section}, which holds initialized writable data; and the @dfn{bss
+section}, which holds uninitialized data.  Some systems have other kinds
+of sections.
+
+@file{varasm.c} provides several well-known sections, such as
+@code{text_section}, @code{data_section} and @code{bss_section}.
+The normal way of controlling a @code{@var{foo}_section} variable
+is to define the associated @code{@var{FOO}_SECTION_ASM_OP} macro,
+as described below.  The macros are only read once, when @file{varasm.c}
+initializes itself, so their values must be run-time constants.
+They may however depend on command-line flags.
+
+@emph{Note:} Some run-time files, such @file{crtstuff.c}, also make
+use of the @code{@var{FOO}_SECTION_ASM_OP} macros, and expect them
+to be string literals.
+
+Some assemblers require a different string to be written every time a
+section is selected.  If your assembler falls into this category, you
+should define the @code{TARGET_ASM_INIT_SECTIONS} hook and use
+@code{get_unnamed_section} to set up the sections.
+
+You must always create a @code{text_section}, either by defining
+@code{TEXT_SECTION_ASM_OP} or by initializing @code{text_section}
+in @code{TARGET_ASM_INIT_SECTIONS}.  The same is true of
+@code{data_section} and @code{DATA_SECTION_ASM_OP}.  If you do not
+create a distinct @code{readonly_data_section}, the default is to
+reuse @code{text_section}.
+
+All the other @file{varasm.c} sections are optional, and are null
+if the target does not provide them.
+
+@defmac TEXT_SECTION_ASM_OP
+A C expression whose value is a string, including spacing, containing the
+assembler operation that should precede instructions and read-only data.
+Normally @code{"\t.text"} is right.
+@end defmac
+
+@defmac HOT_TEXT_SECTION_NAME
+If defined, a C string constant for the name of the section containing most
+frequently executed functions of the program.  If not defined, GCC will provide
+a default definition if the target supports named sections.
+@end defmac
+
+@defmac UNLIKELY_EXECUTED_TEXT_SECTION_NAME
+If defined, a C string constant for the name of the section containing unlikely
+executed functions in the program.
+@end defmac
+
+@defmac DATA_SECTION_ASM_OP
+A C expression whose value is a string, including spacing, containing the
+assembler operation to identify the following data as writable initialized
+data.  Normally @code{"\t.data"} is right.
+@end defmac
+
+@defmac SDATA_SECTION_ASM_OP
+If defined, a C expression whose value is a string, including spacing,
+containing the assembler operation to identify the following data as
+initialized, writable small data.
+@end defmac
+
+@defmac READONLY_DATA_SECTION_ASM_OP
+A C expression whose value is a string, including spacing, containing the
+assembler operation to identify the following data as read-only initialized
+data.
+@end defmac
+
+@defmac BSS_SECTION_ASM_OP
+If defined, a C expression whose value is a string, including spacing,
+containing the assembler operation to identify the following data as
+uninitialized global data.  If not defined, and
+@code{ASM_OUTPUT_ALIGNED_BSS} not defined,
+uninitialized global data will be output in the data section if
+@option{-fno-common} is passed, otherwise @code{ASM_OUTPUT_COMMON} will be
+used.
+@end defmac
+
+@defmac SBSS_SECTION_ASM_OP
+If defined, a C expression whose value is a string, including spacing,
+containing the assembler operation to identify the following data as
+uninitialized, writable small data.
+@end defmac
+
+@defmac TLS_COMMON_ASM_OP
+If defined, a C expression whose value is a string containing the
+assembler operation to identify the following data as thread-local
+common data.  The default is @code{".tls_common"}.
+@end defmac
+
+@defmac TLS_SECTION_ASM_FLAG
+If defined, a C expression whose value is a character constant
+containing the flag used to mark a section as a TLS section.  The
+default is @code{'T'}.
+@end defmac
+
+@defmac INIT_SECTION_ASM_OP
+If defined, a C expression whose value is a string, including spacing,
+containing the assembler operation to identify the following data as
+initialization code.  If not defined, GCC will assume such a section does
+not exist.  This section has no corresponding @code{init_section}
+variable; it is used entirely in runtime code.
+@end defmac
+
+@defmac FINI_SECTION_ASM_OP
+If defined, a C expression whose value is a string, including spacing,
+containing the assembler operation to identify the following data as
+finalization code.  If not defined, GCC will assume such a section does
+not exist.  This section has no corresponding @code{fini_section}
+variable; it is used entirely in runtime code.
+@end defmac
+
+@defmac INIT_ARRAY_SECTION_ASM_OP
+If defined, a C expression whose value is a string, including spacing,
+containing the assembler operation to identify the following data as
+part of the @code{.init_array} (or equivalent) section.  If not
+defined, GCC will assume such a section does not exist.  Do not define
+both this macro and @code{INIT_SECTION_ASM_OP}.
+@end defmac
+
+@defmac FINI_ARRAY_SECTION_ASM_OP
+If defined, a C expression whose value is a string, including spacing,
+containing the assembler operation to identify the following data as
+part of the @code{.fini_array} (or equivalent) section.  If not
+defined, GCC will assume such a section does not exist.  Do not define
+both this macro and @code{FINI_SECTION_ASM_OP}.
+@end defmac
+
+@defmac MACH_DEP_SECTION_ASM_FLAG
+If defined, a C expression whose value is a character constant
+containing the flag used to mark a machine-dependent section.  This
+corresponds to the @code{SECTION_MACH_DEP} section flag.
+@end defmac
+
+@defmac CRT_CALL_STATIC_FUNCTION (@var{section_op}, @var{function})
+If defined, an ASM statement that switches to a different section
+via @var{section_op}, calls @var{function}, and switches back to
+the text section.  This is used in @file{crtstuff.c} if
+@code{INIT_SECTION_ASM_OP} or @code{FINI_SECTION_ASM_OP} to calls
+to initialization and finalization functions from the init and fini
+sections.  By default, this macro uses a simple function call.  Some
+ports need hand-crafted assembly code to avoid dependencies on
+registers initialized in the function prologue or to ensure that
+constant pools don't end up too far way in the text section.
+@end defmac
+
+@defmac TARGET_LIBGCC_SDATA_SECTION
+If defined, a string which names the section into which small
+variables defined in crtstuff and libgcc should go.  This is useful
+when the target has options for optimizing access to small data, and
+you want the crtstuff and libgcc routines to be conservative in what
+they expect of your application yet liberal in what your application
+expects.  For example, for targets with a @code{.sdata} section (like
+MIPS), you could compile crtstuff with @code{-G 0} so that it doesn't
+require small data support from your application, but use this macro
+to put small data into @code{.sdata} so that your application can
+access these variables whether it uses small data or not.
+@end defmac
+
+@defmac FORCE_CODE_SECTION_ALIGN
+If defined, an ASM statement that aligns a code section to some
+arbitrary boundary.  This is used to force all fragments of the
+@code{.init} and @code{.fini} sections to have to same alignment
+and thus prevent the linker from having to add any padding.
+@end defmac
+
+@defmac JUMP_TABLES_IN_TEXT_SECTION
+Define this macro to be an expression with a nonzero value if jump
+tables (for @code{tablejump} insns) should be output in the text
+section, along with the assembler instructions.  Otherwise, the
+readonly data section is used.
+
+This macro is irrelevant if there is no separate readonly data section.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_ASM_INIT_SECTIONS (void)
+Define this hook if you need to do something special to set up the
+@file{varasm.c} sections, or if your target has some special sections
+of its own that you need to create.
+
+GCC calls this hook after processing the command line, but before writing
+any assembly code, and before calling any of the section-returning hooks
+described below.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_ASM_RELOC_RW_MASK (void)
+Return a mask describing how relocations should be treated when
+selecting sections.  Bit 1 should be set if global relocations
+should be placed in a read-write section; bit 0 should be set if
+local relocations should be placed in a read-write section.
+
+The default version of this function returns 3 when @option{-fpic}
+is in effect, and 0 otherwise.  The hook is typically redefined
+when the target cannot support (some kinds of) dynamic relocations
+in read-only sections even in executables.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_ASM_GENERATE_PIC_ADDR_DIFF_VEC (void)
+Return true to generate ADDR_DIF_VEC table
+or false to generate ADDR_VEC table for jumps in case of -fPIC.
+
+The default version of this function returns true if flag_pic
+equals true and false otherwise
+@end deftypefn
+
+@deftypefn {Target Hook} {section *} TARGET_ASM_SELECT_SECTION (tree @var{exp}, int @var{reloc}, unsigned HOST_WIDE_INT @var{align})
+Return the section into which @var{exp} should be placed.  You can
+assume that @var{exp} is either a @code{VAR_DECL} node or a constant of
+some sort.  @var{reloc} indicates whether the initial value of @var{exp}
+requires link-time relocations.  Bit 0 is set when variable contains
+local relocations only, while bit 1 is set for global relocations.
+@var{align} is the constant alignment in bits.
+
+The default version of this function takes care of putting read-only
+variables in @code{readonly_data_section}.
+
+See also @var{USE_SELECT_SECTION_FOR_FUNCTIONS}.
+@end deftypefn
+
+@defmac USE_SELECT_SECTION_FOR_FUNCTIONS
+Define this macro if you wish TARGET_ASM_SELECT_SECTION to be called
+for @code{FUNCTION_DECL}s as well as for variables and constants.
+
+In the case of a @code{FUNCTION_DECL}, @var{reloc} will be zero if the
+function has been determined to be likely to be called, and nonzero if
+it is unlikely to be called.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_ASM_UNIQUE_SECTION (tree @var{decl}, int @var{reloc})
+Build up a unique section name, expressed as a @code{STRING_CST} node,
+and assign it to @samp{DECL_SECTION_NAME (@var{decl})}.
+As with @code{TARGET_ASM_SELECT_SECTION}, @var{reloc} indicates whether
+the initial value of @var{exp} requires link-time relocations.
+
+The default version of this function appends the symbol name to the
+ELF section name that would normally be used for the symbol.  For
+example, the function @code{foo} would be placed in @code{.text.foo}.
+Whatever the actual target object format, this is often good enough.
+@end deftypefn
+
+@deftypefn {Target Hook} {section *} TARGET_ASM_FUNCTION_RODATA_SECTION (tree @var{decl}, bool @var{relocatable})
+Return the readonly data or reloc readonly data section associated with
+@samp{DECL_SECTION_NAME (@var{decl})}. @var{relocatable} selects the latter
+over the former.
+The default version of this function selects @code{.gnu.linkonce.r.name} if
+the function's section is @code{.gnu.linkonce.t.name}, @code{.rodata.name}
+or @code{.data.rel.ro.name} if function is in @code{.text.name}, and
+the normal readonly-data or reloc readonly data section otherwise.
+@end deftypefn
+
+@deftypevr {Target Hook} {const char *} TARGET_ASM_MERGEABLE_RODATA_PREFIX
+Usually, the compiler uses the prefix @code{".rodata"} to construct
+section names for mergeable constant data.  Define this macro to override
+the string if a different section name should be used.
+@end deftypevr
+
+@deftypefn {Target Hook} {section *} TARGET_ASM_TM_CLONE_TABLE_SECTION (void)
+Return the section that should be used for transactional memory clone  tables.
+@end deftypefn
+
+@deftypefn {Target Hook} {section *} TARGET_ASM_SELECT_RTX_SECTION (machine_mode @var{mode}, rtx @var{x}, unsigned HOST_WIDE_INT @var{align})
+Return the section into which a constant @var{x}, of mode @var{mode},
+should be placed.  You can assume that @var{x} is some kind of
+constant in RTL@.  The argument @var{mode} is redundant except in the
+case of a @code{const_int} rtx.  @var{align} is the constant alignment
+in bits.
+
+The default version of this function takes care of putting symbolic
+constants in @code{flag_pic} mode in @code{data_section} and everything
+else in @code{readonly_data_section}.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_MANGLE_DECL_ASSEMBLER_NAME (tree @var{decl}, tree @var{id})
+Define this hook if you need to postprocess the assembler name generated
+by target-independent code.  The @var{id} provided to this hook will be
+the computed name (e.g., the macro @code{DECL_NAME} of the @var{decl} in C,
+or the mangled name of the @var{decl} in C++).  The return value of the
+hook is an @code{IDENTIFIER_NODE} for the appropriate mangled name on
+your target system.  The default implementation of this hook just
+returns the @var{id} provided.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ENCODE_SECTION_INFO (tree @var{decl}, rtx @var{rtl}, int @var{new_decl_p})
+Define this hook if references to a symbol or a constant must be
+treated differently depending on something about the variable or
+function named by the symbol (such as what section it is in).
+
+The hook is executed immediately after rtl has been created for
+@var{decl}, which may be a variable or function declaration or
+an entry in the constant pool.  In either case, @var{rtl} is the
+rtl in question.  Do @emph{not} use @code{DECL_RTL (@var{decl})}
+in this hook; that field may not have been initialized yet.
+
+In the case of a constant, it is safe to assume that the rtl is
+a @code{mem} whose address is a @code{symbol_ref}.  Most decls
+will also have this form, but that is not guaranteed.  Global
+register variables, for instance, will have a @code{reg} for their
+rtl.  (Normally the right thing to do with such unusual rtl is
+leave it alone.)
+
+The @var{new_decl_p} argument will be true if this is the first time
+that @code{TARGET_ENCODE_SECTION_INFO} has been invoked on this decl.  It will
+be false for subsequent invocations, which will happen for duplicate
+declarations.  Whether or not anything must be done for the duplicate
+declaration depends on whether the hook examines @code{DECL_ATTRIBUTES}.
+@var{new_decl_p} is always true when the hook is called for a constant.
+
+@cindex @code{SYMBOL_REF_FLAG}, in @code{TARGET_ENCODE_SECTION_INFO}
+The usual thing for this hook to do is to record flags in the
+@code{symbol_ref}, using @code{SYMBOL_REF_FLAG} or @code{SYMBOL_REF_FLAGS}.
+Historically, the name string was modified if it was necessary to
+encode more than one bit of information, but this practice is now
+discouraged; use @code{SYMBOL_REF_FLAGS}.
+
+The default definition of this hook, @code{default_encode_section_info}
+in @file{varasm.c}, sets a number of commonly-useful bits in
+@code{SYMBOL_REF_FLAGS}.  Check whether the default does what you need
+before overriding it.
+@end deftypefn
+
+@deftypefn {Target Hook} {const char *} TARGET_STRIP_NAME_ENCODING (const char *@var{name})
+Decode @var{name} and return the real name part, sans
+the characters that @code{TARGET_ENCODE_SECTION_INFO}
+may have added.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_IN_SMALL_DATA_P (const_tree @var{exp})
+Returns true if @var{exp} should be placed into a ``small data'' section.
+The default version of this hook always returns false.
+@end deftypefn
+
+@deftypevr {Target Hook} bool TARGET_HAVE_SRODATA_SECTION
+Contains the value true if the target places read-only
+``small data'' into a separate section.  The default value is false.
+@end deftypevr
+
+@deftypefn {Target Hook} bool TARGET_PROFILE_BEFORE_PROLOGUE (void)
+It returns true if target wants profile code emitted before prologue.
+
+The default version of this hook use the target macro
+@code{PROFILE_BEFORE_PROLOGUE}.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_BINDS_LOCAL_P (const_tree @var{exp})
+Returns true if @var{exp} names an object for which name resolution
+rules must resolve to the current ``module'' (dynamic shared library
+or executable image).
+
+The default version of this hook implements the name resolution rules
+for ELF, which has a looser model of global name binding than other
+currently supported object file formats.
+@end deftypefn
+
+@deftypevr {Target Hook} bool TARGET_HAVE_TLS
+Contains the value true if the target supports thread-local storage.
+The default value is false.
+@end deftypevr
+
+
+@node PIC
+@section Position Independent Code
+@cindex position independent code
+@cindex PIC
+
+This section describes macros that help implement generation of position
+independent code.  Simply defining these macros is not enough to
+generate valid PIC; you must also add support to the hook
+@code{TARGET_LEGITIMATE_ADDRESS_P} and to the macro
+@code{PRINT_OPERAND_ADDRESS}, as well as @code{LEGITIMIZE_ADDRESS}.  You
+must modify the definition of @samp{movsi} to do something appropriate
+when the source operand contains a symbolic address.  You may also
+need to alter the handling of switch statements so that they use
+relative addresses.
+@c i rearranged the order of the macros above to try to force one of
+@c them to the next line, to eliminate an overfull hbox. --mew 10feb93
+
+@defmac PIC_OFFSET_TABLE_REGNUM
+The register number of the register used to address a table of static
+data addresses in memory.  In some cases this register is defined by a
+processor's ``application binary interface'' (ABI)@.  When this macro
+is defined, RTL is generated for this register once, as with the stack
+pointer and frame pointer registers.  If this macro is not defined, it
+is up to the machine-dependent files to allocate such a register (if
+necessary).  Note that this register must be fixed when in use (e.g.@:
+when @code{flag_pic} is true).
+@end defmac
+
+@defmac PIC_OFFSET_TABLE_REG_CALL_CLOBBERED
+A C expression that is nonzero if the register defined by
+@code{PIC_OFFSET_TABLE_REGNUM} is clobbered by calls.  If not defined,
+the default is zero.  Do not define
+this macro if @code{PIC_OFFSET_TABLE_REGNUM} is not defined.
+@end defmac
+
+@defmac LEGITIMATE_PIC_OPERAND_P (@var{x})
+A C expression that is nonzero if @var{x} is a legitimate immediate
+operand on the target machine when generating position independent code.
+You can assume that @var{x} satisfies @code{CONSTANT_P}, so you need not
+check this.  You can also assume @var{flag_pic} is true, so you need not
+check it either.  You need not define this macro if all constants
+(including @code{SYMBOL_REF}) can be immediate operands when generating
+position independent code.
+@end defmac
+
+@node Assembler Format
+@section Defining the Output Assembler Language
+
+This section describes macros whose principal purpose is to describe how
+to write instructions in assembler language---rather than what the
+instructions do.
+
+@menu
+* File Framework::       Structural information for the assembler file.
+* Data Output::          Output of constants (numbers, strings, addresses).
+* Uninitialized Data::   Output of uninitialized variables.
+* Label Output::         Output and generation of labels.
+* Initialization::       General principles of initialization
+                         and termination routines.
+* Macros for Initialization::
+                         Specific macros that control the handling of
+                         initialization and termination routines.
+* Instruction Output::   Output of actual instructions.
+* Dispatch Tables::      Output of jump tables.
+* Exception Region Output:: Output of exception region code.
+* Alignment Output::     Pseudo ops for alignment and skipping data.
+@end menu
+
+@node File Framework
+@subsection The Overall Framework of an Assembler File
+@cindex assembler format
+@cindex output of assembler code
+
+@c prevent bad page break with this line
+This describes the overall framework of an assembly file.
+
+@findex default_file_start
+@deftypefn {Target Hook} void TARGET_ASM_FILE_START (void)
+Output to @code{asm_out_file} any text which the assembler expects to
+find at the beginning of a file.  The default behavior is controlled
+by two flags, documented below.  Unless your target's assembler is
+quite unusual, if you override the default, you should call
+@code{default_file_start} at some point in your target hook.  This
+lets other target files rely on these variables.
+@end deftypefn
+
+@deftypevr {Target Hook} bool TARGET_ASM_FILE_START_APP_OFF
+If this flag is true, the text of the macro @code{ASM_APP_OFF} will be
+printed as the very first line in the assembly file, unless
+@option{-fverbose-asm} is in effect.  (If that macro has been defined
+to the empty string, this variable has no effect.)  With the normal
+definition of @code{ASM_APP_OFF}, the effect is to notify the GNU
+assembler that it need not bother stripping comments or extra
+whitespace from its input.  This allows it to work a bit faster.
+
+The default is false.  You should not set it to true unless you have
+verified that your port does not generate any extra whitespace or
+comments that will cause GAS to issue errors in NO_APP mode.
+@end deftypevr
+
+@deftypevr {Target Hook} bool TARGET_ASM_FILE_START_FILE_DIRECTIVE
+If this flag is true, @code{output_file_directive} will be called
+for the primary source file, immediately after printing
+@code{ASM_APP_OFF} (if that is enabled).  Most ELF assemblers expect
+this to be done.  The default is false.
+@end deftypevr
+
+@deftypefn {Target Hook} void TARGET_ASM_FILE_END (void)
+Output to @code{asm_out_file} any text which the assembler expects
+to find at the end of a file.  The default is to output nothing.
+@end deftypefn
+
+@deftypefun void file_end_indicate_exec_stack ()
+Some systems use a common convention, the @samp{.note.GNU-stack}
+special section, to indicate whether or not an object file relies on
+the stack being executable.  If your system uses this convention, you
+should define @code{TARGET_ASM_FILE_END} to this function.  If you
+need to do other things in that hook, have your hook function call
+this function.
+@end deftypefun
+
+@deftypefn {Target Hook} void TARGET_ASM_LTO_START (void)
+Output to @code{asm_out_file} any text which the assembler expects
+to find at the start of an LTO section.  The default is to output
+nothing.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_LTO_END (void)
+Output to @code{asm_out_file} any text which the assembler expects
+to find at the end of an LTO section.  The default is to output
+nothing.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_CODE_END (void)
+Output to @code{asm_out_file} any text which is needed before emitting
+unwind info and debug info at the end of a file.  Some targets emit
+here PIC setup thunks that cannot be emitted at the end of file,
+because they couldn't have unwind info then.  The default is to output
+nothing.
+@end deftypefn
+
+@defmac ASM_COMMENT_START
+A C string constant describing how to begin a comment in the target
+assembler language.  The compiler assumes that the comment will end at
+the end of the line.
+@end defmac
+
+@defmac ASM_APP_ON
+A C string constant for text to be output before each @code{asm}
+statement or group of consecutive ones.  Normally this is
+@code{"#APP"}, which is a comment that has no effect on most
+assemblers but tells the GNU assembler that it must check the lines
+that follow for all valid assembler constructs.
+@end defmac
+
+@defmac ASM_APP_OFF
+A C string constant for text to be output after each @code{asm}
+statement or group of consecutive ones.  Normally this is
+@code{"#NO_APP"}, which tells the GNU assembler to resume making the
+time-saving assumptions that are valid for ordinary compiler output.
+@end defmac
+
+@defmac ASM_OUTPUT_SOURCE_FILENAME (@var{stream}, @var{name})
+A C statement to output COFF information or DWARF debugging information
+which indicates that filename @var{name} is the current source file to
+the stdio stream @var{stream}.
+
+This macro need not be defined if the standard form of output
+for the file format in use is appropriate.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_ASM_OUTPUT_SOURCE_FILENAME (FILE *@var{file}, const char *@var{name})
+Output DWARF debugging information which indicates that filename @var{name} is the current source file to the stdio stream @var{file}.
+ 
+ This target hook need not be defined if the standard form of output for the file format in use is appropriate.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_OUTPUT_IDENT (const char *@var{name})
+Output a string based on @var{name}, suitable for the @samp{#ident}  directive, or the equivalent directive or pragma in non-C-family languages.  If this hook is not defined, nothing is output for the @samp{#ident}  directive.
+@end deftypefn
+
+@defmac OUTPUT_QUOTED_STRING (@var{stream}, @var{string})
+A C statement to output the string @var{string} to the stdio stream
+@var{stream}.  If you do not call the function @code{output_quoted_string}
+in your config files, GCC will only call it to output filenames to
+the assembler source.  So you can use it to canonicalize the format
+of the filename using this macro.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_ASM_NAMED_SECTION (const char *@var{name}, unsigned int @var{flags}, tree @var{decl})
+Output assembly directives to switch to section @var{name}.  The section
+should have attributes as specified by @var{flags}, which is a bit mask
+of the @code{SECTION_*} flags defined in @file{output.h}.  If @var{decl}
+is non-NULL, it is the @code{VAR_DECL} or @code{FUNCTION_DECL} with which
+this section is associated.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_ASM_ELF_FLAGS_NUMERIC (unsigned int @var{flags}, unsigned int *@var{num})
+This hook can be used to encode ELF section flags for which no letter
+code has been defined in the assembler.  It is called by
+@code{default_asm_named_section} whenever the section flags need to be
+emitted in the assembler output.  If the hook returns true, then the
+numerical value for ELF section flags should be calculated from
+@var{flags} and saved in @var{*num}; the value is printed out instead of the
+normal sequence of letter codes.  If the hook is not defined, or if it
+returns false, then @var{num} is ignored and the traditional letter sequence
+is emitted.
+@end deftypefn
+
+@deftypefn {Target Hook} {section *} TARGET_ASM_FUNCTION_SECTION (tree @var{decl}, enum node_frequency @var{freq}, bool @var{startup}, bool @var{exit})
+Return preferred text (sub)section for function @var{decl}.
+Main purpose of this function is to separate cold, normal and hot
+functions. @var{startup} is true when function is known to be used only
+at startup (from static constructors or it is @code{main()}).
+@var{exit} is true when function is known to be used only at exit
+(from static destructors).
+Return NULL if function should go to default text section.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_FUNCTION_SWITCHED_TEXT_SECTIONS (FILE *@var{file}, tree @var{decl}, bool @var{new_is_cold})
+Used by the target to emit any assembler directives or additional  labels needed when a function is partitioned between different  sections.  Output should be written to @var{file}.  The function  decl is available as @var{decl} and the new section is `cold' if  @var{new_is_cold} is @code{true}.
+@end deftypefn
+
+@deftypevr {Common Target Hook} bool TARGET_HAVE_NAMED_SECTIONS
+This flag is true if the target supports @code{TARGET_ASM_NAMED_SECTION}.
+It must not be modified by command-line option processing.
+@end deftypevr
+
+@anchor{TARGET_HAVE_SWITCHABLE_BSS_SECTIONS}
+@deftypevr {Target Hook} bool TARGET_HAVE_SWITCHABLE_BSS_SECTIONS
+This flag is true if we can create zeroed data by switching to a BSS
+section and then using @code{ASM_OUTPUT_SKIP} to allocate the space.
+This is true on most ELF targets.
+@end deftypevr
+
+@deftypefn {Target Hook} {unsigned int} TARGET_SECTION_TYPE_FLAGS (tree @var{decl}, const char *@var{name}, int @var{reloc})
+Choose a set of section attributes for use by @code{TARGET_ASM_NAMED_SECTION}
+based on a variable or function decl, a section name, and whether or not the
+declaration's initializer may contain runtime relocations.  @var{decl} may be
+null, in which case read-write data should be assumed.
+
+The default version of this function handles choosing code vs data,
+read-only vs read-write data, and @code{flag_pic}.  You should only
+need to override this if your target has special flags that might be
+set via @code{__attribute__}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_RECORD_GCC_SWITCHES (const char *@var{})
+Provides the target with the ability to record the gcc command line
+switches provided as argument.
+
+By default this hook is set to NULL, but an example implementation is
+provided for ELF based targets.  Called @var{elf_record_gcc_switches},
+it records the switches as ASCII text inside a new, string mergeable
+section in the assembler output file.  The name of the new section is
+provided by the @code{TARGET_ASM_RECORD_GCC_SWITCHES_SECTION} target
+hook.
+@end deftypefn
+
+@deftypevr {Target Hook} {const char *} TARGET_ASM_RECORD_GCC_SWITCHES_SECTION
+This is the name of the section that will be created by the example
+ELF implementation of the @code{TARGET_ASM_RECORD_GCC_SWITCHES} target
+hook.
+@end deftypevr
+
+@need 2000
+@node Data Output
+@subsection Output of Data
+
+
+@deftypevr {Target Hook} {const char *} TARGET_ASM_BYTE_OP
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_ALIGNED_HI_OP
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_ALIGNED_PSI_OP
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_ALIGNED_SI_OP
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_ALIGNED_PDI_OP
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_ALIGNED_DI_OP
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_ALIGNED_PTI_OP
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_ALIGNED_TI_OP
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_UNALIGNED_HI_OP
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_UNALIGNED_PSI_OP
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_UNALIGNED_SI_OP
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_UNALIGNED_PDI_OP
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_UNALIGNED_DI_OP
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_UNALIGNED_PTI_OP
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_UNALIGNED_TI_OP
+These hooks specify assembly directives for creating certain kinds
+of integer object.  The @code{TARGET_ASM_BYTE_OP} directive creates a
+byte-sized object, the @code{TARGET_ASM_ALIGNED_HI_OP} one creates an
+aligned two-byte object, and so on.  Any of the hooks may be
+@code{NULL}, indicating that no suitable directive is available.
+
+The compiler will print these strings at the start of a new line,
+followed immediately by the object's initial value.  In most cases,
+the string should contain a tab, a pseudo-op, and then another tab.
+@end deftypevr
+
+@deftypefn {Target Hook} bool TARGET_ASM_INTEGER (rtx @var{x}, unsigned int @var{size}, int @var{aligned_p})
+The @code{assemble_integer} function uses this hook to output an
+integer object.  @var{x} is the object's value, @var{size} is its size
+in bytes and @var{aligned_p} indicates whether it is aligned.  The
+function should return @code{true} if it was able to output the
+object.  If it returns false, @code{assemble_integer} will try to
+split the object into smaller parts.
+
+The default implementation of this hook will use the
+@code{TARGET_ASM_BYTE_OP} family of strings, returning @code{false}
+when the relevant string is @code{NULL}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_DECL_END (void)
+Define this hook if the target assembler requires a special marker to
+terminate an initialized variable declaration.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_ASM_OUTPUT_ADDR_CONST_EXTRA (FILE *@var{file}, rtx @var{x})
+A target hook to recognize @var{rtx} patterns that @code{output_addr_const}
+can't deal with, and output assembly code to @var{file} corresponding to
+the pattern @var{x}.  This may be used to allow machine-dependent
+@code{UNSPEC}s to appear within constants.
+
+If target hook fails to recognize a pattern, it must return @code{false},
+so that a standard error message is printed.  If it prints an error message
+itself, by calling, for example, @code{output_operand_lossage}, it may just
+return @code{true}.
+@end deftypefn
+
+@defmac ASM_OUTPUT_ASCII (@var{stream}, @var{ptr}, @var{len})
+A C statement to output to the stdio stream @var{stream} an assembler
+instruction to assemble a string constant containing the @var{len}
+bytes at @var{ptr}.  @var{ptr} will be a C expression of type
+@code{char *} and @var{len} a C expression of type @code{int}.
+
+If the assembler has a @code{.ascii} pseudo-op as found in the
+Berkeley Unix assembler, do not define the macro
+@code{ASM_OUTPUT_ASCII}.
+@end defmac
+
+@defmac ASM_OUTPUT_FDESC (@var{stream}, @var{decl}, @var{n})
+A C statement to output word @var{n} of a function descriptor for
+@var{decl}.  This must be defined if @code{TARGET_VTABLE_USES_DESCRIPTORS}
+is defined, and is otherwise unused.
+@end defmac
+
+@defmac CONSTANT_POOL_BEFORE_FUNCTION
+You may define this macro as a C expression.  You should define the
+expression to have a nonzero value if GCC should output the constant
+pool for a function before the code for the function, or a zero value if
+GCC should output the constant pool after the function.  If you do
+not define this macro, the usual case, GCC will output the constant
+pool before the function.
+@end defmac
+
+@defmac ASM_OUTPUT_POOL_PROLOGUE (@var{file}, @var{funname}, @var{fundecl}, @var{size})
+A C statement to output assembler commands to define the start of the
+constant pool for a function.  @var{funname} is a string giving
+the name of the function.  Should the return type of the function
+be required, it can be obtained via @var{fundecl}.  @var{size}
+is the size, in bytes, of the constant pool that will be written
+immediately after this call.
+
+If no constant-pool prefix is required, the usual case, this macro need
+not be defined.
+@end defmac
+
+@defmac ASM_OUTPUT_SPECIAL_POOL_ENTRY (@var{file}, @var{x}, @var{mode}, @var{align}, @var{labelno}, @var{jumpto})
+A C statement (with or without semicolon) to output a constant in the
+constant pool, if it needs special treatment.  (This macro need not do
+anything for RTL expressions that can be output normally.)
+
+The argument @var{file} is the standard I/O stream to output the
+assembler code on.  @var{x} is the RTL expression for the constant to
+output, and @var{mode} is the machine mode (in case @var{x} is a
+@samp{const_int}).  @var{align} is the required alignment for the value
+@var{x}; you should output an assembler directive to force this much
+alignment.
+
+The argument @var{labelno} is a number to use in an internal label for
+the address of this pool entry.  The definition of this macro is
+responsible for outputting the label definition at the proper place.
+Here is how to do this:
+
+@smallexample
+@code{(*targetm.asm_out.internal_label)} (@var{file}, "LC", @var{labelno});
+@end smallexample
+
+When you output a pool entry specially, you should end with a
+@code{goto} to the label @var{jumpto}.  This will prevent the same pool
+entry from being output a second time in the usual manner.
+
+You need not define this macro if it would do nothing.
+@end defmac
+
+@defmac ASM_OUTPUT_POOL_EPILOGUE (@var{file} @var{funname} @var{fundecl} @var{size})
+A C statement to output assembler commands to at the end of the constant
+pool for a function.  @var{funname} is a string giving the name of the
+function.  Should the return type of the function be required, you can
+obtain it via @var{fundecl}.  @var{size} is the size, in bytes, of the
+constant pool that GCC wrote immediately before this call.
+
+If no constant-pool epilogue is required, the usual case, you need not
+define this macro.
+@end defmac
+
+@defmac IS_ASM_LOGICAL_LINE_SEPARATOR (@var{C}, @var{STR})
+Define this macro as a C expression which is nonzero if @var{C} is
+used as a logical line separator by the assembler.  @var{STR} points
+to the position in the string where @var{C} was found; this can be used if
+a line separator uses multiple characters.
+
+If you do not define this macro, the default is that only
+the character @samp{;} is treated as a logical line separator.
+@end defmac
+
+@deftypevr {Target Hook} {const char *} TARGET_ASM_OPEN_PAREN
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_CLOSE_PAREN
+These target hooks are C string constants, describing the syntax in the
+assembler for grouping arithmetic expressions.  If not overridden, they
+default to normal parentheses, which is correct for most assemblers.
+@end deftypevr
+
+These macros are provided by @file{real.h} for writing the definitions
+of @code{ASM_OUTPUT_DOUBLE} and the like:
+
+@defmac REAL_VALUE_TO_TARGET_SINGLE (@var{x}, @var{l})
+@defmacx REAL_VALUE_TO_TARGET_DOUBLE (@var{x}, @var{l})
+@defmacx REAL_VALUE_TO_TARGET_LONG_DOUBLE (@var{x}, @var{l})
+@defmacx REAL_VALUE_TO_TARGET_DECIMAL32 (@var{x}, @var{l})
+@defmacx REAL_VALUE_TO_TARGET_DECIMAL64 (@var{x}, @var{l})
+@defmacx REAL_VALUE_TO_TARGET_DECIMAL128 (@var{x}, @var{l})
+These translate @var{x}, of type @code{REAL_VALUE_TYPE}, to the
+target's floating point representation, and store its bit pattern in
+the variable @var{l}.  For @code{REAL_VALUE_TO_TARGET_SINGLE} and
+@code{REAL_VALUE_TO_TARGET_DECIMAL32}, this variable should be a
+simple @code{long int}.  For the others, it should be an array of
+@code{long int}.  The number of elements in this array is determined
+by the size of the desired target floating point data type: 32 bits of
+it go in each @code{long int} array element.  Each array element holds
+32 bits of the result, even if @code{long int} is wider than 32 bits
+on the host machine.
+
+The array element values are designed so that you can print them out
+using @code{fprintf} in the order they should appear in the target
+machine's memory.
+@end defmac
+
+@node Uninitialized Data
+@subsection Output of Uninitialized Variables
+
+Each of the macros in this section is used to do the whole job of
+outputting a single uninitialized variable.
+
+@defmac ASM_OUTPUT_COMMON (@var{stream}, @var{name}, @var{size}, @var{rounded})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} the assembler definition of a common-label named
+@var{name} whose size is @var{size} bytes.  The variable @var{rounded}
+is the size rounded up to whatever alignment the caller wants.  It is
+possible that @var{size} may be zero, for instance if a struct with no
+other member than a zero-length array is defined.  In this case, the
+backend must output a symbol definition that allocates at least one
+byte, both so that the address of the resulting object does not compare
+equal to any other, and because some object formats cannot even express
+the concept of a zero-sized common symbol, as that is how they represent
+an ordinary undefined external.
+
+Use the expression @code{assemble_name (@var{stream}, @var{name})} to
+output the name itself; before and after that, output the additional
+assembler syntax for defining the name, and a newline.
+
+This macro controls how the assembler definitions of uninitialized
+common global variables are output.
+@end defmac
+
+@defmac ASM_OUTPUT_ALIGNED_COMMON (@var{stream}, @var{name}, @var{size}, @var{alignment})
+Like @code{ASM_OUTPUT_COMMON} except takes the required alignment as a
+separate, explicit argument.  If you define this macro, it is used in
+place of @code{ASM_OUTPUT_COMMON}, and gives you more flexibility in
+handling the required alignment of the variable.  The alignment is specified
+as the number of bits.
+@end defmac
+
+@defmac ASM_OUTPUT_ALIGNED_DECL_COMMON (@var{stream}, @var{decl}, @var{name}, @var{size}, @var{alignment})
+Like @code{ASM_OUTPUT_ALIGNED_COMMON} except that @var{decl} of the
+variable to be output, if there is one, or @code{NULL_TREE} if there
+is no corresponding variable.  If you define this macro, GCC will use it
+in place of both @code{ASM_OUTPUT_COMMON} and
+@code{ASM_OUTPUT_ALIGNED_COMMON}.  Define this macro when you need to see
+the variable's decl in order to chose what to output.
+@end defmac
+
+@defmac ASM_OUTPUT_ALIGNED_BSS (@var{stream}, @var{decl}, @var{name}, @var{size}, @var{alignment})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} the assembler definition of uninitialized global @var{decl} named
+@var{name} whose size is @var{size} bytes.  The variable @var{alignment}
+is the alignment specified as the number of bits.
+
+Try to use function @code{asm_output_aligned_bss} defined in file
+@file{varasm.c} when defining this macro.  If unable, use the expression
+@code{assemble_name (@var{stream}, @var{name})} to output the name itself;
+before and after that, output the additional assembler syntax for defining
+the name, and a newline.
+
+There are two ways of handling global BSS@.  One is to define this macro.
+The other is to have @code{TARGET_ASM_SELECT_SECTION} return a
+switchable BSS section (@pxref{TARGET_HAVE_SWITCHABLE_BSS_SECTIONS}).
+You do not need to do both.
+
+Some languages do not have @code{common} data, and require a
+non-common form of global BSS in order to handle uninitialized globals
+efficiently.  C++ is one example of this.  However, if the target does
+not support global BSS, the front end may choose to make globals
+common in order to save space in the object file.
+@end defmac
+
+@defmac ASM_OUTPUT_LOCAL (@var{stream}, @var{name}, @var{size}, @var{rounded})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} the assembler definition of a local-common-label named
+@var{name} whose size is @var{size} bytes.  The variable @var{rounded}
+is the size rounded up to whatever alignment the caller wants.
+
+Use the expression @code{assemble_name (@var{stream}, @var{name})} to
+output the name itself; before and after that, output the additional
+assembler syntax for defining the name, and a newline.
+
+This macro controls how the assembler definitions of uninitialized
+static variables are output.
+@end defmac
+
+@defmac ASM_OUTPUT_ALIGNED_LOCAL (@var{stream}, @var{name}, @var{size}, @var{alignment})
+Like @code{ASM_OUTPUT_LOCAL} except takes the required alignment as a
+separate, explicit argument.  If you define this macro, it is used in
+place of @code{ASM_OUTPUT_LOCAL}, and gives you more flexibility in
+handling the required alignment of the variable.  The alignment is specified
+as the number of bits.
+@end defmac
+
+@defmac ASM_OUTPUT_ALIGNED_DECL_LOCAL (@var{stream}, @var{decl}, @var{name}, @var{size}, @var{alignment})
+Like @code{ASM_OUTPUT_ALIGNED_LOCAL} except that @var{decl} of the
+variable to be output, if there is one, or @code{NULL_TREE} if there
+is no corresponding variable.  If you define this macro, GCC will use it
+in place of both @code{ASM_OUTPUT_LOCAL} and
+@code{ASM_OUTPUT_ALIGNED_LOCAL}.  Define this macro when you need to see
+the variable's decl in order to chose what to output.
+@end defmac
+
+@node Label Output
+@subsection Output and Generation of Labels
+
+@c prevent bad page break with this line
+This is about outputting labels.
+
+@findex assemble_name
+@defmac ASM_OUTPUT_LABEL (@var{stream}, @var{name})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} the assembler definition of a label named @var{name}.
+Use the expression @code{assemble_name (@var{stream}, @var{name})} to
+output the name itself; before and after that, output the additional
+assembler syntax for defining the name, and a newline.  A default
+definition of this macro is provided which is correct for most systems.
+@end defmac
+
+@defmac ASM_OUTPUT_FUNCTION_LABEL (@var{stream}, @var{name}, @var{decl})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} the assembler definition of a label named @var{name} of
+a function.
+Use the expression @code{assemble_name (@var{stream}, @var{name})} to
+output the name itself; before and after that, output the additional
+assembler syntax for defining the name, and a newline.  A default
+definition of this macro is provided which is correct for most systems.
+
+If this macro is not defined, then the function name is defined in the
+usual manner as a label (by means of @code{ASM_OUTPUT_LABEL}).
+@end defmac
+
+@findex assemble_name_raw
+@defmac ASM_OUTPUT_INTERNAL_LABEL (@var{stream}, @var{name})
+Identical to @code{ASM_OUTPUT_LABEL}, except that @var{name} is known
+to refer to a compiler-generated label.  The default definition uses
+@code{assemble_name_raw}, which is like @code{assemble_name} except
+that it is more efficient.
+@end defmac
+
+@defmac SIZE_ASM_OP
+A C string containing the appropriate assembler directive to specify the
+size of a symbol, without any arguments.  On systems that use ELF, the
+default (in @file{config/elfos.h}) is @samp{"\t.size\t"}; on other
+systems, the default is not to define this macro.
+
+Define this macro only if it is correct to use the default definitions
+of @code{ASM_OUTPUT_SIZE_DIRECTIVE} and @code{ASM_OUTPUT_MEASURED_SIZE}
+for your system.  If you need your own custom definitions of those
+macros, or if you do not need explicit symbol sizes at all, do not
+define this macro.
+@end defmac
+
+@defmac ASM_OUTPUT_SIZE_DIRECTIVE (@var{stream}, @var{name}, @var{size})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} a directive telling the assembler that the size of the
+symbol @var{name} is @var{size}.  @var{size} is a @code{HOST_WIDE_INT}.
+If you define @code{SIZE_ASM_OP}, a default definition of this macro is
+provided.
+@end defmac
+
+@defmac ASM_OUTPUT_MEASURED_SIZE (@var{stream}, @var{name})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} a directive telling the assembler to calculate the size of
+the symbol @var{name} by subtracting its address from the current
+address.
+
+If you define @code{SIZE_ASM_OP}, a default definition of this macro is
+provided.  The default assumes that the assembler recognizes a special
+@samp{.} symbol as referring to the current address, and can calculate
+the difference between this and another symbol.  If your assembler does
+not recognize @samp{.} or cannot do calculations with it, you will need
+to redefine @code{ASM_OUTPUT_MEASURED_SIZE} to use some other technique.
+@end defmac
+
+@defmac NO_DOLLAR_IN_LABEL
+Define this macro if the assembler does not accept the character
+@samp{$} in label names.  By default constructors and destructors in
+G++ have @samp{$} in the identifiers.  If this macro is defined,
+@samp{.} is used instead.
+@end defmac
+
+@defmac NO_DOT_IN_LABEL
+Define this macro if the assembler does not accept the character
+@samp{.} in label names.  By default constructors and destructors in G++
+have names that use @samp{.}.  If this macro is defined, these names
+are rewritten to avoid @samp{.}.
+@end defmac
+
+@defmac TYPE_ASM_OP
+A C string containing the appropriate assembler directive to specify the
+type of a symbol, without any arguments.  On systems that use ELF, the
+default (in @file{config/elfos.h}) is @samp{"\t.type\t"}; on other
+systems, the default is not to define this macro.
+
+Define this macro only if it is correct to use the default definition of
+@code{ASM_OUTPUT_TYPE_DIRECTIVE} for your system.  If you need your own
+custom definition of this macro, or if you do not need explicit symbol
+types at all, do not define this macro.
+@end defmac
+
+@defmac TYPE_OPERAND_FMT
+A C string which specifies (using @code{printf} syntax) the format of
+the second operand to @code{TYPE_ASM_OP}.  On systems that use ELF, the
+default (in @file{config/elfos.h}) is @samp{"@@%s"}; on other systems,
+the default is not to define this macro.
+
+Define this macro only if it is correct to use the default definition of
+@code{ASM_OUTPUT_TYPE_DIRECTIVE} for your system.  If you need your own
+custom definition of this macro, or if you do not need explicit symbol
+types at all, do not define this macro.
+@end defmac
+
+@defmac ASM_OUTPUT_TYPE_DIRECTIVE (@var{stream}, @var{type})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} a directive telling the assembler that the type of the
+symbol @var{name} is @var{type}.  @var{type} is a C string; currently,
+that string is always either @samp{"function"} or @samp{"object"}, but
+you should not count on this.
+
+If you define @code{TYPE_ASM_OP} and @code{TYPE_OPERAND_FMT}, a default
+definition of this macro is provided.
+@end defmac
+
+@defmac ASM_DECLARE_FUNCTION_NAME (@var{stream}, @var{name}, @var{decl})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} any text necessary for declaring the name @var{name} of a
+function which is being defined.  This macro is responsible for
+outputting the label definition (perhaps using
+@code{ASM_OUTPUT_FUNCTION_LABEL}).  The argument @var{decl} is the
+@code{FUNCTION_DECL} tree node representing the function.
+
+If this macro is not defined, then the function name is defined in the
+usual manner as a label (by means of @code{ASM_OUTPUT_FUNCTION_LABEL}).
+
+You may wish to use @code{ASM_OUTPUT_TYPE_DIRECTIVE} in the definition
+of this macro.
+@end defmac
+
+@defmac ASM_DECLARE_FUNCTION_SIZE (@var{stream}, @var{name}, @var{decl})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} any text necessary for declaring the size of a function
+which is being defined.  The argument @var{name} is the name of the
+function.  The argument @var{decl} is the @code{FUNCTION_DECL} tree node
+representing the function.
+
+If this macro is not defined, then the function size is not defined.
+
+You may wish to use @code{ASM_OUTPUT_MEASURED_SIZE} in the definition
+of this macro.
+@end defmac
+
+@defmac ASM_DECLARE_COLD_FUNCTION_NAME (@var{stream}, @var{name}, @var{decl})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} any text necessary for declaring the name @var{name} of a
+cold function partition which is being defined.  This macro is responsible
+for outputting the label definition (perhaps using
+@code{ASM_OUTPUT_FUNCTION_LABEL}).  The argument @var{decl} is the
+@code{FUNCTION_DECL} tree node representing the function.
+
+If this macro is not defined, then the cold partition name is defined in the
+usual manner as a label (by means of @code{ASM_OUTPUT_LABEL}).
+
+You may wish to use @code{ASM_OUTPUT_TYPE_DIRECTIVE} in the definition
+of this macro.
+@end defmac
+
+@defmac ASM_DECLARE_COLD_FUNCTION_SIZE (@var{stream}, @var{name}, @var{decl})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} any text necessary for declaring the size of a cold function
+partition which is being defined.  The argument @var{name} is the name of the
+cold partition of the function.  The argument @var{decl} is the
+@code{FUNCTION_DECL} tree node representing the function.
+
+If this macro is not defined, then the partition size is not defined.
+
+You may wish to use @code{ASM_OUTPUT_MEASURED_SIZE} in the definition
+of this macro.
+@end defmac
+
+@defmac ASM_DECLARE_OBJECT_NAME (@var{stream}, @var{name}, @var{decl})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} any text necessary for declaring the name @var{name} of an
+initialized variable which is being defined.  This macro must output the
+label definition (perhaps using @code{ASM_OUTPUT_LABEL}).  The argument
+@var{decl} is the @code{VAR_DECL} tree node representing the variable.
+
+If this macro is not defined, then the variable name is defined in the
+usual manner as a label (by means of @code{ASM_OUTPUT_LABEL}).
+
+You may wish to use @code{ASM_OUTPUT_TYPE_DIRECTIVE} and/or
+@code{ASM_OUTPUT_SIZE_DIRECTIVE} in the definition of this macro.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_ASM_DECLARE_CONSTANT_NAME (FILE *@var{file}, const char *@var{name}, const_tree @var{expr}, HOST_WIDE_INT @var{size})
+A target hook to output to the stdio stream @var{file} any text necessary
+for declaring the name @var{name} of a constant which is being defined.  This
+target hook is responsible for outputting the label definition (perhaps using
+@code{assemble_label}).  The argument @var{exp} is the value of the constant,
+and @var{size} is the size of the constant in bytes.  The @var{name}
+will be an internal label.
+
+The default version of this target hook, define the @var{name} in the
+usual manner as a label (by means of @code{assemble_label}).
+
+You may wish to use @code{ASM_OUTPUT_TYPE_DIRECTIVE} in this target hook.
+@end deftypefn
+
+@defmac ASM_DECLARE_REGISTER_GLOBAL (@var{stream}, @var{decl}, @var{regno}, @var{name})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} any text necessary for claiming a register @var{regno}
+for a global variable @var{decl} with name @var{name}.
+
+If you don't define this macro, that is equivalent to defining it to do
+nothing.
+@end defmac
+
+@defmac ASM_FINISH_DECLARE_OBJECT (@var{stream}, @var{decl}, @var{toplevel}, @var{atend})
+A C statement (sans semicolon) to finish up declaring a variable name
+once the compiler has processed its initializer fully and thus has had a
+chance to determine the size of an array when controlled by an
+initializer.  This is used on systems where it's necessary to declare
+something about the size of the object.
+
+If you don't define this macro, that is equivalent to defining it to do
+nothing.
+
+You may wish to use @code{ASM_OUTPUT_SIZE_DIRECTIVE} and/or
+@code{ASM_OUTPUT_MEASURED_SIZE} in the definition of this macro.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_ASM_GLOBALIZE_LABEL (FILE *@var{stream}, const char *@var{name})
+This target hook is a function to output to the stdio stream
+@var{stream} some commands that will make the label @var{name} global;
+that is, available for reference from other files.
+
+The default implementation relies on a proper definition of
+@code{GLOBAL_ASM_OP}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_GLOBALIZE_DECL_NAME (FILE *@var{stream}, tree @var{decl})
+This target hook is a function to output to the stdio stream
+@var{stream} some commands that will make the name associated with @var{decl}
+global; that is, available for reference from other files.
+
+The default implementation uses the TARGET_ASM_GLOBALIZE_LABEL target hook.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_ASSEMBLE_UNDEFINED_DECL (FILE *@var{stream}, const char *@var{name}, const_tree @var{decl})
+This target hook is a function to output to the stdio stream
+@var{stream} some commands that will declare the name associated with
+@var{decl} which is not defined in the current translation unit.  Most
+assemblers do not require anything to be output in this case.
+@end deftypefn
+
+@defmac ASM_WEAKEN_LABEL (@var{stream}, @var{name})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} some commands that will make the label @var{name} weak;
+that is, available for reference from other files but only used if
+no other definition is available.  Use the expression
+@code{assemble_name (@var{stream}, @var{name})} to output the name
+itself; before and after that, output the additional assembler syntax
+for making that name weak, and a newline.
+
+If you don't define this macro or @code{ASM_WEAKEN_DECL}, GCC will not
+support weak symbols and you should not define the @code{SUPPORTS_WEAK}
+macro.
+@end defmac
+
+@defmac ASM_WEAKEN_DECL (@var{stream}, @var{decl}, @var{name}, @var{value})
+Combines (and replaces) the function of @code{ASM_WEAKEN_LABEL} and
+@code{ASM_OUTPUT_WEAK_ALIAS}, allowing access to the associated function
+or variable decl.  If @var{value} is not @code{NULL}, this C statement
+should output to the stdio stream @var{stream} assembler code which
+defines (equates) the weak symbol @var{name} to have the value
+@var{value}.  If @var{value} is @code{NULL}, it should output commands
+to make @var{name} weak.
+@end defmac
+
+@defmac ASM_OUTPUT_WEAKREF (@var{stream}, @var{decl}, @var{name}, @var{value})
+Outputs a directive that enables @var{name} to be used to refer to
+symbol @var{value} with weak-symbol semantics.  @code{decl} is the
+declaration of @code{name}.
+@end defmac
+
+@defmac SUPPORTS_WEAK
+A preprocessor constant expression which evaluates to true if the target
+supports weak symbols.
+
+If you don't define this macro, @file{defaults.h} provides a default
+definition.  If either @code{ASM_WEAKEN_LABEL} or @code{ASM_WEAKEN_DECL}
+is defined, the default definition is @samp{1}; otherwise, it is @samp{0}.
+@end defmac
+
+@defmac TARGET_SUPPORTS_WEAK
+A C expression which evaluates to true if the target supports weak symbols.
+
+If you don't define this macro, @file{defaults.h} provides a default
+definition.  The default definition is @samp{(SUPPORTS_WEAK)}.  Define
+this macro if you want to control weak symbol support with a compiler
+flag such as @option{-melf}.
+@end defmac
+
+@defmac MAKE_DECL_ONE_ONLY (@var{decl})
+A C statement (sans semicolon) to mark @var{decl} to be emitted as a
+public symbol such that extra copies in multiple translation units will
+be discarded by the linker.  Define this macro if your object file
+format provides support for this concept, such as the @samp{COMDAT}
+section flags in the Microsoft Windows PE/COFF format, and this support
+requires changes to @var{decl}, such as putting it in a separate section.
+@end defmac
+
+@defmac SUPPORTS_ONE_ONLY
+A C expression which evaluates to true if the target supports one-only
+semantics.
+
+If you don't define this macro, @file{varasm.c} provides a default
+definition.  If @code{MAKE_DECL_ONE_ONLY} is defined, the default
+definition is @samp{1}; otherwise, it is @samp{0}.  Define this macro if
+you want to control one-only symbol support with a compiler flag, or if
+setting the @code{DECL_ONE_ONLY} flag is enough to mark a declaration to
+be emitted as one-only.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_ASM_ASSEMBLE_VISIBILITY (tree @var{decl}, int @var{visibility})
+This target hook is a function to output to @var{asm_out_file} some
+commands that will make the symbol(s) associated with @var{decl} have
+hidden, protected or internal visibility as specified by @var{visibility}.
+@end deftypefn
+
+@defmac TARGET_WEAK_NOT_IN_ARCHIVE_TOC
+A C expression that evaluates to true if the target's linker expects
+that weak symbols do not appear in a static archive's table of contents.
+The default is @code{0}.
+
+Leaving weak symbols out of an archive's table of contents means that,
+if a symbol will only have a definition in one translation unit and
+will have undefined references from other translation units, that
+symbol should not be weak.  Defining this macro to be nonzero will
+thus have the effect that certain symbols that would normally be weak
+(explicit template instantiations, and vtables for polymorphic classes
+with noninline key methods) will instead be nonweak.
+
+The C++ ABI requires this macro to be zero.  Define this macro for
+targets where full C++ ABI compliance is impossible and where linker
+restrictions require weak symbols to be left out of a static archive's
+table of contents.
+@end defmac
+
+@defmac ASM_OUTPUT_EXTERNAL (@var{stream}, @var{decl}, @var{name})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} any text necessary for declaring the name of an external
+symbol named @var{name} which is referenced in this compilation but
+not defined.  The value of @var{decl} is the tree node for the
+declaration.
+
+This macro need not be defined if it does not need to output anything.
+The GNU assembler and most Unix assemblers don't require anything.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_ASM_EXTERNAL_LIBCALL (rtx @var{symref})
+This target hook is a function to output to @var{asm_out_file} an assembler
+pseudo-op to declare a library function name external.  The name of the
+library function is given by @var{symref}, which is a @code{symbol_ref}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_MARK_DECL_PRESERVED (const char *@var{symbol})
+This target hook is a function to output to @var{asm_out_file} an assembler
+directive to annotate @var{symbol} as used.  The Darwin target uses the
+.no_dead_code_strip directive.
+@end deftypefn
+
+@defmac ASM_OUTPUT_LABELREF (@var{stream}, @var{name})
+A C statement (sans semicolon) to output to the stdio stream
+@var{stream} a reference in assembler syntax to a label named
+@var{name}.  This should add @samp{_} to the front of the name, if that
+is customary on your operating system, as it is in most Berkeley Unix
+systems.  This macro is used in @code{assemble_name}.
+@end defmac
+
+@deftypefn {Target Hook} tree TARGET_MANGLE_ASSEMBLER_NAME (const char *@var{name})
+Given a symbol @var{name}, perform same mangling as @code{varasm.c}'s @code{assemble_name}, but in memory rather than to a file stream, returning result as an @code{IDENTIFIER_NODE}.  Required for correct LTO symtabs.  The default implementation calls the @code{TARGET_STRIP_NAME_ENCODING} hook and then prepends the @code{USER_LABEL_PREFIX}, if any.
+@end deftypefn
+
+@defmac ASM_OUTPUT_SYMBOL_REF (@var{stream}, @var{sym})
+A C statement (sans semicolon) to output a reference to
+@code{SYMBOL_REF} @var{sym}.  If not defined, @code{assemble_name}
+will be used to output the name of the symbol.  This macro may be used
+to modify the way a symbol is referenced depending on information
+encoded by @code{TARGET_ENCODE_SECTION_INFO}.
+@end defmac
+
+@defmac ASM_OUTPUT_LABEL_REF (@var{stream}, @var{buf})
+A C statement (sans semicolon) to output a reference to @var{buf}, the
+result of @code{ASM_GENERATE_INTERNAL_LABEL}.  If not defined,
+@code{assemble_name} will be used to output the name of the symbol.
+This macro is not used by @code{output_asm_label}, or the @code{%l}
+specifier that calls it; the intention is that this macro should be set
+when it is necessary to output a label differently when its address is
+being taken.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_ASM_INTERNAL_LABEL (FILE *@var{stream}, const char *@var{prefix}, unsigned long @var{labelno})
+A function to output to the stdio stream @var{stream} a label whose
+name is made from the string @var{prefix} and the number @var{labelno}.
+
+It is absolutely essential that these labels be distinct from the labels
+used for user-level functions and variables.  Otherwise, certain programs
+will have name conflicts with internal labels.
+
+It is desirable to exclude internal labels from the symbol table of the
+object file.  Most assemblers have a naming convention for labels that
+should be excluded; on many systems, the letter @samp{L} at the
+beginning of a label has this effect.  You should find out what
+convention your system uses, and follow it.
+
+The default version of this function utilizes @code{ASM_GENERATE_INTERNAL_LABEL}.
+@end deftypefn
+
+@defmac ASM_OUTPUT_DEBUG_LABEL (@var{stream}, @var{prefix}, @var{num})
+A C statement to output to the stdio stream @var{stream} a debug info
+label whose name is made from the string @var{prefix} and the number
+@var{num}.  This is useful for VLIW targets, where debug info labels
+may need to be treated differently than branch target labels.  On some
+systems, branch target labels must be at the beginning of instruction
+bundles, but debug info labels can occur in the middle of instruction
+bundles.
+
+If this macro is not defined, then @code{(*targetm.asm_out.internal_label)} will be
+used.
+@end defmac
+
+@defmac ASM_GENERATE_INTERNAL_LABEL (@var{string}, @var{prefix}, @var{num})
+A C statement to store into the string @var{string} a label whose name
+is made from the string @var{prefix} and the number @var{num}.
+
+This string, when output subsequently by @code{assemble_name}, should
+produce the output that @code{(*targetm.asm_out.internal_label)} would produce
+with the same @var{prefix} and @var{num}.
+
+If the string begins with @samp{*}, then @code{assemble_name} will
+output the rest of the string unchanged.  It is often convenient for
+@code{ASM_GENERATE_INTERNAL_LABEL} to use @samp{*} in this way.  If the
+string doesn't start with @samp{*}, then @code{ASM_OUTPUT_LABELREF} gets
+to output the string, and may change it.  (Of course,
+@code{ASM_OUTPUT_LABELREF} is also part of your machine description, so
+you should know what it does on your machine.)
+@end defmac
+
+@defmac ASM_FORMAT_PRIVATE_NAME (@var{outvar}, @var{name}, @var{number})
+A C expression to assign to @var{outvar} (which is a variable of type
+@code{char *}) a newly allocated string made from the string
+@var{name} and the number @var{number}, with some suitable punctuation
+added.  Use @code{alloca} to get space for the string.
+
+The string will be used as an argument to @code{ASM_OUTPUT_LABELREF} to
+produce an assembler label for an internal static variable whose name is
+@var{name}.  Therefore, the string must be such as to result in valid
+assembler code.  The argument @var{number} is different each time this
+macro is executed; it prevents conflicts between similarly-named
+internal static variables in different scopes.
+
+Ideally this string should not be a valid C identifier, to prevent any
+conflict with the user's own symbols.  Most assemblers allow periods
+or percent signs in assembler symbols; putting at least one of these
+between the name and the number will suffice.
+
+If this macro is not defined, a default definition will be provided
+which is correct for most systems.
+@end defmac
+
+@defmac ASM_OUTPUT_DEF (@var{stream}, @var{name}, @var{value})
+A C statement to output to the stdio stream @var{stream} assembler code
+which defines (equates) the symbol @var{name} to have the value @var{value}.
+
+@findex SET_ASM_OP
+If @code{SET_ASM_OP} is defined, a default definition is provided which is
+correct for most systems.
+@end defmac
+
+@defmac ASM_OUTPUT_DEF_FROM_DECLS (@var{stream}, @var{decl_of_name}, @var{decl_of_value})
+A C statement to output to the stdio stream @var{stream} assembler code
+which defines (equates) the symbol whose tree node is @var{decl_of_name}
+to have the value of the tree node @var{decl_of_value}.  This macro will
+be used in preference to @samp{ASM_OUTPUT_DEF} if it is defined and if
+the tree nodes are available.
+
+@findex SET_ASM_OP
+If @code{SET_ASM_OP} is defined, a default definition is provided which is
+correct for most systems.
+@end defmac
+
+@defmac TARGET_DEFERRED_OUTPUT_DEFS (@var{decl_of_name}, @var{decl_of_value})
+A C statement that evaluates to true if the assembler code which defines
+(equates) the symbol whose tree node is @var{decl_of_name} to have the value
+of the tree node @var{decl_of_value} should be emitted near the end of the
+current compilation unit.  The default is to not defer output of defines.
+This macro affects defines output by @samp{ASM_OUTPUT_DEF} and
+@samp{ASM_OUTPUT_DEF_FROM_DECLS}.
+@end defmac
+
+@defmac ASM_OUTPUT_WEAK_ALIAS (@var{stream}, @var{name}, @var{value})
+A C statement to output to the stdio stream @var{stream} assembler code
+which defines (equates) the weak symbol @var{name} to have the value
+@var{value}.  If @var{value} is @code{NULL}, it defines @var{name} as
+an undefined weak symbol.
+
+Define this macro if the target only supports weak aliases; define
+@code{ASM_OUTPUT_DEF} instead if possible.
+@end defmac
+
+@defmac OBJC_GEN_METHOD_LABEL (@var{buf}, @var{is_inst}, @var{class_name}, @var{cat_name}, @var{sel_name})
+Define this macro to override the default assembler names used for
+Objective-C methods.
+
+The default name is a unique method number followed by the name of the
+class (e.g.@: @samp{_1_Foo}).  For methods in categories, the name of
+the category is also included in the assembler name (e.g.@:
+@samp{_1_Foo_Bar}).
+
+These names are safe on most systems, but make debugging difficult since
+the method's selector is not present in the name.  Therefore, particular
+systems define other ways of computing names.
+
+@var{buf} is an expression of type @code{char *} which gives you a
+buffer in which to store the name; its length is as long as
+@var{class_name}, @var{cat_name} and @var{sel_name} put together, plus
+50 characters extra.
+
+The argument @var{is_inst} specifies whether the method is an instance
+method or a class method; @var{class_name} is the name of the class;
+@var{cat_name} is the name of the category (or @code{NULL} if the method is not
+in a category); and @var{sel_name} is the name of the selector.
+
+On systems where the assembler can handle quoted names, you can use this
+macro to provide more human-readable names.
+@end defmac
+
+@node Initialization
+@subsection How Initialization Functions Are Handled
+@cindex initialization routines
+@cindex termination routines
+@cindex constructors, output of
+@cindex destructors, output of
+
+The compiled code for certain languages includes @dfn{constructors}
+(also called @dfn{initialization routines})---functions to initialize
+data in the program when the program is started.  These functions need
+to be called before the program is ``started''---that is to say, before
+@code{main} is called.
+
+Compiling some languages generates @dfn{destructors} (also called
+@dfn{termination routines}) that should be called when the program
+terminates.
+
+To make the initialization and termination functions work, the compiler
+must output something in the assembler code to cause those functions to
+be called at the appropriate time.  When you port the compiler to a new
+system, you need to specify how to do this.
+
+There are two major ways that GCC currently supports the execution of
+initialization and termination functions.  Each way has two variants.
+Much of the structure is common to all four variations.
+
+@findex __CTOR_LIST__
+@findex __DTOR_LIST__
+The linker must build two lists of these functions---a list of
+initialization functions, called @code{__CTOR_LIST__}, and a list of
+termination functions, called @code{__DTOR_LIST__}.
+
+Each list always begins with an ignored function pointer (which may hold
+0, @minus{}1, or a count of the function pointers after it, depending on
+the environment).  This is followed by a series of zero or more function
+pointers to constructors (or destructors), followed by a function
+pointer containing zero.
+
+Depending on the operating system and its executable file format, either
+@file{crtstuff.c} or @file{libgcc2.c} traverses these lists at startup
+time and exit time.  Constructors are called in reverse order of the
+list; destructors in forward order.
+
+The best way to handle static constructors works only for object file
+formats which provide arbitrarily-named sections.  A section is set
+aside for a list of constructors, and another for a list of destructors.
+Traditionally these are called @samp{.ctors} and @samp{.dtors}.  Each
+object file that defines an initialization function also puts a word in
+the constructor section to point to that function.  The linker
+accumulates all these words into one contiguous @samp{.ctors} section.
+Termination functions are handled similarly.
+
+This method will be chosen as the default by @file{target-def.h} if
+@code{TARGET_ASM_NAMED_SECTION} is defined.  A target that does not
+support arbitrary sections, but does support special designated
+constructor and destructor sections may define @code{CTORS_SECTION_ASM_OP}
+and @code{DTORS_SECTION_ASM_OP} to achieve the same effect.
+
+When arbitrary sections are available, there are two variants, depending
+upon how the code in @file{crtstuff.c} is called.  On systems that
+support a @dfn{.init} section which is executed at program startup,
+parts of @file{crtstuff.c} are compiled into that section.  The
+program is linked by the @command{gcc} driver like this:
+
+@smallexample
+ld -o @var{output_file} crti.o crtbegin.o @dots{} -lgcc crtend.o crtn.o
+@end smallexample
+
+The prologue of a function (@code{__init}) appears in the @code{.init}
+section of @file{crti.o}; the epilogue appears in @file{crtn.o}.  Likewise
+for the function @code{__fini} in the @dfn{.fini} section.  Normally these
+files are provided by the operating system or by the GNU C library, but
+are provided by GCC for a few targets.
+
+The objects @file{crtbegin.o} and @file{crtend.o} are (for most targets)
+compiled from @file{crtstuff.c}.  They contain, among other things, code
+fragments within the @code{.init} and @code{.fini} sections that branch
+to routines in the @code{.text} section.  The linker will pull all parts
+of a section together, which results in a complete @code{__init} function
+that invokes the routines we need at startup.
+
+To use this variant, you must define the @code{INIT_SECTION_ASM_OP}
+macro properly.
+
+If no init section is available, when GCC compiles any function called
+@code{main} (or more accurately, any function designated as a program
+entry point by the language front end calling @code{expand_main_function}),
+it inserts a procedure call to @code{__main} as the first executable code
+after the function prologue.  The @code{__main} function is defined
+in @file{libgcc2.c} and runs the global constructors.
+
+In file formats that don't support arbitrary sections, there are again
+two variants.  In the simplest variant, the GNU linker (GNU @code{ld})
+and an `a.out' format must be used.  In this case,
+@code{TARGET_ASM_CONSTRUCTOR} is defined to produce a @code{.stabs}
+entry of type @samp{N_SETT}, referencing the name @code{__CTOR_LIST__},
+and with the address of the void function containing the initialization
+code as its value.  The GNU linker recognizes this as a request to add
+the value to a @dfn{set}; the values are accumulated, and are eventually
+placed in the executable as a vector in the format described above, with
+a leading (ignored) count and a trailing zero element.
+@code{TARGET_ASM_DESTRUCTOR} is handled similarly.  Since no init
+section is available, the absence of @code{INIT_SECTION_ASM_OP} causes
+the compilation of @code{main} to call @code{__main} as above, starting
+the initialization process.
+
+The last variant uses neither arbitrary sections nor the GNU linker.
+This is preferable when you want to do dynamic linking and when using
+file formats which the GNU linker does not support, such as `ECOFF'@.  In
+this case, @code{TARGET_HAVE_CTORS_DTORS} is false, initialization and
+termination functions are recognized simply by their names.  This requires
+an extra program in the linkage step, called @command{collect2}.  This program
+pretends to be the linker, for use with GCC; it does its job by running
+the ordinary linker, but also arranges to include the vectors of
+initialization and termination functions.  These functions are called
+via @code{__main} as described above.  In order to use this method,
+@code{use_collect2} must be defined in the target in @file{config.gcc}.
+
+@ifinfo
+The following section describes the specific macros that control and
+customize the handling of initialization and termination functions.
+@end ifinfo
+
+@node Macros for Initialization
+@subsection Macros Controlling Initialization Routines
+
+Here are the macros that control how the compiler handles initialization
+and termination functions:
+
+@defmac INIT_SECTION_ASM_OP
+If defined, a C string constant, including spacing, for the assembler
+operation to identify the following data as initialization code.  If not
+defined, GCC will assume such a section does not exist.  When you are
+using special sections for initialization and termination functions, this
+macro also controls how @file{crtstuff.c} and @file{libgcc2.c} arrange to
+run the initialization functions.
+@end defmac
+
+@defmac HAS_INIT_SECTION
+If defined, @code{main} will not call @code{__main} as described above.
+This macro should be defined for systems that control start-up code
+on a symbol-by-symbol basis, such as OSF/1, and should not
+be defined explicitly for systems that support @code{INIT_SECTION_ASM_OP}.
+@end defmac
+
+@defmac LD_INIT_SWITCH
+If defined, a C string constant for a switch that tells the linker that
+the following symbol is an initialization routine.
+@end defmac
+
+@defmac LD_FINI_SWITCH
+If defined, a C string constant for a switch that tells the linker that
+the following symbol is a finalization routine.
+@end defmac
+
+@defmac COLLECT_SHARED_INIT_FUNC (@var{stream}, @var{func})
+If defined, a C statement that will write a function that can be
+automatically called when a shared library is loaded.  The function
+should call @var{func}, which takes no arguments.  If not defined, and
+the object format requires an explicit initialization function, then a
+function called @code{_GLOBAL__DI} will be generated.
+
+This function and the following one are used by collect2 when linking a
+shared library that needs constructors or destructors, or has DWARF2
+exception tables embedded in the code.
+@end defmac
+
+@defmac COLLECT_SHARED_FINI_FUNC (@var{stream}, @var{func})
+If defined, a C statement that will write a function that can be
+automatically called when a shared library is unloaded.  The function
+should call @var{func}, which takes no arguments.  If not defined, and
+the object format requires an explicit finalization function, then a
+function called @code{_GLOBAL__DD} will be generated.
+@end defmac
+
+@defmac INVOKE__main
+If defined, @code{main} will call @code{__main} despite the presence of
+@code{INIT_SECTION_ASM_OP}.  This macro should be defined for systems
+where the init section is not actually run automatically, but is still
+useful for collecting the lists of constructors and destructors.
+@end defmac
+
+@defmac SUPPORTS_INIT_PRIORITY
+If nonzero, the C++ @code{init_priority} attribute is supported and the
+compiler should emit instructions to control the order of initialization
+of objects.  If zero, the compiler will issue an error message upon
+encountering an @code{init_priority} attribute.
+@end defmac
+
+@deftypevr {Target Hook} bool TARGET_HAVE_CTORS_DTORS
+This value is true if the target supports some ``native'' method of
+collecting constructors and destructors to be run at startup and exit.
+It is false if we must use @command{collect2}.
+@end deftypevr
+
+@deftypefn {Target Hook} void TARGET_ASM_CONSTRUCTOR (rtx @var{symbol}, int @var{priority})
+If defined, a function that outputs assembler code to arrange to call
+the function referenced by @var{symbol} at initialization time.
+
+Assume that @var{symbol} is a @code{SYMBOL_REF} for a function taking
+no arguments and with no return value.  If the target supports initialization
+priorities, @var{priority} is a value between 0 and @code{MAX_INIT_PRIORITY};
+otherwise it must be @code{DEFAULT_INIT_PRIORITY}.
+
+If this macro is not defined by the target, a suitable default will
+be chosen if (1) the target supports arbitrary section names, (2) the
+target defines @code{CTORS_SECTION_ASM_OP}, or (3) @code{USE_COLLECT2}
+is not defined.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_DESTRUCTOR (rtx @var{symbol}, int @var{priority})
+This is like @code{TARGET_ASM_CONSTRUCTOR} but used for termination
+functions rather than initialization functions.
+@end deftypefn
+
+If @code{TARGET_HAVE_CTORS_DTORS} is true, the initialization routine
+generated for the generated object file will have static linkage.
+
+If your system uses @command{collect2} as the means of processing
+constructors, then that program normally uses @command{nm} to scan
+an object file for constructor functions to be called.
+
+On certain kinds of systems, you can define this macro to make
+@command{collect2} work faster (and, in some cases, make it work at all):
+
+@defmac OBJECT_FORMAT_COFF
+Define this macro if the system uses COFF (Common Object File Format)
+object files, so that @command{collect2} can assume this format and scan
+object files directly for dynamic constructor/destructor functions.
+
+This macro is effective only in a native compiler; @command{collect2} as
+part of a cross compiler always uses @command{nm} for the target machine.
+@end defmac
+
+@defmac REAL_NM_FILE_NAME
+Define this macro as a C string constant containing the file name to use
+to execute @command{nm}.  The default is to search the path normally for
+@command{nm}.
+@end defmac
+
+@defmac NM_FLAGS
+@command{collect2} calls @command{nm} to scan object files for static
+constructors and destructors and LTO info.  By default, @option{-n} is
+passed.  Define @code{NM_FLAGS} to a C string constant if other options
+are needed to get the same output format as GNU @command{nm -n}
+produces.
+@end defmac
+
+If your system supports shared libraries and has a program to list the
+dynamic dependencies of a given library or executable, you can define
+these macros to enable support for running initialization and
+termination functions in shared libraries:
+
+@defmac LDD_SUFFIX
+Define this macro to a C string constant containing the name of the program
+which lists dynamic dependencies, like @command{ldd} under SunOS 4.
+@end defmac
+
+@defmac PARSE_LDD_OUTPUT (@var{ptr})
+Define this macro to be C code that extracts filenames from the output
+of the program denoted by @code{LDD_SUFFIX}.  @var{ptr} is a variable
+of type @code{char *} that points to the beginning of a line of output
+from @code{LDD_SUFFIX}.  If the line lists a dynamic dependency, the
+code must advance @var{ptr} to the beginning of the filename on that
+line.  Otherwise, it must set @var{ptr} to @code{NULL}.
+@end defmac
+
+@defmac SHLIB_SUFFIX
+Define this macro to a C string constant containing the default shared
+library extension of the target (e.g., @samp{".so"}).  @command{collect2}
+strips version information after this suffix when generating global
+constructor and destructor names.  This define is only needed on targets
+that use @command{collect2} to process constructors and destructors.
+@end defmac
+
+@node Instruction Output
+@subsection Output of Assembler Instructions
+
+@c prevent bad page break with this line
+This describes assembler instruction output.
+
+@defmac REGISTER_NAMES
+A C initializer containing the assembler's names for the machine
+registers, each one as a C string constant.  This is what translates
+register numbers in the compiler into assembler language.
+@end defmac
+
+@defmac ADDITIONAL_REGISTER_NAMES
+If defined, a C initializer for an array of structures containing a name
+and a register number.  This macro defines additional names for hard
+registers, thus allowing the @code{asm} option in declarations to refer
+to registers using alternate names.
+@end defmac
+
+@defmac OVERLAPPING_REGISTER_NAMES
+If defined, a C initializer for an array of structures containing a
+name, a register number and a count of the number of consecutive
+machine registers the name overlaps.  This macro defines additional
+names for hard registers, thus allowing the @code{asm} option in
+declarations to refer to registers using alternate names.  Unlike
+@code{ADDITIONAL_REGISTER_NAMES}, this macro should be used when the
+register name implies multiple underlying registers.
+
+This macro should be used when it is important that a clobber in an
+@code{asm} statement clobbers all the underlying values implied by the
+register name.  For example, on ARM, clobbering the double-precision
+VFP register ``d0'' implies clobbering both single-precision registers
+``s0'' and ``s1''.
+@end defmac
+
+@defmac ASM_OUTPUT_OPCODE (@var{stream}, @var{ptr})
+Define this macro if you are using an unusual assembler that
+requires different names for the machine instructions.
+
+The definition is a C statement or statements which output an
+assembler instruction opcode to the stdio stream @var{stream}.  The
+macro-operand @var{ptr} is a variable of type @code{char *} which
+points to the opcode name in its ``internal'' form---the form that is
+written in the machine description.  The definition should output the
+opcode name to @var{stream}, performing any translation you desire, and
+increment the variable @var{ptr} to point at the end of the opcode
+so that it will not be output twice.
+
+In fact, your macro definition may process less than the entire opcode
+name, or more than the opcode name; but if you want to process text
+that includes @samp{%}-sequences to substitute operands, you must take
+care of the substitution yourself.  Just be sure to increment
+@var{ptr} over whatever text should not be output normally.
+
+@findex recog_data.operand
+If you need to look at the operand values, they can be found as the
+elements of @code{recog_data.operand}.
+
+If the macro definition does nothing, the instruction is output
+in the usual way.
+@end defmac
+
+@defmac FINAL_PRESCAN_INSN (@var{insn}, @var{opvec}, @var{noperands})
+If defined, a C statement to be executed just prior to the output of
+assembler code for @var{insn}, to modify the extracted operands so
+they will be output differently.
+
+Here the argument @var{opvec} is the vector containing the operands
+extracted from @var{insn}, and @var{noperands} is the number of
+elements of the vector which contain meaningful data for this insn.
+The contents of this vector are what will be used to convert the insn
+template into assembler code, so you can change the assembler output
+by changing the contents of the vector.
+
+This macro is useful when various assembler syntaxes share a single
+file of instruction patterns; by defining this macro differently, you
+can cause a large class of instructions to be output differently (such
+as with rearranged operands).  Naturally, variations in assembler
+syntax affecting individual insn patterns ought to be handled by
+writing conditional output routines in those patterns.
+
+If this macro is not defined, it is equivalent to a null statement.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_ASM_FINAL_POSTSCAN_INSN (FILE *@var{file}, rtx_insn *@var{insn}, rtx *@var{opvec}, int @var{noperands})
+If defined, this target hook is a function which is executed just after the
+output of assembler code for @var{insn}, to change the mode of the assembler
+if necessary.
+
+Here the argument @var{opvec} is the vector containing the operands
+extracted from @var{insn}, and @var{noperands} is the number of
+elements of the vector which contain meaningful data for this insn.
+The contents of this vector are what was used to convert the insn
+template into assembler code, so you can change the assembler mode
+by checking the contents of the vector.
+@end deftypefn
+
+@defmac PRINT_OPERAND (@var{stream}, @var{x}, @var{code})
+A C compound statement to output to stdio stream @var{stream} the
+assembler syntax for an instruction operand @var{x}.  @var{x} is an
+RTL expression.
+
+@var{code} is a value that can be used to specify one of several ways
+of printing the operand.  It is used when identical operands must be
+printed differently depending on the context.  @var{code} comes from
+the @samp{%} specification that was used to request printing of the
+operand.  If the specification was just @samp{%@var{digit}} then
+@var{code} is 0; if the specification was @samp{%@var{ltr}
+@var{digit}} then @var{code} is the ASCII code for @var{ltr}.
+
+@findex reg_names
+If @var{x} is a register, this macro should print the register's name.
+The names can be found in an array @code{reg_names} whose type is
+@code{char *[]}.  @code{reg_names} is initialized from
+@code{REGISTER_NAMES}.
+
+When the machine description has a specification @samp{%@var{punct}}
+(a @samp{%} followed by a punctuation character), this macro is called
+with a null pointer for @var{x} and the punctuation character for
+@var{code}.
+@end defmac
+
+@defmac PRINT_OPERAND_PUNCT_VALID_P (@var{code})
+A C expression which evaluates to true if @var{code} is a valid
+punctuation character for use in the @code{PRINT_OPERAND} macro.  If
+@code{PRINT_OPERAND_PUNCT_VALID_P} is not defined, it means that no
+punctuation characters (except for the standard one, @samp{%}) are used
+in this way.
+@end defmac
+
+@defmac PRINT_OPERAND_ADDRESS (@var{stream}, @var{x})
+A C compound statement to output to stdio stream @var{stream} the
+assembler syntax for an instruction operand that is a memory reference
+whose address is @var{x}.  @var{x} is an RTL expression.
+
+@cindex @code{TARGET_ENCODE_SECTION_INFO} usage
+On some machines, the syntax for a symbolic address depends on the
+section that the address refers to.  On these machines, define the hook
+@code{TARGET_ENCODE_SECTION_INFO} to store the information into the
+@code{symbol_ref}, and then check for it here.  @xref{Assembler
+Format}.
+@end defmac
+
+@findex dbr_sequence_length
+@defmac DBR_OUTPUT_SEQEND (@var{file})
+A C statement, to be executed after all slot-filler instructions have
+been output.  If necessary, call @code{dbr_sequence_length} to
+determine the number of slots filled in a sequence (zero if not
+currently outputting a sequence), to decide how many no-ops to output,
+or whatever.
+
+Don't define this macro if it has nothing to do, but it is helpful in
+reading assembly output if the extent of the delay sequence is made
+explicit (e.g.@: with white space).
+@end defmac
+
+@findex final_sequence
+Note that output routines for instructions with delay slots must be
+prepared to deal with not being output as part of a sequence
+(i.e.@: when the scheduling pass is not run, or when no slot fillers could be
+found.)  The variable @code{final_sequence} is null when not
+processing a sequence, otherwise it contains the @code{sequence} rtx
+being output.
+
+@findex asm_fprintf
+@defmac REGISTER_PREFIX
+@defmacx LOCAL_LABEL_PREFIX
+@defmacx USER_LABEL_PREFIX
+@defmacx IMMEDIATE_PREFIX
+If defined, C string expressions to be used for the @samp{%R}, @samp{%L},
+@samp{%U}, and @samp{%I} options of @code{asm_fprintf} (see
+@file{final.c}).  These are useful when a single @file{md} file must
+support multiple assembler formats.  In that case, the various @file{tm.h}
+files can define these macros differently.
+@end defmac
+
+@defmac ASM_FPRINTF_EXTENSIONS (@var{file}, @var{argptr}, @var{format})
+If defined this macro should expand to a series of @code{case}
+statements which will be parsed inside the @code{switch} statement of
+the @code{asm_fprintf} function.  This allows targets to define extra
+printf formats which may useful when generating their assembler
+statements.  Note that uppercase letters are reserved for future
+generic extensions to asm_fprintf, and so are not available to target
+specific code.  The output file is given by the parameter @var{file}.
+The varargs input pointer is @var{argptr} and the rest of the format
+string, starting the character after the one that is being switched
+upon, is pointed to by @var{format}.
+@end defmac
+
+@defmac ASSEMBLER_DIALECT
+If your target supports multiple dialects of assembler language (such as
+different opcodes), define this macro as a C expression that gives the
+numeric index of the assembler language dialect to use, with zero as the
+first variant.
+
+If this macro is defined, you may use constructs of the form
+@smallexample
+@samp{@{option0|option1|option2@dots{}@}}
+@end smallexample
+@noindent
+in the output templates of patterns (@pxref{Output Template}) or in the
+first argument of @code{asm_fprintf}.  This construct outputs
+@samp{option0}, @samp{option1}, @samp{option2}, etc., if the value of
+@code{ASSEMBLER_DIALECT} is zero, one, two, etc.  Any special characters
+within these strings retain their usual meaning.  If there are fewer
+alternatives within the braces than the value of
+@code{ASSEMBLER_DIALECT}, the construct outputs nothing. If it's needed
+to print curly braces or @samp{|} character in assembler output directly,
+@samp{%@{}, @samp{%@}} and @samp{%|} can be used.
+
+If you do not define this macro, the characters @samp{@{}, @samp{|} and
+@samp{@}} do not have any special meaning when used in templates or
+operands to @code{asm_fprintf}.
+
+Define the macros @code{REGISTER_PREFIX}, @code{LOCAL_LABEL_PREFIX},
+@code{USER_LABEL_PREFIX} and @code{IMMEDIATE_PREFIX} if you can express
+the variations in assembler language syntax with that mechanism.  Define
+@code{ASSEMBLER_DIALECT} and use the @samp{@{option0|option1@}} syntax
+if the syntax variant are larger and involve such things as different
+opcodes or operand order.
+@end defmac
+
+@defmac ASM_OUTPUT_REG_PUSH (@var{stream}, @var{regno})
+A C expression to output to @var{stream} some assembler code
+which will push hard register number @var{regno} onto the stack.
+The code need not be optimal, since this macro is used only when
+profiling.
+@end defmac
+
+@defmac ASM_OUTPUT_REG_POP (@var{stream}, @var{regno})
+A C expression to output to @var{stream} some assembler code
+which will pop hard register number @var{regno} off of the stack.
+The code need not be optimal, since this macro is used only when
+profiling.
+@end defmac
+
+@node Dispatch Tables
+@subsection Output of Dispatch Tables
+
+@c prevent bad page break with this line
+This concerns dispatch tables.
+
+@cindex dispatch table
+@defmac ASM_OUTPUT_ADDR_DIFF_ELT (@var{stream}, @var{body}, @var{value}, @var{rel})
+A C statement to output to the stdio stream @var{stream} an assembler
+pseudo-instruction to generate a difference between two labels.
+@var{value} and @var{rel} are the numbers of two internal labels.  The
+definitions of these labels are output using
+@code{(*targetm.asm_out.internal_label)}, and they must be printed in the same
+way here.  For example,
+
+@smallexample
+fprintf (@var{stream}, "\t.word L%d-L%d\n",
+         @var{value}, @var{rel})
+@end smallexample
+
+You must provide this macro on machines where the addresses in a
+dispatch table are relative to the table's own address.  If defined, GCC
+will also use this macro on all machines when producing PIC@.
+@var{body} is the body of the @code{ADDR_DIFF_VEC}; it is provided so that the
+mode and flags can be read.
+@end defmac
+
+@defmac ASM_OUTPUT_ADDR_VEC_ELT (@var{stream}, @var{value})
+This macro should be provided on machines where the addresses
+in a dispatch table are absolute.
+
+The definition should be a C statement to output to the stdio stream
+@var{stream} an assembler pseudo-instruction to generate a reference to
+a label.  @var{value} is the number of an internal label whose
+definition is output using @code{(*targetm.asm_out.internal_label)}.
+For example,
+
+@smallexample
+fprintf (@var{stream}, "\t.word L%d\n", @var{value})
+@end smallexample
+@end defmac
+
+@defmac ASM_OUTPUT_CASE_LABEL (@var{stream}, @var{prefix}, @var{num}, @var{table})
+Define this if the label before a jump-table needs to be output
+specially.  The first three arguments are the same as for
+@code{(*targetm.asm_out.internal_label)}; the fourth argument is the
+jump-table which follows (a @code{jump_table_data} containing an
+@code{addr_vec} or @code{addr_diff_vec}).
+
+This feature is used on system V to output a @code{swbeg} statement
+for the table.
+
+If this macro is not defined, these labels are output with
+@code{(*targetm.asm_out.internal_label)}.
+@end defmac
+
+@defmac ASM_OUTPUT_CASE_END (@var{stream}, @var{num}, @var{table})
+Define this if something special must be output at the end of a
+jump-table.  The definition should be a C statement to be executed
+after the assembler code for the table is written.  It should write
+the appropriate code to stdio stream @var{stream}.  The argument
+@var{table} is the jump-table insn, and @var{num} is the label-number
+of the preceding label.
+
+If this macro is not defined, nothing special is output at the end of
+the jump-table.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_ASM_POST_CFI_STARTPROC (FILE *@var{}, @var{tree})
+This target hook is used to emit assembly strings required by the target
+after the .cfi_startproc directive.  The first argument is the file stream to
+write the strings to and the second argument is the function's declaration.  The
+expected use is to add more .cfi_* directives.
+
+The default is to not output any assembly strings.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_EMIT_UNWIND_LABEL (FILE *@var{stream}, tree @var{decl}, int @var{for_eh}, int @var{empty})
+This target hook emits a label at the beginning of each FDE@.  It
+should be defined on targets where FDEs need special labels, and it
+should write the appropriate label, for the FDE associated with the
+function declaration @var{decl}, to the stdio stream @var{stream}.
+The third argument, @var{for_eh}, is a boolean: true if this is for an
+exception table.  The fourth argument, @var{empty}, is a boolean:
+true if this is a placeholder label for an omitted FDE@.
+
+The default is that FDEs are not given nonlocal labels.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_EMIT_EXCEPT_TABLE_LABEL (FILE *@var{stream})
+This target hook emits a label at the beginning of the exception table.
+It should be defined on targets where it is desirable for the table
+to be broken up according to function.
+
+The default is that no label is emitted.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_EMIT_EXCEPT_PERSONALITY (rtx @var{personality})
+If the target implements @code{TARGET_ASM_UNWIND_EMIT}, this hook may be used to emit a directive to install a personality hook into the unwind info.  This hook should not be used if dwarf2 unwind info is used.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ASM_UNWIND_EMIT (FILE *@var{stream}, rtx_insn *@var{insn})
+This target hook emits assembly directives required to unwind the
+given instruction.  This is only used when @code{TARGET_EXCEPT_UNWIND_INFO}
+returns @code{UI_TARGET}.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_ASM_MAKE_EH_SYMBOL_INDIRECT (rtx @var{origsymbol}, bool @var{pubvis})
+If necessary, modify personality and LSDA references to handle indirection.  The original symbol is in @code{origsymbol} and if @code{pubvis} is true  the symbol is visible outside the TU.
+@end deftypefn
+
+@deftypevr {Target Hook} bool TARGET_ASM_UNWIND_EMIT_BEFORE_INSN
+True if the @code{TARGET_ASM_UNWIND_EMIT} hook should be called before the assembly for @var{insn} has been emitted, false if the hook should be called afterward.
+@end deftypevr
+
+@deftypefn {Target Hook} bool TARGET_ASM_SHOULD_RESTORE_CFA_STATE (void)
+For DWARF-based unwind frames, two CFI instructions provide for save and restore of register state.  GCC maintains the current frame address (CFA) separately from the register bank but the unwinder in libgcc preserves this state along with the registers (and this is expected by the code that writes the unwind frames).  This hook allows the target to specify that the CFA data is not saved/restored along with the registers by the target unwinder so that suitable additional instructions should be emitted to restore it.
+@end deftypefn
+
+@node Exception Region Output
+@subsection Assembler Commands for Exception Regions
+
+@c prevent bad page break with this line
+
+This describes commands marking the start and the end of an exception
+region.
+
+@defmac EH_FRAME_SECTION_NAME
+If defined, a C string constant for the name of the section containing
+exception handling frame unwind information.  If not defined, GCC will
+provide a default definition if the target supports named sections.
+@file{crtstuff.c} uses this macro to switch to the appropriate section.
+
+You should define this symbol if your target supports DWARF 2 frame
+unwind information and the default definition does not work.
+@end defmac
+
+@defmac EH_FRAME_THROUGH_COLLECT2
+If defined, DWARF 2 frame unwind information will identified by
+specially named labels.  The collect2 process will locate these
+labels and generate code to register the frames.
+
+This might be necessary, for instance, if the system linker will not
+place the eh_frames in-between the sentinals from @file{crtstuff.c},
+or if the system linker does garbage collection and sections cannot
+be marked as not to be collected.
+@end defmac
+
+@defmac EH_TABLES_CAN_BE_READ_ONLY
+Define this macro to 1 if your target is such that no frame unwind
+information encoding used with non-PIC code will ever require a
+runtime relocation, but the linker may not support merging read-only
+and read-write sections into a single read-write section.
+@end defmac
+
+@defmac MASK_RETURN_ADDR
+An rtx used to mask the return address found via @code{RETURN_ADDR_RTX}, so
+that it does not contain any extraneous set bits in it.
+@end defmac
+
+@defmac DWARF2_UNWIND_INFO
+Define this macro to 0 if your target supports DWARF 2 frame unwind
+information, but it does not yet work with exception handling.
+Otherwise, if your target supports this information (if it defines
+@code{INCOMING_RETURN_ADDR_RTX} and @code{OBJECT_FORMAT_ELF}),
+GCC will provide a default definition of 1.
+@end defmac
+
+@deftypefn {Common Target Hook} {enum unwind_info_type} TARGET_EXCEPT_UNWIND_INFO (struct gcc_options *@var{opts})
+This hook defines the mechanism that will be used for exception handling
+by the target.  If the target has ABI specified unwind tables, the hook
+should return @code{UI_TARGET}.  If the target is to use the
+@code{setjmp}/@code{longjmp}-based exception handling scheme, the hook
+should return @code{UI_SJLJ}.  If the target supports DWARF 2 frame unwind
+information, the hook should return @code{UI_DWARF2}.
+
+A target may, if exceptions are disabled, choose to return @code{UI_NONE}.
+This may end up simplifying other parts of target-specific code.  The
+default implementation of this hook never returns @code{UI_NONE}.
+
+Note that the value returned by this hook should be constant.  It should
+not depend on anything except the command-line switches described by
+@var{opts}.  In particular, the
+setting @code{UI_SJLJ} must be fixed at compiler start-up as C pre-processor
+macros and builtin functions related to exception handling are set up
+depending on this setting.
+
+The default implementation of the hook first honors the
+@option{--enable-sjlj-exceptions} configure option, then
+@code{DWARF2_UNWIND_INFO}, and finally defaults to @code{UI_SJLJ}.  If
+@code{DWARF2_UNWIND_INFO} depends on command-line options, the target
+must define this hook so that @var{opts} is used correctly.
+@end deftypefn
+
+@deftypevr {Common Target Hook} bool TARGET_UNWIND_TABLES_DEFAULT
+This variable should be set to @code{true} if the target ABI requires unwinding
+tables even when exceptions are not used.  It must not be modified by
+command-line option processing.
+@end deftypevr
+
+@defmac DONT_USE_BUILTIN_SETJMP
+Define this macro to 1 if the @code{setjmp}/@code{longjmp}-based scheme
+should use the @code{setjmp}/@code{longjmp} functions from the C library
+instead of the @code{__builtin_setjmp}/@code{__builtin_longjmp} machinery.
+@end defmac
+
+@defmac JMP_BUF_SIZE
+This macro has no effect unless @code{DONT_USE_BUILTIN_SETJMP} is also
+defined.  Define this macro if the default size of @code{jmp_buf} buffer
+for the @code{setjmp}/@code{longjmp}-based exception handling mechanism
+is not large enough, or if it is much too large.
+The default size is @code{FIRST_PSEUDO_REGISTER * sizeof(void *)}.
+@end defmac
+
+@defmac DWARF_CIE_DATA_ALIGNMENT
+This macro need only be defined if the target might save registers in the
+function prologue at an offset to the stack pointer that is not aligned to
+@code{UNITS_PER_WORD}.  The definition should be the negative minimum
+alignment if @code{STACK_GROWS_DOWNWARD} is true, and the positive
+minimum alignment otherwise.  @xref{DWARF}.  Only applicable if
+the target supports DWARF 2 frame unwind information.
+@end defmac
+
+@deftypevr {Target Hook} bool TARGET_TERMINATE_DW2_EH_FRAME_INFO
+Contains the value true if the target should add a zero word onto the
+end of a Dwarf-2 frame info section when used for exception handling.
+Default value is false if @code{EH_FRAME_SECTION_NAME} is defined, and
+true otherwise.
+@end deftypevr
+
+@deftypefn {Target Hook} rtx TARGET_DWARF_REGISTER_SPAN (rtx @var{reg})
+Given a register, this hook should return a parallel of registers to
+represent where to find the register pieces.  Define this hook if the
+register and its mode are represented in Dwarf in non-contiguous
+locations, or if the register should be represented in more than one
+register in Dwarf.  Otherwise, this hook should return @code{NULL_RTX}.
+If not defined, the default is to return @code{NULL_RTX}.
+@end deftypefn
+
+@deftypefn {Target Hook} machine_mode TARGET_DWARF_FRAME_REG_MODE (int @var{regno})
+Given a register, this hook should return the mode which the
+corresponding Dwarf frame register should have.  This is normally
+used to return a smaller mode than the raw mode to prevent call
+clobbered parts of a register altering the frame register size
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_INIT_DWARF_REG_SIZES_EXTRA (tree @var{address})
+If some registers are represented in Dwarf-2 unwind information in
+multiple pieces, define this hook to fill in information about the
+sizes of those pieces in the table used by the unwinder at runtime.
+It will be called by @code{expand_builtin_init_dwarf_reg_sizes} after
+filling in a single size corresponding to each hard register;
+@var{address} is the address of the table.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_ASM_TTYPE (rtx @var{sym})
+This hook is used to output a reference from a frame unwinding table to
+the type_info object identified by @var{sym}.  It should return @code{true}
+if the reference was output.  Returning @code{false} will cause the
+reference to be output using the normal Dwarf2 routines.
+@end deftypefn
+
+@deftypevr {Target Hook} bool TARGET_ARM_EABI_UNWINDER
+This flag should be set to @code{true} on targets that use an ARM EABI
+based unwinding library, and @code{false} on other targets.  This effects
+the format of unwinding tables, and how the unwinder in entered after
+running a cleanup.  The default is @code{false}.
+@end deftypevr
+
+@node Alignment Output
+@subsection Assembler Commands for Alignment
+
+@c prevent bad page break with this line
+This describes commands for alignment.
+
+@defmac JUMP_ALIGN (@var{label})
+The alignment (log base 2) to put in front of @var{label}, which is
+a common destination of jumps and has no fallthru incoming edge.
+
+This macro need not be defined if you don't want any special alignment
+to be done at such a time.  Most machine descriptions do not currently
+define the macro.
+
+Unless it's necessary to inspect the @var{label} parameter, it is better
+to set the variable @var{align_jumps} in the target's
+@code{TARGET_OPTION_OVERRIDE}.  Otherwise, you should try to honor the user's
+selection in @var{align_jumps} in a @code{JUMP_ALIGN} implementation.
+@end defmac
+
+@defmac LABEL_ALIGN_AFTER_BARRIER (@var{label})
+The alignment (log base 2) to put in front of @var{label}, which follows
+a @code{BARRIER}.
+
+This macro need not be defined if you don't want any special alignment
+to be done at such a time.  Most machine descriptions do not currently
+define the macro.
+@end defmac
+
+@defmac LOOP_ALIGN (@var{label})
+The alignment (log base 2) to put in front of @var{label} that heads
+a frequently executed basic block (usually the header of a loop).
+
+This macro need not be defined if you don't want any special alignment
+to be done at such a time.  Most machine descriptions do not currently
+define the macro.
+
+Unless it's necessary to inspect the @var{label} parameter, it is better
+to set the variable @code{align_loops} in the target's
+@code{TARGET_OPTION_OVERRIDE}.  Otherwise, you should try to honor the user's
+selection in @code{align_loops} in a @code{LOOP_ALIGN} implementation.
+@end defmac
+
+@defmac LABEL_ALIGN (@var{label})
+The alignment (log base 2) to put in front of @var{label}.
+If @code{LABEL_ALIGN_AFTER_BARRIER} / @code{LOOP_ALIGN} specify a different alignment,
+the maximum of the specified values is used.
+
+Unless it's necessary to inspect the @var{label} parameter, it is better
+to set the variable @code{align_labels} in the target's
+@code{TARGET_OPTION_OVERRIDE}.  Otherwise, you should try to honor the user's
+selection in @code{align_labels} in a @code{LABEL_ALIGN} implementation.
+@end defmac
+
+@defmac ASM_OUTPUT_SKIP (@var{stream}, @var{nbytes})
+A C statement to output to the stdio stream @var{stream} an assembler
+instruction to advance the location counter by @var{nbytes} bytes.
+Those bytes should be zero when loaded.  @var{nbytes} will be a C
+expression of type @code{unsigned HOST_WIDE_INT}.
+@end defmac
+
+@defmac ASM_NO_SKIP_IN_TEXT
+Define this macro if @code{ASM_OUTPUT_SKIP} should not be used in the
+text section because it fails to put zeros in the bytes that are skipped.
+This is true on many Unix systems, where the pseudo--op to skip bytes
+produces no-op instructions rather than zeros when used in the text
+section.
+@end defmac
+
+@defmac ASM_OUTPUT_ALIGN (@var{stream}, @var{power})
+A C statement to output to the stdio stream @var{stream} an assembler
+command to advance the location counter to a multiple of 2 to the
+@var{power} bytes.  @var{power} will be a C expression of type @code{int}.
+@end defmac
+
+@defmac ASM_OUTPUT_ALIGN_WITH_NOP (@var{stream}, @var{power})
+Like @code{ASM_OUTPUT_ALIGN}, except that the ``nop'' instruction is used
+for padding, if necessary.
+@end defmac
+
+@defmac ASM_OUTPUT_MAX_SKIP_ALIGN (@var{stream}, @var{power}, @var{max_skip})
+A C statement to output to the stdio stream @var{stream} an assembler
+command to advance the location counter to a multiple of 2 to the
+@var{power} bytes, but only if @var{max_skip} or fewer bytes are needed to
+satisfy the alignment request.  @var{power} and @var{max_skip} will be
+a C expression of type @code{int}.
+@end defmac
+
+@need 3000
+@node Debugging Info
+@section Controlling Debugging Information Format
+
+@c prevent bad page break with this line
+This describes how to specify debugging information.
+
+@menu
+* All Debuggers::      Macros that affect all debugging formats uniformly.
+* DBX Options::        Macros enabling specific options in DBX format.
+* DBX Hooks::          Hook macros for varying DBX format.
+* File Names and DBX:: Macros controlling output of file names in DBX format.
+* DWARF::              Macros for DWARF format.
+* VMS Debug::          Macros for VMS debug format.
+@end menu
+
+@node All Debuggers
+@subsection Macros Affecting All Debugging Formats
+
+@c prevent bad page break with this line
+These macros affect all debugging formats.
+
+@defmac DBX_REGISTER_NUMBER (@var{regno})
+A C expression that returns the DBX register number for the compiler
+register number @var{regno}.  In the default macro provided, the value
+of this expression will be @var{regno} itself.  But sometimes there are
+some registers that the compiler knows about and DBX does not, or vice
+versa.  In such cases, some register may need to have one number in the
+compiler and another for DBX@.
+
+If two registers have consecutive numbers inside GCC, and they can be
+used as a pair to hold a multiword value, then they @emph{must} have
+consecutive numbers after renumbering with @code{DBX_REGISTER_NUMBER}.
+Otherwise, debuggers will be unable to access such a pair, because they
+expect register pairs to be consecutive in their own numbering scheme.
+
+If you find yourself defining @code{DBX_REGISTER_NUMBER} in way that
+does not preserve register pairs, then what you must do instead is
+redefine the actual register numbering scheme.
+@end defmac
+
+@defmac DEBUGGER_AUTO_OFFSET (@var{x})
+A C expression that returns the integer offset value for an automatic
+variable having address @var{x} (an RTL expression).  The default
+computation assumes that @var{x} is based on the frame-pointer and
+gives the offset from the frame-pointer.  This is required for targets
+that produce debugging output for DBX and allow the frame-pointer to be
+eliminated when the @option{-g} option is used.
+@end defmac
+
+@defmac DEBUGGER_ARG_OFFSET (@var{offset}, @var{x})
+A C expression that returns the integer offset value for an argument
+having address @var{x} (an RTL expression).  The nominal offset is
+@var{offset}.
+@end defmac
+
+@defmac PREFERRED_DEBUGGING_TYPE
+A C expression that returns the type of debugging output GCC should
+produce when the user specifies just @option{-g}.  Define
+this if you have arranged for GCC to support more than one format of
+debugging output.  Currently, the allowable values are @code{DBX_DEBUG},
+@code{DWARF2_DEBUG}, @code{XCOFF_DEBUG}, @code{VMS_DEBUG},
+and @code{VMS_AND_DWARF2_DEBUG}.
+
+When the user specifies @option{-ggdb}, GCC normally also uses the
+value of this macro to select the debugging output format, but with two
+exceptions.  If @code{DWARF2_DEBUGGING_INFO} is defined, GCC uses the
+value @code{DWARF2_DEBUG}.  Otherwise, if @code{DBX_DEBUGGING_INFO} is
+defined, GCC uses @code{DBX_DEBUG}.
+
+The value of this macro only affects the default debugging output; the
+user can always get a specific type of output by using @option{-gstabs},
+@option{-gdwarf-2}, @option{-gxcoff}, or @option{-gvms}.
+@end defmac
+
+@node DBX Options
+@subsection Specific Options for DBX Output
+
+@c prevent bad page break with this line
+These are specific options for DBX output.
+
+@defmac DBX_DEBUGGING_INFO
+Define this macro if GCC should produce debugging output for DBX
+in response to the @option{-g} option.
+@end defmac
+
+@defmac XCOFF_DEBUGGING_INFO
+Define this macro if GCC should produce XCOFF format debugging output
+in response to the @option{-g} option.  This is a variant of DBX format.
+@end defmac
+
+@defmac DEFAULT_GDB_EXTENSIONS
+Define this macro to control whether GCC should by default generate
+GDB's extended version of DBX debugging information (assuming DBX-format
+debugging information is enabled at all).  If you don't define the
+macro, the default is 1: always generate the extended information
+if there is any occasion to.
+@end defmac
+
+@defmac DEBUG_SYMS_TEXT
+Define this macro if all @code{.stabs} commands should be output while
+in the text section.
+@end defmac
+
+@defmac ASM_STABS_OP
+A C string constant, including spacing, naming the assembler pseudo op to
+use instead of @code{"\t.stabs\t"} to define an ordinary debugging symbol.
+If you don't define this macro, @code{"\t.stabs\t"} is used.  This macro
+applies only to DBX debugging information format.
+@end defmac
+
+@defmac ASM_STABD_OP
+A C string constant, including spacing, naming the assembler pseudo op to
+use instead of @code{"\t.stabd\t"} to define a debugging symbol whose
+value is the current location.  If you don't define this macro,
+@code{"\t.stabd\t"} is used.  This macro applies only to DBX debugging
+information format.
+@end defmac
+
+@defmac ASM_STABN_OP
+A C string constant, including spacing, naming the assembler pseudo op to
+use instead of @code{"\t.stabn\t"} to define a debugging symbol with no
+name.  If you don't define this macro, @code{"\t.stabn\t"} is used.  This
+macro applies only to DBX debugging information format.
+@end defmac
+
+@defmac DBX_NO_XREFS
+Define this macro if DBX on your system does not support the construct
+@samp{xs@var{tagname}}.  On some systems, this construct is used to
+describe a forward reference to a structure named @var{tagname}.
+On other systems, this construct is not supported at all.
+@end defmac
+
+@defmac DBX_CONTIN_LENGTH
+A symbol name in DBX-format debugging information is normally
+continued (split into two separate @code{.stabs} directives) when it
+exceeds a certain length (by default, 80 characters).  On some
+operating systems, DBX requires this splitting; on others, splitting
+must not be done.  You can inhibit splitting by defining this macro
+with the value zero.  You can override the default splitting-length by
+defining this macro as an expression for the length you desire.
+@end defmac
+
+@defmac DBX_CONTIN_CHAR
+Normally continuation is indicated by adding a @samp{\} character to
+the end of a @code{.stabs} string when a continuation follows.  To use
+a different character instead, define this macro as a character
+constant for the character you want to use.  Do not define this macro
+if backslash is correct for your system.
+@end defmac
+
+@defmac DBX_STATIC_STAB_DATA_SECTION
+Define this macro if it is necessary to go to the data section before
+outputting the @samp{.stabs} pseudo-op for a non-global static
+variable.
+@end defmac
+
+@defmac DBX_TYPE_DECL_STABS_CODE
+The value to use in the ``code'' field of the @code{.stabs} directive
+for a typedef.  The default is @code{N_LSYM}.
+@end defmac
+
+@defmac DBX_STATIC_CONST_VAR_CODE
+The value to use in the ``code'' field of the @code{.stabs} directive
+for a static variable located in the text section.  DBX format does not
+provide any ``right'' way to do this.  The default is @code{N_FUN}.
+@end defmac
+
+@defmac DBX_REGPARM_STABS_CODE
+The value to use in the ``code'' field of the @code{.stabs} directive
+for a parameter passed in registers.  DBX format does not provide any
+``right'' way to do this.  The default is @code{N_RSYM}.
+@end defmac
+
+@defmac DBX_REGPARM_STABS_LETTER
+The letter to use in DBX symbol data to identify a symbol as a parameter
+passed in registers.  DBX format does not customarily provide any way to
+do this.  The default is @code{'P'}.
+@end defmac
+
+@defmac DBX_FUNCTION_FIRST
+Define this macro if the DBX information for a function and its
+arguments should precede the assembler code for the function.  Normally,
+in DBX format, the debugging information entirely follows the assembler
+code.
+@end defmac
+
+@defmac DBX_BLOCKS_FUNCTION_RELATIVE
+Define this macro, with value 1, if the value of a symbol describing
+the scope of a block (@code{N_LBRAC} or @code{N_RBRAC}) should be
+relative to the start of the enclosing function.  Normally, GCC uses
+an absolute address.
+@end defmac
+
+@defmac DBX_LINES_FUNCTION_RELATIVE
+Define this macro, with value 1, if the value of a symbol indicating
+the current line number (@code{N_SLINE}) should be relative to the
+start of the enclosing function.  Normally, GCC uses an absolute address.
+@end defmac
+
+@defmac DBX_USE_BINCL
+Define this macro if GCC should generate @code{N_BINCL} and
+@code{N_EINCL} stabs for included header files, as on Sun systems.  This
+macro also directs GCC to output a type number as a pair of a file
+number and a type number within the file.  Normally, GCC does not
+generate @code{N_BINCL} or @code{N_EINCL} stabs, and it outputs a single
+number for a type number.
+@end defmac
+
+@node DBX Hooks
+@subsection Open-Ended Hooks for DBX Format
+
+@c prevent bad page break with this line
+These are hooks for DBX format.
+
+@defmac DBX_OUTPUT_SOURCE_LINE (@var{stream}, @var{line}, @var{counter})
+A C statement to output DBX debugging information before code for line
+number @var{line} of the current source file to the stdio stream
+@var{stream}.  @var{counter} is the number of time the macro was
+invoked, including the current invocation; it is intended to generate
+unique labels in the assembly output.
+
+This macro should not be defined if the default output is correct, or
+if it can be made correct by defining @code{DBX_LINES_FUNCTION_RELATIVE}.
+@end defmac
+
+@defmac NO_DBX_FUNCTION_END
+Some stabs encapsulation formats (in particular ECOFF), cannot handle the
+@code{.stabs "",N_FUN,,0,0,Lscope-function-1} gdb dbx extension construct.
+On those machines, define this macro to turn this feature off without
+disturbing the rest of the gdb extensions.
+@end defmac
+
+@defmac NO_DBX_BNSYM_ENSYM
+Some assemblers cannot handle the @code{.stabd BNSYM/ENSYM,0,0} gdb dbx
+extension construct.  On those machines, define this macro to turn this
+feature off without disturbing the rest of the gdb extensions.
+@end defmac
+
+@node File Names and DBX
+@subsection File Names in DBX Format
+
+@c prevent bad page break with this line
+This describes file names in DBX format.
+
+@defmac DBX_OUTPUT_MAIN_SOURCE_FILENAME (@var{stream}, @var{name})
+A C statement to output DBX debugging information to the stdio stream
+@var{stream}, which indicates that file @var{name} is the main source
+file---the file specified as the input file for compilation.
+This macro is called only once, at the beginning of compilation.
+
+This macro need not be defined if the standard form of output
+for DBX debugging information is appropriate.
+
+It may be necessary to refer to a label equal to the beginning of the
+text section.  You can use @samp{assemble_name (stream, ltext_label_name)}
+to do so.  If you do this, you must also set the variable
+@var{used_ltext_label_name} to @code{true}.
+@end defmac
+
+@defmac NO_DBX_MAIN_SOURCE_DIRECTORY
+Define this macro, with value 1, if GCC should not emit an indication
+of the current directory for compilation and current source language at
+the beginning of the file.
+@end defmac
+
+@defmac NO_DBX_GCC_MARKER
+Define this macro, with value 1, if GCC should not emit an indication
+that this object file was compiled by GCC@.  The default is to emit
+an @code{N_OPT} stab at the beginning of every source file, with
+@samp{gcc2_compiled.} for the string and value 0.
+@end defmac
+
+@defmac DBX_OUTPUT_MAIN_SOURCE_FILE_END (@var{stream}, @var{name})
+A C statement to output DBX debugging information at the end of
+compilation of the main source file @var{name}.  Output should be
+written to the stdio stream @var{stream}.
+
+If you don't define this macro, nothing special is output at the end
+of compilation, which is correct for most machines.
+@end defmac
+
+@defmac DBX_OUTPUT_NULL_N_SO_AT_MAIN_SOURCE_FILE_END
+Define this macro @emph{instead of} defining
+@code{DBX_OUTPUT_MAIN_SOURCE_FILE_END}, if what needs to be output at
+the end of compilation is an @code{N_SO} stab with an empty string,
+whose value is the highest absolute text address in the file.
+@end defmac
+
+@need 2000
+@node DWARF
+@subsection Macros for DWARF Output
+
+@c prevent bad page break with this line
+Here are macros for DWARF output.
+
+@defmac DWARF2_DEBUGGING_INFO
+Define this macro if GCC should produce dwarf version 2 format
+debugging output in response to the @option{-g} option.
+
+@deftypefn {Target Hook} int TARGET_DWARF_CALLING_CONVENTION (const_tree @var{function})
+Define this to enable the dwarf attribute @code{DW_AT_calling_convention} to
+be emitted for each function.  Instead of an integer return the enum
+value for the @code{DW_CC_} tag.
+@end deftypefn
+
+To support optional call frame debugging information, you must also
+define @code{INCOMING_RETURN_ADDR_RTX} and either set
+@code{RTX_FRAME_RELATED_P} on the prologue insns if you use RTL for the
+prologue, or call @code{dwarf2out_def_cfa} and @code{dwarf2out_reg_save}
+as appropriate from @code{TARGET_ASM_FUNCTION_PROLOGUE} if you don't.
+@end defmac
+
+@defmac DWARF2_FRAME_INFO
+Define this macro to a nonzero value if GCC should always output
+Dwarf 2 frame information.  If @code{TARGET_EXCEPT_UNWIND_INFO}
+(@pxref{Exception Region Output}) returns @code{UI_DWARF2}, and
+exceptions are enabled, GCC will output this information not matter
+how you define @code{DWARF2_FRAME_INFO}.
+@end defmac
+
+@deftypefn {Target Hook} {enum unwind_info_type} TARGET_DEBUG_UNWIND_INFO (void)
+This hook defines the mechanism that will be used for describing frame
+unwind information to the debugger.  Normally the hook will return
+@code{UI_DWARF2} if DWARF 2 debug information is enabled, and
+return @code{UI_NONE} otherwise.
+
+A target may return @code{UI_DWARF2} even when DWARF 2 debug information
+is disabled in order to always output DWARF 2 frame information.
+
+A target may return @code{UI_TARGET} if it has ABI specified unwind tables.
+This will suppress generation of the normal debug frame unwind information.
+@end deftypefn
+
+@defmac DWARF2_ASM_LINE_DEBUG_INFO
+Define this macro to be a nonzero value if the assembler can generate Dwarf 2
+line debug info sections.  This will result in much more compact line number
+tables, and hence is desirable if it works.
+@end defmac
+
+@defmac DWARF2_ASM_VIEW_DEBUG_INFO
+Define this macro to be a nonzero value if the assembler supports view
+assignment and verification in @code{.loc}.  If it does not, but the
+user enables location views, the compiler may have to fallback to
+internal line number tables.
+@end defmac
+
+@deftypefn {Target Hook} int TARGET_RESET_LOCATION_VIEW (rtx_insn *@var{})
+This hook, if defined, enables -ginternal-reset-location-views, and
+uses its result to override cases in which the estimated min insn
+length might be nonzero even when a PC advance (i.e., a view reset)
+cannot be taken for granted.
+
+If the hook is defined, it must return a positive value to indicate
+the insn definitely advances the PC, and so the view number can be
+safely assumed to be reset; a negative value to mean the insn
+definitely does not advance the PC, and os the view number must not
+be reset; or zero to decide based on the estimated insn length.
+
+If insn length is to be regarded as reliable, set the hook to
+@code{hook_int_rtx_insn_0}.
+@end deftypefn
+
+@deftypevr {Target Hook} bool TARGET_WANT_DEBUG_PUB_SECTIONS
+True if the @code{.debug_pubtypes} and @code{.debug_pubnames} sections should be emitted.  These sections are not used on most platforms, and in particular GDB does not use them.
+@end deftypevr
+
+@deftypevr {Target Hook} bool TARGET_DELAY_SCHED2
+True if sched2 is not to be run at its normal place.
+This usually means it will be run as part of machine-specific reorg.
+@end deftypevr
+
+@deftypevr {Target Hook} bool TARGET_DELAY_VARTRACK
+True if vartrack is not to be run at its normal place.
+This usually means it will be run as part of machine-specific reorg.
+@end deftypevr
+
+@deftypevr {Target Hook} bool TARGET_NO_REGISTER_ALLOCATION
+True if register allocation and the passes
+following it should not be run.  Usually true only for virtual assembler
+targets.
+@end deftypevr
+
+@defmac ASM_OUTPUT_DWARF_DELTA (@var{stream}, @var{size}, @var{label1}, @var{label2})
+A C statement to issue assembly directives that create a difference
+@var{lab1} minus @var{lab2}, using an integer of the given @var{size}.
+@end defmac
+
+@defmac ASM_OUTPUT_DWARF_VMS_DELTA (@var{stream}, @var{size}, @var{label1}, @var{label2})
+A C statement to issue assembly directives that create a difference
+between the two given labels in system defined units, e.g.@: instruction
+slots on IA64 VMS, using an integer of the given size.
+@end defmac
+
+@defmac ASM_OUTPUT_DWARF_OFFSET (@var{stream}, @var{size}, @var{label}, @var{offset}, @var{section})
+A C statement to issue assembly directives that create a
+section-relative reference to the given @var{label} plus @var{offset}, using
+an integer of the given @var{size}.  The label is known to be defined in the
+given @var{section}.
+@end defmac
+
+@defmac ASM_OUTPUT_DWARF_PCREL (@var{stream}, @var{size}, @var{label})
+A C statement to issue assembly directives that create a self-relative
+reference to the given @var{label}, using an integer of the given @var{size}.
+@end defmac
+
+@defmac ASM_OUTPUT_DWARF_DATAREL (@var{stream}, @var{size}, @var{label})
+A C statement to issue assembly directives that create a reference to the
+given @var{label} relative to the dbase, using an integer of the given @var{size}.
+@end defmac
+
+@defmac ASM_OUTPUT_DWARF_TABLE_REF (@var{label})
+A C statement to issue assembly directives that create a reference to
+the DWARF table identifier @var{label} from the current section.  This
+is used on some systems to avoid garbage collecting a DWARF table which
+is referenced by a function.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_ASM_OUTPUT_DWARF_DTPREL (FILE *@var{file}, int @var{size}, rtx @var{x})
+If defined, this target hook is a function which outputs a DTP-relative
+reference to the given TLS symbol of the specified size.
+@end deftypefn
+
+@need 2000
+@node VMS Debug
+@subsection Macros for VMS Debug Format
+
+@c prevent bad page break with this line
+Here are macros for VMS debug format.
+
+@defmac VMS_DEBUGGING_INFO
+Define this macro if GCC should produce debugging output for VMS
+in response to the @option{-g} option.  The default behavior for VMS
+is to generate minimal debug info for a traceback in the absence of
+@option{-g} unless explicitly overridden with @option{-g0}.  This
+behavior is controlled by @code{TARGET_OPTION_OPTIMIZATION} and
+@code{TARGET_OPTION_OVERRIDE}.
+@end defmac
+
+@node Floating Point
+@section Cross Compilation and Floating Point
+@cindex cross compilation and floating point
+@cindex floating point and cross compilation
+
+While all modern machines use twos-complement representation for integers,
+there are a variety of representations for floating point numbers.  This
+means that in a cross-compiler the representation of floating point numbers
+in the compiled program may be different from that used in the machine
+doing the compilation.
+
+Because different representation systems may offer different amounts of
+range and precision, all floating point constants must be represented in
+the target machine's format.  Therefore, the cross compiler cannot
+safely use the host machine's floating point arithmetic; it must emulate
+the target's arithmetic.  To ensure consistency, GCC always uses
+emulation to work with floating point values, even when the host and
+target floating point formats are identical.
+
+The following macros are provided by @file{real.h} for the compiler to
+use.  All parts of the compiler which generate or optimize
+floating-point calculations must use these macros.  They may evaluate
+their operands more than once, so operands must not have side effects.
+
+@defmac REAL_VALUE_TYPE
+The C data type to be used to hold a floating point value in the target
+machine's format.  Typically this is a @code{struct} containing an
+array of @code{HOST_WIDE_INT}, but all code should treat it as an opaque
+quantity.
+@end defmac
+
+@deftypefn Macro HOST_WIDE_INT REAL_VALUE_FIX (REAL_VALUE_TYPE @var{x})
+Truncates @var{x} to a signed integer, rounding toward zero.
+@end deftypefn
+
+@deftypefn Macro {unsigned HOST_WIDE_INT} REAL_VALUE_UNSIGNED_FIX (REAL_VALUE_TYPE @var{x})
+Truncates @var{x} to an unsigned integer, rounding toward zero.  If
+@var{x} is negative, returns zero.
+@end deftypefn
+
+@deftypefn Macro REAL_VALUE_TYPE REAL_VALUE_ATOF (const char *@var{string}, machine_mode @var{mode})
+Converts @var{string} into a floating point number in the target machine's
+representation for mode @var{mode}.  This routine can handle both
+decimal and hexadecimal floating point constants, using the syntax
+defined by the C language for both.
+@end deftypefn
+
+@deftypefn Macro int REAL_VALUE_NEGATIVE (REAL_VALUE_TYPE @var{x})
+Returns 1 if @var{x} is negative (including negative zero), 0 otherwise.
+@end deftypefn
+
+@deftypefn Macro int REAL_VALUE_ISINF (REAL_VALUE_TYPE @var{x})
+Determines whether @var{x} represents infinity (positive or negative).
+@end deftypefn
+
+@deftypefn Macro int REAL_VALUE_ISNAN (REAL_VALUE_TYPE @var{x})
+Determines whether @var{x} represents a ``NaN'' (not-a-number).
+@end deftypefn
+
+@deftypefn Macro REAL_VALUE_TYPE REAL_VALUE_NEGATE (REAL_VALUE_TYPE @var{x})
+Returns the negative of the floating point value @var{x}.
+@end deftypefn
+
+@deftypefn Macro REAL_VALUE_TYPE REAL_VALUE_ABS (REAL_VALUE_TYPE @var{x})
+Returns the absolute value of @var{x}.
+@end deftypefn
+
+@node Mode Switching
+@section Mode Switching Instructions
+@cindex mode switching
+The following macros control mode switching optimizations:
+
+@defmac OPTIMIZE_MODE_SWITCHING (@var{entity})
+Define this macro if the port needs extra instructions inserted for mode
+switching in an optimizing compilation.
+
+For an example, the SH4 can perform both single and double precision
+floating point operations, but to perform a single precision operation,
+the FPSCR PR bit has to be cleared, while for a double precision
+operation, this bit has to be set.  Changing the PR bit requires a general
+purpose register as a scratch register, hence these FPSCR sets have to
+be inserted before reload, i.e.@: you cannot put this into instruction emitting
+or @code{TARGET_MACHINE_DEPENDENT_REORG}.
+
+You can have multiple entities that are mode-switched, and select at run time
+which entities actually need it.  @code{OPTIMIZE_MODE_SWITCHING} should
+return nonzero for any @var{entity} that needs mode-switching.
+If you define this macro, you also have to define
+@code{NUM_MODES_FOR_MODE_SWITCHING}, @code{TARGET_MODE_NEEDED},
+@code{TARGET_MODE_PRIORITY} and @code{TARGET_MODE_EMIT}.
+@code{TARGET_MODE_AFTER}, @code{TARGET_MODE_ENTRY}, and @code{TARGET_MODE_EXIT}
+are optional.
+@end defmac
+
+@defmac NUM_MODES_FOR_MODE_SWITCHING
+If you define @code{OPTIMIZE_MODE_SWITCHING}, you have to define this as
+initializer for an array of integers.  Each initializer element
+N refers to an entity that needs mode switching, and specifies the number
+of different modes that might need to be set for this entity.
+The position of the initializer in the initializer---starting counting at
+zero---determines the integer that is used to refer to the mode-switched
+entity in question.
+In macros that take mode arguments / yield a mode result, modes are
+represented as numbers 0 @dots{} N @minus{} 1.  N is used to specify that no mode
+switch is needed / supplied.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_MODE_EMIT (int @var{entity}, int @var{mode}, int @var{prev_mode}, HARD_REG_SET @var{regs_live})
+Generate one or more insns to set @var{entity} to @var{mode}. @var{hard_reg_live} is the set of hard registers live at the point where the insn(s) are to be inserted. @var{prev_moxde} indicates the mode to switch from. Sets of a lower numbered entity will be emitted before sets of a higher numbered entity to a mode of the same or lower priority.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_MODE_NEEDED (int @var{entity}, rtx_insn *@var{insn})
+@var{entity} is an integer specifying a mode-switched entity.  If @code{OPTIMIZE_MODE_SWITCHING} is defined, you must define this macro to return an integer value not larger than the corresponding element in @code{NUM_MODES_FOR_MODE_SWITCHING}, to denote the mode that @var{entity} must be switched into prior to the execution of @var{insn}.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_MODE_AFTER (int @var{entity}, int @var{mode}, rtx_insn *@var{insn})
+@var{entity} is an integer specifying a mode-switched entity.  If this macro is defined, it is evaluated for every @var{insn} during mode switching.  It determines the mode that an insn results in (if different from the incoming mode).
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_MODE_ENTRY (int @var{entity})
+If this macro is defined, it is evaluated for every @var{entity} that needs mode switching.  It should evaluate to an integer, which is a mode that @var{entity} is assumed to be switched to at function entry.  If @code{TARGET_MODE_ENTRY} is defined then @code{TARGET_MODE_EXIT} must be defined.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_MODE_EXIT (int @var{entity})
+If this macro is defined, it is evaluated for every @var{entity} that needs mode switching.  It should evaluate to an integer, which is a mode that @var{entity} is assumed to be switched to at function exit.  If @code{TARGET_MODE_EXIT} is defined then @code{TARGET_MODE_ENTRY} must be defined.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_MODE_PRIORITY (int @var{entity}, int @var{n})
+This macro specifies the order in which modes for @var{entity} are processed. 0 is the highest priority, @code{NUM_MODES_FOR_MODE_SWITCHING[@var{entity}] - 1} the lowest.  The value of the macro should be an integer designating a mode for @var{entity}.  For any fixed @var{entity}, @code{mode_priority} (@var{entity}, @var{n}) shall be a bijection in 0 @dots{} @code{num_modes_for_mode_switching[@var{entity}] - 1}.
+@end deftypefn
+
+@node Target Attributes
+@section Defining target-specific uses of @code{__attribute__}
+@cindex target attributes
+@cindex machine attributes
+@cindex attributes, target-specific
+
+Target-specific attributes may be defined for functions, data and types.
+These are described using the following target hooks; they also need to
+be documented in @file{extend.texi}.
+
+@deftypevr {Target Hook} {const struct attribute_spec *} TARGET_ATTRIBUTE_TABLE
+If defined, this target hook points to an array of @samp{struct
+attribute_spec} (defined in @file{tree-core.h}) specifying the machine
+specific attributes for this target and some of the restrictions on the
+entities to which these attributes are applied and the arguments they
+take.
+@end deftypevr
+
+@deftypefn {Target Hook} bool TARGET_ATTRIBUTE_TAKES_IDENTIFIER_P (const_tree @var{name})
+If defined, this target hook is a function which returns true if the
+machine-specific attribute named @var{name} expects an identifier
+given as its first argument to be passed on as a plain identifier, not
+subjected to name lookup.  If this is not defined, the default is
+false for all machine-specific attributes.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_COMP_TYPE_ATTRIBUTES (const_tree @var{type1}, const_tree @var{type2})
+If defined, this target hook is a function which returns zero if the attributes on
+@var{type1} and @var{type2} are incompatible, one if they are compatible,
+and two if they are nearly compatible (which causes a warning to be
+generated).  If this is not defined, machine-specific attributes are
+supposed always to be compatible.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SET_DEFAULT_TYPE_ATTRIBUTES (tree @var{type})
+If defined, this target hook is a function which assigns default attributes to
+the newly defined @var{type}.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_MERGE_TYPE_ATTRIBUTES (tree @var{type1}, tree @var{type2})
+Define this target hook if the merging of type attributes needs special
+handling.  If defined, the result is a list of the combined
+@code{TYPE_ATTRIBUTES} of @var{type1} and @var{type2}.  It is assumed
+that @code{comptypes} has already been called and returned 1.  This
+function may call @code{merge_attributes} to handle machine-independent
+merging.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_MERGE_DECL_ATTRIBUTES (tree @var{olddecl}, tree @var{newdecl})
+Define this target hook if the merging of decl attributes needs special
+handling.  If defined, the result is a list of the combined
+@code{DECL_ATTRIBUTES} of @var{olddecl} and @var{newdecl}.
+@var{newdecl} is a duplicate declaration of @var{olddecl}.  Examples of
+when this is needed are when one attribute overrides another, or when an
+attribute is nullified by a subsequent definition.  This function may
+call @code{merge_attributes} to handle machine-independent merging.
+
+@findex TARGET_DLLIMPORT_DECL_ATTRIBUTES
+If the only target-specific handling you require is @samp{dllimport}
+for Microsoft Windows targets, you should define the macro
+@code{TARGET_DLLIMPORT_DECL_ATTRIBUTES} to @code{1}.  The compiler
+will then define a function called
+@code{merge_dllimport_decl_attributes} which can then be defined as
+the expansion of @code{TARGET_MERGE_DECL_ATTRIBUTES}.  You can also
+add @code{handle_dll_attribute} in the attribute table for your port
+to perform initial processing of the @samp{dllimport} and
+@samp{dllexport} attributes.  This is done in @file{i386/cygwin.h} and
+@file{i386/i386.c}, for example.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_VALID_DLLIMPORT_ATTRIBUTE_P (const_tree @var{decl})
+@var{decl} is a variable or function with @code{__attribute__((dllimport))} specified.  Use this hook if the target needs to add extra validation checks to @code{handle_dll_attribute}.
+@end deftypefn
+
+@defmac TARGET_DECLSPEC
+Define this macro to a nonzero value if you want to treat
+@code{__declspec(X)} as equivalent to @code{__attribute((X))}.  By
+default, this behavior is enabled only for targets that define
+@code{TARGET_DLLIMPORT_DECL_ATTRIBUTES}.  The current implementation
+of @code{__declspec} is via a built-in macro, but you should not rely
+on this implementation detail.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_INSERT_ATTRIBUTES (tree @var{node}, tree *@var{attr_ptr})
+Define this target hook if you want to be able to add attributes to a decl
+when it is being created.  This is normally useful for back ends which
+wish to implement a pragma by using the attributes which correspond to
+the pragma's effect.  The @var{node} argument is the decl which is being
+created.  The @var{attr_ptr} argument is a pointer to the attribute list
+for this decl.  The list itself should not be modified, since it may be
+shared with other decls, but attributes may be chained on the head of
+the list and @code{*@var{attr_ptr}} modified to point to the new
+attributes, or a copy of the list may be made if further changes are
+needed.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_HANDLE_GENERIC_ATTRIBUTE (tree *@var{node}, tree @var{name}, tree @var{args}, int @var{flags}, bool *@var{no_add_attrs})
+Define this target hook if you want to be able to perform additional
+target-specific processing of an attribute which is handled generically
+by a front end.  The arguments are the same as those which are passed to
+attribute handlers.  So far this only affects the @var{noinit} and
+@var{section} attribute.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_FUNCTION_ATTRIBUTE_INLINABLE_P (const_tree @var{fndecl})
+@cindex inlining
+This target hook returns @code{true} if it is OK to inline @var{fndecl}
+into the current function, despite its having target-specific
+attributes, @code{false} otherwise.  By default, if a function has a
+target specific attribute attached to it, it will not be inlined.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_OPTION_VALID_ATTRIBUTE_P (tree @var{fndecl}, tree @var{name}, tree @var{args}, int @var{flags})
+This hook is called to parse @code{attribute(target("..."))}, which
+allows setting target-specific options on individual functions.
+These function-specific options may differ
+from the options specified on the command line.  The hook should return
+@code{true} if the options are valid.
+
+The hook should set the @code{DECL_FUNCTION_SPECIFIC_TARGET} field in
+the function declaration to hold a pointer to a target-specific
+@code{struct cl_target_option} structure.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_OPTION_SAVE (struct cl_target_option *@var{ptr}, struct gcc_options *@var{opts}, struct gcc_options *@var{opts_set})
+This hook is called to save any additional target-specific information
+in the @code{struct cl_target_option} structure for function-specific
+options from the @code{struct gcc_options} structure.
+@xref{Option file format}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_OPTION_RESTORE (struct gcc_options *@var{opts}, struct gcc_options *@var{opts_set}, struct cl_target_option *@var{ptr})
+This hook is called to restore any additional target-specific
+information in the @code{struct cl_target_option} structure for
+function-specific options to the @code{struct gcc_options} structure.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_OPTION_POST_STREAM_IN (struct cl_target_option *@var{ptr})
+This hook is called to update target-specific information in the
+@code{struct cl_target_option} structure after it is streamed in from
+LTO bytecode.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_OPTION_PRINT (FILE *@var{file}, int @var{indent}, struct cl_target_option *@var{ptr})
+This hook is called to print any additional target-specific
+information in the @code{struct cl_target_option} structure for
+function-specific options.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_OPTION_PRAGMA_PARSE (tree @var{args}, tree @var{pop_target})
+This target hook parses the options for @code{#pragma GCC target}, which
+sets the target-specific options for functions that occur later in the
+input stream.  The options accepted should be the same as those handled by the
+@code{TARGET_OPTION_VALID_ATTRIBUTE_P} hook.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_OPTION_OVERRIDE (void)
+Sometimes certain combinations of command options do not make sense on
+a particular target machine.  You can override the hook
+@code{TARGET_OPTION_OVERRIDE} to take account of this.  This hooks is called
+once just after all the command options have been parsed.
+
+Don't use this hook to turn on various extra optimizations for
+@option{-O}.  That is what @code{TARGET_OPTION_OPTIMIZATION} is for.
+
+If you need to do something whenever the optimization level is
+changed via the optimize attribute or pragma, see
+@code{TARGET_OVERRIDE_OPTIONS_AFTER_CHANGE}
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_OPTION_FUNCTION_VERSIONS (tree @var{decl1}, tree @var{decl2})
+This target hook returns @code{true} if @var{DECL1} and @var{DECL2} are
+versions of the same function.  @var{DECL1} and @var{DECL2} are function
+versions if and only if they have the same function signature and
+different target specific attributes, that is, they are compiled for
+different target machines.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_CAN_INLINE_P (tree @var{caller}, tree @var{callee})
+This target hook returns @code{false} if the @var{caller} function
+cannot inline @var{callee}, based on target specific information.  By
+default, inlining is not allowed if the callee function has function
+specific target options and the caller does not use the same options.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_RELAYOUT_FUNCTION (tree @var{fndecl})
+This target hook fixes function @var{fndecl} after attributes are processed. Default does nothing. On ARM, the default function's alignment is updated with the attribute target.
+@end deftypefn
+
+@node Emulated TLS
+@section Emulating TLS
+@cindex Emulated TLS
+
+For targets whose psABI does not provide Thread Local Storage via
+specific relocations and instruction sequences, an emulation layer is
+used.  A set of target hooks allows this emulation layer to be
+configured for the requirements of a particular target.  For instance
+the psABI may in fact specify TLS support in terms of an emulation
+layer.
+
+The emulation layer works by creating a control object for every TLS
+object.  To access the TLS object, a lookup function is provided
+which, when given the address of the control object, will return the
+address of the current thread's instance of the TLS object.
+
+@deftypevr {Target Hook} {const char *} TARGET_EMUTLS_GET_ADDRESS
+Contains the name of the helper function that uses a TLS control
+object to locate a TLS instance.  The default causes libgcc's
+emulated TLS helper function to be used.
+@end deftypevr
+
+@deftypevr {Target Hook} {const char *} TARGET_EMUTLS_REGISTER_COMMON
+Contains the name of the helper function that should be used at
+program startup to register TLS objects that are implicitly
+initialized to zero.  If this is @code{NULL}, all TLS objects will
+have explicit initializers.  The default causes libgcc's emulated TLS
+registration function to be used.
+@end deftypevr
+
+@deftypevr {Target Hook} {const char *} TARGET_EMUTLS_VAR_SECTION
+Contains the name of the section in which TLS control variables should
+be placed.  The default of @code{NULL} allows these to be placed in
+any section.
+@end deftypevr
+
+@deftypevr {Target Hook} {const char *} TARGET_EMUTLS_TMPL_SECTION
+Contains the name of the section in which TLS initializers should be
+placed.  The default of @code{NULL} allows these to be placed in any
+section.
+@end deftypevr
+
+@deftypevr {Target Hook} {const char *} TARGET_EMUTLS_VAR_PREFIX
+Contains the prefix to be prepended to TLS control variable names.
+The default of @code{NULL} uses a target-specific prefix.
+@end deftypevr
+
+@deftypevr {Target Hook} {const char *} TARGET_EMUTLS_TMPL_PREFIX
+Contains the prefix to be prepended to TLS initializer objects.  The
+default of @code{NULL} uses a target-specific prefix.
+@end deftypevr
+
+@deftypefn {Target Hook} tree TARGET_EMUTLS_VAR_FIELDS (tree @var{type}, tree *@var{name})
+Specifies a function that generates the FIELD_DECLs for a TLS control
+object type.  @var{type} is the RECORD_TYPE the fields are for and
+@var{name} should be filled with the structure tag, if the default of
+@code{__emutls_object} is unsuitable.  The default creates a type suitable
+for libgcc's emulated TLS function.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_EMUTLS_VAR_INIT (tree @var{var}, tree @var{decl}, tree @var{tmpl_addr})
+Specifies a function that generates the CONSTRUCTOR to initialize a
+TLS control object.  @var{var} is the TLS control object, @var{decl}
+is the TLS object and @var{tmpl_addr} is the address of the
+initializer.  The default initializes libgcc's emulated TLS control object.
+@end deftypefn
+
+@deftypevr {Target Hook} bool TARGET_EMUTLS_VAR_ALIGN_FIXED
+Specifies whether the alignment of TLS control variable objects is
+fixed and should not be increased as some backends may do to optimize
+single objects.  The default is false.
+@end deftypevr
+
+@deftypevr {Target Hook} bool TARGET_EMUTLS_DEBUG_FORM_TLS_ADDRESS
+Specifies whether a DWARF @code{DW_OP_form_tls_address} location descriptor
+may be used to describe emulated TLS control objects.
+@end deftypevr
+
+@node MIPS Coprocessors
+@section Defining coprocessor specifics for MIPS targets.
+@cindex MIPS coprocessor-definition macros
+
+The MIPS specification allows MIPS implementations to have as many as 4
+coprocessors, each with as many as 32 private registers.  GCC supports
+accessing these registers and transferring values between the registers
+and memory using asm-ized variables.  For example:
+
+@smallexample
+  register unsigned int cp0count asm ("c0r1");
+  unsigned int d;
+
+  d = cp0count + 3;
+@end smallexample
+
+(``c0r1'' is the default name of register 1 in coprocessor 0; alternate
+names may be added as described below, or the default names may be
+overridden entirely in @code{SUBTARGET_CONDITIONAL_REGISTER_USAGE}.)
+
+Coprocessor registers are assumed to be epilogue-used; sets to them will
+be preserved even if it does not appear that the register is used again
+later in the function.
+
+Another note: according to the MIPS spec, coprocessor 1 (if present) is
+the FPU@.  One accesses COP1 registers through standard mips
+floating-point support; they are not included in this mechanism.
+
+@node PCH Target
+@section Parameters for Precompiled Header Validity Checking
+@cindex parameters, precompiled headers
+
+@deftypefn {Target Hook} {void *} TARGET_GET_PCH_VALIDITY (size_t *@var{sz})
+This hook returns a pointer to the data needed by
+@code{TARGET_PCH_VALID_P} and sets
+@samp{*@var{sz}} to the size of the data in bytes.
+@end deftypefn
+
+@deftypefn {Target Hook} {const char *} TARGET_PCH_VALID_P (const void *@var{data}, size_t @var{sz})
+This hook checks whether the options used to create a PCH file are
+compatible with the current settings.  It returns @code{NULL}
+if so and a suitable error message if not.  Error messages will
+be presented to the user and must be localized using @samp{_(@var{msg})}.
+
+@var{data} is the data that was returned by @code{TARGET_GET_PCH_VALIDITY}
+when the PCH file was created and @var{sz} is the size of that data in bytes.
+It's safe to assume that the data was created by the same version of the
+compiler, so no format checking is needed.
+
+The default definition of @code{default_pch_valid_p} should be
+suitable for most targets.
+@end deftypefn
+
+@deftypefn {Target Hook} {const char *} TARGET_CHECK_PCH_TARGET_FLAGS (int @var{pch_flags})
+If this hook is nonnull, the default implementation of
+@code{TARGET_PCH_VALID_P} will use it to check for compatible values
+of @code{target_flags}.  @var{pch_flags} specifies the value that
+@code{target_flags} had when the PCH file was created.  The return
+value is the same as for @code{TARGET_PCH_VALID_P}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_PREPARE_PCH_SAVE (void)
+Called before writing out a PCH file.  If the target has some
+garbage-collected data that needs to be in a particular state on PCH loads,
+it can use this hook to enforce that state.  Very few targets need
+to do anything here.
+@end deftypefn
+
+@node C++ ABI
+@section C++ ABI parameters
+@cindex parameters, c++ abi
+
+@deftypefn {Target Hook} tree TARGET_CXX_GUARD_TYPE (void)
+Define this hook to override the integer type used for guard variables.
+These are used to implement one-time construction of static objects.  The
+default is long_long_integer_type_node.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_CXX_GUARD_MASK_BIT (void)
+This hook determines how guard variables are used.  It should return
+@code{false} (the default) if the first byte should be used.  A return value of
+@code{true} indicates that only the least significant bit should be used.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_CXX_GET_COOKIE_SIZE (tree @var{type})
+This hook returns the size of the cookie to use when allocating an array
+whose elements have the indicated @var{type}.  Assumes that it is already
+known that a cookie is needed.  The default is
+@code{max(sizeof (size_t), alignof(type))}, as defined in section 2.7 of the
+IA64/Generic C++ ABI@.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_CXX_COOKIE_HAS_SIZE (void)
+This hook should return @code{true} if the element size should be stored in
+array cookies.  The default is to return @code{false}.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_CXX_IMPORT_EXPORT_CLASS (tree @var{type}, int @var{import_export})
+If defined by a backend this hook allows the decision made to export
+class @var{type} to be overruled.  Upon entry @var{import_export}
+will contain 1 if the class is going to be exported, @minus{}1 if it is going
+to be imported and 0 otherwise.  This function should return the
+modified value and perform any other actions necessary to support the
+backend's targeted operating system.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_CXX_CDTOR_RETURNS_THIS (void)
+This hook should return @code{true} if constructors and destructors return
+the address of the object created/destroyed.  The default is to return
+@code{false}.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_CXX_KEY_METHOD_MAY_BE_INLINE (void)
+This hook returns true if the key method for a class (i.e., the method
+which, if defined in the current translation unit, causes the virtual
+table to be emitted) may be an inline function.  Under the standard
+Itanium C++ ABI the key method may be an inline function so long as
+the function is not declared inline in the class definition.  Under
+some variants of the ABI, an inline function can never be the key
+method.  The default is to return @code{true}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_CXX_DETERMINE_CLASS_DATA_VISIBILITY (tree @var{decl})
+@var{decl} is a virtual table, virtual table table, typeinfo object, or other similar implicit class data object that will be emitted with external linkage in this translation unit.  No ELF visibility has been explicitly specified.  If the target needs to specify a visibility other than that of the containing class, use this hook to set @code{DECL_VISIBILITY} and @code{DECL_VISIBILITY_SPECIFIED}.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_CXX_CLASS_DATA_ALWAYS_COMDAT (void)
+This hook returns true (the default) if virtual tables and other
+similar implicit class data objects are always COMDAT if they have
+external linkage.  If this hook returns false, then class data for
+classes whose virtual table will be emitted in only one translation
+unit will not be COMDAT.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_CXX_LIBRARY_RTTI_COMDAT (void)
+This hook returns true (the default) if the RTTI information for
+the basic types which is defined in the C++ runtime should always
+be COMDAT, false if it should not be COMDAT.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_CXX_USE_AEABI_ATEXIT (void)
+This hook returns true if @code{__aeabi_atexit} (as defined by the ARM EABI)
+should be used to register static destructors when @option{-fuse-cxa-atexit}
+is in effect.  The default is to return false to use @code{__cxa_atexit}.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_CXX_USE_ATEXIT_FOR_CXA_ATEXIT (void)
+This hook returns true if the target @code{atexit} function can be used
+in the same manner as @code{__cxa_atexit} to register C++ static
+destructors. This requires that @code{atexit}-registered functions in
+shared libraries are run in the correct order when the libraries are
+unloaded. The default is to return false.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_CXX_ADJUST_CLASS_AT_DEFINITION (tree @var{type})
+@var{type} is a C++ class (i.e., RECORD_TYPE or UNION_TYPE) that has just been defined.  Use this hook to make adjustments to the class (eg, tweak visibility or perform any other required target modifications).
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_CXX_DECL_MANGLING_CONTEXT (const_tree @var{decl})
+Return target-specific mangling context of @var{decl} or @code{NULL_TREE}.
+@end deftypefn
+
+@node D Language and ABI
+@section D ABI parameters
+@cindex parameters, d abi
+
+@deftypefn {D Target Hook} void TARGET_D_CPU_VERSIONS (void)
+Declare all environmental version identifiers relating to the target CPU
+using the function @code{builtin_version}, which takes a string representing
+the name of the version.  Version identifiers predefined by this hook apply
+to all modules that are being compiled and imported.
+@end deftypefn
+
+@deftypefn {D Target Hook} void TARGET_D_OS_VERSIONS (void)
+Similarly to @code{TARGET_D_CPU_VERSIONS}, but is used for versions
+relating to the target operating system.
+@end deftypefn
+
+@deftypefn {D Target Hook} void TARGET_D_REGISTER_CPU_TARGET_INFO (void)
+Register all target information keys relating to the target CPU using the
+function @code{d_add_target_info_handlers}, which takes a
+@samp{struct d_target_info_spec} (defined in @file{d/d-target.h}).  The keys
+added by this hook are made available at compile time by the
+@code{__traits(getTargetInfo)} extension, the result is an expression
+describing the requested target information.
+@end deftypefn
+
+@deftypefn {D Target Hook} void TARGET_D_REGISTER_OS_TARGET_INFO (void)
+Same as @code{TARGET_D_CPU_TARGET_INFO}, but is used for keys relating to
+the target operating system.
+@end deftypefn
+
+@deftypevr {D Target Hook} {const char *} TARGET_D_MINFO_SECTION
+Contains the name of the section in which module info references should be
+placed.  This section is expected to be bracketed by two symbols to indicate
+the start and end address of the section, so that the runtime library can
+collect all modules for each loaded shared library and executable.  The
+default value of @code{NULL} disables the use of sections altogether.
+@end deftypevr
+
+@deftypevr {D Target Hook} {const char *} TARGET_D_MINFO_START_NAME
+If @code{TARGET_D_MINFO_SECTION} is defined, then this must also be defined
+as the name of the symbol indicating the start address of the module info
+section
+@end deftypevr
+
+@deftypevr {D Target Hook} {const char *} TARGET_D_MINFO_END_NAME
+If @code{TARGET_D_MINFO_SECTION} is defined, then this must also be defined
+as the name of the symbol indicating the end address of the module info
+section
+@end deftypevr
+
+@deftypefn {D Target Hook} bool TARGET_D_HAS_STDCALL_CONVENTION (unsigned int *@var{link_system}, unsigned int *@var{link_windows})
+Returns @code{true} if the target supports the stdcall calling convention.
+The hook should also set @var{link_system} to @code{1} if the @code{stdcall}
+attribute should be applied to functions with @code{extern(System)} linkage,
+and @var{link_windows} to @code{1} to apply @code{stdcall} to functions with
+@code{extern(Windows)} linkage.
+@end deftypefn
+
+@deftypevr {D Target Hook} bool TARGET_D_TEMPLATES_ALWAYS_COMDAT
+This flag is true if instantiated functions and variables are always COMDAT
+if they have external linkage.  If this flag is false, then instantiated
+decls will be emitted as weak symbols.  The default is @code{false}.
+@end deftypevr
+
+@node Named Address Spaces
+@section Adding support for named address spaces
+@cindex named address spaces
+
+The draft technical report of the ISO/IEC JTC1 S22 WG14 N1275
+standards committee, @cite{Programming Languages - C - Extensions to
+support embedded processors}, specifies a syntax for embedded
+processors to specify alternate address spaces.  You can configure a
+GCC port to support section 5.1 of the draft report to add support for
+address spaces other than the default address space.  These address
+spaces are new keywords that are similar to the @code{volatile} and
+@code{const} type attributes.
+
+Pointers to named address spaces can have a different size than
+pointers to the generic address space.
+
+For example, the SPU port uses the @code{__ea} address space to refer
+to memory in the host processor, rather than memory local to the SPU
+processor.  Access to memory in the @code{__ea} address space involves
+issuing DMA operations to move data between the host processor and the
+local processor memory address space.  Pointers in the @code{__ea}
+address space are either 32 bits or 64 bits based on the
+@option{-mea32} or @option{-mea64} switches (native SPU pointers are
+always 32 bits).
+
+Internally, address spaces are represented as a small integer in the
+range 0 to 15 with address space 0 being reserved for the generic
+address space.
+
+To register a named address space qualifier keyword with the C front end,
+the target may call the @code{c_register_addr_space} routine.  For example,
+the SPU port uses the following to declare @code{__ea} as the keyword for
+named address space #1:
+@smallexample
+#define ADDR_SPACE_EA 1
+c_register_addr_space ("__ea", ADDR_SPACE_EA);
+@end smallexample
+
+@deftypefn {Target Hook} scalar_int_mode TARGET_ADDR_SPACE_POINTER_MODE (addr_space_t @var{address_space})
+Define this to return the machine mode to use for pointers to
+@var{address_space} if the target supports named address spaces.
+The default version of this hook returns @code{ptr_mode}.
+@end deftypefn
+
+@deftypefn {Target Hook} scalar_int_mode TARGET_ADDR_SPACE_ADDRESS_MODE (addr_space_t @var{address_space})
+Define this to return the machine mode to use for addresses in
+@var{address_space} if the target supports named address spaces.
+The default version of this hook returns @code{Pmode}.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_ADDR_SPACE_VALID_POINTER_MODE (scalar_int_mode @var{mode}, addr_space_t @var{as})
+Define this to return nonzero if the port can handle pointers
+with machine mode @var{mode} to address space @var{as}.  This target
+hook is the same as the @code{TARGET_VALID_POINTER_MODE} target hook,
+except that it includes explicit named address space support.  The default
+version of this hook returns true for the modes returned by either the
+@code{TARGET_ADDR_SPACE_POINTER_MODE} or @code{TARGET_ADDR_SPACE_ADDRESS_MODE}
+target hooks for the given address space.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_ADDR_SPACE_LEGITIMATE_ADDRESS_P (machine_mode @var{mode}, rtx @var{exp}, bool @var{strict}, addr_space_t @var{as})
+Define this to return true if @var{exp} is a valid address for mode
+@var{mode} in the named address space @var{as}.  The @var{strict}
+parameter says whether strict addressing is in effect after reload has
+finished.  This target hook is the same as the
+@code{TARGET_LEGITIMATE_ADDRESS_P} target hook, except that it includes
+explicit named address space support.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_ADDR_SPACE_LEGITIMIZE_ADDRESS (rtx @var{x}, rtx @var{oldx}, machine_mode @var{mode}, addr_space_t @var{as})
+Define this to modify an invalid address @var{x} to be a valid address
+with mode @var{mode} in the named address space @var{as}.  This target
+hook is the same as the @code{TARGET_LEGITIMIZE_ADDRESS} target hook,
+except that it includes explicit named address space support.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_ADDR_SPACE_SUBSET_P (addr_space_t @var{subset}, addr_space_t @var{superset})
+Define this to return whether the @var{subset} named address space is
+contained within the @var{superset} named address space.  Pointers to
+a named address space that is a subset of another named address space
+will be converted automatically without a cast if used together in
+arithmetic operations.  Pointers to a superset address space can be
+converted to pointers to a subset address space via explicit casts.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_ADDR_SPACE_ZERO_ADDRESS_VALID (addr_space_t @var{as})
+Define this to modify the default handling of address 0 for the
+address space.  Return true if 0 should be considered a valid address.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_ADDR_SPACE_CONVERT (rtx @var{op}, tree @var{from_type}, tree @var{to_type})
+Define this to convert the pointer expression represented by the RTL
+@var{op} with type @var{from_type} that points to a named address
+space to a new pointer expression with type @var{to_type} that points
+to a different named address space.  When this hook it called, it is
+guaranteed that one of the two address spaces is a subset of the other,
+as determined by the @code{TARGET_ADDR_SPACE_SUBSET_P} target hook.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_ADDR_SPACE_DEBUG (addr_space_t @var{as})
+Define this to define how the address space is encoded in dwarf.
+The result is the value to be used with @code{DW_AT_address_class}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ADDR_SPACE_DIAGNOSE_USAGE (addr_space_t @var{as}, location_t @var{loc})
+Define this hook if the availability of an address space depends on
+command line options and some diagnostics should be printed when the
+address space is used.  This hook is called during parsing and allows
+to emit a better diagnostic compared to the case where the address space
+was not registered with @code{c_register_addr_space}.  @var{as} is
+the address space as registered with @code{c_register_addr_space}.
+@var{loc} is the location of the address space qualifier token.
+The default implementation does nothing.
+@end deftypefn
+
+@node Misc
+@section Miscellaneous Parameters
+@cindex parameters, miscellaneous
+
+@c prevent bad page break with this line
+Here are several miscellaneous parameters.
+
+@defmac HAS_LONG_COND_BRANCH
+Define this boolean macro to indicate whether or not your architecture
+has conditional branches that can span all of memory.  It is used in
+conjunction with an optimization that partitions hot and cold basic
+blocks into separate sections of the executable.  If this macro is
+set to false, gcc will convert any conditional branches that attempt
+to cross between sections into unconditional branches or indirect jumps.
+@end defmac
+
+@defmac HAS_LONG_UNCOND_BRANCH
+Define this boolean macro to indicate whether or not your architecture
+has unconditional branches that can span all of memory.  It is used in
+conjunction with an optimization that partitions hot and cold basic
+blocks into separate sections of the executable.  If this macro is
+set to false, gcc will convert any unconditional branches that attempt
+to cross between sections into indirect jumps.
+@end defmac
+
+@defmac CASE_VECTOR_MODE
+An alias for a machine mode name.  This is the machine mode that
+elements of a jump-table should have.
+@end defmac
+
+@defmac CASE_VECTOR_SHORTEN_MODE (@var{min_offset}, @var{max_offset}, @var{body})
+Optional: return the preferred mode for an @code{addr_diff_vec}
+when the minimum and maximum offset are known.  If you define this,
+it enables extra code in branch shortening to deal with @code{addr_diff_vec}.
+To make this work, you also have to define @code{INSN_ALIGN} and
+make the alignment for @code{addr_diff_vec} explicit.
+The @var{body} argument is provided so that the offset_unsigned and scale
+flags can be updated.
+@end defmac
+
+@defmac CASE_VECTOR_PC_RELATIVE
+Define this macro to be a C expression to indicate when jump-tables
+should contain relative addresses.  You need not define this macro if
+jump-tables never contain relative addresses, or jump-tables should
+contain relative addresses only when @option{-fPIC} or @option{-fPIC}
+is in effect.
+@end defmac
+
+@deftypefn {Target Hook} {unsigned int} TARGET_CASE_VALUES_THRESHOLD (void)
+This function return the smallest number of different values for which it
+is best to use a jump-table instead of a tree of conditional branches.
+The default is four for machines with a @code{casesi} instruction and
+five otherwise.  This is best for most machines.
+@end deftypefn
+
+@defmac WORD_REGISTER_OPERATIONS
+Define this macro to 1 if operations between registers with integral mode
+smaller than a word are always performed on the entire register.  To be
+more explicit, if you start with a pair of @code{word_mode} registers with
+known values and you do a subword, for example @code{QImode}, addition on
+the low part of the registers, then the compiler may consider that the
+result has a known value in @code{word_mode} too if the macro is defined
+to 1.  Most RISC machines have this property and most CISC machines do not.
+@end defmac
+
+@deftypefn {Target Hook} {unsigned int} TARGET_MIN_ARITHMETIC_PRECISION (void)
+On some RISC architectures with 64-bit registers, the processor also
+maintains 32-bit condition codes that make it possible to do real 32-bit
+arithmetic, although the operations are performed on the full registers.
+
+On such architectures, defining this hook to 32 tells the compiler to try
+using 32-bit arithmetical operations setting the condition codes instead
+of doing full 64-bit arithmetic.
+
+More generally, define this hook on RISC architectures if you want the
+compiler to try using arithmetical operations setting the condition codes
+with a precision lower than the word precision.
+
+You need not define this hook if @code{WORD_REGISTER_OPERATIONS} is not
+defined to 1.
+@end deftypefn
+
+@defmac LOAD_EXTEND_OP (@var{mem_mode})
+Define this macro to be a C expression indicating when insns that read
+memory in @var{mem_mode}, an integral mode narrower than a word, set the
+bits outside of @var{mem_mode} to be either the sign-extension or the
+zero-extension of the data read.  Return @code{SIGN_EXTEND} for values
+of @var{mem_mode} for which the
+insn sign-extends, @code{ZERO_EXTEND} for which it zero-extends, and
+@code{UNKNOWN} for other modes.
+
+This macro is not called with @var{mem_mode} non-integral or with a width
+greater than or equal to @code{BITS_PER_WORD}, so you may return any
+value in this case.  Do not define this macro if it would always return
+@code{UNKNOWN}.  On machines where this macro is defined, you will normally
+define it as the constant @code{SIGN_EXTEND} or @code{ZERO_EXTEND}.
+
+You may return a non-@code{UNKNOWN} value even if for some hard registers
+the sign extension is not performed, if for the @code{REGNO_REG_CLASS}
+of these hard registers @code{TARGET_CAN_CHANGE_MODE_CLASS} returns false
+when the @var{from} mode is @var{mem_mode} and the @var{to} mode is any
+integral mode larger than this but not larger than @code{word_mode}.
+
+You must return @code{UNKNOWN} if for some hard registers that allow this
+mode, @code{TARGET_CAN_CHANGE_MODE_CLASS} says that they cannot change to
+@code{word_mode}, but that they can change to another integral mode that
+is larger then @var{mem_mode} but still smaller than @code{word_mode}.
+@end defmac
+
+@defmac SHORT_IMMEDIATES_SIGN_EXTEND
+Define this macro to 1 if loading short immediate values into registers sign
+extends.
+@end defmac
+
+@deftypefn {Target Hook} {unsigned int} TARGET_MIN_DIVISIONS_FOR_RECIP_MUL (machine_mode @var{mode})
+When @option{-ffast-math} is in effect, GCC tries to optimize
+divisions by the same divisor, by turning them into multiplications by
+the reciprocal.  This target hook specifies the minimum number of divisions
+that should be there for GCC to perform the optimization for a variable
+of mode @var{mode}.  The default implementation returns 3 if the machine
+has an instruction for the division, and 2 if it does not.
+@end deftypefn
+
+@defmac MOVE_MAX
+The maximum number of bytes that a single instruction can move quickly
+between memory and registers or between two memory locations.
+@end defmac
+
+@defmac MAX_MOVE_MAX
+The maximum number of bytes that a single instruction can move quickly
+between memory and registers or between two memory locations.  If this
+is undefined, the default is @code{MOVE_MAX}.  Otherwise, it is the
+constant value that is the largest value that @code{MOVE_MAX} can have
+at run-time.
+@end defmac
+
+@defmac SHIFT_COUNT_TRUNCATED
+A C expression that is nonzero if on this machine the number of bits
+actually used for the count of a shift operation is equal to the number
+of bits needed to represent the size of the object being shifted.  When
+this macro is nonzero, the compiler will assume that it is safe to omit
+a sign-extend, zero-extend, and certain bitwise `and' instructions that
+truncates the count of a shift operation.  On machines that have
+instructions that act on bit-fields at variable positions, which may
+include `bit test' instructions, a nonzero @code{SHIFT_COUNT_TRUNCATED}
+also enables deletion of truncations of the values that serve as
+arguments to bit-field instructions.
+
+If both types of instructions truncate the count (for shifts) and
+position (for bit-field operations), or if no variable-position bit-field
+instructions exist, you should define this macro.
+
+However, on some machines, such as the 80386 and the 680x0, truncation
+only applies to shift operations and not the (real or pretended)
+bit-field operations.  Define @code{SHIFT_COUNT_TRUNCATED} to be zero on
+such machines.  Instead, add patterns to the @file{md} file that include
+the implied truncation of the shift instructions.
+
+You need not define this macro if it would always have the value of zero.
+@end defmac
+
+@anchor{TARGET_SHIFT_TRUNCATION_MASK}
+@deftypefn {Target Hook} {unsigned HOST_WIDE_INT} TARGET_SHIFT_TRUNCATION_MASK (machine_mode @var{mode})
+This function describes how the standard shift patterns for @var{mode}
+deal with shifts by negative amounts or by more than the width of the mode.
+@xref{shift patterns}.
+
+On many machines, the shift patterns will apply a mask @var{m} to the
+shift count, meaning that a fixed-width shift of @var{x} by @var{y} is
+equivalent to an arbitrary-width shift of @var{x} by @var{y & m}.  If
+this is true for mode @var{mode}, the function should return @var{m},
+otherwise it should return 0.  A return value of 0 indicates that no
+particular behavior is guaranteed.
+
+Note that, unlike @code{SHIFT_COUNT_TRUNCATED}, this function does
+@emph{not} apply to general shift rtxes; it applies only to instructions
+that are generated by the named shift patterns.
+
+The default implementation of this function returns
+@code{GET_MODE_BITSIZE (@var{mode}) - 1} if @code{SHIFT_COUNT_TRUNCATED}
+and 0 otherwise.  This definition is always safe, but if
+@code{SHIFT_COUNT_TRUNCATED} is false, and some shift patterns
+nevertheless truncate the shift count, you may get better code
+by overriding it.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_TRULY_NOOP_TRUNCATION (poly_uint64 @var{outprec}, poly_uint64 @var{inprec})
+This hook returns true if it is safe to ``convert'' a value of
+@var{inprec} bits to one of @var{outprec} bits (where @var{outprec} is
+smaller than @var{inprec}) by merely operating on it as if it had only
+@var{outprec} bits.  The default returns true unconditionally, which
+is correct for most machines.  When @code{TARGET_TRULY_NOOP_TRUNCATION}
+returns false, the machine description should provide a @code{trunc}
+optab to specify the RTL that performs the required truncation.
+
+If @code{TARGET_MODES_TIEABLE_P} returns false for a pair of modes,
+suboptimal code can result if this hook returns true for the corresponding
+mode sizes.  Making this hook return false in such cases may improve things.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_MODE_REP_EXTENDED (scalar_int_mode @var{mode}, scalar_int_mode @var{rep_mode})
+The representation of an integral mode can be such that the values
+are always extended to a wider integral mode.  Return
+@code{SIGN_EXTEND} if values of @var{mode} are represented in
+sign-extended form to @var{rep_mode}.  Return @code{UNKNOWN}
+otherwise.  (Currently, none of the targets use zero-extended
+representation this way so unlike @code{LOAD_EXTEND_OP},
+@code{TARGET_MODE_REP_EXTENDED} is expected to return either
+@code{SIGN_EXTEND} or @code{UNKNOWN}.  Also no target extends
+@var{mode} to @var{rep_mode} so that @var{rep_mode} is not the next
+widest integral mode and currently we take advantage of this fact.)
+
+Similarly to @code{LOAD_EXTEND_OP} you may return a non-@code{UNKNOWN}
+value even if the extension is not performed on certain hard registers
+as long as for the @code{REGNO_REG_CLASS} of these hard registers
+@code{TARGET_CAN_CHANGE_MODE_CLASS} returns false.
+
+Note that @code{TARGET_MODE_REP_EXTENDED} and @code{LOAD_EXTEND_OP}
+describe two related properties.  If you define
+@code{TARGET_MODE_REP_EXTENDED (mode, word_mode)} you probably also want
+to define @code{LOAD_EXTEND_OP (mode)} to return the same type of
+extension.
+
+In order to enforce the representation of @code{mode},
+@code{TARGET_TRULY_NOOP_TRUNCATION} should return false when truncating to
+@code{mode}.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_SETJMP_PRESERVES_NONVOLATILE_REGS_P (void)
+On some targets, it is assumed that the compiler will spill all pseudos
+  that are live across a call to @code{setjmp}, while other targets treat
+  @code{setjmp} calls as normal function calls.
+  
+  This hook returns false if @code{setjmp} calls do not preserve all
+  non-volatile registers so that gcc that must spill all pseudos that are
+  live across @code{setjmp} calls.  Define this to return true if the
+  target does not need to spill all pseudos live across @code{setjmp} calls.
+  The default implementation conservatively assumes all pseudos must be
+  spilled across @code{setjmp} calls.
+@end deftypefn
+
+@defmac STORE_FLAG_VALUE
+A C expression describing the value returned by a comparison operator
+with an integral mode and stored by a store-flag instruction
+(@samp{cstore@var{mode}4}) when the condition is true.  This description must
+apply to @emph{all} the @samp{cstore@var{mode}4} patterns and all the
+comparison operators whose results have a @code{MODE_INT} mode.
+
+A value of 1 or @minus{}1 means that the instruction implementing the
+comparison operator returns exactly 1 or @minus{}1 when the comparison is true
+and 0 when the comparison is false.  Otherwise, the value indicates
+which bits of the result are guaranteed to be 1 when the comparison is
+true.  This value is interpreted in the mode of the comparison
+operation, which is given by the mode of the first operand in the
+@samp{cstore@var{mode}4} pattern.  Either the low bit or the sign bit of
+@code{STORE_FLAG_VALUE} be on.  Presently, only those bits are used by
+the compiler.
+
+If @code{STORE_FLAG_VALUE} is neither 1 or @minus{}1, the compiler will
+generate code that depends only on the specified bits.  It can also
+replace comparison operators with equivalent operations if they cause
+the required bits to be set, even if the remaining bits are undefined.
+For example, on a machine whose comparison operators return an
+@code{SImode} value and where @code{STORE_FLAG_VALUE} is defined as
+@samp{0x80000000}, saying that just the sign bit is relevant, the
+expression
+
+@smallexample
+(ne:SI (and:SI @var{x} (const_int @var{power-of-2})) (const_int 0))
+@end smallexample
+
+@noindent
+can be converted to
+
+@smallexample
+(ashift:SI @var{x} (const_int @var{n}))
+@end smallexample
+
+@noindent
+where @var{n} is the appropriate shift count to move the bit being
+tested into the sign bit.
+
+There is no way to describe a machine that always sets the low-order bit
+for a true value, but does not guarantee the value of any other bits,
+but we do not know of any machine that has such an instruction.  If you
+are trying to port GCC to such a machine, include an instruction to
+perform a logical-and of the result with 1 in the pattern for the
+comparison operators and let us know at @email{gcc@@gcc.gnu.org}.
+
+Often, a machine will have multiple instructions that obtain a value
+from a comparison (or the condition codes).  Here are rules to guide the
+choice of value for @code{STORE_FLAG_VALUE}, and hence the instructions
+to be used:
+
+@itemize @bullet
+@item
+Use the shortest sequence that yields a valid definition for
+@code{STORE_FLAG_VALUE}.  It is more efficient for the compiler to
+``normalize'' the value (convert it to, e.g., 1 or 0) than for the
+comparison operators to do so because there may be opportunities to
+combine the normalization with other operations.
+
+@item
+For equal-length sequences, use a value of 1 or @minus{}1, with @minus{}1 being
+slightly preferred on machines with expensive jumps and 1 preferred on
+other machines.
+
+@item
+As a second choice, choose a value of @samp{0x80000001} if instructions
+exist that set both the sign and low-order bits but do not define the
+others.
+
+@item
+Otherwise, use a value of @samp{0x80000000}.
+@end itemize
+
+Many machines can produce both the value chosen for
+@code{STORE_FLAG_VALUE} and its negation in the same number of
+instructions.  On those machines, you should also define a pattern for
+those cases, e.g., one matching
+
+@smallexample
+(set @var{A} (neg:@var{m} (ne:@var{m} @var{B} @var{C})))
+@end smallexample
+
+Some machines can also perform @code{and} or @code{plus} operations on
+condition code values with less instructions than the corresponding
+@samp{cstore@var{mode}4} insn followed by @code{and} or @code{plus}.  On those
+machines, define the appropriate patterns.  Use the names @code{incscc}
+and @code{decscc}, respectively, for the patterns which perform
+@code{plus} or @code{minus} operations on condition code values.  See
+@file{rs6000.md} for some examples.  The GNU Superoptimizer can be used to
+find such instruction sequences on other machines.
+
+If this macro is not defined, the default value, 1, is used.  You need
+not define @code{STORE_FLAG_VALUE} if the machine has no store-flag
+instructions, or if the value generated by these instructions is 1.
+@end defmac
+
+@defmac FLOAT_STORE_FLAG_VALUE (@var{mode})
+A C expression that gives a nonzero @code{REAL_VALUE_TYPE} value that is
+returned when comparison operators with floating-point results are true.
+Define this macro on machines that have comparison operations that return
+floating-point values.  If there are no such operations, do not define
+this macro.
+@end defmac
+
+@defmac VECTOR_STORE_FLAG_VALUE (@var{mode})
+A C expression that gives an rtx representing the nonzero true element
+for vector comparisons.  The returned rtx should be valid for the inner
+mode of @var{mode} which is guaranteed to be a vector mode.  Define
+this macro on machines that have vector comparison operations that
+return a vector result.  If there are no such operations, do not define
+this macro.  Typically, this macro is defined as @code{const1_rtx} or
+@code{constm1_rtx}.  This macro may return @code{NULL_RTX} to prevent
+the compiler optimizing such vector comparison operations for the
+given mode.
+@end defmac
+
+@defmac CLZ_DEFINED_VALUE_AT_ZERO (@var{mode}, @var{value})
+@defmacx CTZ_DEFINED_VALUE_AT_ZERO (@var{mode}, @var{value})
+A C expression that indicates whether the architecture defines a value
+for @code{clz} or @code{ctz} with a zero operand.
+A result of @code{0} indicates the value is undefined.
+If the value is defined for only the RTL expression, the macro should
+evaluate to @code{1}; if the value applies also to the corresponding optab
+entry (which is normally the case if it expands directly into
+the corresponding RTL), then the macro should evaluate to @code{2}.
+In the cases where the value is defined, @var{value} should be set to
+this value.
+
+If this macro is not defined, the value of @code{clz} or
+@code{ctz} at zero is assumed to be undefined.
+
+This macro must be defined if the target's expansion for @code{ffs}
+relies on a particular value to get correct results.  Otherwise it
+is not necessary, though it may be used to optimize some corner cases, and
+to provide a default expansion for the @code{ffs} optab.
+
+Note that regardless of this macro the ``definedness'' of @code{clz}
+and @code{ctz} at zero do @emph{not} extend to the builtin functions
+visible to the user.  Thus one may be free to adjust the value at will
+to match the target expansion of these operations without fear of
+breaking the API@.
+@end defmac
+
+@defmac Pmode
+An alias for the machine mode for pointers.  On most machines, define
+this to be the integer mode corresponding to the width of a hardware
+pointer; @code{SImode} on 32-bit machine or @code{DImode} on 64-bit machines.
+On some machines you must define this to be one of the partial integer
+modes, such as @code{PSImode}.
+
+The width of @code{Pmode} must be at least as large as the value of
+@code{POINTER_SIZE}.  If it is not equal, you must define the macro
+@code{POINTERS_EXTEND_UNSIGNED} to specify how pointers are extended
+to @code{Pmode}.
+@end defmac
+
+@defmac FUNCTION_MODE
+An alias for the machine mode used for memory references to functions
+being called, in @code{call} RTL expressions.  On most CISC machines,
+where an instruction can begin at any byte address, this should be
+@code{QImode}.  On most RISC machines, where all instructions have fixed
+size and alignment, this should be a mode with the same size and alignment
+as the machine instruction words - typically @code{SImode} or @code{HImode}.
+@end defmac
+
+@defmac STDC_0_IN_SYSTEM_HEADERS
+In normal operation, the preprocessor expands @code{__STDC__} to the
+constant 1, to signify that GCC conforms to ISO Standard C@.  On some
+hosts, like Solaris, the system compiler uses a different convention,
+where @code{__STDC__} is normally 0, but is 1 if the user specifies
+strict conformance to the C Standard.
+
+Defining @code{STDC_0_IN_SYSTEM_HEADERS} makes GNU CPP follows the host
+convention when processing system header files, but when processing user
+files @code{__STDC__} will always expand to 1.
+@end defmac
+
+@deftypefn {C Target Hook} {const char *} TARGET_C_PREINCLUDE (void)
+Define this hook to return the name of a header file to be included at the start of all compilations, as if it had been included with @code{#include <@var{file}>}.  If this hook returns @code{NULL}, or is not defined, or the header is not found, or if the user specifies @option{-ffreestanding} or @option{-nostdinc}, no header is included.
+
+ This hook can be used together with a header provided by the system C library to implement ISO C requirements for certain macros to be predefined that describe properties of the whole implementation rather than just the compiler.
+@end deftypefn
+
+@deftypefn {C Target Hook} bool TARGET_CXX_IMPLICIT_EXTERN_C (const char*@var{})
+Define this hook to add target-specific C++ implicit extern C functions. If this function returns true for the name of a file-scope function, that function implicitly gets extern "C" linkage rather than whatever language linkage the declaration would normally have.  An example of such function is WinMain on Win32 targets.
+@end deftypefn
+
+@defmac SYSTEM_IMPLICIT_EXTERN_C
+Define this macro if the system header files do not support C++@.
+This macro handles system header files by pretending that system
+header files are enclosed in @samp{extern "C" @{@dots{}@}}.
+@end defmac
+
+@findex #pragma
+@findex pragma
+@defmac REGISTER_TARGET_PRAGMAS ()
+Define this macro if you want to implement any target-specific pragmas.
+If defined, it is a C expression which makes a series of calls to
+@code{c_register_pragma} or @code{c_register_pragma_with_expansion}
+for each pragma.  The macro may also do any
+setup required for the pragmas.
+
+The primary reason to define this macro is to provide compatibility with
+other compilers for the same target.  In general, we discourage
+definition of target-specific pragmas for GCC@.
+
+If the pragma can be implemented by attributes then you should consider
+defining the target hook @samp{TARGET_INSERT_ATTRIBUTES} as well.
+
+Preprocessor macros that appear on pragma lines are not expanded.  All
+@samp{#pragma} directives that do not match any registered pragma are
+silently ignored, unless the user specifies @option{-Wunknown-pragmas}.
+@end defmac
+
+@deftypefun void c_register_pragma (const char *@var{space}, const char *@var{name}, void (*@var{callback}) (struct cpp_reader *))
+@deftypefunx void c_register_pragma_with_expansion (const char *@var{space}, const char *@var{name}, void (*@var{callback}) (struct cpp_reader *))
+
+Each call to @code{c_register_pragma} or
+@code{c_register_pragma_with_expansion} establishes one pragma.  The
+@var{callback} routine will be called when the preprocessor encounters a
+pragma of the form
+
+@smallexample
+#pragma [@var{space}] @var{name} @dots{}
+@end smallexample
+
+@var{space} is the case-sensitive namespace of the pragma, or
+@code{NULL} to put the pragma in the global namespace.  The callback
+routine receives @var{pfile} as its first argument, which can be passed
+on to cpplib's functions if necessary.  You can lex tokens after the
+@var{name} by calling @code{pragma_lex}.  Tokens that are not read by the
+callback will be silently ignored.  The end of the line is indicated by
+a token of type @code{CPP_EOF}.  Macro expansion occurs on the
+arguments of pragmas registered with
+@code{c_register_pragma_with_expansion} but not on the arguments of
+pragmas registered with @code{c_register_pragma}.
+
+Note that the use of @code{pragma_lex} is specific to the C and C++
+compilers.  It will not work in the Java or Fortran compilers, or any
+other language compilers for that matter.  Thus if @code{pragma_lex} is going
+to be called from target-specific code, it must only be done so when
+building the C and C++ compilers.  This can be done by defining the
+variables @code{c_target_objs} and @code{cxx_target_objs} in the
+target entry in the @file{config.gcc} file.  These variables should name
+the target-specific, language-specific object file which contains the
+code that uses @code{pragma_lex}.  Note it will also be necessary to add a
+rule to the makefile fragment pointed to by @code{tmake_file} that shows
+how to build this object file.
+@end deftypefun
+
+@defmac HANDLE_PRAGMA_PACK_WITH_EXPANSION
+Define this macro if macros should be expanded in the
+arguments of @samp{#pragma pack}.
+@end defmac
+
+@defmac TARGET_DEFAULT_PACK_STRUCT
+If your target requires a structure packing default other than 0 (meaning
+the machine default), define this macro to the necessary value (in bytes).
+This must be a value that would also be valid to use with
+@samp{#pragma pack()} (that is, a small power of two).
+@end defmac
+
+@defmac DOLLARS_IN_IDENTIFIERS
+Define this macro to control use of the character @samp{$} in
+identifier names for the C family of languages.  0 means @samp{$} is
+not allowed by default; 1 means it is allowed.  1 is the default;
+there is no need to define this macro in that case.
+@end defmac
+
+@defmac INSN_SETS_ARE_DELAYED (@var{insn})
+Define this macro as a C expression that is nonzero if it is safe for the
+delay slot scheduler to place instructions in the delay slot of @var{insn},
+even if they appear to use a resource set or clobbered in @var{insn}.
+@var{insn} is always a @code{jump_insn} or an @code{insn}; GCC knows that
+every @code{call_insn} has this behavior.  On machines where some @code{insn}
+or @code{jump_insn} is really a function call and hence has this behavior,
+you should define this macro.
+
+You need not define this macro if it would always return zero.
+@end defmac
+
+@defmac INSN_REFERENCES_ARE_DELAYED (@var{insn})
+Define this macro as a C expression that is nonzero if it is safe for the
+delay slot scheduler to place instructions in the delay slot of @var{insn},
+even if they appear to set or clobber a resource referenced in @var{insn}.
+@var{insn} is always a @code{jump_insn} or an @code{insn}.  On machines where
+some @code{insn} or @code{jump_insn} is really a function call and its operands
+are registers whose use is actually in the subroutine it calls, you should
+define this macro.  Doing so allows the delay slot scheduler to move
+instructions which copy arguments into the argument registers into the delay
+slot of @var{insn}.
+
+You need not define this macro if it would always return zero.
+@end defmac
+
+@defmac MULTIPLE_SYMBOL_SPACES
+Define this macro as a C expression that is nonzero if, in some cases,
+global symbols from one translation unit may not be bound to undefined
+symbols in another translation unit without user intervention.  For
+instance, under Microsoft Windows symbols must be explicitly imported
+from shared libraries (DLLs).
+
+You need not define this macro if it would always evaluate to zero.
+@end defmac
+
+@deftypefn {Target Hook} {rtx_insn *} TARGET_MD_ASM_ADJUST (vec<rtx>& @var{outputs}, vec<rtx>& @var{inputs}, vec<machine_mode>& @var{input_modes}, vec<const char *>& @var{constraints}, vec<rtx>& @var{clobbers}, HARD_REG_SET& @var{clobbered_regs})
+This target hook may add @dfn{clobbers} to @var{clobbers} and
+@var{clobbered_regs} for any hard regs the port wishes to automatically
+clobber for an asm.  The @var{outputs} and @var{inputs} may be inspected
+to avoid clobbering a register that is already used by the asm.
+
+It may modify the @var{outputs}, @var{inputs}, @var{input_modes}, and
+@var{constraints} as necessary for other pre-processing.  In this case the
+return value is a sequence of insns to emit after the asm.  Note that
+changes to @var{inputs} must be accompanied by the corresponding changes
+to @var{input_modes}.
+@end deftypefn
+
+@defmac MATH_LIBRARY
+Define this macro as a C string constant for the linker argument to link
+in the system math library, minus the initial @samp{"-l"}, or
+@samp{""} if the target does not have a
+separate math library.
+
+You need only define this macro if the default of @samp{"m"} is wrong.
+@end defmac
+
+@defmac LIBRARY_PATH_ENV
+Define this macro as a C string constant for the environment variable that
+specifies where the linker should look for libraries.
+
+You need only define this macro if the default of @samp{"LIBRARY_PATH"}
+is wrong.
+@end defmac
+
+@defmac TARGET_POSIX_IO
+Define this macro if the target supports the following POSIX@ file
+functions, access, mkdir and  file locking with fcntl / F_SETLKW@.
+Defining @code{TARGET_POSIX_IO} will enable the test coverage code
+to use file locking when exiting a program, which avoids race conditions
+if the program has forked. It will also create directories at run-time
+for cross-profiling.
+@end defmac
+
+@defmac MAX_CONDITIONAL_EXECUTE
+
+A C expression for the maximum number of instructions to execute via
+conditional execution instructions instead of a branch.  A value of
+@code{BRANCH_COST}+1 is the default if the machine does not use cc0, and
+1 if it does use cc0.
+@end defmac
+
+@defmac IFCVT_MODIFY_TESTS (@var{ce_info}, @var{true_expr}, @var{false_expr})
+Used if the target needs to perform machine-dependent modifications on the
+conditionals used for turning basic blocks into conditionally executed code.
+@var{ce_info} points to a data structure, @code{struct ce_if_block}, which
+contains information about the currently processed blocks.  @var{true_expr}
+and @var{false_expr} are the tests that are used for converting the
+then-block and the else-block, respectively.  Set either @var{true_expr} or
+@var{false_expr} to a null pointer if the tests cannot be converted.
+@end defmac
+
+@defmac IFCVT_MODIFY_MULTIPLE_TESTS (@var{ce_info}, @var{bb}, @var{true_expr}, @var{false_expr})
+Like @code{IFCVT_MODIFY_TESTS}, but used when converting more complicated
+if-statements into conditions combined by @code{and} and @code{or} operations.
+@var{bb} contains the basic block that contains the test that is currently
+being processed and about to be turned into a condition.
+@end defmac
+
+@defmac IFCVT_MODIFY_INSN (@var{ce_info}, @var{pattern}, @var{insn})
+A C expression to modify the @var{PATTERN} of an @var{INSN} that is to
+be converted to conditional execution format.  @var{ce_info} points to
+a data structure, @code{struct ce_if_block}, which contains information
+about the currently processed blocks.
+@end defmac
+
+@defmac IFCVT_MODIFY_FINAL (@var{ce_info})
+A C expression to perform any final machine dependent modifications in
+converting code to conditional execution.  The involved basic blocks
+can be found in the @code{struct ce_if_block} structure that is pointed
+to by @var{ce_info}.
+@end defmac
+
+@defmac IFCVT_MODIFY_CANCEL (@var{ce_info})
+A C expression to cancel any machine dependent modifications in
+converting code to conditional execution.  The involved basic blocks
+can be found in the @code{struct ce_if_block} structure that is pointed
+to by @var{ce_info}.
+@end defmac
+
+@defmac IFCVT_MACHDEP_INIT (@var{ce_info})
+A C expression to initialize any machine specific data for if-conversion
+of the if-block in the @code{struct ce_if_block} structure that is pointed
+to by @var{ce_info}.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_MACHINE_DEPENDENT_REORG (void)
+If non-null, this hook performs a target-specific pass over the
+instruction stream.  The compiler will run it at all optimization levels,
+just before the point at which it normally does delayed-branch scheduling.
+
+The exact purpose of the hook varies from target to target.  Some use
+it to do transformations that are necessary for correctness, such as
+laying out in-function constant pools or avoiding hardware hazards.
+Others use it as an opportunity to do some machine-dependent optimizations.
+
+You need not implement the hook if it has nothing to do.  The default
+definition is null.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_INIT_BUILTINS (void)
+Define this hook if you have any machine-specific built-in functions
+that need to be defined.  It should be a function that performs the
+necessary setup.
+
+Machine specific built-in functions can be useful to expand special machine
+instructions that would otherwise not normally be generated because
+they have no equivalent in the source language (for example, SIMD vector
+instructions or prefetch instructions).
+
+To create a built-in function, call the function
+@code{lang_hooks.builtin_function}
+which is defined by the language front end.  You can use any type nodes set
+up by @code{build_common_tree_nodes};
+only language front ends that use those two functions will call
+@samp{TARGET_INIT_BUILTINS}.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_BUILTIN_DECL (unsigned @var{code}, bool @var{initialize_p})
+Define this hook if you have any machine-specific built-in functions
+that need to be defined.  It should be a function that returns the
+builtin function declaration for the builtin function code @var{code}.
+If there is no such builtin and it cannot be initialized at this time
+if @var{initialize_p} is true the function should return @code{NULL_TREE}.
+If @var{code} is out of range the function should return
+@code{error_mark_node}.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_EXPAND_BUILTIN (tree @var{exp}, rtx @var{target}, rtx @var{subtarget}, machine_mode @var{mode}, int @var{ignore})
+
+Expand a call to a machine specific built-in function that was set up by
+@samp{TARGET_INIT_BUILTINS}.  @var{exp} is the expression for the
+function call; the result should go to @var{target} if that is
+convenient, and have mode @var{mode} if that is convenient.
+@var{subtarget} may be used as the target for computing one of
+@var{exp}'s operands.  @var{ignore} is nonzero if the value is to be
+ignored.  This function should return the result of the call to the
+built-in function.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_RESOLVE_OVERLOADED_BUILTIN (unsigned int @var{loc}, tree @var{fndecl}, void *@var{arglist})
+Select a replacement for a machine specific built-in function that
+was set up by @samp{TARGET_INIT_BUILTINS}.  This is done
+@emph{before} regular type checking, and so allows the target to
+implement a crude form of function overloading.  @var{fndecl} is the
+declaration of the built-in function.  @var{arglist} is the list of
+arguments passed to the built-in function.  The result is a
+complete expression that implements the operation, usually
+another @code{CALL_EXPR}.
+@var{arglist} really has type @samp{VEC(tree,gc)*}
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_CHECK_BUILTIN_CALL (location_t @var{loc}, vec<location_t> @var{arg_loc}, tree @var{fndecl}, tree @var{orig_fndecl}, unsigned int @var{nargs}, tree *@var{args})
+Perform semantic checking on a call to a machine-specific built-in
+function after its arguments have been constrained to the function
+signature.  Return true if the call is valid, otherwise report an error
+and return false.
+
+This hook is called after @code{TARGET_RESOLVE_OVERLOADED_BUILTIN}.
+The call was originally to built-in function @var{orig_fndecl},
+but after the optional @code{TARGET_RESOLVE_OVERLOADED_BUILTIN}
+step is now to built-in function @var{fndecl}.  @var{loc} is the
+location of the call and @var{args} is an array of function arguments,
+of which there are @var{nargs}.  @var{arg_loc} specifies the location
+of each argument.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_FOLD_BUILTIN (tree @var{fndecl}, int @var{n_args}, tree *@var{argp}, bool @var{ignore})
+Fold a call to a machine specific built-in function that was set up by
+@samp{TARGET_INIT_BUILTINS}.  @var{fndecl} is the declaration of the
+built-in function.  @var{n_args} is the number of arguments passed to
+the function; the arguments themselves are pointed to by @var{argp}.
+The result is another tree, valid for both GIMPLE and GENERIC,
+containing a simplified expression for the call's result.  If
+@var{ignore} is true the value will be ignored.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_GIMPLE_FOLD_BUILTIN (gimple_stmt_iterator *@var{gsi})
+Fold a call to a machine specific built-in function that was set up
+by @samp{TARGET_INIT_BUILTINS}.  @var{gsi} points to the gimple
+statement holding the function call.  Returns true if any change
+was made to the GIMPLE stream.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_COMPARE_VERSION_PRIORITY (tree @var{decl1}, tree @var{decl2})
+This hook is used to compare the target attributes in two functions to
+determine which function's features get higher priority.  This is used
+during function multi-versioning to figure out the order in which two
+versions must be dispatched.  A function version with a higher priority
+is checked for dispatching earlier.  @var{decl1} and @var{decl2} are
+ the two function decls that will be compared.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_GET_FUNCTION_VERSIONS_DISPATCHER (void *@var{decl})
+This hook is used to get the dispatcher function for a set of function
+versions.  The dispatcher function is called to invoke the right function
+version at run-time. @var{decl} is one version from a set of semantically
+identical versions.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_GENERATE_VERSION_DISPATCHER_BODY (void *@var{arg})
+This hook is used to generate the dispatcher logic to invoke the right
+function version at run-time for a given set of function versions.
+@var{arg} points to the callgraph node of the dispatcher function whose
+body must be generated.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_PREDICT_DOLOOP_P (class loop *@var{loop})
+Return true if we can predict it is possible to use a low-overhead loop
+for a particular loop.  The parameter @var{loop} is a pointer to the loop.
+This target hook is required only when the target supports low-overhead
+loops, and will help ivopts to make some decisions.
+The default version of this hook returns false.
+@end deftypefn
+
+@deftypevr {Target Hook} bool TARGET_HAVE_COUNT_REG_DECR_P
+Return true if the target supports hardware count register for decrement
+and branch.
+The default value is false.
+@end deftypevr
+
+@deftypevr {Target Hook} int64_t TARGET_DOLOOP_COST_FOR_GENERIC
+One IV candidate dedicated for doloop is introduced in IVOPTs, we can
+calculate the computation cost of adopting it to any generic IV use by
+function get_computation_cost as before.  But for targets which have
+hardware count register support for decrement and branch, it may have to
+move IV value from hardware count register to general purpose register
+while doloop IV candidate is used for generic IV uses.  It probably takes
+expensive penalty.  This hook allows target owners to define the cost for
+this especially for generic IV uses.
+The default value is zero.
+@end deftypevr
+
+@deftypevr {Target Hook} int64_t TARGET_DOLOOP_COST_FOR_ADDRESS
+One IV candidate dedicated for doloop is introduced in IVOPTs, we can
+calculate the computation cost of adopting it to any address IV use by
+function get_computation_cost as before.  But for targets which have
+hardware count register support for decrement and branch, it may have to
+move IV value from hardware count register to general purpose register
+while doloop IV candidate is used for address IV uses.  It probably takes
+expensive penalty.  This hook allows target owners to define the cost for
+this escpecially for address IV uses.
+The default value is zero.
+@end deftypevr
+
+@deftypefn {Target Hook} bool TARGET_CAN_USE_DOLOOP_P (const widest_int @var{&iterations}, const widest_int @var{&iterations_max}, unsigned int @var{loop_depth}, bool @var{entered_at_top})
+Return true if it is possible to use low-overhead loops (@code{doloop_end}
+and @code{doloop_begin}) for a particular loop.  @var{iterations} gives the
+exact number of iterations, or 0 if not known.  @var{iterations_max} gives
+the maximum number of iterations, or 0 if not known.  @var{loop_depth} is
+the nesting depth of the loop, with 1 for innermost loops, 2 for loops that
+contain innermost loops, and so on.  @var{entered_at_top} is true if the
+loop is only entered from the top.
+
+This hook is only used if @code{doloop_end} is available.  The default
+implementation returns true.  You can use @code{can_use_doloop_if_innermost}
+if the loop must be the innermost, and if there are no other restrictions.
+@end deftypefn
+
+@deftypefn {Target Hook} {const char *} TARGET_INVALID_WITHIN_DOLOOP (const rtx_insn *@var{insn})
+
+Take an instruction in @var{insn} and return NULL if it is valid within a
+low-overhead loop, otherwise return a string explaining why doloop
+could not be applied.
+
+Many targets use special registers for low-overhead looping. For any
+instruction that clobbers these this function should return a string indicating
+the reason why the doloop could not be applied.
+By default, the RTL loop optimizer does not use a present doloop pattern for
+loops containing function calls or branch on table instructions.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_LEGITIMATE_COMBINED_INSN (rtx_insn *@var{insn})
+Take an instruction in @var{insn} and return @code{false} if the instruction is not appropriate as a combination of two or more instructions.  The default is to accept all instructions.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_CAN_FOLLOW_JUMP (const rtx_insn *@var{follower}, const rtx_insn *@var{followee})
+FOLLOWER and FOLLOWEE are JUMP_INSN instructions;  return true if FOLLOWER may be modified to follow FOLLOWEE;  false, if it can't.  For example, on some targets, certain kinds of branches can't be made to  follow through a hot/cold partitioning.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_COMMUTATIVE_P (const_rtx @var{x}, int @var{outer_code})
+This target hook returns @code{true} if @var{x} is considered to be commutative.
+Usually, this is just COMMUTATIVE_P (@var{x}), but the HP PA doesn't consider
+PLUS to be commutative inside a MEM@.  @var{outer_code} is the rtx code
+of the enclosing rtl, if known, otherwise it is UNKNOWN.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_ALLOCATE_INITIAL_VALUE (rtx @var{hard_reg})
+
+When the initial value of a hard register has been copied in a pseudo
+register, it is often not necessary to actually allocate another register
+to this pseudo register, because the original hard register or a stack slot
+it has been saved into can be used.  @code{TARGET_ALLOCATE_INITIAL_VALUE}
+is called at the start of register allocation once for each hard register
+that had its initial value copied by using
+@code{get_func_hard_reg_initial_val} or @code{get_hard_reg_initial_val}.
+Possible values are @code{NULL_RTX}, if you don't want
+to do any special allocation, a @code{REG} rtx---that would typically be
+the hard register itself, if it is known not to be clobbered---or a
+@code{MEM}.
+If you are returning a @code{MEM}, this is only a hint for the allocator;
+it might decide to use another register anyways.
+You may use @code{current_function_is_leaf} or 
+@code{REG_N_SETS} in the hook to determine if the hard
+register in question will not be clobbered.
+The default value of this hook is @code{NULL}, which disables any special
+allocation.
+@end deftypefn
+
+@deftypefn {Target Hook} int TARGET_UNSPEC_MAY_TRAP_P (const_rtx @var{x}, unsigned @var{flags})
+This target hook returns nonzero if @var{x}, an @code{unspec} or
+@code{unspec_volatile} operation, might cause a trap.  Targets can use
+this hook to enhance precision of analysis for @code{unspec} and
+@code{unspec_volatile} operations.  You may call @code{may_trap_p_1}
+to analyze inner elements of @var{x} in which case @var{flags} should be
+passed along.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_SET_CURRENT_FUNCTION (tree @var{decl})
+The compiler invokes this hook whenever it changes its current function
+context (@code{cfun}).  You can define this function if
+the back end needs to perform any initialization or reset actions on a
+per-function basis.  For example, it may be used to implement function
+attributes that affect register usage or code generation patterns.
+The argument @var{decl} is the declaration for the new function context,
+and may be null to indicate that the compiler has left a function context
+and is returning to processing at the top level.
+The default hook function does nothing.
+
+GCC sets @code{cfun} to a dummy function context during initialization of
+some parts of the back end.  The hook function is not invoked in this
+situation; you need not worry about the hook being invoked recursively,
+or when the back end is in a partially-initialized state.
+@code{cfun} might be @code{NULL} to indicate processing at top level,
+outside of any function scope.
+@end deftypefn
+
+@defmac TARGET_OBJECT_SUFFIX
+Define this macro to be a C string representing the suffix for object
+files on your target machine.  If you do not define this macro, GCC will
+use @samp{.o} as the suffix for object files.
+@end defmac
+
+@defmac TARGET_EXECUTABLE_SUFFIX
+Define this macro to be a C string representing the suffix to be
+automatically added to executable files on your target machine.  If you
+do not define this macro, GCC will use the null string as the suffix for
+executable files.
+@end defmac
+
+@defmac COLLECT_EXPORT_LIST
+If defined, @code{collect2} will scan the individual object files
+specified on its command line and create an export list for the linker.
+Define this macro for systems like AIX, where the linker discards
+object files that are not referenced from @code{main} and uses export
+lists.
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_CANNOT_MODIFY_JUMPS_P (void)
+This target hook returns @code{true} past the point in which new jump
+instructions could be created.  On machines that require a register for
+every jump such as the SHmedia ISA of SH5, this point would typically be
+reload, so this target hook should be defined to a function such as:
+
+@smallexample
+static bool
+cannot_modify_jumps_past_reload_p ()
+@{
+  return (reload_completed || reload_in_progress);
+@}
+@end smallexample
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_HAVE_CONDITIONAL_EXECUTION (void)
+This target hook returns true if the target supports conditional execution.
+This target hook is required only when the target has several different
+modes and they have different conditional execution capability, such as ARM.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_GEN_CCMP_FIRST (rtx_insn **@var{prep_seq}, rtx_insn **@var{gen_seq}, int @var{code}, tree @var{op0}, tree @var{op1})
+This function prepares to emit a comparison insn for the first compare in a
+ sequence of conditional comparisions.  It returns an appropriate comparison
+ with @code{CC} for passing to @code{gen_ccmp_next} or @code{cbranch_optab}.
+ The insns to prepare the compare are saved in @var{prep_seq} and the compare
+ insns are saved in @var{gen_seq}.  They will be emitted when all the
+ compares in the conditional comparision are generated without error.
+ @var{code} is the @code{rtx_code} of the compare for @var{op0} and @var{op1}.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_GEN_CCMP_NEXT (rtx_insn **@var{prep_seq}, rtx_insn **@var{gen_seq}, rtx @var{prev}, int @var{cmp_code}, tree @var{op0}, tree @var{op1}, int @var{bit_code})
+This function prepares to emit a conditional comparison within a sequence
+ of conditional comparisons.  It returns an appropriate comparison with
+ @code{CC} for passing to @code{gen_ccmp_next} or @code{cbranch_optab}.
+ The insns to prepare the compare are saved in @var{prep_seq} and the compare
+ insns are saved in @var{gen_seq}.  They will be emitted when all the
+ compares in the conditional comparision are generated without error.  The
+ @var{prev} expression is the result of a prior call to @code{gen_ccmp_first}
+ or @code{gen_ccmp_next}.  It may return @code{NULL} if the combination of
+ @var{prev} and this comparison is not supported, otherwise the result must
+ be appropriate for passing to @code{gen_ccmp_next} or @code{cbranch_optab}.
+ @var{code} is the @code{rtx_code} of the compare for @var{op0} and @var{op1}.
+ @var{bit_code} is @code{AND} or @code{IOR}, which is the op on the compares.
+@end deftypefn
+
+@deftypefn {Target Hook} unsigned TARGET_LOOP_UNROLL_ADJUST (unsigned @var{nunroll}, class loop *@var{loop})
+This target hook returns a new value for the number of times @var{loop}
+should be unrolled. The parameter @var{nunroll} is the number of times
+the loop is to be unrolled. The parameter @var{loop} is a pointer to
+the loop, which is going to be checked for unrolling. This target hook
+is required only when the target has special constraints like maximum
+number of memory accesses.
+@end deftypefn
+
+@defmac POWI_MAX_MULTS
+If defined, this macro is interpreted as a signed integer C expression
+that specifies the maximum number of floating point multiplications
+that should be emitted when expanding exponentiation by an integer
+constant inline.  When this value is defined, exponentiation requiring
+more than this number of multiplications is implemented by calling the
+system library's @code{pow}, @code{powf} or @code{powl} routines.
+The default value places no upper bound on the multiplication count.
+@end defmac
+
+@deftypefn Macro void TARGET_EXTRA_INCLUDES (const char *@var{sysroot}, const char *@var{iprefix}, int @var{stdinc})
+This target hook should register any extra include files for the
+target.  The parameter @var{stdinc} indicates if normal include files
+are present.  The parameter @var{sysroot} is the system root directory.
+The parameter @var{iprefix} is the prefix for the gcc directory.
+@end deftypefn
+
+@deftypefn Macro void TARGET_EXTRA_PRE_INCLUDES (const char *@var{sysroot}, const char *@var{iprefix}, int @var{stdinc})
+This target hook should register any extra include files for the
+target before any standard headers.  The parameter @var{stdinc}
+indicates if normal include files are present.  The parameter
+@var{sysroot} is the system root directory.  The parameter
+@var{iprefix} is the prefix for the gcc directory.
+@end deftypefn
+
+@deftypefn Macro void TARGET_OPTF (char *@var{path})
+This target hook should register special include paths for the target.
+The parameter @var{path} is the include to register.  On Darwin
+systems, this is used for Framework includes, which have semantics
+that are different from @option{-I}.
+@end deftypefn
+
+@defmac bool TARGET_USE_LOCAL_THUNK_ALIAS_P (tree @var{fndecl})
+This target macro returns @code{true} if it is safe to use a local alias
+for a virtual function @var{fndecl} when constructing thunks,
+@code{false} otherwise.  By default, the macro returns @code{true} for all
+functions, if a target supports aliases (i.e.@: defines
+@code{ASM_OUTPUT_DEF}), @code{false} otherwise,
+@end defmac
+
+@defmac TARGET_FORMAT_TYPES
+If defined, this macro is the name of a global variable containing
+target-specific format checking information for the @option{-Wformat}
+option.  The default is to have no target-specific format checks.
+@end defmac
+
+@defmac TARGET_N_FORMAT_TYPES
+If defined, this macro is the number of entries in
+@code{TARGET_FORMAT_TYPES}.
+@end defmac
+
+@defmac TARGET_OVERRIDES_FORMAT_ATTRIBUTES
+If defined, this macro is the name of a global variable containing
+target-specific format overrides for the @option{-Wformat} option. The
+default is to have no target-specific format overrides. If defined,
+@code{TARGET_FORMAT_TYPES} must be defined, too.
+@end defmac
+
+@defmac TARGET_OVERRIDES_FORMAT_ATTRIBUTES_COUNT
+If defined, this macro specifies the number of entries in
+@code{TARGET_OVERRIDES_FORMAT_ATTRIBUTES}.
+@end defmac
+
+@defmac TARGET_OVERRIDES_FORMAT_INIT
+If defined, this macro specifies the optional initialization
+routine for target specific customizations of the system printf
+and scanf formatter settings.
+@end defmac
+
+@deftypefn {Target Hook} {const char *} TARGET_INVALID_ARG_FOR_UNPROTOTYPED_FN (const_tree @var{typelist}, const_tree @var{funcdecl}, const_tree @var{val})
+If defined, this macro returns the diagnostic message when it is
+illegal to pass argument @var{val} to function @var{funcdecl}
+with prototype @var{typelist}.
+@end deftypefn
+
+@deftypefn {Target Hook} {const char *} TARGET_INVALID_CONVERSION (const_tree @var{fromtype}, const_tree @var{totype})
+If defined, this macro returns the diagnostic message when it is
+invalid to convert from @var{fromtype} to @var{totype}, or @code{NULL}
+if validity should be determined by the front end.
+@end deftypefn
+
+@deftypefn {Target Hook} {const char *} TARGET_INVALID_UNARY_OP (int @var{op}, const_tree @var{type})
+If defined, this macro returns the diagnostic message when it is
+invalid to apply operation @var{op} (where unary plus is denoted by
+@code{CONVERT_EXPR}) to an operand of type @var{type}, or @code{NULL}
+if validity should be determined by the front end.
+@end deftypefn
+
+@deftypefn {Target Hook} {const char *} TARGET_INVALID_BINARY_OP (int @var{op}, const_tree @var{type1}, const_tree @var{type2})
+If defined, this macro returns the diagnostic message when it is
+invalid to apply operation @var{op} to operands of types @var{type1}
+and @var{type2}, or @code{NULL} if validity should be determined by
+the front end.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_PROMOTED_TYPE (const_tree @var{type})
+If defined, this target hook returns the type to which values of
+@var{type} should be promoted when they appear in expressions,
+analogous to the integer promotions, or @code{NULL_TREE} to use the
+front end's normal promotion rules.  This hook is useful when there are
+target-specific types with special promotion rules.
+This is currently used only by the C and C++ front ends.
+@end deftypefn
+
+@deftypefn {Target Hook} tree TARGET_CONVERT_TO_TYPE (tree @var{type}, tree @var{expr})
+If defined, this hook returns the result of converting @var{expr} to
+@var{type}.  It should return the converted expression,
+or @code{NULL_TREE} to apply the front end's normal conversion rules.
+This hook is useful when there are target-specific types with special
+conversion rules.
+This is currently used only by the C and C++ front ends.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_VERIFY_TYPE_CONTEXT (location_t @var{loc}, type_context_kind @var{context}, const_tree @var{type}, bool @var{silent_p})
+If defined, this hook returns false if there is a target-specific reason
+why type @var{type} cannot be used in the source language context described
+by @var{context}.  When @var{silent_p} is false, the hook also reports an
+error against @var{loc} for invalid uses of @var{type}.
+
+Calls to this hook should be made through the global function
+@code{verify_type_context}, which makes the @var{silent_p} parameter
+default to false and also handles @code{error_mark_node}.
+
+The default implementation always returns true.
+@end deftypefn
+
+@defmac OBJC_JBLEN
+This macro determines the size of the objective C jump buffer for the
+NeXT runtime. By default, OBJC_JBLEN is defined to an innocuous value.
+@end defmac
+
+@defmac LIBGCC2_UNWIND_ATTRIBUTE
+Define this macro if any target-specific attributes need to be attached
+to the functions in @file{libgcc} that provide low-level support for
+call stack unwinding.  It is used in declarations in @file{unwind-generic.h}
+and the associated definitions of those functions.
+@end defmac
+
+@deftypefn {Target Hook} void TARGET_UPDATE_STACK_BOUNDARY (void)
+Define this macro to update the current function stack boundary if
+necessary.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_GET_DRAP_RTX (void)
+This hook should return an rtx for Dynamic Realign Argument Pointer (DRAP) if a
+different argument pointer register is needed to access the function's
+argument list due to stack realignment.  Return @code{NULL} if no DRAP
+is needed.
+@end deftypefn
+
+@deftypefn {Target Hook} HARD_REG_SET TARGET_ZERO_CALL_USED_REGS (HARD_REG_SET @var{selected_regs})
+This target hook emits instructions to zero the subset of @var{selected_regs}
+that could conceivably contain values that are useful to an attacker.
+Return the set of registers that were actually cleared.
+
+The default implementation uses normal move instructions to zero
+all the registers in @var{selected_regs}.  Define this hook if the
+target has more efficient ways of zeroing certain registers,
+or if you believe that certain registers would never contain
+values that are useful to an attacker.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_ALLOCATE_STACK_SLOTS_FOR_ARGS (void)
+When optimization is disabled, this hook indicates whether or not
+arguments should be allocated to stack slots.  Normally, GCC allocates
+stacks slots for arguments when not optimizing in order to make
+debugging easier.  However, when a function is declared with
+@code{__attribute__((naked))}, there is no stack frame, and the compiler
+cannot safely move arguments from the registers in which they are passed
+to the stack.  Therefore, this hook should return true in general, but
+false for naked functions.  The default implementation always returns true.
+@end deftypefn
+
+@deftypevr {Target Hook} {unsigned HOST_WIDE_INT} TARGET_CONST_ANCHOR
+On some architectures it can take multiple instructions to synthesize
+a constant.  If there is another constant already in a register that
+is close enough in value then it is preferable that the new constant
+is computed from this register using immediate addition or
+subtraction.  We accomplish this through CSE.  Besides the value of
+the constant we also add a lower and an upper constant anchor to the
+available expressions.  These are then queried when encountering new
+constants.  The anchors are computed by rounding the constant up and
+down to a multiple of the value of @code{TARGET_CONST_ANCHOR}.
+@code{TARGET_CONST_ANCHOR} should be the maximum positive value
+accepted by immediate-add plus one.  We currently assume that the
+value of @code{TARGET_CONST_ANCHOR} is a power of 2.  For example, on
+MIPS, where add-immediate takes a 16-bit signed value,
+@code{TARGET_CONST_ANCHOR} is set to @samp{0x8000}.  The default value
+is zero, which disables this optimization.
+@end deftypevr
+
+@deftypefn {Target Hook} {unsigned HOST_WIDE_INT} TARGET_ASAN_SHADOW_OFFSET (void)
+Return the offset bitwise ored into shifted address to get corresponding
+Address Sanitizer shadow memory address.  NULL if Address Sanitizer is not
+supported by the target.  May return 0 if Address Sanitizer is not supported
+by a subtarget.
+@end deftypefn
+
+@deftypefn {Target Hook} {unsigned HOST_WIDE_INT} TARGET_MEMMODEL_CHECK (unsigned HOST_WIDE_INT @var{val})
+Validate target specific memory model mask bits. When NULL no target specific
+memory model bits are allowed.
+@end deftypefn
+
+@deftypevr {Target Hook} {unsigned char} TARGET_ATOMIC_TEST_AND_SET_TRUEVAL
+This value should be set if the result written by @code{atomic_test_and_set} is not exactly 1, i.e.@: the @code{bool} @code{true}.
+@end deftypevr
+
+@deftypefn {Target Hook} bool TARGET_HAS_IFUNC_P (void)
+It returns true if the target supports GNU indirect functions.
+The support includes the assembler, linker and dynamic linker.
+The default value of this hook is based on target's libc.
+@end deftypefn
+
+@deftypefn {Target Hook} {unsigned int} TARGET_ATOMIC_ALIGN_FOR_MODE (machine_mode @var{mode})
+If defined, this function returns an appropriate alignment in bits for an atomic object of machine_mode @var{mode}.  If 0 is returned then the default alignment for the specified mode is used. 
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_ATOMIC_ASSIGN_EXPAND_FENV (tree *@var{hold}, tree *@var{clear}, tree *@var{update})
+ISO C11 requires atomic compound assignments that may raise floating-point exceptions to raise exceptions corresponding to the arithmetic operation whose result was successfully stored in a compare-and-exchange sequence.  This requires code equivalent to calls to @code{feholdexcept}, @code{feclearexcept} and @code{feupdateenv} to be generated at appropriate points in the compare-and-exchange sequence.  This hook should set @code{*@var{hold}} to an expression equivalent to the call to @code{feholdexcept}, @code{*@var{clear}} to an expression equivalent to the call to @code{feclearexcept} and @code{*@var{update}} to an expression equivalent to the call to @code{feupdateenv}.  The three expressions are @code{NULL_TREE} on entry to the hook and may be left as @code{NULL_TREE} if no code is required in a particular place.  The default implementation leaves all three expressions as @code{NULL_TREE}.  The @code{__atomic_feraiseexcept} function from @code{libatomic} may be of use as part of the code generated in @code{*@var{update}}.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_RECORD_OFFLOAD_SYMBOL (tree)
+Used when offloaded functions are seen in the compilation unit and no named
+sections are available.  It is called once for each symbol that must be
+recorded in the offload function and variable table.
+@end deftypefn
+
+@deftypefn {Target Hook} {char *} TARGET_OFFLOAD_OPTIONS (void)
+Used when writing out the list of options into an LTO file.  It should
+translate any relevant target-specific options (such as the ABI in use)
+into one of the @option{-foffload} options that exist as a common interface
+to express such options.  It should return a string containing these options,
+separated by spaces, which the caller will free.
+
+@end deftypefn
+
+@defmac TARGET_SUPPORTS_WIDE_INT
+
+On older ports, large integers are stored in @code{CONST_DOUBLE} rtl
+objects.  Newer ports define @code{TARGET_SUPPORTS_WIDE_INT} to be nonzero
+to indicate that large integers are stored in
+@code{CONST_WIDE_INT} rtl objects.  The @code{CONST_WIDE_INT} allows
+very large integer constants to be represented.  @code{CONST_DOUBLE}
+is limited to twice the size of the host's @code{HOST_WIDE_INT}
+representation.
+
+Converting a port mostly requires looking for the places where
+@code{CONST_DOUBLE}s are used with @code{VOIDmode} and replacing that
+code with code that accesses @code{CONST_WIDE_INT}s.  @samp{"grep -i
+const_double"} at the port level gets you to 95% of the changes that
+need to be made.  There are a few places that require a deeper look.
+
+@itemize @bullet
+@item
+There is no equivalent to @code{hval} and @code{lval} for
+@code{CONST_WIDE_INT}s.  This would be difficult to express in the md
+language since there are a variable number of elements.
+
+Most ports only check that @code{hval} is either 0 or -1 to see if the
+value is small.  As mentioned above, this will no longer be necessary
+since small constants are always @code{CONST_INT}.  Of course there
+are still a few exceptions, the alpha's constraint used by the zap
+instruction certainly requires careful examination by C code.
+However, all the current code does is pass the hval and lval to C
+code, so evolving the c code to look at the @code{CONST_WIDE_INT} is
+not really a large change.
+
+@item
+Because there is no standard template that ports use to materialize
+constants, there is likely to be some futzing that is unique to each
+port in this code.
+
+@item
+The rtx costs may have to be adjusted to properly account for larger
+constants that are represented as @code{CONST_WIDE_INT}.
+@end itemize
+
+All and all it does not take long to convert ports that the
+maintainer is familiar with.
+
+@end defmac
+
+@deftypefn {Target Hook} bool TARGET_HAVE_SPECULATION_SAFE_VALUE (bool @var{active})
+This hook is used to determine the level of target support for
+ @code{__builtin_speculation_safe_value}.  If called with an argument
+ of false, it returns true if the target has been modified to support
+ this builtin.  If called with an argument of true, it returns true
+ if the target requires active mitigation execution might be speculative.
+ 
+ The default implementation returns false if the target does not define
+ a pattern named @code{speculation_barrier}.  Else it returns true
+ for the first case and whether the pattern is enabled for the current
+ compilation for the second case.
+ 
+ For targets that have no processors that can execute instructions
+ speculatively an alternative implemenation of this hook is available:
+ simply redefine this hook to @code{speculation_safe_value_not_needed}
+ along with your other target hooks.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_SPECULATION_SAFE_VALUE (machine_mode @var{mode}, rtx @var{result}, rtx @var{val}, rtx @var{failval})
+This target hook can be used to generate a target-specific code
+ sequence that implements the @code{__builtin_speculation_safe_value}
+ built-in function.  The function must always return @var{val} in
+ @var{result} in mode @var{mode} when the cpu is not executing
+ speculatively, but must never return that when speculating until it
+ is known that the speculation will not be unwound.  The hook supports
+ two primary mechanisms for implementing the requirements.  The first
+ is to emit a speculation barrier which forces the processor to wait
+ until all prior speculative operations have been resolved; the second
+ is to use a target-specific mechanism that can track the speculation
+ state and to return @var{failval} if it can determine that
+ speculation must be unwound at a later time.
+ 
+ The default implementation simply copies @var{val} to @var{result} and
+ emits a @code{speculation_barrier} instruction if that is defined.
+@end deftypefn
+
+@deftypefn {Target Hook} void TARGET_RUN_TARGET_SELFTESTS (void)
+If selftests are enabled, run any selftests for this target.
+@end deftypefn
+
+@deftypefn {Target Hook} bool TARGET_MEMTAG_CAN_TAG_ADDRESSES ()
+True if the backend architecture naturally supports ignoring some region
+of pointers.  This feature means that @option{-fsanitize=hwaddress} can
+work.
+
+At preset, this feature does not support address spaces.  It also requires
+@code{Pmode} to be the same as @code{ptr_mode}.
+@end deftypefn
+
+@deftypefn {Target Hook} uint8_t TARGET_MEMTAG_TAG_SIZE ()
+Return the size of a tag (in bits) for this platform.
+
+The default returns 8.
+@end deftypefn
+
+@deftypefn {Target Hook} uint8_t TARGET_MEMTAG_GRANULE_SIZE ()
+Return the size in real memory that each byte in shadow memory refers to.
+I.e. if a variable is @var{X} bytes long in memory, then this hook should
+return the value @var{Y} such that the tag in shadow memory spans
+@var{X}/@var{Y} bytes.
+
+Most variables will need to be aligned to this amount since two variables
+that are neighbors in memory and share a tag granule would need to share
+the same tag.
+
+The default returns 16.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_MEMTAG_INSERT_RANDOM_TAG (rtx @var{untagged}, rtx @var{target})
+Return an RTX representing the value of @var{untagged} but with a
+(possibly) random tag in it.
+Put that value into @var{target} if it is convenient to do so.
+This function is used to generate a tagged base for the current stack frame.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_MEMTAG_ADD_TAG (rtx @var{base}, poly_int64 @var{addr_offset}, uint8_t @var{tag_offset})
+Return an RTX that represents the result of adding @var{addr_offset} to
+the address in pointer @var{base} and @var{tag_offset} to the tag in pointer
+@var{base}.
+The resulting RTX must either be a valid memory address or be able to get
+put into an operand with @code{force_operand}.
+
+Unlike other memtag hooks, this must return an expression and not emit any
+RTL.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_MEMTAG_SET_TAG (rtx @var{untagged_base}, rtx @var{tag}, rtx @var{target})
+Return an RTX representing @var{untagged_base} but with the tag @var{tag}.
+Try and store this in @var{target} if convenient.
+@var{untagged_base} is required to have a zero tag when this hook is called.
+The default of this hook is to set the top byte of @var{untagged_base} to
+@var{tag}.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_MEMTAG_EXTRACT_TAG (rtx @var{tagged_pointer}, rtx @var{target})
+Return an RTX representing the tag stored in @var{tagged_pointer}.
+Store the result in @var{target} if it is convenient.
+The default represents the top byte of the original pointer.
+@end deftypefn
+
+@deftypefn {Target Hook} rtx TARGET_MEMTAG_UNTAGGED_POINTER (rtx @var{tagged_pointer}, rtx @var{target})
+Return an RTX representing @var{tagged_pointer} with its tag set to zero.
+Store the result in @var{target} if convenient.
+The default clears the top byte of the original pointer.
+@end deftypefn
diff -uNr gcc-11.2.0.orig/gcc/target.def gcc-11.2.0/gcc/target.def
--- gcc-11.2.0.orig/gcc/target.def	2021-07-28 01:55:07.504290371 -0500
+++ gcc-11.2.0/gcc/target.def	2021-08-06 00:34:01.884709574 -0500
@@ -3674,6 +3674,13 @@
  default_slow_unaligned_access)
 
 DEFHOOK
+(clear_by_pieces,
+ "This target hook should return an rtx that is used to clear memory.\n\
+The default returns @code{const0_rtx}.",
+ rtx, (void *, void *, HOST_WIDE_INT, scalar_int_mode),
+ default_clear_by_pieces)
+
+DEFHOOK
 (optab_supported_p,
  "Return true if the optimizers should use optab @var{op} with\n\
 modes @var{mode1} and @var{mode2} for optimization type @var{opt_type}.\n\
diff -uNr gcc-11.2.0.orig/gcc/target.def.orig gcc-11.2.0/gcc/target.def.orig
--- gcc-11.2.0.orig/gcc/target.def.orig	1969-12-31 18:00:00.000000000 -0600
+++ gcc-11.2.0/gcc/target.def.orig	2021-07-28 01:55:07.504290371 -0500
@@ -0,0 +1,6985 @@
+/* Target hook definitions.
+   Copyright (C) 2001-2021 Free Software Foundation, Inc.
+
+   This program is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published by the
+   Free Software Foundation; either version 3, or (at your option) any
+   later version.
+
+   This program is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.
+
+   In other words, you are welcome to use, share and improve this program.
+   You are forbidden to forbid anyone else to use, share and improve
+   what you give them.   Help stamp out software-hoarding!  */
+
+/* See target-hooks-macros.h for details of macros that should be
+   provided by the including file, and how to use them here.  */
+#include "target-hooks-macros.h"
+
+#undef HOOK_TYPE
+#define HOOK_TYPE "Target Hook"
+
+HOOK_VECTOR (TARGET_INITIALIZER, gcc_target)
+
+/* Functions that output assembler for the target.  */
+#define HOOK_PREFIX "TARGET_ASM_"
+HOOK_VECTOR (TARGET_ASM_OUT, asm_out)
+
+/* Opening and closing parentheses for asm expression grouping.  */
+DEFHOOKPOD
+(open_paren,
+ "These target hooks are C string constants, describing the syntax in the\n\
+assembler for grouping arithmetic expressions.  If not overridden, they\n\
+default to normal parentheses, which is correct for most assemblers.",
+ const char *, "(")
+DEFHOOKPODX (close_paren, const char *, ")")
+
+/* Assembler instructions for creating various kinds of integer object.  */
+DEFHOOKPOD
+(byte_op,
+ "@deftypevrx {Target Hook} {const char *} TARGET_ASM_ALIGNED_HI_OP\n\
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_ALIGNED_PSI_OP\n\
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_ALIGNED_SI_OP\n\
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_ALIGNED_PDI_OP\n\
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_ALIGNED_DI_OP\n\
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_ALIGNED_PTI_OP\n\
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_ALIGNED_TI_OP\n\
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_UNALIGNED_HI_OP\n\
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_UNALIGNED_PSI_OP\n\
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_UNALIGNED_SI_OP\n\
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_UNALIGNED_PDI_OP\n\
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_UNALIGNED_DI_OP\n\
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_UNALIGNED_PTI_OP\n\
+@deftypevrx {Target Hook} {const char *} TARGET_ASM_UNALIGNED_TI_OP\n\
+These hooks specify assembly directives for creating certain kinds\n\
+of integer object.  The @code{TARGET_ASM_BYTE_OP} directive creates a\n\
+byte-sized object, the @code{TARGET_ASM_ALIGNED_HI_OP} one creates an\n\
+aligned two-byte object, and so on.  Any of the hooks may be\n\
+@code{NULL}, indicating that no suitable directive is available.\n\
+\n\
+The compiler will print these strings at the start of a new line,\n\
+followed immediately by the object's initial value.  In most cases,\n\
+the string should contain a tab, a pseudo-op, and then another tab.",
+ const char *, "\t.byte\t")
+DEFHOOKPOD (aligned_op, "*", struct asm_int_op, TARGET_ASM_ALIGNED_INT_OP)
+DEFHOOKPOD (unaligned_op, "*", struct asm_int_op, TARGET_ASM_UNALIGNED_INT_OP)
+
+/* Try to output the assembler code for an integer object whose
+   value is given by X.  SIZE is the size of the object in bytes and
+   ALIGNED_P indicates whether it is aligned.  Return true if
+   successful.  Only handles cases for which BYTE_OP, ALIGNED_OP
+   and UNALIGNED_OP are NULL.  */
+DEFHOOK
+(integer,
+ "The @code{assemble_integer} function uses this hook to output an\n\
+integer object.  @var{x} is the object's value, @var{size} is its size\n\
+in bytes and @var{aligned_p} indicates whether it is aligned.  The\n\
+function should return @code{true} if it was able to output the\n\
+object.  If it returns false, @code{assemble_integer} will try to\n\
+split the object into smaller parts.\n\
+\n\
+The default implementation of this hook will use the\n\
+@code{TARGET_ASM_BYTE_OP} family of strings, returning @code{false}\n\
+when the relevant string is @code{NULL}.",
+ /* Only handles cases for which BYTE_OP, ALIGNED_OP and UNALIGNED_OP are
+    NULL.  */
+ bool, (rtx x, unsigned int size, int aligned_p),
+ default_assemble_integer)
+
+/* Assembly strings required after the .cfi_startproc label.  */
+DEFHOOK
+(post_cfi_startproc,
+  "This target hook is used to emit assembly strings required by the target\n\
+after the .cfi_startproc directive.  The first argument is the file stream to\n\
+write the strings to and the second argument is the function\'s declaration.  The\n\
+expected use is to add more .cfi_* directives.\n\
+\n\
+The default is to not output any assembly strings.",
+  void, (FILE *, tree),
+  hook_void_FILEptr_tree)
+
+/* Notify the backend that we have completed emitting the data for a
+   decl.  */
+DEFHOOK
+(decl_end,
+ "Define this hook if the target assembler requires a special marker to\n\
+terminate an initialized variable declaration.",
+ void, (void),
+ hook_void_void)
+
+/* Output code that will globalize a label.  */
+DEFHOOK
+(globalize_label,
+ "This target hook is a function to output to the stdio stream\n\
+@var{stream} some commands that will make the label @var{name} global;\n\
+that is, available for reference from other files.\n\
+\n\
+The default implementation relies on a proper definition of\n\
+@code{GLOBAL_ASM_OP}.",
+ void, (FILE *stream, const char *name),
+ default_globalize_label)
+
+/* Output code that will globalize a declaration.  */
+DEFHOOK
+(globalize_decl_name,
+ "This target hook is a function to output to the stdio stream\n\
+@var{stream} some commands that will make the name associated with @var{decl}\n\
+global; that is, available for reference from other files.\n\
+\n\
+The default implementation uses the TARGET_ASM_GLOBALIZE_LABEL target hook.",
+ void, (FILE *stream, tree decl), default_globalize_decl_name)
+
+/* Output code that will declare an external variable.  */
+DEFHOOK
+(assemble_undefined_decl,
+ "This target hook is a function to output to the stdio stream\n\
+@var{stream} some commands that will declare the name associated with\n\
+@var{decl} which is not defined in the current translation unit.  Most\n\
+assemblers do not require anything to be output in this case.",
+ void, (FILE *stream, const char *name, const_tree decl),
+ hook_void_FILEptr_constcharptr_const_tree)
+
+/* Output code that will emit a label for unwind info, if this
+   target requires such labels.  Second argument is the decl the
+   unwind info is associated with, third is a boolean: true if
+   this is for exception handling, fourth is a boolean: true if
+   this is only a placeholder for an omitted FDE.  */
+DEFHOOK
+(emit_unwind_label,
+ "This target hook emits a label at the beginning of each FDE@.  It\n\
+should be defined on targets where FDEs need special labels, and it\n\
+should write the appropriate label, for the FDE associated with the\n\
+function declaration @var{decl}, to the stdio stream @var{stream}.\n\
+The third argument, @var{for_eh}, is a boolean: true if this is for an\n\
+exception table.  The fourth argument, @var{empty}, is a boolean:\n\
+true if this is a placeholder label for an omitted FDE@.\n\
+\n\
+The default is that FDEs are not given nonlocal labels.",
+ void, (FILE *stream, tree decl, int for_eh, int empty),
+ default_emit_unwind_label)
+
+/* Output code that will emit a label to divide up the exception table.  */
+DEFHOOK
+(emit_except_table_label,
+ "This target hook emits a label at the beginning of the exception table.\n\
+It should be defined on targets where it is desirable for the table\n\
+to be broken up according to function.\n\
+\n\
+The default is that no label is emitted.",
+ void, (FILE *stream),
+ default_emit_except_table_label)
+
+/* Emit a directive for setting the personality for the function.  */
+DEFHOOK
+(emit_except_personality,
+ "If the target implements @code{TARGET_ASM_UNWIND_EMIT}, this hook may be\
+ used to emit a directive to install a personality hook into the unwind\
+ info.  This hook should not be used if dwarf2 unwind info is used.",
+ void, (rtx personality),
+ NULL)
+
+/* If necessary, modify personality and LSDA references to handle
+   indirection.  This is used when the assembler supports CFI directives.  */
+DEFHOOK
+(make_eh_symbol_indirect,
+ "If necessary, modify personality and LSDA references to handle indirection.\
+  The original symbol is in @code{origsymbol} and if @code{pubvis} is true\
+  the symbol is visible outside the TU.",
+ rtx, (rtx origsymbol, bool pubvis),
+ NULL)
+
+/* Emit any directives required to unwind this instruction.  */
+DEFHOOK
+(unwind_emit,
+ "This target hook emits assembly directives required to unwind the\n\
+given instruction.  This is only used when @code{TARGET_EXCEPT_UNWIND_INFO}\n\
+returns @code{UI_TARGET}.",
+ void, (FILE *stream, rtx_insn *insn),
+ NULL)
+
+DEFHOOKPOD
+(unwind_emit_before_insn,
+ "True if the @code{TARGET_ASM_UNWIND_EMIT} hook should be called before\
+ the assembly for @var{insn} has been emitted, false if the hook should\
+ be called afterward.",
+ bool, true)
+
+/* Return true if the target needs extra instructions to restore the current
+   frame address after a DW_CFA_restore_state opcode.  */
+DEFHOOK
+(should_restore_cfa_state,
+ "For DWARF-based unwind frames, two CFI instructions provide for save and\
+ restore of register state.  GCC maintains the current frame address (CFA)\
+ separately from the register bank but the unwinder in libgcc preserves this\
+ state along with the registers (and this is expected by the code that writes\
+ the unwind frames).  This hook allows the target to specify that the CFA data\
+ is not saved/restored along with the registers by the target unwinder so that\
+ suitable additional instructions should be emitted to restore it.",
+ bool, (void),
+ hook_bool_void_false)
+
+/* Generate an internal label.
+   For now this is just a wrapper for ASM_GENERATE_INTERNAL_LABEL.  */
+DEFHOOK_UNDOC
+(generate_internal_label,
+ "",
+ void, (char *buf, const char *prefix, unsigned long labelno),
+ default_generate_internal_label)
+
+/* Output an internal label.  */
+DEFHOOK
+(internal_label,
+ "A function to output to the stdio stream @var{stream} a label whose\n\
+name is made from the string @var{prefix} and the number @var{labelno}.\n\
+\n\
+It is absolutely essential that these labels be distinct from the labels\n\
+used for user-level functions and variables.  Otherwise, certain programs\n\
+will have name conflicts with internal labels.\n\
+\n\
+It is desirable to exclude internal labels from the symbol table of the\n\
+object file.  Most assemblers have a naming convention for labels that\n\
+should be excluded; on many systems, the letter @samp{L} at the\n\
+beginning of a label has this effect.  You should find out what\n\
+convention your system uses, and follow it.\n\
+\n\
+The default version of this function utilizes @code{ASM_GENERATE_INTERNAL_LABEL}.",
+ void, (FILE *stream, const char *prefix, unsigned long labelno),
+ default_internal_label)
+
+/* Output label for the constant.  */
+DEFHOOK
+(declare_constant_name,
+ "A target hook to output to the stdio stream @var{file} any text necessary\n\
+for declaring the name @var{name} of a constant which is being defined.  This\n\
+target hook is responsible for outputting the label definition (perhaps using\n\
+@code{assemble_label}).  The argument @var{exp} is the value of the constant,\n\
+and @var{size} is the size of the constant in bytes.  The @var{name}\n\
+will be an internal label.\n\
+\n\
+The default version of this target hook, define the @var{name} in the\n\
+usual manner as a label (by means of @code{assemble_label}).\n\
+\n\
+You may wish to use @code{ASM_OUTPUT_TYPE_DIRECTIVE} in this target hook.",
+ void, (FILE *file, const char *name, const_tree expr, HOST_WIDE_INT size),
+ default_asm_declare_constant_name)
+
+/* Emit a ttype table reference to a typeinfo object.  */
+DEFHOOK
+(ttype,
+ "This hook is used to output a reference from a frame unwinding table to\n\
+the type_info object identified by @var{sym}.  It should return @code{true}\n\
+if the reference was output.  Returning @code{false} will cause the\n\
+reference to be output using the normal Dwarf2 routines.",
+ bool, (rtx sym),
+ hook_bool_rtx_false)
+
+/* Emit an assembler directive to set visibility for the symbol
+   associated with the tree decl.  */
+DEFHOOK
+(assemble_visibility,
+ "This target hook is a function to output to @var{asm_out_file} some\n\
+commands that will make the symbol(s) associated with @var{decl} have\n\
+hidden, protected or internal visibility as specified by @var{visibility}.",
+ void, (tree decl, int visibility),
+ default_assemble_visibility)
+
+DEFHOOK
+(print_patchable_function_entry,
+ "Generate a patchable area at the function start, consisting of\n\
+@var{patch_area_size} NOP instructions.  If the target supports named\n\
+sections and if @var{record_p} is true, insert a pointer to the current\n\
+location in the table of patchable functions.  The default implementation\n\
+of the hook places the table of pointers in the special section named\n\
+@code{__patchable_function_entries}.",
+ void, (FILE *file, unsigned HOST_WIDE_INT patch_area_size, bool record_p),
+ default_print_patchable_function_entry)
+
+/* Output the assembler code for entry to a function.  */
+DEFHOOK
+(function_prologue,
+ "If defined, a function that outputs the assembler code for entry to a\n\
+function.  The prologue is responsible for setting up the stack frame,\n\
+initializing the frame pointer register, saving registers that must be\n\
+saved, and allocating @var{size} additional bytes of storage for the\n\
+local variables.  @var{file} is a stdio stream to which the assembler\n\
+code should be output.\n\
+\n\
+The label for the beginning of the function need not be output by this\n\
+macro.  That has already been done when the macro is run.\n\
+\n\
+@findex regs_ever_live\n\
+To determine which registers to save, the macro can refer to the array\n\
+@code{regs_ever_live}: element @var{r} is nonzero if hard register\n\
+@var{r} is used anywhere within the function.  This implies the function\n\
+prologue should save register @var{r}, provided it is not one of the\n\
+call-used registers.  (@code{TARGET_ASM_FUNCTION_EPILOGUE} must likewise use\n\
+@code{regs_ever_live}.)\n\
+\n\
+On machines that have ``register windows'', the function entry code does\n\
+not save on the stack the registers that are in the windows, even if\n\
+they are supposed to be preserved by function calls; instead it takes\n\
+appropriate steps to ``push'' the register stack, if any non-call-used\n\
+registers are used in the function.\n\
+\n\
+@findex frame_pointer_needed\n\
+On machines where functions may or may not have frame-pointers, the\n\
+function entry code must vary accordingly; it must set up the frame\n\
+pointer if one is wanted, and not otherwise.  To determine whether a\n\
+frame pointer is in wanted, the macro can refer to the variable\n\
+@code{frame_pointer_needed}.  The variable's value will be 1 at run\n\
+time in a function that needs a frame pointer.  @xref{Elimination}.\n\
+\n\
+The function entry code is responsible for allocating any stack space\n\
+required for the function.  This stack space consists of the regions\n\
+listed below.  In most cases, these regions are allocated in the\n\
+order listed, with the last listed region closest to the top of the\n\
+stack (the lowest address if @code{STACK_GROWS_DOWNWARD} is defined, and\n\
+the highest address if it is not defined).  You can use a different order\n\
+for a machine if doing so is more convenient or required for\n\
+compatibility reasons.  Except in cases where required by standard\n\
+or by a debugger, there is no reason why the stack layout used by GCC\n\
+need agree with that used by other compilers for a machine.",
+ void, (FILE *file),
+ default_function_pro_epilogue)
+
+/* Output the assembler code for end of prologue.  */
+DEFHOOK
+(function_end_prologue,
+ "If defined, a function that outputs assembler code at the end of a\n\
+prologue.  This should be used when the function prologue is being\n\
+emitted as RTL, and you have some extra assembler that needs to be\n\
+emitted.  @xref{prologue instruction pattern}.",
+ void, (FILE *file),
+ no_asm_to_stream)
+
+/* Output the assembler code for start of epilogue.  */
+DEFHOOK
+(function_begin_epilogue,
+ "If defined, a function that outputs assembler code at the start of an\n\
+epilogue.  This should be used when the function epilogue is being\n\
+emitted as RTL, and you have some extra assembler that needs to be\n\
+emitted.  @xref{epilogue instruction pattern}.",
+ void, (FILE *file),
+ no_asm_to_stream)
+
+/* Output the assembler code for function exit.  */
+DEFHOOK
+(function_epilogue,
+ "If defined, a function that outputs the assembler code for exit from a\n\
+function.  The epilogue is responsible for restoring the saved\n\
+registers and stack pointer to their values when the function was\n\
+called, and returning control to the caller.  This macro takes the\n\
+same argument as the macro @code{TARGET_ASM_FUNCTION_PROLOGUE}, and the\n\
+registers to restore are determined from @code{regs_ever_live} and\n\
+@code{CALL_USED_REGISTERS} in the same way.\n\
+\n\
+On some machines, there is a single instruction that does all the work\n\
+of returning from the function.  On these machines, give that\n\
+instruction the name @samp{return} and do not define the macro\n\
+@code{TARGET_ASM_FUNCTION_EPILOGUE} at all.\n\
+\n\
+Do not define a pattern named @samp{return} if you want the\n\
+@code{TARGET_ASM_FUNCTION_EPILOGUE} to be used.  If you want the target\n\
+switches to control whether return instructions or epilogues are used,\n\
+define a @samp{return} pattern with a validity condition that tests the\n\
+target switches appropriately.  If the @samp{return} pattern's validity\n\
+condition is false, epilogues will be used.\n\
+\n\
+On machines where functions may or may not have frame-pointers, the\n\
+function exit code must vary accordingly.  Sometimes the code for these\n\
+two cases is completely different.  To determine whether a frame pointer\n\
+is wanted, the macro can refer to the variable\n\
+@code{frame_pointer_needed}.  The variable's value will be 1 when compiling\n\
+a function that needs a frame pointer.\n\
+\n\
+Normally, @code{TARGET_ASM_FUNCTION_PROLOGUE} and\n\
+@code{TARGET_ASM_FUNCTION_EPILOGUE} must treat leaf functions specially.\n\
+The C variable @code{current_function_is_leaf} is nonzero for such a\n\
+function.  @xref{Leaf Functions}.\n\
+\n\
+On some machines, some functions pop their arguments on exit while\n\
+others leave that for the caller to do.  For example, the 68020 when\n\
+given @option{-mrtd} pops arguments in functions that take a fixed\n\
+number of arguments.\n\
+\n\
+@findex pops_args\n\
+@findex crtl->args.pops_args\n\
+Your definition of the macro @code{RETURN_POPS_ARGS} decides which\n\
+functions pop their own arguments.  @code{TARGET_ASM_FUNCTION_EPILOGUE}\n\
+needs to know what was decided.  The number of bytes of the current\n\
+function's arguments that this function should pop is available in\n\
+@code{crtl->args.pops_args}.  @xref{Scalar Return}.",
+ void, (FILE *file),
+ default_function_pro_epilogue)
+
+/* Initialize target-specific sections.  */
+DEFHOOK
+(init_sections,
+ "Define this hook if you need to do something special to set up the\n\
+@file{varasm.c} sections, or if your target has some special sections\n\
+of its own that you need to create.\n\
+\n\
+GCC calls this hook after processing the command line, but before writing\n\
+any assembly code, and before calling any of the section-returning hooks\n\
+described below.",
+ void, (void),
+ hook_void_void)
+
+/* Tell assembler to change to section NAME with attributes FLAGS.
+   If DECL is non-NULL, it is the VAR_DECL or FUNCTION_DECL with
+   which this section is associated.  */
+DEFHOOK
+(named_section,
+ "Output assembly directives to switch to section @var{name}.  The section\n\
+should have attributes as specified by @var{flags}, which is a bit mask\n\
+of the @code{SECTION_*} flags defined in @file{output.h}.  If @var{decl}\n\
+is non-NULL, it is the @code{VAR_DECL} or @code{FUNCTION_DECL} with which\n\
+this section is associated.",
+ void, (const char *name, unsigned int flags, tree decl),
+ default_no_named_section)
+
+/* Tell assembler what section attributes to assign this elf section
+   declaration, using their numerical value.  */
+DEFHOOK
+(elf_flags_numeric,
+ "This hook can be used to encode ELF section flags for which no letter\n\
+code has been defined in the assembler.  It is called by\n\
+@code{default_asm_named_section} whenever the section flags need to be\n\
+emitted in the assembler output.  If the hook returns true, then the\n\
+numerical value for ELF section flags should be calculated from\n\
+@var{flags} and saved in @var{*num}; the value is printed out instead of the\n\
+normal sequence of letter codes.  If the hook is not defined, or if it\n\
+returns false, then @var{num} is ignored and the traditional letter sequence\n\
+is emitted.",
+ bool, (unsigned int flags, unsigned int *num),
+ hook_bool_uint_uintp_false)
+
+/* Return preferred text (sub)section for function DECL.
+   Main purpose of this function is to separate cold, normal and hot
+   functions. STARTUP is true when function is known to be used only 
+   at startup (from static constructors or it is main()).
+   EXIT is true when function is known to be used only at exit
+   (from static destructors).
+   Return NULL if function should go to default text section.  */
+DEFHOOK
+(function_section,
+ "Return preferred text (sub)section for function @var{decl}.\n\
+Main purpose of this function is to separate cold, normal and hot\n\
+functions. @var{startup} is true when function is known to be used only\n\
+at startup (from static constructors or it is @code{main()}).\n\
+@var{exit} is true when function is known to be used only at exit\n\
+(from static destructors).\n\
+Return NULL if function should go to default text section.",
+ section *, (tree decl, enum node_frequency freq, bool startup, bool exit),
+ default_function_section)
+
+/* Output the assembler code for function exit.  */
+DEFHOOK
+(function_switched_text_sections,
+ "Used by the target to emit any assembler directives or additional\
+  labels needed when a function is partitioned between different\
+  sections.  Output should be written to @var{file}.  The function\
+  decl is available as @var{decl} and the new section is `cold' if\
+  @var{new_is_cold} is @code{true}.",
+ void, (FILE *file, tree decl, bool new_is_cold),
+ default_function_switched_text_sections)
+
+/* Return a mask describing how relocations should be treated when
+   selecting sections.  Bit 1 should be set if global relocations
+   should be placed in a read-write section; bit 0 should be set if
+   local relocations should be placed in a read-write section.  */
+DEFHOOK
+(reloc_rw_mask,
+ "Return a mask describing how relocations should be treated when\n\
+selecting sections.  Bit 1 should be set if global relocations\n\
+should be placed in a read-write section; bit 0 should be set if\n\
+local relocations should be placed in a read-write section.\n\
+\n\
+The default version of this function returns 3 when @option{-fpic}\n\
+is in effect, and 0 otherwise.  The hook is typically redefined\n\
+when the target cannot support (some kinds of) dynamic relocations\n\
+in read-only sections even in executables.",
+ int, (void),
+ default_reloc_rw_mask)
+
+ /* Return a flag for either generating ADDR_DIF_VEC table
+ or ADDR_VEC table for jumps in case of -fPIC/-fPIE.  */
+DEFHOOK
+(generate_pic_addr_diff_vec,
+"Return true to generate ADDR_DIF_VEC table\n\
+or false to generate ADDR_VEC table for jumps in case of -fPIC.\n\
+\n\
+The default version of this function returns true if flag_pic\n\
+equals true and false otherwise",
+ bool, (void),
+ default_generate_pic_addr_diff_vec)
+
+ /* Return a section for EXP.  It may be a DECL or a constant.  RELOC
+    is nonzero if runtime relocations must be applied; bit 1 will be
+    set if the runtime relocations require non-local name resolution.
+    ALIGN is the required alignment of the data.  */
+DEFHOOK
+(select_section,
+ "Return the section into which @var{exp} should be placed.  You can\n\
+assume that @var{exp} is either a @code{VAR_DECL} node or a constant of\n\
+some sort.  @var{reloc} indicates whether the initial value of @var{exp}\n\
+requires link-time relocations.  Bit 0 is set when variable contains\n\
+local relocations only, while bit 1 is set for global relocations.\n\
+@var{align} is the constant alignment in bits.\n\
+\n\
+The default version of this function takes care of putting read-only\n\
+variables in @code{readonly_data_section}.\n\
+\n\
+See also @var{USE_SELECT_SECTION_FOR_FUNCTIONS}.",
+ section *, (tree exp, int reloc, unsigned HOST_WIDE_INT align),
+ default_select_section)
+
+/* Return a section for X.  MODE is X's mode and ALIGN is its
+   alignment in bits.  */
+DEFHOOK
+(select_rtx_section,
+ "Return the section into which a constant @var{x}, of mode @var{mode},\n\
+should be placed.  You can assume that @var{x} is some kind of\n\
+constant in RTL@.  The argument @var{mode} is redundant except in the\n\
+case of a @code{const_int} rtx.  @var{align} is the constant alignment\n\
+in bits.\n\
+\n\
+The default version of this function takes care of putting symbolic\n\
+constants in @code{flag_pic} mode in @code{data_section} and everything\n\
+else in @code{readonly_data_section}.",
+ section *, (machine_mode mode, rtx x, unsigned HOST_WIDE_INT align),
+ default_select_rtx_section)
+
+/* Select a unique section name for DECL.  RELOC is the same as
+   for SELECT_SECTION.  */
+DEFHOOK
+(unique_section,
+ "Build up a unique section name, expressed as a @code{STRING_CST} node,\n\
+and assign it to @samp{DECL_SECTION_NAME (@var{decl})}.\n\
+As with @code{TARGET_ASM_SELECT_SECTION}, @var{reloc} indicates whether\n\
+the initial value of @var{exp} requires link-time relocations.\n\
+\n\
+The default version of this function appends the symbol name to the\n\
+ELF section name that would normally be used for the symbol.  For\n\
+example, the function @code{foo} would be placed in @code{.text.foo}.\n\
+Whatever the actual target object format, this is often good enough.",
+ void, (tree decl, int reloc),
+ default_unique_section)
+
+/* Return the readonly data or relocated readonly data section
+   associated with function DECL.  */
+DEFHOOK
+(function_rodata_section,
+ "Return the readonly data or reloc readonly data section associated with\n\
+@samp{DECL_SECTION_NAME (@var{decl})}. @var{relocatable} selects the latter\n\
+over the former.\n\
+The default version of this function selects @code{.gnu.linkonce.r.name} if\n\
+the function's section is @code{.gnu.linkonce.t.name}, @code{.rodata.name}\n\
+or @code{.data.rel.ro.name} if function is in @code{.text.name}, and\n\
+the normal readonly-data or reloc readonly data section otherwise.",
+ section *, (tree decl, bool relocatable),
+ default_function_rodata_section)
+
+/* Nonnull if the target wants to override the default ".rodata" prefix
+   for mergeable data sections.  */
+DEFHOOKPOD
+(mergeable_rodata_prefix,
+ "Usually, the compiler uses the prefix @code{\".rodata\"} to construct\n\
+section names for mergeable constant data.  Define this macro to override\n\
+the string if a different section name should be used.",
+ const char *, ".rodata")
+
+/* Return the section to be used for transactional memory clone tables.  */
+DEFHOOK
+(tm_clone_table_section,
+ "Return the section that should be used for transactional memory clone\
+  tables.",
+ section *, (void), default_clone_table_section)
+
+/* Output a constructor for a symbol with a given priority.  */
+DEFHOOK
+(constructor,
+ "If defined, a function that outputs assembler code to arrange to call\n\
+the function referenced by @var{symbol} at initialization time.\n\
+\n\
+Assume that @var{symbol} is a @code{SYMBOL_REF} for a function taking\n\
+no arguments and with no return value.  If the target supports initialization\n\
+priorities, @var{priority} is a value between 0 and @code{MAX_INIT_PRIORITY};\n\
+otherwise it must be @code{DEFAULT_INIT_PRIORITY}.\n\
+\n\
+If this macro is not defined by the target, a suitable default will\n\
+be chosen if (1) the target supports arbitrary section names, (2) the\n\
+target defines @code{CTORS_SECTION_ASM_OP}, or (3) @code{USE_COLLECT2}\n\
+is not defined.",
+ void, (rtx symbol, int priority), NULL)
+
+/* Output a destructor for a symbol with a given priority.  */
+DEFHOOK
+(destructor,
+ "This is like @code{TARGET_ASM_CONSTRUCTOR} but used for termination\n\
+functions rather than initialization functions.",
+ void, (rtx symbol, int priority), NULL)
+
+/* Output the assembler code for a thunk function.  THUNK_DECL is the
+   declaration for the thunk function itself, FUNCTION is the decl for
+   the target function.  DELTA is an immediate constant offset to be
+   added to THIS.  If VCALL_OFFSET is nonzero, the word at
+   *(*this + vcall_offset) should be added to THIS.  */
+DEFHOOK
+(output_mi_thunk,
+ "A function that outputs the assembler code for a thunk\n\
+function, used to implement C++ virtual function calls with multiple\n\
+inheritance.  The thunk acts as a wrapper around a virtual function,\n\
+adjusting the implicit object parameter before handing control off to\n\
+the real function.\n\
+\n\
+First, emit code to add the integer @var{delta} to the location that\n\
+contains the incoming first argument.  Assume that this argument\n\
+contains a pointer, and is the one used to pass the @code{this} pointer\n\
+in C++.  This is the incoming argument @emph{before} the function prologue,\n\
+e.g.@: @samp{%o0} on a sparc.  The addition must preserve the values of\n\
+all other incoming arguments.\n\
+\n\
+Then, if @var{vcall_offset} is nonzero, an additional adjustment should be\n\
+made after adding @code{delta}.  In particular, if @var{p} is the\n\
+adjusted pointer, the following adjustment should be made:\n\
+\n\
+@smallexample\n\
+p += (*((ptrdiff_t **)p))[vcall_offset/sizeof(ptrdiff_t)]\n\
+@end smallexample\n\
+\n\
+After the additions, emit code to jump to @var{function}, which is a\n\
+@code{FUNCTION_DECL}.  This is a direct pure jump, not a call, and does\n\
+not touch the return address.  Hence returning from @var{FUNCTION} will\n\
+return to whoever called the current @samp{thunk}.\n\
+\n\
+The effect must be as if @var{function} had been called directly with\n\
+the adjusted first argument.  This macro is responsible for emitting all\n\
+of the code for a thunk function; @code{TARGET_ASM_FUNCTION_PROLOGUE}\n\
+and @code{TARGET_ASM_FUNCTION_EPILOGUE} are not invoked.\n\
+\n\
+The @var{thunk_fndecl} is redundant.  (@var{delta} and @var{function}\n\
+have already been extracted from it.)  It might possibly be useful on\n\
+some targets, but probably not.\n\
+\n\
+If you do not define this macro, the target-independent code in the C++\n\
+front end will generate a less efficient heavyweight thunk that calls\n\
+@var{function} instead of jumping to it.  The generic approach does\n\
+not support varargs.",
+ void, (FILE *file, tree thunk_fndecl, HOST_WIDE_INT delta,
+	HOST_WIDE_INT vcall_offset, tree function),
+ NULL)
+
+/* Determine whether output_mi_thunk would succeed.  */
+/* ??? Ideally, this hook would not exist, and success or failure
+   would be returned from output_mi_thunk directly.  But there's
+   too much undo-able setup involved in invoking output_mi_thunk.
+   Could be fixed by making output_mi_thunk emit rtl instead of
+   text to the output file.  */
+DEFHOOK
+(can_output_mi_thunk,
+ "A function that returns true if TARGET_ASM_OUTPUT_MI_THUNK would be able\n\
+to output the assembler code for the thunk function specified by the\n\
+arguments it is passed, and false otherwise.  In the latter case, the\n\
+generic approach will be used by the C++ front end, with the limitations\n\
+previously exposed.",
+ bool, (const_tree thunk_fndecl, HOST_WIDE_INT delta,
+	HOST_WIDE_INT vcall_offset, const_tree function),
+ hook_bool_const_tree_hwi_hwi_const_tree_false)
+
+/* Output any boilerplate text needed at the beginning of a
+   translation unit.  */
+DEFHOOK
+(file_start,
+ "Output to @code{asm_out_file} any text which the assembler expects to\n\
+find at the beginning of a file.  The default behavior is controlled\n\
+by two flags, documented below.  Unless your target's assembler is\n\
+quite unusual, if you override the default, you should call\n\
+@code{default_file_start} at some point in your target hook.  This\n\
+lets other target files rely on these variables.",
+ void, (void),
+ default_file_start)
+
+/* Output any boilerplate text needed at the end of a translation unit.  */
+DEFHOOK
+(file_end,
+ "Output to @code{asm_out_file} any text which the assembler expects\n\
+to find at the end of a file.  The default is to output nothing.",
+ void, (void),
+ hook_void_void)
+
+/* Output any boilerplate text needed at the beginning of an
+   LTO output stream.  */
+DEFHOOK
+(lto_start,
+ "Output to @code{asm_out_file} any text which the assembler expects\n\
+to find at the start of an LTO section.  The default is to output\n\
+nothing.",
+ void, (void),
+ hook_void_void)
+
+/* Output any boilerplate text needed at the end of an
+   LTO output stream.  */
+DEFHOOK
+(lto_end,
+ "Output to @code{asm_out_file} any text which the assembler expects\n\
+to find at the end of an LTO section.  The default is to output\n\
+nothing.",
+ void, (void),
+ hook_void_void)
+
+/* Output any boilerplace text needed at the end of a
+   translation unit before debug and unwind info is emitted.  */
+DEFHOOK
+(code_end,
+ "Output to @code{asm_out_file} any text which is needed before emitting\n\
+unwind info and debug info at the end of a file.  Some targets emit\n\
+here PIC setup thunks that cannot be emitted at the end of file,\n\
+because they couldn't have unwind info then.  The default is to output\n\
+nothing.",
+ void, (void),
+ hook_void_void)
+
+/* Output an assembler pseudo-op to declare a library function name
+   external.  */
+DEFHOOK
+(external_libcall,
+ "This target hook is a function to output to @var{asm_out_file} an assembler\n\
+pseudo-op to declare a library function name external.  The name of the\n\
+library function is given by @var{symref}, which is a @code{symbol_ref}.",
+ void, (rtx symref),
+ default_external_libcall)
+
+/* Output an assembler directive to mark decl live. This instructs
+   linker to not dead code strip this symbol.  */
+DEFHOOK
+(mark_decl_preserved,
+ "This target hook is a function to output to @var{asm_out_file} an assembler\n\
+directive to annotate @var{symbol} as used.  The Darwin target uses the\n\
+.no_dead_code_strip directive.",
+ void, (const char *symbol),
+ hook_void_constcharptr)
+
+/* Output a record of the command line switches that have been passed.  */
+DEFHOOK
+(record_gcc_switches,
+ "Provides the target with the ability to record the gcc command line\n\
+switches provided as argument.\n\
+\n\
+By default this hook is set to NULL, but an example implementation is\n\
+provided for ELF based targets.  Called @var{elf_record_gcc_switches},\n\
+it records the switches as ASCII text inside a new, string mergeable\n\
+section in the assembler output file.  The name of the new section is\n\
+provided by the @code{TARGET_ASM_RECORD_GCC_SWITCHES_SECTION} target\n\
+hook.",
+ void, (const char *),
+ NULL)
+
+/* The name of the section that the example ELF implementation of
+   record_gcc_switches will use to store the information.  Target
+   specific versions of record_gcc_switches may or may not use
+   this information.  */
+DEFHOOKPOD
+(record_gcc_switches_section,
+ "This is the name of the section that will be created by the example\n\
+ELF implementation of the @code{TARGET_ASM_RECORD_GCC_SWITCHES} target\n\
+hook.",
+ const char *, ".GCC.command.line")
+
+/* Output the definition of a section anchor.  */
+DEFHOOK
+(output_anchor,
+ "Write the assembly code to define section anchor @var{x}, which is a\n\
+@code{SYMBOL_REF} for which @samp{SYMBOL_REF_ANCHOR_P (@var{x})} is true.\n\
+The hook is called with the assembly output position set to the beginning\n\
+of @code{SYMBOL_REF_BLOCK (@var{x})}.\n\
+\n\
+If @code{ASM_OUTPUT_DEF} is available, the hook's default definition uses\n\
+it to define the symbol as @samp{. + SYMBOL_REF_BLOCK_OFFSET (@var{x})}.\n\
+If @code{ASM_OUTPUT_DEF} is not available, the hook's default definition\n\
+is @code{NULL}, which disables the use of section anchors altogether.",
+ void, (rtx x),
+ default_asm_output_anchor)
+
+DEFHOOK
+(output_ident,
+ "Output a string based on @var{name}, suitable for the @samp{#ident} \
+ directive, or the equivalent directive or pragma in non-C-family languages. \
+ If this hook is not defined, nothing is output for the @samp{#ident} \
+ directive.",
+ void, (const char *name),
+ hook_void_constcharptr)
+
+/* Output a DTP-relative reference to a TLS symbol.  */
+DEFHOOK
+(output_dwarf_dtprel,
+ "If defined, this target hook is a function which outputs a DTP-relative\n\
+reference to the given TLS symbol of the specified size.",
+ void, (FILE *file, int size, rtx x),
+ NULL)
+
+/* Some target machines need to postscan each insn after it is output.  */
+DEFHOOK
+(final_postscan_insn,
+ "If defined, this target hook is a function which is executed just after the\n\
+output of assembler code for @var{insn}, to change the mode of the assembler\n\
+if necessary.\n\
+\n\
+Here the argument @var{opvec} is the vector containing the operands\n\
+extracted from @var{insn}, and @var{noperands} is the number of\n\
+elements of the vector which contain meaningful data for this insn.\n\
+The contents of this vector are what was used to convert the insn\n\
+template into assembler code, so you can change the assembler mode\n\
+by checking the contents of the vector.",
+ void, (FILE *file, rtx_insn *insn, rtx *opvec, int noperands),
+ NULL)
+
+/* Emit the trampoline template.  This hook may be NULL.  */
+DEFHOOK
+(trampoline_template,
+ "This hook is called by @code{assemble_trampoline_template} to output,\n\
+on the stream @var{f}, assembler code for a block of data that contains\n\
+the constant parts of a trampoline.  This code should not include a\n\
+label---the label is taken care of automatically.\n\
+\n\
+If you do not define this hook, it means no template is needed\n\
+for the target.  Do not define this hook on systems where the block move\n\
+code to copy the trampoline into place would be larger than the code\n\
+to generate it on the spot.",
+ void, (FILE *f),
+ NULL)
+
+DEFHOOK
+(output_source_filename,
+ "Output DWARF debugging information which indicates that filename\
+ @var{name} is the current source file to the stdio stream @var{file}.\n\
+ \n\
+ This target hook need not be defined if the standard form of output\
+ for the file format in use is appropriate.",
+ void ,(FILE *file, const char *name),
+ default_asm_output_source_filename)
+
+DEFHOOK
+(output_addr_const_extra,
+ "A target hook to recognize @var{rtx} patterns that @code{output_addr_const}\n\
+can't deal with, and output assembly code to @var{file} corresponding to\n\
+the pattern @var{x}.  This may be used to allow machine-dependent\n\
+@code{UNSPEC}s to appear within constants.\n\
+\n\
+If target hook fails to recognize a pattern, it must return @code{false},\n\
+so that a standard error message is printed.  If it prints an error message\n\
+itself, by calling, for example, @code{output_operand_lossage}, it may just\n\
+return @code{true}.",
+ bool, (FILE *file, rtx x),
+ hook_bool_FILEptr_rtx_false)
+
+/* ??? The TARGET_PRINT_OPERAND* hooks are part of the asm_out struct,
+   even though that is not reflected in the macro name to override their
+   initializers.  */
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_"
+
+/* Emit a machine-specific insn operand.  */
+/* ??? tm.texi only documents the old macro PRINT_OPERAND,
+   not this  hook, and uses a different name for the argument FILE.  */
+DEFHOOK_UNDOC
+(print_operand,
+ "",
+ void, (FILE *file, rtx x, int code),
+ default_print_operand)
+
+/* Emit a machine-specific memory address.  */
+/* ??? tm.texi only documents the old macro PRINT_OPERAND_ADDRESS,
+   not this  hook, and uses different argument names.  */
+DEFHOOK_UNDOC
+(print_operand_address,
+ "",
+ void, (FILE *file, machine_mode mode, rtx addr),
+ default_print_operand_address)
+
+/* Determine whether CODE is a valid punctuation character for the
+   `print_operand' hook.  */
+/* ??? tm.texi only documents the old macro PRINT_OPERAND_PUNCT_VALID_P,
+   not this  hook.  */
+DEFHOOK_UNDOC
+(print_operand_punct_valid_p,
+ "",
+ bool ,(unsigned char code),
+ default_print_operand_punct_valid_p)
+
+/* Given a symbol name, perform same mangling as assemble_name and
+   ASM_OUTPUT_LABELREF, returning result as an IDENTIFIER_NODE.  */
+DEFHOOK
+(mangle_assembler_name,
+ "Given a symbol @var{name}, perform same mangling as @code{varasm.c}'s\
+ @code{assemble_name}, but in memory rather than to a file stream, returning\
+ result as an @code{IDENTIFIER_NODE}.  Required for correct LTO symtabs.  The\
+ default implementation calls the @code{TARGET_STRIP_NAME_ENCODING} hook and\
+ then prepends the @code{USER_LABEL_PREFIX}, if any.",
+ tree, (const char *name),
+ default_mangle_assembler_name)
+
+HOOK_VECTOR_END (asm_out)
+
+/* Functions relating to instruction scheduling.  All of these
+   default to null pointers, which haifa-sched.c looks for and handles.  */
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_SCHED_"
+HOOK_VECTOR (TARGET_SCHED, sched)
+
+/* Given the current cost, COST, of an insn, INSN, calculate and
+   return a new cost based on its relationship to DEP_INSN through
+   the dependence LINK.  The default is to make no adjustment.  */
+DEFHOOK
+(adjust_cost,
+ "This function corrects the value of @var{cost} based on the\n\
+relationship between @var{insn} and @var{dep_insn} through a\n\
+dependence of type dep_type, and strength @var{dw}.  It should return the new\n\
+value.  The default is to make no adjustment to @var{cost}.  This can be\n\
+used for example to specify to the scheduler using the traditional pipeline\n\
+description that an output- or anti-dependence does not incur the same cost\n\
+as a data-dependence.  If the scheduler using the automaton based pipeline\n\
+description, the cost of anti-dependence is zero and the cost of\n\
+output-dependence is maximum of one and the difference of latency\n\
+times of the first and the second insns.  If these values are not\n\
+acceptable, you could use the hook to modify them too.  See also\n\
+@pxref{Processor pipeline description}.",
+ int, (rtx_insn *insn, int dep_type1, rtx_insn *dep_insn, int cost,
+       unsigned int dw),
+ NULL)
+
+/* Adjust the priority of an insn as you see fit.  Returns the new priority.  */
+DEFHOOK
+(adjust_priority,
+ "This hook adjusts the integer scheduling priority @var{priority} of\n\
+@var{insn}.  It should return the new priority.  Increase the priority to\n\
+execute @var{insn} earlier, reduce the priority to execute @var{insn}\n\
+later.  Do not define this hook if you do not need to adjust the\n\
+scheduling priorities of insns.",
+ int, (rtx_insn *insn, int priority), NULL)
+
+/* Function which returns the maximum number of insns that can be
+   scheduled in the same machine cycle.  This must be constant
+   over an entire compilation.  The default is 1.  */
+DEFHOOK
+(issue_rate,
+ "This hook returns the maximum number of instructions that can ever\n\
+issue at the same time on the target machine.  The default is one.\n\
+Although the insn scheduler can define itself the possibility of issue\n\
+an insn on the same cycle, the value can serve as an additional\n\
+constraint to issue insns on the same simulated processor cycle (see\n\
+hooks @samp{TARGET_SCHED_REORDER} and @samp{TARGET_SCHED_REORDER2}).\n\
+This value must be constant over the entire compilation.  If you need\n\
+it to vary depending on what the instructions are, you must use\n\
+@samp{TARGET_SCHED_VARIABLE_ISSUE}.",
+ int, (void), NULL)
+
+/* Calculate how much this insn affects how many more insns we
+   can emit this cycle.  Default is they all cost the same.  */
+DEFHOOK
+(variable_issue,
+ "This hook is executed by the scheduler after it has scheduled an insn\n\
+from the ready list.  It should return the number of insns which can\n\
+still be issued in the current cycle.  The default is\n\
+@samp{@w{@var{more} - 1}} for insns other than @code{CLOBBER} and\n\
+@code{USE}, which normally are not counted against the issue rate.\n\
+You should define this hook if some insns take more machine resources\n\
+than others, so that fewer insns can follow them in the same cycle.\n\
+@var{file} is either a null pointer, or a stdio stream to write any\n\
+debug output to.  @var{verbose} is the verbose level provided by\n\
+@option{-fsched-verbose-@var{n}}.  @var{insn} is the instruction that\n\
+was scheduled.",
+ int, (FILE *file, int verbose, rtx_insn *insn, int more), NULL)
+
+/* Initialize machine-dependent scheduling code.  */
+DEFHOOK
+(init,
+ "This hook is executed by the scheduler at the beginning of each block of\n\
+instructions that are to be scheduled.  @var{file} is either a null\n\
+pointer, or a stdio stream to write any debug output to.  @var{verbose}\n\
+is the verbose level provided by @option{-fsched-verbose-@var{n}}.\n\
+@var{max_ready} is the maximum number of insns in the current scheduling\n\
+region that can be live at the same time.  This can be used to allocate\n\
+scratch space if it is needed, e.g.@: by @samp{TARGET_SCHED_REORDER}.",
+ void, (FILE *file, int verbose, int max_ready), NULL)
+
+/* Finalize machine-dependent scheduling code.  */
+DEFHOOK
+(finish,
+ "This hook is executed by the scheduler at the end of each block of\n\
+instructions that are to be scheduled.  It can be used to perform\n\
+cleanup of any actions done by the other scheduling hooks.  @var{file}\n\
+is either a null pointer, or a stdio stream to write any debug output\n\
+to.  @var{verbose} is the verbose level provided by\n\
+@option{-fsched-verbose-@var{n}}.",
+ void, (FILE *file, int verbose), NULL)
+
+ /* Initialize machine-dependent function wide scheduling code.  */
+DEFHOOK
+(init_global,
+ "This hook is executed by the scheduler after function level initializations.\n\
+@var{file} is either a null pointer, or a stdio stream to write any debug output to.\n\
+@var{verbose} is the verbose level provided by @option{-fsched-verbose-@var{n}}.\n\
+@var{old_max_uid} is the maximum insn uid when scheduling begins.",
+ void, (FILE *file, int verbose, int old_max_uid), NULL)
+
+/* Finalize machine-dependent function wide scheduling code.  */
+DEFHOOK
+(finish_global,
+ "This is the cleanup hook corresponding to @code{TARGET_SCHED_INIT_GLOBAL}.\n\
+@var{file} is either a null pointer, or a stdio stream to write any debug output to.\n\
+@var{verbose} is the verbose level provided by @option{-fsched-verbose-@var{n}}.",
+ void, (FILE *file, int verbose), NULL)
+
+/* Reorder insns in a machine-dependent fashion, in two different
+       places.  Default does nothing.  */
+DEFHOOK
+(reorder,
+ "This hook is executed by the scheduler after it has scheduled the ready\n\
+list, to allow the machine description to reorder it (for example to\n\
+combine two small instructions together on @samp{VLIW} machines).\n\
+@var{file} is either a null pointer, or a stdio stream to write any\n\
+debug output to.  @var{verbose} is the verbose level provided by\n\
+@option{-fsched-verbose-@var{n}}.  @var{ready} is a pointer to the ready\n\
+list of instructions that are ready to be scheduled.  @var{n_readyp} is\n\
+a pointer to the number of elements in the ready list.  The scheduler\n\
+reads the ready list in reverse order, starting with\n\
+@var{ready}[@var{*n_readyp} @minus{} 1] and going to @var{ready}[0].  @var{clock}\n\
+is the timer tick of the scheduler.  You may modify the ready list and\n\
+the number of ready insns.  The return value is the number of insns that\n\
+can issue this cycle; normally this is just @code{issue_rate}.  See also\n\
+@samp{TARGET_SCHED_REORDER2}.",
+ int, (FILE *file, int verbose, rtx_insn **ready, int *n_readyp, int clock), NULL)
+
+DEFHOOK
+(reorder2,
+ "Like @samp{TARGET_SCHED_REORDER}, but called at a different time.  That\n\
+function is called whenever the scheduler starts a new cycle.  This one\n\
+is called once per iteration over a cycle, immediately after\n\
+@samp{TARGET_SCHED_VARIABLE_ISSUE}; it can reorder the ready list and\n\
+return the number of insns to be scheduled in the same cycle.  Defining\n\
+this hook can be useful if there are frequent situations where\n\
+scheduling one insn causes other insns to become ready in the same\n\
+cycle.  These other insns can then be taken into account properly.",
+ int, (FILE *file, int verbose, rtx_insn **ready, int *n_readyp, int clock), NULL)
+
+DEFHOOK
+(macro_fusion_p,
+ "This hook is used to check whether target platform supports macro fusion.",
+ bool, (void), NULL)
+
+DEFHOOK
+(macro_fusion_pair_p,
+ "This hook is used to check whether two insns should be macro fused for\n\
+a target microarchitecture. If this hook returns true for the given insn pair\n\
+(@var{prev} and @var{curr}), the scheduler will put them into a sched\n\
+group, and they will not be scheduled apart.  The two insns will be either\n\
+two SET insns or a compare and a conditional jump and this hook should\n\
+validate any dependencies needed to fuse the two insns together.",
+ bool, (rtx_insn *prev, rtx_insn *curr), NULL)
+
+/* The following member value is a pointer to a function called
+   after evaluation forward dependencies of insns in chain given
+   by two parameter values (head and tail correspondingly).  */
+DEFHOOK
+(dependencies_evaluation_hook,
+ "This hook is called after evaluation forward dependencies of insns in\n\
+chain given by two parameter values (@var{head} and @var{tail}\n\
+correspondingly) but before insns scheduling of the insn chain.  For\n\
+example, it can be used for better insn classification if it requires\n\
+analysis of dependencies.  This hook can use backward and forward\n\
+dependencies of the insn scheduler because they are already\n\
+calculated.",
+ void, (rtx_insn *head, rtx_insn *tail), NULL)
+
+/* The values of the following four members are pointers to functions
+   used to simplify the automaton descriptions.  dfa_pre_cycle_insn and
+   dfa_post_cycle_insn give functions returning insns which are used to
+   change the pipeline hazard recognizer state when the new simulated
+   processor cycle correspondingly starts and finishes.  The function
+   defined by init_dfa_pre_cycle_insn and init_dfa_post_cycle_insn are
+   used to initialize the corresponding insns.  The default values of
+   the members result in not changing the automaton state when the
+   new simulated processor cycle correspondingly starts and finishes.  */
+
+DEFHOOK
+(init_dfa_pre_cycle_insn,
+ "The hook can be used to initialize data used by the previous hook.",
+ void, (void), NULL)
+
+DEFHOOK
+(dfa_pre_cycle_insn,
+ "The hook returns an RTL insn.  The automaton state used in the\n\
+pipeline hazard recognizer is changed as if the insn were scheduled\n\
+when the new simulated processor cycle starts.  Usage of the hook may\n\
+simplify the automaton pipeline description for some @acronym{VLIW}\n\
+processors.  If the hook is defined, it is used only for the automaton\n\
+based pipeline description.  The default is not to change the state\n\
+when the new simulated processor cycle starts.",
+ rtx, (void), NULL)
+
+DEFHOOK
+(init_dfa_post_cycle_insn,
+ "The hook is analogous to @samp{TARGET_SCHED_INIT_DFA_PRE_CYCLE_INSN} but\n\
+used to initialize data used by the previous hook.",
+ void, (void), NULL)
+
+DEFHOOK
+(dfa_post_cycle_insn,
+ "The hook is analogous to @samp{TARGET_SCHED_DFA_PRE_CYCLE_INSN} but used\n\
+to changed the state as if the insn were scheduled when the new\n\
+simulated processor cycle finishes.",
+ rtx_insn *, (void), NULL)
+
+/* The values of the following two members are pointers to
+   functions used to simplify the automaton descriptions.
+   dfa_pre_advance_cycle and dfa_post_advance_cycle are getting called
+   immediately before and after cycle is advanced.  */
+
+DEFHOOK
+(dfa_pre_advance_cycle,
+ "The hook to notify target that the current simulated cycle is about to finish.\n\
+The hook is analogous to @samp{TARGET_SCHED_DFA_PRE_CYCLE_INSN} but used\n\
+to change the state in more complicated situations - e.g., when advancing\n\
+state on a single insn is not enough.",
+ void, (void), NULL)
+
+DEFHOOK
+(dfa_post_advance_cycle,
+ "The hook to notify target that new simulated cycle has just started.\n\
+The hook is analogous to @samp{TARGET_SCHED_DFA_POST_CYCLE_INSN} but used\n\
+to change the state in more complicated situations - e.g., when advancing\n\
+state on a single insn is not enough.",
+ void, (void), NULL)
+
+/* The following member value is a pointer to a function returning value
+   which defines how many insns in queue `ready' will we try for
+   multi-pass scheduling.  If the member value is nonzero and the
+   function returns positive value, the DFA based scheduler will make
+   multi-pass scheduling for the first cycle.  In other words, we will
+   try to choose ready insn which permits to start maximum number of
+   insns on the same cycle.  */
+DEFHOOK
+(first_cycle_multipass_dfa_lookahead,
+ "This hook controls better choosing an insn from the ready insn queue\n\
+for the @acronym{DFA}-based insn scheduler.  Usually the scheduler\n\
+chooses the first insn from the queue.  If the hook returns a positive\n\
+value, an additional scheduler code tries all permutations of\n\
+@samp{TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD ()}\n\
+subsequent ready insns to choose an insn whose issue will result in\n\
+maximal number of issued insns on the same cycle.  For the\n\
+@acronym{VLIW} processor, the code could actually solve the problem of\n\
+packing simple insns into the @acronym{VLIW} insn.  Of course, if the\n\
+rules of @acronym{VLIW} packing are described in the automaton.\n\
+\n\
+This code also could be used for superscalar @acronym{RISC}\n\
+processors.  Let us consider a superscalar @acronym{RISC} processor\n\
+with 3 pipelines.  Some insns can be executed in pipelines @var{A} or\n\
+@var{B}, some insns can be executed only in pipelines @var{B} or\n\
+@var{C}, and one insn can be executed in pipeline @var{B}.  The\n\
+processor may issue the 1st insn into @var{A} and the 2nd one into\n\
+@var{B}.  In this case, the 3rd insn will wait for freeing @var{B}\n\
+until the next cycle.  If the scheduler issues the 3rd insn the first,\n\
+the processor could issue all 3 insns per cycle.\n\
+\n\
+Actually this code demonstrates advantages of the automaton based\n\
+pipeline hazard recognizer.  We try quickly and easy many insn\n\
+schedules to choose the best one.\n\
+\n\
+The default is no multipass scheduling.",
+ int, (void), NULL)
+
+/* The following member value is pointer to a function controlling
+   what insns from the ready insn queue will be considered for the
+   multipass insn scheduling.  If the hook returns zero for insn
+   passed as the parameter, the insn will be not chosen to be issued.  */
+DEFHOOK
+(first_cycle_multipass_dfa_lookahead_guard,
+ "\n\
+This hook controls what insns from the ready insn queue will be\n\
+considered for the multipass insn scheduling.  If the hook returns\n\
+zero for @var{insn}, the insn will be considered in multipass scheduling.\n\
+Positive return values will remove @var{insn} from consideration on\n\
+the current round of multipass scheduling.\n\
+Negative return values will remove @var{insn} from consideration for given\n\
+number of cycles.\n\
+Backends should be careful about returning non-zero for highest priority\n\
+instruction at position 0 in the ready list.  @var{ready_index} is passed\n\
+to allow backends make correct judgements.\n\
+\n\
+The default is that any ready insns can be chosen to be issued.",
+ int, (rtx_insn *insn, int ready_index), NULL)
+
+/* This hook prepares the target for a new round of multipass
+   scheduling.
+   DATA is a pointer to target-specific data used for multipass scheduling.
+   READY_TRY and N_READY represent the current state of search in the
+   optimization space.  The target can filter out instructions that
+   should not be tried during current round by setting corresponding
+   elements in READY_TRY to non-zero.
+   FIRST_CYCLE_INSN_P is true if this is the first round of multipass
+   scheduling on current cycle.  */
+DEFHOOK
+(first_cycle_multipass_begin,
+ "This hook prepares the target backend for a new round of multipass\n\
+scheduling.",
+ void, (void *data, signed char *ready_try, int n_ready, bool first_cycle_insn_p),
+ NULL)
+
+/* This hook is called when multipass scheduling evaluates instruction INSN.
+   DATA is a pointer to target-specific data that can be used to record effects
+   of INSN on CPU that are not described in DFA.
+   READY_TRY and N_READY represent the current state of search in the
+   optimization space.  The target can filter out instructions that
+   should not be tried after issuing INSN by setting corresponding
+   elements in READY_TRY to non-zero.
+   INSN is the instruction being evaluated.
+   PREV_DATA is a pointer to target-specific data corresponding
+   to a state before issuing INSN.  */
+DEFHOOK
+(first_cycle_multipass_issue,
+ "This hook is called when multipass scheduling evaluates instruction INSN.",
+ void, (void *data, signed char *ready_try, int n_ready, rtx_insn *insn,
+	const void *prev_data), NULL)
+
+/* This hook is called when multipass scheduling backtracks from evaluation of
+   instruction corresponding to DATA.
+   DATA is a pointer to target-specific data that stores the effects
+   of instruction from which the algorithm backtracks on CPU that are not
+   described in DFA.
+   READY_TRY and N_READY represent the current state of search in the
+   optimization space.  The target can filter out instructions that
+   should not be tried after issuing INSN by setting corresponding
+   elements in READY_TRY to non-zero.  */
+DEFHOOK
+(first_cycle_multipass_backtrack,
+ "This is called when multipass scheduling backtracks from evaluation of\n\
+an instruction.",
+ void, (const void *data, signed char *ready_try, int n_ready), NULL)
+
+/* This hook notifies the target about the result of the concluded current
+   round of multipass scheduling.
+   DATA is a pointer.
+   If DATA is non-NULL it points to target-specific data used for multipass
+   scheduling which corresponds to instruction at the start of the chain of
+   the winning solution.  DATA is NULL when multipass scheduling cannot find
+   a good enough solution on current cycle and decides to retry later,
+   usually after advancing the cycle count.  */
+DEFHOOK
+(first_cycle_multipass_end,
+ "This hook notifies the target about the result of the concluded current\n\
+round of multipass scheduling.",
+ void, (const void *data), NULL)
+
+/* This hook is called to initialize target-specific data for multipass
+   scheduling after it has been allocated.
+   DATA is a pointer to target-specific data that stores the effects
+   of instruction from which the algorithm backtracks on CPU that are not
+   described in DFA.  */
+DEFHOOK
+(first_cycle_multipass_init,
+ "This hook initializes target-specific data used in multipass scheduling.",
+ void, (void *data), NULL)
+
+/* This hook is called to finalize target-specific data for multipass
+   scheduling before it is deallocated.
+   DATA is a pointer to target-specific data that stores the effects
+   of instruction from which the algorithm backtracks on CPU that are not
+   described in DFA.  */
+DEFHOOK
+(first_cycle_multipass_fini,
+ "This hook finalizes target-specific data used in multipass scheduling.",
+ void, (void *data), NULL)
+
+/* The following member value is pointer to a function called by
+   the insn scheduler before issuing insn passed as the third
+   parameter on given cycle.  If the hook returns nonzero, the
+   insn is not issued on given processors cycle.  Instead of that,
+   the processor cycle is advanced.  If the value passed through
+   the last parameter is zero, the insn ready queue is not sorted
+   on the new cycle start as usually.  The first parameter passes
+   file for debugging output.  The second one passes the scheduler
+   verbose level of the debugging output.  The forth and the fifth
+   parameter values are correspondingly processor cycle on which
+   the previous insn has been issued and the current processor cycle.  */
+DEFHOOK
+(dfa_new_cycle,
+ "This hook is called by the insn scheduler before issuing @var{insn}\n\
+on cycle @var{clock}.  If the hook returns nonzero,\n\
+@var{insn} is not issued on this processor cycle.  Instead,\n\
+the processor cycle is advanced.  If *@var{sort_p}\n\
+is zero, the insn ready queue is not sorted on the new cycle\n\
+start as usually.  @var{dump} and @var{verbose} specify the file and\n\
+verbosity level to use for debugging output.\n\
+@var{last_clock} and @var{clock} are, respectively, the\n\
+processor cycle on which the previous insn has been issued,\n\
+and the current processor cycle.",
+ int, (FILE *dump, int verbose, rtx_insn *insn, int last_clock,
+       int clock, int *sort_p),
+ NULL)
+
+/* The following member value is a pointer to a function called by the
+   insn scheduler.  It should return true if there exists a dependence
+   which is considered costly by the target, between the insn
+   DEP_PRO (&_DEP), and the insn DEP_CON (&_DEP).  The first parameter is
+   the dep that represents the dependence between the two insns.  The
+   second argument is the cost of the dependence as estimated by
+   the scheduler.  The last argument is the distance in cycles
+   between the already scheduled insn (first parameter) and the
+   second insn (second parameter).  */
+DEFHOOK
+(is_costly_dependence,
+ "This hook is used to define which dependences are considered costly by\n\
+the target, so costly that it is not advisable to schedule the insns that\n\
+are involved in the dependence too close to one another.  The parameters\n\
+to this hook are as follows:  The first parameter @var{_dep} is the dependence\n\
+being evaluated.  The second parameter @var{cost} is the cost of the\n\
+dependence as estimated by the scheduler, and the third\n\
+parameter @var{distance} is the distance in cycles between the two insns.\n\
+The hook returns @code{true} if considering the distance between the two\n\
+insns the dependence between them is considered costly by the target,\n\
+and @code{false} otherwise.\n\
+\n\
+Defining this hook can be useful in multiple-issue out-of-order machines,\n\
+where (a) it's practically hopeless to predict the actual data/resource\n\
+delays, however: (b) there's a better chance to predict the actual grouping\n\
+that will be formed, and (c) correctly emulating the grouping can be very\n\
+important.  In such targets one may want to allow issuing dependent insns\n\
+closer to one another---i.e., closer than the dependence distance;  however,\n\
+not in cases of ``costly dependences'', which this hooks allows to define.",
+ bool, (struct _dep *_dep, int cost, int distance), NULL)
+
+/* The following member value is a pointer to a function called
+   by the insn scheduler. This hook is called to notify the backend
+   that new instructions were emitted.  */
+DEFHOOK
+(h_i_d_extended,
+ "This hook is called by the insn scheduler after emitting a new instruction to\n\
+the instruction stream.  The hook notifies a target backend to extend its\n\
+per instruction data structures.",
+ void, (void), NULL)
+
+/* Next 5 functions are for multi-point scheduling.  */
+
+/* Allocate memory for scheduler context.  */
+DEFHOOK
+(alloc_sched_context,
+ "Return a pointer to a store large enough to hold target scheduling context.",
+ void *, (void), NULL)
+
+/* Fills the context from the local machine scheduler context.  */
+DEFHOOK
+(init_sched_context,
+ "Initialize store pointed to by @var{tc} to hold target scheduling context.\n\
+It @var{clean_p} is true then initialize @var{tc} as if scheduler is at the\n\
+beginning of the block.  Otherwise, copy the current context into @var{tc}.",
+ void, (void *tc, bool clean_p), NULL)
+
+/* Sets local machine scheduler context to a saved value.  */
+DEFHOOK
+(set_sched_context,
+ "Copy target scheduling context pointed to by @var{tc} to the current context.",
+ void, (void *tc), NULL)
+
+/* Clears a scheduler context so it becomes like after init.  */
+DEFHOOK
+(clear_sched_context,
+ "Deallocate internal data in target scheduling context pointed to by @var{tc}.",
+ void, (void *tc), NULL)
+
+/* Frees the scheduler context.  */
+DEFHOOK
+(free_sched_context,
+ "Deallocate a store for target scheduling context pointed to by @var{tc}.",
+ void, (void *tc), NULL)
+
+/* The following member value is a pointer to a function called
+   by the insn scheduler.
+   The first parameter is an instruction, the second parameter is the type
+   of the requested speculation, and the third parameter is a pointer to the
+   speculative pattern of the corresponding type (set if return value == 1).
+   It should return
+   -1, if there is no pattern, that will satisfy the requested speculation type,
+   0, if current pattern satisfies the requested speculation type,
+   1, if pattern of the instruction should be changed to the newly
+   generated one.  */
+DEFHOOK
+(speculate_insn,
+ "This hook is called by the insn scheduler when @var{insn} has only\n\
+speculative dependencies and therefore can be scheduled speculatively.\n\
+The hook is used to check if the pattern of @var{insn} has a speculative\n\
+version and, in case of successful check, to generate that speculative\n\
+pattern.  The hook should return 1, if the instruction has a speculative form,\n\
+or @minus{}1, if it doesn't.  @var{request} describes the type of requested\n\
+speculation.  If the return value equals 1 then @var{new_pat} is assigned\n\
+the generated speculative pattern.",
+ int, (rtx_insn *insn, unsigned int dep_status, rtx *new_pat), NULL)
+
+/* The following member value is a pointer to a function called
+   by the insn scheduler.  It should return true if the check instruction
+   passed as the parameter needs a recovery block.  */
+DEFHOOK
+(needs_block_p,
+ "This hook is called by the insn scheduler during generation of recovery code\n\
+for @var{insn}.  It should return @code{true}, if the corresponding check\n\
+instruction should branch to recovery code, or @code{false} otherwise.",
+ bool, (unsigned int dep_status), NULL)
+
+/* The following member value is a pointer to a function called
+   by the insn scheduler.  It should return a pattern for the check
+   instruction.
+   The first parameter is a speculative instruction, the second parameter
+   is the label of the corresponding recovery block (or null, if it is a
+   simple check).  The third parameter is the kind of speculation that
+   is being performed.  */
+DEFHOOK
+(gen_spec_check,
+ "This hook is called by the insn scheduler to generate a pattern for recovery\n\
+check instruction.  If @var{mutate_p} is zero, then @var{insn} is a\n\
+speculative instruction for which the check should be generated.\n\
+@var{label} is either a label of a basic block, where recovery code should\n\
+be emitted, or a null pointer, when requested check doesn't branch to\n\
+recovery code (a simple check).  If @var{mutate_p} is nonzero, then\n\
+a pattern for a branchy check corresponding to a simple check denoted by\n\
+@var{insn} should be generated.  In this case @var{label} can't be null.",
+ rtx, (rtx_insn *insn, rtx_insn *label, unsigned int ds), NULL)
+
+/* The following member value is a pointer to a function that provides
+   information about the speculation capabilities of the target.
+   The parameter is a pointer to spec_info variable.  */
+DEFHOOK
+(set_sched_flags,
+ "This hook is used by the insn scheduler to find out what features should be\n\
+enabled/used.\n\
+The structure *@var{spec_info} should be filled in by the target.\n\
+The structure describes speculation types that can be used in the scheduler.",
+ void, (struct spec_info_def *spec_info), NULL)
+
+DEFHOOK_UNDOC
+(get_insn_spec_ds,
+ "Return speculation types of instruction @var{insn}.",
+ unsigned int, (rtx_insn *insn), NULL)
+
+DEFHOOK_UNDOC
+(get_insn_checked_ds,
+ "Return speculation types that are checked for instruction @var{insn}",
+ unsigned int, (rtx_insn *insn), NULL)
+
+DEFHOOK
+(can_speculate_insn,
+ "Some instructions should never be speculated by the schedulers, usually\n\
+ because the instruction is too expensive to get this wrong.  Often such\n\
+ instructions have long latency, and often they are not fully modeled in the\n\
+ pipeline descriptions.  This hook should return @code{false} if @var{insn}\n\
+ should not be speculated.",
+ bool, (rtx_insn *insn), hook_bool_rtx_insn_true)
+
+DEFHOOK_UNDOC
+(skip_rtx_p,
+ "Return bool if rtx scanning should just skip current layer and\
+ advance to the inner rtxes.",
+ bool, (const_rtx x), NULL)
+
+/* The following member value is a pointer to a function that provides
+   information about the target resource-based lower bound which is
+   used by the swing modulo scheduler.  The parameter is a pointer
+   to ddg variable.  */
+DEFHOOK
+(sms_res_mii,
+ "This hook is called by the swing modulo scheduler to calculate a\n\
+resource-based lower bound which is based on the resources available in\n\
+the machine and the resources required by each instruction.  The target\n\
+backend can use @var{g} to calculate such bound.  A very simple lower\n\
+bound will be used in case this hook is not implemented: the total number\n\
+of instructions divided by the issue rate.",
+ int, (struct ddg *g), NULL)
+
+/* The following member value is a function that initializes dispatch
+   schedling and adds instructions to dispatch window according to its
+   parameters.  */
+DEFHOOK
+(dispatch_do,
+"This hook is called by Haifa Scheduler.  It performs the operation specified\n\
+in its second parameter.",
+void, (rtx_insn *insn, int x),
+hook_void_rtx_insn_int)
+
+/* The following member value is a function that returns true is
+   dispatch schedling is supported in hardware and condition passed
+   as the second parameter is true.  */
+DEFHOOK
+(dispatch,
+"This hook is called by Haifa Scheduler.  It returns true if dispatch scheduling\n\
+is supported in hardware and the condition specified in the parameter is true.",
+bool, (rtx_insn *insn, int x),
+hook_bool_rtx_insn_int_false)
+
+DEFHOOKPOD
+(exposed_pipeline,
+"True if the processor has an exposed pipeline, which means that not just\n\
+the order of instructions is important for correctness when scheduling, but\n\
+also the latencies of operations.",
+bool, false)
+
+/* The following member value is a function that returns number
+   of operations reassociator should try to put in parallel for
+   statements of the given type.  By default 1 is used.  */
+DEFHOOK
+(reassociation_width,
+"This hook is called by tree reassociator to determine a level of\n\
+parallelism required in output calculations chain.",
+int, (unsigned int opc, machine_mode mode),
+hook_int_uint_mode_1)
+
+/* The following member value is a function that returns priority for
+   fusion of each instruction via pointer parameters.  */
+DEFHOOK
+(fusion_priority,
+"This hook is called by scheduling fusion pass.  It calculates fusion\n\
+priorities for each instruction passed in by parameter.  The priorities\n\
+are returned via pointer parameters.\n\
+\n\
+@var{insn} is the instruction whose priorities need to be calculated.\n\
+@var{max_pri} is the maximum priority can be returned in any cases.\n\
+@var{fusion_pri} is the pointer parameter through which @var{insn}'s\n\
+fusion priority should be calculated and returned.\n\
+@var{pri} is the pointer parameter through which @var{insn}'s priority\n\
+should be calculated and returned.\n\
+\n\
+Same @var{fusion_pri} should be returned for instructions which should\n\
+be scheduled together.  Different @var{pri} should be returned for\n\
+instructions with same @var{fusion_pri}.  @var{fusion_pri} is the major\n\
+sort key, @var{pri} is the minor sort key.  All instructions will be\n\
+scheduled according to the two priorities.  All priorities calculated\n\
+should be between 0 (exclusive) and @var{max_pri} (inclusive).  To avoid\n\
+false dependencies, @var{fusion_pri} of instructions which need to be\n\
+scheduled together should be smaller than @var{fusion_pri} of irrelevant\n\
+instructions.\n\
+\n\
+Given below example:\n\
+\n\
+@smallexample\n\
+    ldr r10, [r1, 4]\n\
+    add r4, r4, r10\n\
+    ldr r15, [r2, 8]\n\
+    sub r5, r5, r15\n\
+    ldr r11, [r1, 0]\n\
+    add r4, r4, r11\n\
+    ldr r16, [r2, 12]\n\
+    sub r5, r5, r16\n\
+@end smallexample\n\
+\n\
+On targets like ARM/AArch64, the two pairs of consecutive loads should be\n\
+merged.  Since peephole2 pass can't help in this case unless consecutive\n\
+loads are actually next to each other in instruction flow.  That's where\n\
+this scheduling fusion pass works.  This hook calculates priority for each\n\
+instruction based on its fustion type, like:\n\
+\n\
+@smallexample\n\
+    ldr r10, [r1, 4]  ; fusion_pri=99,  pri=96\n\
+    add r4, r4, r10   ; fusion_pri=100, pri=100\n\
+    ldr r15, [r2, 8]  ; fusion_pri=98,  pri=92\n\
+    sub r5, r5, r15   ; fusion_pri=100, pri=100\n\
+    ldr r11, [r1, 0]  ; fusion_pri=99,  pri=100\n\
+    add r4, r4, r11   ; fusion_pri=100, pri=100\n\
+    ldr r16, [r2, 12] ; fusion_pri=98,  pri=88\n\
+    sub r5, r5, r16   ; fusion_pri=100, pri=100\n\
+@end smallexample\n\
+\n\
+Scheduling fusion pass then sorts all ready to issue instructions according\n\
+to the priorities.  As a result, instructions of same fusion type will be\n\
+pushed together in instruction flow, like:\n\
+\n\
+@smallexample\n\
+    ldr r11, [r1, 0]\n\
+    ldr r10, [r1, 4]\n\
+    ldr r15, [r2, 8]\n\
+    ldr r16, [r2, 12]\n\
+    add r4, r4, r10\n\
+    sub r5, r5, r15\n\
+    add r4, r4, r11\n\
+    sub r5, r5, r16\n\
+@end smallexample\n\
+\n\
+Now peephole2 pass can simply merge the two pairs of loads.\n\
+\n\
+Since scheduling fusion pass relies on peephole2 to do real fusion\n\
+work, it is only enabled by default when peephole2 is in effect.\n\
+\n\
+This is firstly introduced on ARM/AArch64 targets, please refer to\n\
+the hook implementation for how different fusion types are supported.",
+void, (rtx_insn *insn, int max_pri, int *fusion_pri, int *pri), NULL)
+
+HOOK_VECTOR_END (sched)
+
+/* Functions relating to OpenMP SIMD and __attribute__((simd)) clones.  */
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_SIMD_CLONE_"
+HOOK_VECTOR (TARGET_SIMD_CLONE, simd_clone)
+
+DEFHOOK
+(compute_vecsize_and_simdlen,
+"This hook should set @var{vecsize_mangle}, @var{vecsize_int}, @var{vecsize_float}\n\
+fields in @var{simd_clone} structure pointed by @var{clone_info} argument and also\n\
+@var{simdlen} field if it was previously 0.\n\
+The hook should return 0 if SIMD clones shouldn't be emitted,\n\
+or number of @var{vecsize_mangle} variants that should be emitted.",
+int, (struct cgraph_node *, struct cgraph_simd_clone *, tree, int), NULL)
+
+DEFHOOK
+(adjust,
+"This hook should add implicit @code{attribute(target(\"...\"))} attribute\n\
+to SIMD clone @var{node} if needed.",
+void, (struct cgraph_node *), NULL)
+
+DEFHOOK
+(usable,
+"This hook should return -1 if SIMD clone @var{node} shouldn't be used\n\
+in vectorized loops in current function, or non-negative number if it is\n\
+usable.  In that case, the smaller the number is, the more desirable it is\n\
+to use it.",
+int, (struct cgraph_node *), NULL)
+
+HOOK_VECTOR_END (simd_clone)
+
+/* Functions relating to OpenMP SIMT vectorization transform.  */
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_SIMT_"
+HOOK_VECTOR (TARGET_SIMT, simt)
+
+DEFHOOK
+(vf,
+"Return number of threads in SIMT thread group on the target.",
+int, (void), NULL)
+
+HOOK_VECTOR_END (simt)
+
+/* Functions relating to OpenMP.  */
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_OMP_"
+HOOK_VECTOR (TARGET_OMP, omp)
+
+DEFHOOK
+(device_kind_arch_isa,
+"Return 1 if @var{trait} @var{name} is present in the OpenMP context's\n\
+device trait set, return 0 if not present in any OpenMP context in the\n\
+whole translation unit, or -1 if not present in the current OpenMP context\n\
+but might be present in another OpenMP context in the same TU.",
+int, (enum omp_device_kind_arch_isa trait, const char *name), NULL)
+
+HOOK_VECTOR_END (omp)
+
+/* Functions relating to openacc.  */
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_GOACC_"
+HOOK_VECTOR (TARGET_GOACC, goacc)
+
+DEFHOOK
+(validate_dims,
+"This hook should check the launch dimensions provided for an OpenACC\n\
+compute region, or routine.  Defaulted values are represented as -1\n\
+and non-constant values as 0.  The @var{fn_level} is negative for the\n\
+function corresponding to the compute region.  For a routine it is the\n\
+outermost level at which partitioned execution may be spawned.  The hook\n\
+should verify non-default values.  If DECL is NULL, global defaults\n\
+are being validated and unspecified defaults should be filled in.\n\
+Diagnostics should be issued as appropriate.  Return\n\
+true, if changes have been made.  You must override this hook to\n\
+provide dimensions larger than 1.",
+bool, (tree decl, int *dims, int fn_level, unsigned used),
+default_goacc_validate_dims)
+
+DEFHOOK
+(dim_limit,
+"This hook should return the maximum size of a particular dimension,\n\
+or zero if unbounded.",
+int, (int axis),
+default_goacc_dim_limit)
+
+DEFHOOK
+(fork_join,
+"This hook can be used to convert IFN_GOACC_FORK and IFN_GOACC_JOIN\n\
+function calls to target-specific gimple, or indicate whether they\n\
+should be retained.  It is executed during the oacc_device_lower pass.\n\
+It should return true, if the call should be retained.  It should\n\
+return false, if it is to be deleted (either because target-specific\n\
+gimple has been inserted before it, or there is no need for it).\n\
+The default hook returns false, if there are no RTL expanders for them.",
+bool, (gcall *call, const int *dims, bool is_fork),
+default_goacc_fork_join)
+
+DEFHOOK
+(reduction,
+"This hook is used by the oacc_transform pass to expand calls to the\n\
+@var{GOACC_REDUCTION} internal function, into a sequence of gimple\n\
+instructions.  @var{call} is gimple statement containing the call to\n\
+the function.  This hook removes statement @var{call} after the\n\
+expanded sequence has been inserted.  This hook is also responsible\n\
+for allocating any storage for reductions when necessary.",
+void, (gcall *call),
+default_goacc_reduction)
+
+HOOK_VECTOR_END (goacc)
+
+/* Functions relating to vectorization.  */
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_VECTORIZE_"
+HOOK_VECTOR (TARGET_VECTORIZE, vectorize)
+
+/* The following member value is a pointer to a function called
+   by the vectorizer, and return the decl of the target builtin
+   function.  */
+DEFHOOK
+(builtin_mask_for_load,
+ "This hook should return the DECL of a function @var{f} that given an\n\
+address @var{addr} as an argument returns a mask @var{m} that can be\n\
+used to extract from two vectors the relevant data that resides in\n\
+@var{addr} in case @var{addr} is not properly aligned.\n\
+\n\
+The autovectorizer, when vectorizing a load operation from an address\n\
+@var{addr} that may be unaligned, will generate two vector loads from\n\
+the two aligned addresses around @var{addr}. It then generates a\n\
+@code{REALIGN_LOAD} operation to extract the relevant data from the\n\
+two loaded vectors. The first two arguments to @code{REALIGN_LOAD},\n\
+@var{v1} and @var{v2}, are the two vectors, each of size @var{VS}, and\n\
+the third argument, @var{OFF}, defines how the data will be extracted\n\
+from these two vectors: if @var{OFF} is 0, then the returned vector is\n\
+@var{v2}; otherwise, the returned vector is composed from the last\n\
+@var{VS}-@var{OFF} elements of @var{v1} concatenated to the first\n\
+@var{OFF} elements of @var{v2}.\n\
+\n\
+If this hook is defined, the autovectorizer will generate a call\n\
+to @var{f} (using the DECL tree that this hook returns) and will\n\
+use the return value of @var{f} as the argument @var{OFF} to\n\
+@code{REALIGN_LOAD}. Therefore, the mask @var{m} returned by @var{f}\n\
+should comply with the semantics expected by @code{REALIGN_LOAD}\n\
+described above.\n\
+If this hook is not defined, then @var{addr} will be used as\n\
+the argument @var{OFF} to @code{REALIGN_LOAD}, in which case the low\n\
+log2(@var{VS}) @minus{} 1 bits of @var{addr} will be considered.",
+ tree, (void), NULL)
+
+/* Returns a built-in function that realizes the vectorized version of
+   a target-independent function, or NULL_TREE if not available.  */
+DEFHOOK
+(builtin_vectorized_function,
+ "This hook should return the decl of a function that implements the\n\
+vectorized variant of the function with the @code{combined_fn} code\n\
+@var{code} or @code{NULL_TREE} if such a function is not available.\n\
+The return type of the vectorized function shall be of vector type\n\
+@var{vec_type_out} and the argument types should be @var{vec_type_in}.",
+ tree, (unsigned code, tree vec_type_out, tree vec_type_in),
+ default_builtin_vectorized_function)
+
+/* Returns a built-in function that realizes the vectorized version of
+   a target-specific function, or NULL_TREE if not available.  */
+DEFHOOK
+(builtin_md_vectorized_function,
+ "This hook should return the decl of a function that implements the\n\
+vectorized variant of target built-in function @code{fndecl}.  The\n\
+return type of the vectorized function shall be of vector type\n\
+@var{vec_type_out} and the argument types should be @var{vec_type_in}.",
+ tree, (tree fndecl, tree vec_type_out, tree vec_type_in),
+ default_builtin_md_vectorized_function)
+
+/* Cost of different vector/scalar statements in vectorization cost
+   model. In case of misaligned vector loads and stores the cost depends
+   on the data type and misalignment value.  */
+DEFHOOK
+(builtin_vectorization_cost,
+ "Returns cost of different scalar or vector statements for vectorization cost model.\n\
+For vector memory operations the cost may depend on type (@var{vectype}) and\n\
+misalignment value (@var{misalign}).",
+ int, (enum vect_cost_for_stmt type_of_cost, tree vectype, int misalign),
+ default_builtin_vectorization_cost)
+
+DEFHOOK
+(preferred_vector_alignment,
+ "This hook returns the preferred alignment in bits for accesses to\n\
+vectors of type @var{type} in vectorized code.  This might be less than\n\
+or greater than the ABI-defined value returned by\n\
+@code{TARGET_VECTOR_ALIGNMENT}.  It can be equal to the alignment of\n\
+a single element, in which case the vectorizer will not try to optimize\n\
+for alignment.\n\
+\n\
+The default hook returns @code{TYPE_ALIGN (@var{type})}, which is\n\
+correct for most targets.",
+ poly_uint64, (const_tree type),
+ default_preferred_vector_alignment)
+
+/* Return true if vector alignment is reachable (by peeling N
+   iterations) for the given scalar type.  */
+DEFHOOK
+(vector_alignment_reachable,
+ "Return true if vector alignment is reachable (by peeling N iterations) for the given scalar type @var{type}.  @var{is_packed} is false if the scalar access using @var{type} is known to be naturally aligned.",
+ bool, (const_tree type, bool is_packed),
+ default_builtin_vector_alignment_reachable)
+
+DEFHOOK
+(vec_perm_const,
+ "This hook is used to test whether the target can permute up to two\n\
+vectors of mode @var{mode} using the permutation vector @code{sel}, and\n\
+also to emit such a permutation.  In the former case @var{in0}, @var{in1}\n\
+and @var{out} are all null.  In the latter case @var{in0} and @var{in1} are\n\
+the source vectors and @var{out} is the destination vector; all three are\n\
+registers of mode @var{mode}.  @var{in1} is the same as @var{in0} if\n\
+@var{sel} describes a permutation on one vector instead of two.\n\
+\n\
+Return true if the operation is possible, emitting instructions for it\n\
+if rtxes are provided.\n\
+\n\
+@cindex @code{vec_perm@var{m}} instruction pattern\n\
+If the hook returns false for a mode with multibyte elements, GCC will\n\
+try the equivalent byte operation.  If that also fails, it will try forcing\n\
+the selector into a register and using the @var{vec_perm@var{mode}}\n\
+instruction pattern.  There is no need for the hook to handle these two\n\
+implementation approaches itself.",
+ bool, (machine_mode mode, rtx output, rtx in0, rtx in1,
+	const vec_perm_indices &sel),
+ NULL)
+
+/* Return true if the target supports misaligned store/load of a
+   specific factor denoted in the third parameter.  The last parameter
+   is true if the access is defined in a packed struct.  */
+DEFHOOK
+(support_vector_misalignment,
+ "This hook should return true if the target supports misaligned vector\n\
+store/load of a specific factor denoted in the @var{misalignment}\n\
+parameter.  The vector store/load should be of machine mode @var{mode} and\n\
+the elements in the vectors should be of type @var{type}.  @var{is_packed}\n\
+parameter is true if the memory access is defined in a packed struct.",
+ bool,
+ (machine_mode mode, const_tree type, int misalignment, bool is_packed),
+ default_builtin_support_vector_misalignment)
+
+/* Returns the preferred mode for SIMD operations for the specified
+   scalar mode.  */
+DEFHOOK
+(preferred_simd_mode,
+ "This hook should return the preferred mode for vectorizing scalar\n\
+mode @var{mode}.  The default is\n\
+equal to @code{word_mode}, because the vectorizer can do some\n\
+transformations even in absence of specialized @acronym{SIMD} hardware.",
+ machine_mode,
+ (scalar_mode mode),
+ default_preferred_simd_mode)
+
+/* Returns the preferred mode for splitting SIMD reductions to.  */
+DEFHOOK
+(split_reduction,
+ "This hook should return the preferred mode to split the final reduction\n\
+step on @var{mode} to.  The reduction is then carried out reducing upper\n\
+against lower halves of vectors recursively until the specified mode is\n\
+reached.  The default is @var{mode} which means no splitting.",
+  machine_mode,
+  (machine_mode),
+  default_split_reduction)
+
+/* Returns a mask of vector sizes to iterate over when auto-vectorizing
+   after processing the preferred one derived from preferred_simd_mode.  */
+DEFHOOK
+(autovectorize_vector_modes,
+ "If using the mode returned by @code{TARGET_VECTORIZE_PREFERRED_SIMD_MODE}\n\
+is not the only approach worth considering, this hook should add one mode to\n\
+@var{modes} for each useful alternative approach.  These modes are then\n\
+passed to @code{TARGET_VECTORIZE_RELATED_MODE} to obtain the vector mode\n\
+for a given element mode.\n\
+\n\
+The modes returned in @var{modes} should use the smallest element mode\n\
+possible for the vectorization approach that they represent, preferring\n\
+integer modes over floating-poing modes in the event of a tie.  The first\n\
+mode should be the @code{TARGET_VECTORIZE_PREFERRED_SIMD_MODE} for its\n\
+element mode.\n\
+\n\
+If @var{all} is true, add suitable vector modes even when they are generally\n\
+not expected to be worthwhile.\n\
+\n\
+The hook returns a bitmask of flags that control how the modes in\n\
+@var{modes} are used.  The flags are:\n\
+@table @code\n\
+@item VECT_COMPARE_COSTS\n\
+Tells the loop vectorizer to try all the provided modes and pick the one\n\
+with the lowest cost.  By default the vectorizer will choose the first\n\
+mode that works.\n\
+@end table\n\
+\n\
+The hook does not need to do anything if the vector returned by\n\
+@code{TARGET_VECTORIZE_PREFERRED_SIMD_MODE} is the only one relevant\n\
+for autovectorization.  The default implementation adds no modes and\n\
+returns 0.",
+ unsigned int,
+ (vector_modes *modes, bool all),
+ default_autovectorize_vector_modes)
+
+DEFHOOK
+(related_mode,
+ "If a piece of code is using vector mode @var{vector_mode} and also wants\n\
+to operate on elements of mode @var{element_mode}, return the vector mode\n\
+it should use for those elements.  If @var{nunits} is nonzero, ensure that\n\
+the mode has exactly @var{nunits} elements, otherwise pick whichever vector\n\
+size pairs the most naturally with @var{vector_mode}.  Return an empty\n\
+@code{opt_machine_mode} if there is no supported vector mode with the\n\
+required properties.\n\
+\n\
+There is no prescribed way of handling the case in which @var{nunits}\n\
+is zero.  One common choice is to pick a vector mode with the same size\n\
+as @var{vector_mode}; this is the natural choice if the target has a\n\
+fixed vector size.  Another option is to choose a vector mode with the\n\
+same number of elements as @var{vector_mode}; this is the natural choice\n\
+if the target has a fixed number of elements.  Alternatively, the hook\n\
+might choose a middle ground, such as trying to keep the number of\n\
+elements as similar as possible while applying maximum and minimum\n\
+vector sizes.\n\
+\n\
+The default implementation uses @code{mode_for_vector} to find the\n\
+requested mode, returning a mode with the same size as @var{vector_mode}\n\
+when @var{nunits} is zero.  This is the correct behavior for most targets.",
+ opt_machine_mode,
+ (machine_mode vector_mode, scalar_mode element_mode, poly_uint64 nunits),
+ default_vectorize_related_mode)
+
+/* Function to get a target mode for a vector mask.  */
+DEFHOOK
+(get_mask_mode,
+ "Return the mode to use for a vector mask that holds one boolean\n\
+result for each element of vector mode @var{mode}.  The returned mask mode\n\
+can be a vector of integers (class @code{MODE_VECTOR_INT}), a vector of\n\
+booleans (class @code{MODE_VECTOR_BOOL}) or a scalar integer (class\n\
+@code{MODE_INT}).  Return an empty @code{opt_machine_mode} if no such\n\
+mask mode exists.\n\
+\n\
+The default implementation returns a @code{MODE_VECTOR_INT} with the\n\
+same size and number of elements as @var{mode}, if such a mode exists.",
+ opt_machine_mode,
+ (machine_mode mode),
+ default_get_mask_mode)
+
+/* Function to say whether a masked operation is expensive when the
+   mask is all zeros.  */
+DEFHOOK
+(empty_mask_is_expensive,
+ "This hook returns true if masked internal function @var{ifn} (really of\n\
+type @code{internal_fn}) should be considered expensive when the mask is\n\
+all zeros.  GCC can then try to branch around the instruction instead.",
+ bool,
+ (unsigned ifn),
+ default_empty_mask_is_expensive)
+
+/* Target builtin that implements vector gather operation.  */
+DEFHOOK
+(builtin_gather,
+ "Target builtin that implements vector gather operation.  @var{mem_vectype}\n\
+is the vector type of the load and @var{index_type} is scalar type of\n\
+the index, scaled by @var{scale}.\n\
+The default is @code{NULL_TREE} which means to not vectorize gather\n\
+loads.",
+ tree,
+ (const_tree mem_vectype, const_tree index_type, int scale),
+ NULL)
+
+/* Target builtin that implements vector scatter operation.  */
+DEFHOOK
+(builtin_scatter,
+"Target builtin that implements vector scatter operation.  @var{vectype}\n\
+is the vector type of the store and @var{index_type} is scalar type of\n\
+the index, scaled by @var{scale}.\n\
+The default is @code{NULL_TREE} which means to not vectorize scatter\n\
+stores.",
+ tree,
+ (const_tree vectype, const_tree index_type, int scale),
+ NULL)
+
+/* Target function to initialize the cost model for a loop or block.  */
+DEFHOOK
+(init_cost,
+ "This hook should initialize target-specific data structures in preparation "
+ "for modeling the costs of vectorizing a loop or basic block.  The default "
+ "allocates three unsigned integers for accumulating costs for the prologue, "
+ "body, and epilogue of the loop or basic block.  If @var{loop_info} is "
+ "non-NULL, it identifies the loop being vectorized; otherwise a single block "
+ "is being vectorized.",
+ void *,
+ (class loop *loop_info),
+ default_init_cost)
+
+/* Target function to record N statements of the given kind using the
+   given vector type within the cost model data for the current loop or
+    block.  */
+DEFHOOK
+(add_stmt_cost,
+ "This hook should update the target-specific @var{data} in response to "
+ "adding @var{count} copies of the given @var{kind} of statement to a "
+ "loop or basic block.  The default adds the builtin vectorizer cost for "
+ "the copies of the statement to the accumulator specified by @var{where}, "
+ "(the prologue, body, or epilogue) and returns the amount added.  The "
+ "return value should be viewed as a tentative cost that may later be "
+ "revised.",
+ unsigned,
+ (class vec_info *, void *data, int count, enum vect_cost_for_stmt kind,
+  class _stmt_vec_info *stmt_info, tree vectype, int misalign,
+  enum vect_cost_model_location where),
+ default_add_stmt_cost)
+
+/* Target function to calculate the total cost of the current vectorized
+   loop or block.  */
+DEFHOOK
+(finish_cost,
+ "This hook should complete calculations of the cost of vectorizing a loop "
+ "or basic block based on @var{data}, and return the prologue, body, and "
+ "epilogue costs as unsigned integers.  The default returns the value of "
+ "the three accumulators.",
+ void,
+ (void *data, unsigned *prologue_cost, unsigned *body_cost,
+  unsigned *epilogue_cost),
+ default_finish_cost)
+
+/* Function to delete target-specific cost modeling data.  */
+DEFHOOK
+(destroy_cost_data,
+ "This hook should release @var{data} and any related data structures "
+ "allocated by TARGET_VECTORIZE_INIT_COST.  The default releases the "
+ "accumulator.",
+ void,
+ (void *data),
+ default_destroy_cost_data)
+
+HOOK_VECTOR_END (vectorize)
+
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_"
+
+DEFHOOK
+(preferred_else_value,
+ "This hook returns the target's preferred final argument for a call\n\
+to conditional internal function @var{ifn} (really of type\n\
+@code{internal_fn}).  @var{type} specifies the return type of the\n\
+function and @var{ops} are the operands to the conditional operation,\n\
+of which there are @var{nops}.\n\
+\n\
+For example, if @var{ifn} is @code{IFN_COND_ADD}, the hook returns\n\
+a value of type @var{type} that should be used when @samp{@var{ops}[0]}\n\
+and @samp{@var{ops}[1]} are conditionally added together.\n\
+\n\
+This hook is only relevant if the target supports conditional patterns\n\
+like @code{cond_add@var{m}}.  The default implementation returns a zero\n\
+constant of type @var{type}.",
+ tree,
+ (unsigned ifn, tree type, unsigned nops, tree *ops),
+ default_preferred_else_value)
+
+DEFHOOK
+(record_offload_symbol,
+ "Used when offloaded functions are seen in the compilation unit and no named\n\
+sections are available.  It is called once for each symbol that must be\n\
+recorded in the offload function and variable table.",
+ void, (tree),
+ hook_void_tree)
+
+DEFHOOKPOD
+(absolute_biggest_alignment,
+ "If defined, this target hook specifies the absolute biggest alignment\n\
+that a type or variable can have on this machine, otherwise,\n\
+@code{BIGGEST_ALIGNMENT} is used.",
+ HOST_WIDE_INT, BIGGEST_ALIGNMENT)
+
+/* Allow target specific overriding of option settings after options have
+  been changed by an attribute or pragma or when it is reset at the
+  end of the code affected by an attribute or pragma.  */
+DEFHOOK
+(override_options_after_change,
+ "This target function is similar to the hook @code{TARGET_OPTION_OVERRIDE}\n\
+but is called when the optimize level is changed via an attribute or\n\
+pragma or when it is reset at the end of the code affected by the\n\
+attribute or pragma.  It is not called at the beginning of compilation\n\
+when @code{TARGET_OPTION_OVERRIDE} is called so if you want to perform these\n\
+actions then, you should have @code{TARGET_OPTION_OVERRIDE} call\n\
+@code{TARGET_OVERRIDE_OPTIONS_AFTER_CHANGE}.",
+ void, (void),
+ hook_void_void)
+
+DEFHOOK
+(offload_options,
+ "Used when writing out the list of options into an LTO file.  It should\n\
+translate any relevant target-specific options (such as the ABI in use)\n\
+into one of the @option{-foffload} options that exist as a common interface\n\
+to express such options.  It should return a string containing these options,\n\
+separated by spaces, which the caller will free.\n",
+char *, (void), hook_charptr_void_null)
+
+DEFHOOK_UNDOC
+(eh_return_filter_mode,
+ "Return machine mode for filter value.",
+ scalar_int_mode, (void),
+ default_eh_return_filter_mode)
+
+/* Return machine mode for libgcc expanded cmp instructions.  */
+DEFHOOK
+(libgcc_cmp_return_mode,
+ "This target hook should return the mode to be used for the return value\n\
+of compare instructions expanded to libgcc calls.  If not defined\n\
+@code{word_mode} is returned which is the right choice for a majority of\n\
+targets.",
+ scalar_int_mode, (void),
+ default_libgcc_cmp_return_mode)
+
+/* Return machine mode for libgcc expanded shift instructions.  */
+DEFHOOK
+(libgcc_shift_count_mode,
+ "This target hook should return the mode to be used for the shift count operand\n\
+of shift instructions expanded to libgcc calls.  If not defined\n\
+@code{word_mode} is returned which is the right choice for a majority of\n\
+targets.",
+ scalar_int_mode, (void),
+ default_libgcc_shift_count_mode)
+
+/* Return machine mode to be used for _Unwind_Word type.  */
+DEFHOOK
+(unwind_word_mode,
+ "Return machine mode to be used for @code{_Unwind_Word} type.\n\
+The default is to use @code{word_mode}.",
+ scalar_int_mode, (void),
+ default_unwind_word_mode)
+
+/* Given two decls, merge their attributes and return the result.  */
+DEFHOOK
+(merge_decl_attributes,
+ "Define this target hook if the merging of decl attributes needs special\n\
+handling.  If defined, the result is a list of the combined\n\
+@code{DECL_ATTRIBUTES} of @var{olddecl} and @var{newdecl}.\n\
+@var{newdecl} is a duplicate declaration of @var{olddecl}.  Examples of\n\
+when this is needed are when one attribute overrides another, or when an\n\
+attribute is nullified by a subsequent definition.  This function may\n\
+call @code{merge_attributes} to handle machine-independent merging.\n\
+\n\
+@findex TARGET_DLLIMPORT_DECL_ATTRIBUTES\n\
+If the only target-specific handling you require is @samp{dllimport}\n\
+for Microsoft Windows targets, you should define the macro\n\
+@code{TARGET_DLLIMPORT_DECL_ATTRIBUTES} to @code{1}.  The compiler\n\
+will then define a function called\n\
+@code{merge_dllimport_decl_attributes} which can then be defined as\n\
+the expansion of @code{TARGET_MERGE_DECL_ATTRIBUTES}.  You can also\n\
+add @code{handle_dll_attribute} in the attribute table for your port\n\
+to perform initial processing of the @samp{dllimport} and\n\
+@samp{dllexport} attributes.  This is done in @file{i386/cygwin.h} and\n\
+@file{i386/i386.c}, for example.",
+ tree, (tree olddecl, tree newdecl),
+ merge_decl_attributes)
+
+/* Given two types, merge their attributes and return the result.  */
+DEFHOOK
+(merge_type_attributes,
+ "Define this target hook if the merging of type attributes needs special\n\
+handling.  If defined, the result is a list of the combined\n\
+@code{TYPE_ATTRIBUTES} of @var{type1} and @var{type2}.  It is assumed\n\
+that @code{comptypes} has already been called and returned 1.  This\n\
+function may call @code{merge_attributes} to handle machine-independent\n\
+merging.",
+ tree, (tree type1, tree type2),
+ merge_type_attributes)
+
+/* Table of machine attributes and functions to handle them.
+   Ignored if NULL.  */
+DEFHOOKPOD
+(attribute_table,
+ "If defined, this target hook points to an array of @samp{struct\n\
+attribute_spec} (defined in @file{tree-core.h}) specifying the machine\n\
+specific attributes for this target and some of the restrictions on the\n\
+entities to which these attributes are applied and the arguments they\n\
+take.",
+ const struct attribute_spec *, NULL)
+
+/* Return true iff attribute NAME expects a plain identifier as its first
+   argument.  */
+DEFHOOK
+(attribute_takes_identifier_p,
+ "If defined, this target hook is a function which returns true if the\n\
+machine-specific attribute named @var{name} expects an identifier\n\
+given as its first argument to be passed on as a plain identifier, not\n\
+subjected to name lookup.  If this is not defined, the default is\n\
+false for all machine-specific attributes.",
+ bool, (const_tree name),
+ hook_bool_const_tree_false)
+
+/* Return zero if the attributes on TYPE1 and TYPE2 are incompatible,
+   one if they are compatible and two if they are nearly compatible
+   (which causes a warning to be generated).  */
+DEFHOOK
+(comp_type_attributes,
+ "If defined, this target hook is a function which returns zero if the attributes on\n\
+@var{type1} and @var{type2} are incompatible, one if they are compatible,\n\
+and two if they are nearly compatible (which causes a warning to be\n\
+generated).  If this is not defined, machine-specific attributes are\n\
+supposed always to be compatible.",
+ int, (const_tree type1, const_tree type2),
+ hook_int_const_tree_const_tree_1)
+
+/* Assign default attributes to the newly defined TYPE.  */
+DEFHOOK
+(set_default_type_attributes,
+ "If defined, this target hook is a function which assigns default attributes to\n\
+the newly defined @var{type}.",
+ void, (tree type),
+ hook_void_tree)
+
+/* Insert attributes on the newly created DECL.  */
+DEFHOOK
+(insert_attributes,
+ "Define this target hook if you want to be able to add attributes to a decl\n\
+when it is being created.  This is normally useful for back ends which\n\
+wish to implement a pragma by using the attributes which correspond to\n\
+the pragma's effect.  The @var{node} argument is the decl which is being\n\
+created.  The @var{attr_ptr} argument is a pointer to the attribute list\n\
+for this decl.  The list itself should not be modified, since it may be\n\
+shared with other decls, but attributes may be chained on the head of\n\
+the list and @code{*@var{attr_ptr}} modified to point to the new\n\
+attributes, or a copy of the list may be made if further changes are\n\
+needed.",
+ void, (tree node, tree *attr_ptr),
+ hook_void_tree_treeptr)
+
+/* Perform additional target-specific processing of generic attributes.  */
+DEFHOOK
+(handle_generic_attribute,
+ "Define this target hook if you want to be able to perform additional\n\
+target-specific processing of an attribute which is handled generically\n\
+by a front end.  The arguments are the same as those which are passed to\n\
+attribute handlers.  So far this only affects the @var{noinit} and\n\
+@var{section} attribute.",
+ tree, (tree *node, tree name, tree args, int flags, bool *no_add_attrs),
+ hook_tree_treeptr_tree_tree_int_boolptr_null)
+
+/* Return true if FNDECL (which has at least one machine attribute)
+   can be inlined despite its machine attributes, false otherwise.  */
+DEFHOOK
+(function_attribute_inlinable_p,
+ "@cindex inlining\n\
+This target hook returns @code{true} if it is OK to inline @var{fndecl}\n\
+into the current function, despite its having target-specific\n\
+attributes, @code{false} otherwise.  By default, if a function has a\n\
+target specific attribute attached to it, it will not be inlined.",
+ bool, (const_tree fndecl),
+ hook_bool_const_tree_false)
+
+/* Return true if bitfields in RECORD_TYPE should follow the
+   Microsoft Visual C++ bitfield layout rules.  */
+DEFHOOK
+(ms_bitfield_layout_p,
+ "This target hook returns @code{true} if bit-fields in the given\n\
+@var{record_type} are to be laid out following the rules of Microsoft\n\
+Visual C/C++, namely: (i) a bit-field won't share the same storage\n\
+unit with the previous bit-field if their underlying types have\n\
+different sizes, and the bit-field will be aligned to the highest\n\
+alignment of the underlying types of itself and of the previous\n\
+bit-field; (ii) a zero-sized bit-field will affect the alignment of\n\
+the whole enclosing structure, even if it is unnamed; except that\n\
+(iii) a zero-sized bit-field will be disregarded unless it follows\n\
+another bit-field of nonzero size.  If this hook returns @code{true},\n\
+other macros that control bit-field layout are ignored.\n\
+\n\
+When a bit-field is inserted into a packed record, the whole size\n\
+of the underlying type is used by one or more same-size adjacent\n\
+bit-fields (that is, if its long:3, 32 bits is used in the record,\n\
+and any additional adjacent long bit-fields are packed into the same\n\
+chunk of 32 bits.  However, if the size changes, a new field of that\n\
+size is allocated).  In an unpacked record, this is the same as using\n\
+alignment, but not equivalent when packing.\n\
+\n\
+If both MS bit-fields and @samp{__attribute__((packed))} are used,\n\
+the latter will take precedence.  If @samp{__attribute__((packed))} is\n\
+used on a single field when MS bit-fields are in use, it will take\n\
+precedence for that field, but the alignment of the rest of the structure\n\
+may affect its placement.",
+ bool, (const_tree record_type),
+ hook_bool_const_tree_false)
+
+/* For now this is only an interface to WORDS_BIG_ENDIAN for
+   target-independent code like the front ends, need performance testing
+   before switching completely to the target hook.  */
+DEFHOOK_UNDOC
+(words_big_endian,
+ "",
+ bool, (void),
+ targhook_words_big_endian)
+
+/* Likewise for FLOAT_WORDS_BIG_ENDIAN.  */
+DEFHOOK_UNDOC
+(float_words_big_endian,
+ "",
+ bool, (void),
+ targhook_float_words_big_endian)
+
+DEFHOOK
+(float_exceptions_rounding_supported_p,
+ "Returns true if the target supports IEEE 754 floating-point exceptions\
+ and rounding modes, false otherwise.  This is intended to relate to the\
+ @code{float} and @code{double} types, but not necessarily @code{long double}.\
+ By default, returns true if the @code{adddf3} instruction pattern is\
+ available and false otherwise, on the assumption that hardware floating\
+ point supports exceptions and rounding modes but software floating point\
+ does not.",
+ bool, (void),
+ default_float_exceptions_rounding_supported_p)
+
+/* True if the target supports decimal floating point.  */
+DEFHOOK
+(decimal_float_supported_p,
+ "Returns true if the target supports decimal floating point.",
+ bool, (void),
+ default_decimal_float_supported_p)
+
+/* True if the target supports fixed-point.  */
+DEFHOOK
+(fixed_point_supported_p,
+ "Returns true if the target supports fixed-point arithmetic.",
+ bool, (void),
+ default_fixed_point_supported_p)
+
+/* Return true if anonymous bitfields affect structure alignment.  */
+DEFHOOK
+(align_anon_bitfield,
+ "When @code{PCC_BITFIELD_TYPE_MATTERS} is true this hook will determine\n\
+whether unnamed bitfields affect the alignment of the containing\n\
+structure.  The hook should return true if the structure should inherit\n\
+the alignment requirements of an unnamed bitfield's type.",
+ bool, (void),
+ hook_bool_void_false)
+
+/* Return true if volatile bitfields should use the narrowest type possible.
+   Return false if they should use the container type.  */
+DEFHOOK
+(narrow_volatile_bitfield,
+ "This target hook should return @code{true} if accesses to volatile bitfields\n\
+should use the narrowest mode possible.  It should return @code{false} if\n\
+these accesses should use the bitfield container type.\n\
+\n\
+The default is @code{false}.",
+ bool, (void),
+ hook_bool_void_false)
+
+/* Set up target-specific built-in functions.  */
+DEFHOOK
+(init_builtins,
+ "Define this hook if you have any machine-specific built-in functions\n\
+that need to be defined.  It should be a function that performs the\n\
+necessary setup.\n\
+\n\
+Machine specific built-in functions can be useful to expand special machine\n\
+instructions that would otherwise not normally be generated because\n\
+they have no equivalent in the source language (for example, SIMD vector\n\
+instructions or prefetch instructions).\n\
+\n\
+To create a built-in function, call the function\n\
+@code{lang_hooks.builtin_function}\n\
+which is defined by the language front end.  You can use any type nodes set\n\
+up by @code{build_common_tree_nodes};\n\
+only language front ends that use those two functions will call\n\
+@samp{TARGET_INIT_BUILTINS}.",
+ void, (void),
+ hook_void_void)
+
+/* Initialize (if INITIALIZE_P is true) and return the target-specific
+   built-in function decl for CODE.
+   Return NULL if that is not possible.  Return error_mark_node if CODE
+   is outside of the range of valid target builtin function codes.  */
+DEFHOOK
+(builtin_decl,
+ "Define this hook if you have any machine-specific built-in functions\n\
+that need to be defined.  It should be a function that returns the\n\
+builtin function declaration for the builtin function code @var{code}.\n\
+If there is no such builtin and it cannot be initialized at this time\n\
+if @var{initialize_p} is true the function should return @code{NULL_TREE}.\n\
+If @var{code} is out of range the function should return\n\
+@code{error_mark_node}.",
+ tree, (unsigned code, bool initialize_p), NULL)
+
+/* Expand a target-specific builtin.  */
+DEFHOOK
+(expand_builtin,
+ "\n\
+Expand a call to a machine specific built-in function that was set up by\n\
+@samp{TARGET_INIT_BUILTINS}.  @var{exp} is the expression for the\n\
+function call; the result should go to @var{target} if that is\n\
+convenient, and have mode @var{mode} if that is convenient.\n\
+@var{subtarget} may be used as the target for computing one of\n\
+@var{exp}'s operands.  @var{ignore} is nonzero if the value is to be\n\
+ignored.  This function should return the result of the call to the\n\
+built-in function.",
+ rtx,
+ (tree exp, rtx target, rtx subtarget, machine_mode mode, int ignore),
+ default_expand_builtin)
+
+/* Select a replacement for a target-specific builtin.  This is done
+   *before* regular type checking, and so allows the target to
+   implement a crude form of function overloading.  The result is a
+   complete expression that implements the operation.  PARAMS really
+   has type VEC(tree,gc)*, but we don't want to include tree.h here.  */
+DEFHOOK
+(resolve_overloaded_builtin,
+ "Select a replacement for a machine specific built-in function that\n\
+was set up by @samp{TARGET_INIT_BUILTINS}.  This is done\n\
+@emph{before} regular type checking, and so allows the target to\n\
+implement a crude form of function overloading.  @var{fndecl} is the\n\
+declaration of the built-in function.  @var{arglist} is the list of\n\
+arguments passed to the built-in function.  The result is a\n\
+complete expression that implements the operation, usually\n\
+another @code{CALL_EXPR}.\n\
+@var{arglist} really has type @samp{VEC(tree,gc)*}",
+ tree, (unsigned int /*location_t*/ loc, tree fndecl, void *arglist), NULL)
+
+DEFHOOK
+(check_builtin_call,
+ "Perform semantic checking on a call to a machine-specific built-in\n\
+function after its arguments have been constrained to the function\n\
+signature.  Return true if the call is valid, otherwise report an error\n\
+and return false.\n\
+\n\
+This hook is called after @code{TARGET_RESOLVE_OVERLOADED_BUILTIN}.\n\
+The call was originally to built-in function @var{orig_fndecl},\n\
+but after the optional @code{TARGET_RESOLVE_OVERLOADED_BUILTIN}\n\
+step is now to built-in function @var{fndecl}.  @var{loc} is the\n\
+location of the call and @var{args} is an array of function arguments,\n\
+of which there are @var{nargs}.  @var{arg_loc} specifies the location\n\
+of each argument.",
+ bool, (location_t loc, vec<location_t> arg_loc, tree fndecl,
+	tree orig_fndecl, unsigned int nargs, tree *args),
+ NULL)
+
+/* Fold a target-specific builtin to a tree valid for both GIMPLE
+   and GENERIC.  */
+DEFHOOK
+(fold_builtin,
+ "Fold a call to a machine specific built-in function that was set up by\n\
+@samp{TARGET_INIT_BUILTINS}.  @var{fndecl} is the declaration of the\n\
+built-in function.  @var{n_args} is the number of arguments passed to\n\
+the function; the arguments themselves are pointed to by @var{argp}.\n\
+The result is another tree, valid for both GIMPLE and GENERIC,\n\
+containing a simplified expression for the call's result.  If\n\
+@var{ignore} is true the value will be ignored.",
+ tree, (tree fndecl, int n_args, tree *argp, bool ignore),
+ hook_tree_tree_int_treep_bool_null)
+
+/* Fold a target-specific builtin to a valid GIMPLE tree.  */
+DEFHOOK
+(gimple_fold_builtin,
+ "Fold a call to a machine specific built-in function that was set up\n\
+by @samp{TARGET_INIT_BUILTINS}.  @var{gsi} points to the gimple\n\
+statement holding the function call.  Returns true if any change\n\
+was made to the GIMPLE stream.",
+ bool, (gimple_stmt_iterator *gsi),
+ hook_bool_gsiptr_false)
+
+/* Target hook is used to compare the target attributes in two functions to
+   determine which function's features get higher priority.  This is used
+   during function multi-versioning to figure out the order in which two
+   versions must be dispatched.  A function version with a higher priority
+   is checked for dispatching earlier.  DECL1 and DECL2 are
+   the two function decls that will be compared. It returns positive value
+   if DECL1 is higher priority,  negative value if DECL2 is higher priority
+   and 0 if they are the same. */
+DEFHOOK
+(compare_version_priority,
+ "This hook is used to compare the target attributes in two functions to\n\
+determine which function's features get higher priority.  This is used\n\
+during function multi-versioning to figure out the order in which two\n\
+versions must be dispatched.  A function version with a higher priority\n\
+is checked for dispatching earlier.  @var{decl1} and @var{decl2} are\n\
+ the two function decls that will be compared.",
+ int, (tree decl1, tree decl2), NULL)
+
+/*  Target hook is used to generate the dispatcher logic to invoke the right
+    function version at run-time for a given set of function versions.
+    ARG points to the callgraph node of the dispatcher function whose body
+    must be generated.  */
+DEFHOOK
+(generate_version_dispatcher_body,
+ "This hook is used to generate the dispatcher logic to invoke the right\n\
+function version at run-time for a given set of function versions.\n\
+@var{arg} points to the callgraph node of the dispatcher function whose\n\
+body must be generated.",
+ tree, (void *arg), NULL) 
+
+/* Target hook is used to get the dispatcher function for a set of function
+   versions.  The dispatcher function is called to invoke the right function
+   version at run-time.  DECL is one version from a set of semantically
+   identical versions.  */
+DEFHOOK
+(get_function_versions_dispatcher,
+ "This hook is used to get the dispatcher function for a set of function\n\
+versions.  The dispatcher function is called to invoke the right function\n\
+version at run-time. @var{decl} is one version from a set of semantically\n\
+identical versions.",
+ tree, (void *decl), NULL)
+
+/* Returns a code for a target-specific builtin that implements
+   reciprocal of a target-specific function, or NULL_TREE if not available.  */
+DEFHOOK
+(builtin_reciprocal,
+ "This hook should return the DECL of a function that implements the\n\
+reciprocal of the machine-specific builtin function @var{fndecl}, or\n\
+@code{NULL_TREE} if such a function is not available.",
+ tree, (tree fndecl),
+ default_builtin_reciprocal)
+
+/* For a vendor-specific TYPE, return a pointer to a statically-allocated
+   string containing the C++ mangling for TYPE.  In all other cases, return
+   NULL.  */
+DEFHOOK
+(mangle_type,
+ "If your target defines any fundamental types, or any types your target\n\
+uses should be mangled differently from the default, define this hook\n\
+to return the appropriate encoding for these types as part of a C++\n\
+mangled name.  The @var{type} argument is the tree structure representing\n\
+the type to be mangled.  The hook may be applied to trees which are\n\
+not target-specific fundamental types; it should return @code{NULL}\n\
+for all such types, as well as arguments it does not recognize.  If the\n\
+return value is not @code{NULL}, it must point to a statically-allocated\n\
+string constant.\n\
+\n\
+Target-specific fundamental types might be new fundamental types or\n\
+qualified versions of ordinary fundamental types.  Encode new\n\
+fundamental types as @samp{@w{u @var{n} @var{name}}}, where @var{name}\n\
+is the name used for the type in source code, and @var{n} is the\n\
+length of @var{name} in decimal.  Encode qualified versions of\n\
+ordinary types as @samp{@w{U @var{n} @var{name} @var{code}}}, where\n\
+@var{name} is the name used for the type qualifier in source code,\n\
+@var{n} is the length of @var{name} as above, and @var{code} is the\n\
+code used to represent the unqualified version of this type.  (See\n\
+@code{write_builtin_type} in @file{cp/mangle.c} for the list of\n\
+codes.)  In both cases the spaces are for clarity; do not include any\n\
+spaces in your string.\n\
+\n\
+This hook is applied to types prior to typedef resolution.  If the mangled\n\
+name for a particular type depends only on that type's main variant, you\n\
+can perform typedef resolution yourself using @code{TYPE_MAIN_VARIANT}\n\
+before mangling.\n\
+\n\
+The default version of this hook always returns @code{NULL}, which is\n\
+appropriate for a target that does not define any new fundamental\n\
+types.",
+ const char *, (const_tree type),
+ hook_constcharptr_const_tree_null)
+
+/* Make any adjustments to libfunc names needed for this target.  */
+DEFHOOK
+(init_libfuncs,
+ "This hook should declare additional library routines or rename\n\
+existing ones, using the functions @code{set_optab_libfunc} and\n\
+@code{init_one_libfunc} defined in @file{optabs.c}.\n\
+@code{init_optabs} calls this macro after initializing all the normal\n\
+library routines.\n\
+\n\
+The default is to do nothing.  Most ports don't need to define this hook.",
+ void, (void),
+ hook_void_void)
+
+ /* Add a __gnu_ prefix to library functions rather than just __.  */
+DEFHOOKPOD
+(libfunc_gnu_prefix,
+ "If false (the default), internal library routines start with two\n\
+underscores.  If set to true, these routines start with @code{__gnu_}\n\
+instead.  E.g., @code{__muldi3} changes to @code{__gnu_muldi3}.  This\n\
+currently only affects functions defined in @file{libgcc2.c}.  If this\n\
+is set to true, the @file{tm.h} file must also\n\
+@code{#define LIBGCC2_GNU_PREFIX}.",
+  bool, false)
+
+/* Given a decl, a section name, and whether the decl initializer
+   has relocs, choose attributes for the section.  */
+/* ??? Should be merged with SELECT_SECTION and UNIQUE_SECTION.  */
+DEFHOOK
+(section_type_flags,
+ "Choose a set of section attributes for use by @code{TARGET_ASM_NAMED_SECTION}\n\
+based on a variable or function decl, a section name, and whether or not the\n\
+declaration's initializer may contain runtime relocations.  @var{decl} may be\n\
+null, in which case read-write data should be assumed.\n\
+\n\
+The default version of this function handles choosing code vs data,\n\
+read-only vs read-write data, and @code{flag_pic}.  You should only\n\
+need to override this if your target has special flags that might be\n\
+set via @code{__attribute__}.",
+ unsigned int, (tree decl, const char *name, int reloc),
+ default_section_type_flags)
+
+DEFHOOK
+(libc_has_function,
+ "This hook determines whether a function from a class of functions\n\
+@var{fn_class} is present in the target C library.  If @var{type} is NULL,\n\
+the caller asks for support for all standard (float, double, long double)\n\
+types.  If @var{type} is non-NULL, the caller asks for support for a\n\
+specific type.",
+ bool, (enum function_class fn_class, tree type),
+ default_libc_has_function)
+
+DEFHOOK
+(libc_has_fast_function,
+ "This hook determines whether a function from a class of functions\n\
+@code{(enum function_class)}@var{fcode} has a fast implementation.",
+ bool, (int fcode),
+ default_libc_has_fast_function)
+
+/* True if new jumps cannot be created, to replace existing ones or
+   not, at the current point in the compilation.  */
+DEFHOOK
+(cannot_modify_jumps_p,
+ "This target hook returns @code{true} past the point in which new jump\n\
+instructions could be created.  On machines that require a register for\n\
+every jump such as the SHmedia ISA of SH5, this point would typically be\n\
+reload, so this target hook should be defined to a function such as:\n\
+\n\
+@smallexample\n\
+static bool\n\
+cannot_modify_jumps_past_reload_p ()\n\
+@{\n\
+  return (reload_completed || reload_in_progress);\n\
+@}\n\
+@end smallexample",
+ bool, (void),
+ hook_bool_void_false)
+
+/* True if FOLLOWER may be modified to follow FOLLOWEE.  */
+DEFHOOK
+(can_follow_jump,
+ "FOLLOWER and FOLLOWEE are JUMP_INSN instructions;\
+  return true if FOLLOWER may be modified to follow FOLLOWEE;\
+  false, if it can't.\
+  For example, on some targets, certain kinds of branches can't be made to\
+  follow through a hot/cold partitioning.",
+ bool, (const rtx_insn *follower, const rtx_insn *followee),
+ hook_bool_const_rtx_insn_const_rtx_insn_true)
+
+/* Return true if the target supports conditional execution.  */
+DEFHOOK
+(have_conditional_execution,
+ "This target hook returns true if the target supports conditional execution.\n\
+This target hook is required only when the target has several different\n\
+modes and they have different conditional execution capability, such as ARM.",
+ bool, (void),
+ default_have_conditional_execution)
+
+DEFHOOK
+(gen_ccmp_first,
+ "This function prepares to emit a comparison insn for the first compare in a\n\
+ sequence of conditional comparisions.  It returns an appropriate comparison\n\
+ with @code{CC} for passing to @code{gen_ccmp_next} or @code{cbranch_optab}.\n\
+ The insns to prepare the compare are saved in @var{prep_seq} and the compare\n\
+ insns are saved in @var{gen_seq}.  They will be emitted when all the\n\
+ compares in the conditional comparision are generated without error.\n\
+ @var{code} is the @code{rtx_code} of the compare for @var{op0} and @var{op1}.",
+ rtx, (rtx_insn **prep_seq, rtx_insn **gen_seq, int code, tree op0, tree op1),
+ NULL)
+
+DEFHOOK
+(gen_ccmp_next,
+ "This function prepares to emit a conditional comparison within a sequence\n\
+ of conditional comparisons.  It returns an appropriate comparison with\n\
+ @code{CC} for passing to @code{gen_ccmp_next} or @code{cbranch_optab}.\n\
+ The insns to prepare the compare are saved in @var{prep_seq} and the compare\n\
+ insns are saved in @var{gen_seq}.  They will be emitted when all the\n\
+ compares in the conditional comparision are generated without error.  The\n\
+ @var{prev} expression is the result of a prior call to @code{gen_ccmp_first}\n\
+ or @code{gen_ccmp_next}.  It may return @code{NULL} if the combination of\n\
+ @var{prev} and this comparison is not supported, otherwise the result must\n\
+ be appropriate for passing to @code{gen_ccmp_next} or @code{cbranch_optab}.\n\
+ @var{code} is the @code{rtx_code} of the compare for @var{op0} and @var{op1}.\n\
+ @var{bit_code} is @code{AND} or @code{IOR}, which is the op on the compares.",
+ rtx, (rtx_insn **prep_seq, rtx_insn **gen_seq, rtx prev, int cmp_code, tree op0, tree op1, int bit_code),
+ NULL)
+
+/* Return a new value for loop unroll size.  */
+DEFHOOK
+(loop_unroll_adjust,
+ "This target hook returns a new value for the number of times @var{loop}\n\
+should be unrolled. The parameter @var{nunroll} is the number of times\n\
+the loop is to be unrolled. The parameter @var{loop} is a pointer to\n\
+the loop, which is going to be checked for unrolling. This target hook\n\
+is required only when the target has special constraints like maximum\n\
+number of memory accesses.",
+ unsigned, (unsigned nunroll, class loop *loop),
+ NULL)
+
+/* True if X is a legitimate MODE-mode immediate operand.  */
+DEFHOOK
+(legitimate_constant_p,
+ "This hook returns true if @var{x} is a legitimate constant for a\n\
+@var{mode}-mode immediate operand on the target machine.  You can assume that\n\
+@var{x} satisfies @code{CONSTANT_P}, so you need not check this.\n\
+\n\
+The default definition returns true.",
+ bool, (machine_mode mode, rtx x),
+ hook_bool_mode_rtx_true)
+
+/* True if X is a TLS operand whose value should be pre-computed.  */
+DEFHOOK
+(precompute_tls_p,
+ "This hook returns true if @var{x} is a TLS operand on the target\n\
+machine that should be pre-computed when used as the argument in a call.\n\
+You can assume that @var{x} satisfies @code{CONSTANT_P}, so you need not \n\
+check this.\n\
+\n\
+The default definition returns false.",
+ bool, (machine_mode mode, rtx x),
+ hook_bool_mode_rtx_false)
+
+/* True if the constant X cannot be placed in the constant pool.  */
+DEFHOOK
+(cannot_force_const_mem,
+ "This hook should return true if @var{x} is of a form that cannot (or\n\
+should not) be spilled to the constant pool.  @var{mode} is the mode\n\
+of @var{x}.\n\
+\n\
+The default version of this hook returns false.\n\
+\n\
+The primary reason to define this hook is to prevent reload from\n\
+deciding that a non-legitimate constant would be better reloaded\n\
+from the constant pool instead of spilling and reloading a register\n\
+holding the constant.  This restriction is often true of addresses\n\
+of TLS symbols for various targets.",
+ bool, (machine_mode mode, rtx x),
+ hook_bool_mode_rtx_false)
+
+DEFHOOK_UNDOC
+(cannot_copy_insn_p,
+ "True if the insn @var{x} cannot be duplicated.",
+ bool, (rtx_insn *), NULL)
+
+/* True if X is considered to be commutative.  */
+DEFHOOK
+(commutative_p,
+ "This target hook returns @code{true} if @var{x} is considered to be commutative.\n\
+Usually, this is just COMMUTATIVE_P (@var{x}), but the HP PA doesn't consider\n\
+PLUS to be commutative inside a MEM@.  @var{outer_code} is the rtx code\n\
+of the enclosing rtl, if known, otherwise it is UNKNOWN.",
+ bool, (const_rtx x, int outer_code),
+ hook_bool_const_rtx_commutative_p)
+
+/* True if ADDR is an address-expression whose effect depends
+   on the mode of the memory reference it is used in.  */
+DEFHOOK
+(mode_dependent_address_p,
+ "This hook returns @code{true} if memory address @var{addr} in address\n\
+space @var{addrspace} can have\n\
+different meanings depending on the machine mode of the memory\n\
+reference it is used for or if the address is valid for some modes\n\
+but not others.\n\
+\n\
+Autoincrement and autodecrement addresses typically have mode-dependent\n\
+effects because the amount of the increment or decrement is the size\n\
+of the operand being addressed.  Some machines have other mode-dependent\n\
+addresses.  Many RISC machines have no mode-dependent addresses.\n\
+\n\
+You may assume that @var{addr} is a valid address for the machine.\n\
+\n\
+The default version of this hook returns @code{false}.",
+ bool, (const_rtx addr, addr_space_t addrspace),
+ default_mode_dependent_address_p)
+
+/* Given an invalid address X for a given machine mode, try machine-specific
+   ways to make it legitimate.  Return X or an invalid address on failure.  */
+DEFHOOK
+(legitimize_address,
+ "This hook is given an invalid memory address @var{x} for an\n\
+operand of mode @var{mode} and should try to return a valid memory\n\
+address.\n\
+\n\
+@findex break_out_memory_refs\n\
+@var{x} will always be the result of a call to @code{break_out_memory_refs},\n\
+and @var{oldx} will be the operand that was given to that function to produce\n\
+@var{x}.\n\
+\n\
+The code of the hook should not alter the substructure of\n\
+@var{x}.  If it transforms @var{x} into a more legitimate form, it\n\
+should return the new @var{x}.\n\
+\n\
+It is not necessary for this hook to come up with a legitimate address,\n\
+with the exception of native TLS addresses (@pxref{Emulated TLS}).\n\
+The compiler has standard ways of doing so in all cases.  In fact, if\n\
+the target supports only emulated TLS, it\n\
+is safe to omit this hook or make it return @var{x} if it cannot find\n\
+a valid way to legitimize the address.  But often a machine-dependent\n\
+strategy can generate better code.",
+ rtx, (rtx x, rtx oldx, machine_mode mode),
+ default_legitimize_address)
+
+/* Given an address RTX, undo the effects of LEGITIMIZE_ADDRESS.  */
+DEFHOOK
+(delegitimize_address,
+ "This hook is used to undo the possibly obfuscating effects of the\n\
+@code{LEGITIMIZE_ADDRESS} and @code{LEGITIMIZE_RELOAD_ADDRESS} target\n\
+macros.  Some backend implementations of these macros wrap symbol\n\
+references inside an @code{UNSPEC} rtx to represent PIC or similar\n\
+addressing modes.  This target hook allows GCC's optimizers to understand\n\
+the semantics of these opaque @code{UNSPEC}s by converting them back\n\
+into their original form.",
+ rtx, (rtx x),
+ delegitimize_mem_from_attrs)
+
+/* Given an RTX, return true if it is not ok to emit it into debug info
+   section.  */
+DEFHOOK
+(const_not_ok_for_debug_p,
+ "This hook should return true if @var{x} should not be emitted into\n\
+debug sections.",
+ bool, (rtx x),
+ default_const_not_ok_for_debug_p)
+
+/* Given an address RTX, say whether it is valid.  */
+DEFHOOK
+(legitimate_address_p,
+ "A function that returns whether @var{x} (an RTX) is a legitimate memory\n\
+address on the target machine for a memory operand of mode @var{mode}.\n\
+\n\
+Legitimate addresses are defined in two variants: a strict variant and a\n\
+non-strict one.  The @var{strict} parameter chooses which variant is\n\
+desired by the caller.\n\
+\n\
+The strict variant is used in the reload pass.  It must be defined so\n\
+that any pseudo-register that has not been allocated a hard register is\n\
+considered a memory reference.  This is because in contexts where some\n\
+kind of register is required, a pseudo-register with no hard register\n\
+must be rejected.  For non-hard registers, the strict variant should look\n\
+up the @code{reg_renumber} array; it should then proceed using the hard\n\
+register number in the array, or treat the pseudo as a memory reference\n\
+if the array holds @code{-1}.\n\
+\n\
+The non-strict variant is used in other passes.  It must be defined to\n\
+accept all pseudo-registers in every context where some kind of\n\
+register is required.\n\
+\n\
+Normally, constant addresses which are the sum of a @code{symbol_ref}\n\
+and an integer are stored inside a @code{const} RTX to mark them as\n\
+constant.  Therefore, there is no need to recognize such sums\n\
+specifically as legitimate addresses.  Normally you would simply\n\
+recognize any @code{const} as legitimate.\n\
+\n\
+Usually @code{PRINT_OPERAND_ADDRESS} is not prepared to handle constant\n\
+sums that are not marked with  @code{const}.  It assumes that a naked\n\
+@code{plus} indicates indexing.  If so, then you @emph{must} reject such\n\
+naked constant sums as illegitimate addresses, so that none of them will\n\
+be given to @code{PRINT_OPERAND_ADDRESS}.\n\
+\n\
+@cindex @code{TARGET_ENCODE_SECTION_INFO} and address validation\n\
+On some machines, whether a symbolic address is legitimate depends on\n\
+the section that the address refers to.  On these machines, define the\n\
+target hook @code{TARGET_ENCODE_SECTION_INFO} to store the information\n\
+into the @code{symbol_ref}, and then check for it here.  When you see a\n\
+@code{const}, you will have to look inside it to find the\n\
+@code{symbol_ref} in order to determine the section.  @xref{Assembler\n\
+Format}.\n\
+\n\
+@cindex @code{GO_IF_LEGITIMATE_ADDRESS}\n\
+Some ports are still using a deprecated legacy substitute for\n\
+this hook, the @code{GO_IF_LEGITIMATE_ADDRESS} macro.  This macro\n\
+has this syntax:\n\
+\n\
+@example\n\
+#define GO_IF_LEGITIMATE_ADDRESS (@var{mode}, @var{x}, @var{label})\n\
+@end example\n\
+\n\
+@noindent\n\
+and should @code{goto @var{label}} if the address @var{x} is a valid\n\
+address on the target machine for a memory operand of mode @var{mode}.\n\
+\n\
+@findex REG_OK_STRICT\n\
+Compiler source files that want to use the strict variant of this\n\
+macro define the macro @code{REG_OK_STRICT}.  You should use an\n\
+@code{#ifdef REG_OK_STRICT} conditional to define the strict variant in\n\
+that case and the non-strict variant otherwise.\n\
+\n\
+Using the hook is usually simpler because it limits the number of\n\
+files that are recompiled when changes are made.",
+ bool, (machine_mode mode, rtx x, bool strict),
+ default_legitimate_address_p)
+
+/* True if the given constant can be put into an object_block.  */
+DEFHOOK
+(use_blocks_for_constant_p,
+ "This hook should return true if pool entries for constant @var{x} can\n\
+be placed in an @code{object_block} structure.  @var{mode} is the mode\n\
+of @var{x}.\n\
+\n\
+The default version returns false for all constants.",
+ bool, (machine_mode mode, const_rtx x),
+ hook_bool_mode_const_rtx_false)
+
+/* True if the given decl can be put into an object_block.  */
+DEFHOOK
+(use_blocks_for_decl_p,
+ "This hook should return true if pool entries for @var{decl} should\n\
+be placed in an @code{object_block} structure.\n\
+\n\
+The default version returns true for all decls.",
+ bool, (const_tree decl),
+ hook_bool_const_tree_true)
+
+/* The minimum and maximum byte offsets for anchored addresses.  */
+DEFHOOKPOD
+(min_anchor_offset,
+ "The minimum offset that should be applied to a section anchor.\n\
+On most targets, it should be the smallest offset that can be\n\
+applied to a base register while still giving a legitimate address\n\
+for every mode.  The default value is 0.",
+ HOST_WIDE_INT, 0)
+
+DEFHOOKPOD
+(max_anchor_offset,
+ "Like @code{TARGET_MIN_ANCHOR_OFFSET}, but the maximum (inclusive)\n\
+offset that should be applied to section anchors.  The default\n\
+value is 0.",
+ HOST_WIDE_INT, 0)
+
+/* True if section anchors can be used to access the given symbol.  */
+DEFHOOK
+(use_anchors_for_symbol_p,
+ "Return true if GCC should attempt to use anchors to access @code{SYMBOL_REF}\n\
+@var{x}.  You can assume @samp{SYMBOL_REF_HAS_BLOCK_INFO_P (@var{x})} and\n\
+@samp{!SYMBOL_REF_ANCHOR_P (@var{x})}.\n\
+\n\
+The default version is correct for most targets, but you might need to\n\
+intercept this hook to handle things like target-specific attributes\n\
+or target-specific sections.",
+ bool, (const_rtx x),
+ default_use_anchors_for_symbol_p)
+
+/* True if target supports indirect functions.  */
+DEFHOOK
+(has_ifunc_p,
+ "It returns true if the target supports GNU indirect functions.\n\
+The support includes the assembler, linker and dynamic linker.\n\
+The default value of this hook is based on target's libc.",
+ bool, (void),
+ default_has_ifunc_p)
+
+/* True if it is OK to do sibling call optimization for the specified
+   call expression EXP.  DECL will be the called function, or NULL if
+   this is an indirect call.  */
+DEFHOOK
+(function_ok_for_sibcall,
+ "True if it is OK to do sibling call optimization for the specified\n\
+call expression @var{exp}.  @var{decl} will be the called function,\n\
+or @code{NULL} if this is an indirect call.\n\
+\n\
+It is not uncommon for limitations of calling conventions to prevent\n\
+tail calls to functions outside the current unit of translation, or\n\
+during PIC compilation.  The hook is used to enforce these restrictions,\n\
+as the @code{sibcall} md pattern cannot fail, or fall over to a\n\
+``normal'' call.  The criteria for successful sibling call optimization\n\
+may vary greatly between different architectures.",
+ bool, (tree decl, tree exp),
+ hook_bool_tree_tree_false)
+
+/* Establish appropriate back-end context for processing the function
+   FNDECL.  The argument might be NULL to indicate processing at top
+   level, outside of any function scope.  */
+DEFHOOK
+(set_current_function,
+ "The compiler invokes this hook whenever it changes its current function\n\
+context (@code{cfun}).  You can define this function if\n\
+the back end needs to perform any initialization or reset actions on a\n\
+per-function basis.  For example, it may be used to implement function\n\
+attributes that affect register usage or code generation patterns.\n\
+The argument @var{decl} is the declaration for the new function context,\n\
+and may be null to indicate that the compiler has left a function context\n\
+and is returning to processing at the top level.\n\
+The default hook function does nothing.\n\
+\n\
+GCC sets @code{cfun} to a dummy function context during initialization of\n\
+some parts of the back end.  The hook function is not invoked in this\n\
+situation; you need not worry about the hook being invoked recursively,\n\
+or when the back end is in a partially-initialized state.\n\
+@code{cfun} might be @code{NULL} to indicate processing at top level,\n\
+outside of any function scope.",
+ void, (tree decl), hook_void_tree)
+
+/* True if EXP should be placed in a "small data" section.  */
+DEFHOOK
+(in_small_data_p,
+ "Returns true if @var{exp} should be placed into a ``small data'' section.\n\
+The default version of this hook always returns false.",
+ bool, (const_tree exp),
+ hook_bool_const_tree_false)
+
+/* True if EXP names an object for which name resolution must resolve
+   to the current executable or shared library.  */
+DEFHOOK
+(binds_local_p,
+ "Returns true if @var{exp} names an object for which name resolution\n\
+rules must resolve to the current ``module'' (dynamic shared library\n\
+or executable image).\n\
+\n\
+The default version of this hook implements the name resolution rules\n\
+for ELF, which has a looser model of global name binding than other\n\
+currently supported object file formats.",
+ bool, (const_tree exp),
+ default_binds_local_p)
+
+/* Check if profiling code is before or after prologue.  */
+DEFHOOK
+(profile_before_prologue,
+ "It returns true if target wants profile code emitted before prologue.\n\n\
+The default version of this hook use the target macro\n\
+@code{PROFILE_BEFORE_PROLOGUE}.",
+ bool, (void),
+ default_profile_before_prologue)
+
+/* Return true if a leaf function should stay leaf even with profiling
+   enabled.  */
+DEFHOOK
+(keep_leaf_when_profiled,
+ "This target hook returns true if the target wants the leaf flag for\
+ the current function to stay true even if it calls mcount.  This might\
+ make sense for targets using the leaf flag only to determine whether a\
+ stack frame needs to be generated or not and for which the call to\
+ mcount is generated before the function prologue.",
+ bool, (void),
+ default_keep_leaf_when_profiled)
+
+/* Modify and return the identifier of a DECL's external name,
+   originally identified by ID, as required by the target,
+   (eg, append @nn to windows32 stdcall function names).
+   The default is to return ID without modification. */
+DEFHOOK
+(mangle_decl_assembler_name,
+ "Define this hook if you need to postprocess the assembler name generated\n\
+by target-independent code.  The @var{id} provided to this hook will be\n\
+the computed name (e.g., the macro @code{DECL_NAME} of the @var{decl} in C,\n\
+or the mangled name of the @var{decl} in C++).  The return value of the\n\
+hook is an @code{IDENTIFIER_NODE} for the appropriate mangled name on\n\
+your target system.  The default implementation of this hook just\n\
+returns the @var{id} provided.",
+ tree, (tree decl, tree  id),
+ default_mangle_decl_assembler_name)
+
+/* Do something target-specific to record properties of the DECL into
+   the associated SYMBOL_REF.  */
+DEFHOOK
+(encode_section_info,
+ "Define this hook if references to a symbol or a constant must be\n\
+treated differently depending on something about the variable or\n\
+function named by the symbol (such as what section it is in).\n\
+\n\
+The hook is executed immediately after rtl has been created for\n\
+@var{decl}, which may be a variable or function declaration or\n\
+an entry in the constant pool.  In either case, @var{rtl} is the\n\
+rtl in question.  Do @emph{not} use @code{DECL_RTL (@var{decl})}\n\
+in this hook; that field may not have been initialized yet.\n\
+\n\
+In the case of a constant, it is safe to assume that the rtl is\n\
+a @code{mem} whose address is a @code{symbol_ref}.  Most decls\n\
+will also have this form, but that is not guaranteed.  Global\n\
+register variables, for instance, will have a @code{reg} for their\n\
+rtl.  (Normally the right thing to do with such unusual rtl is\n\
+leave it alone.)\n\
+\n\
+The @var{new_decl_p} argument will be true if this is the first time\n\
+that @code{TARGET_ENCODE_SECTION_INFO} has been invoked on this decl.  It will\n\
+be false for subsequent invocations, which will happen for duplicate\n\
+declarations.  Whether or not anything must be done for the duplicate\n\
+declaration depends on whether the hook examines @code{DECL_ATTRIBUTES}.\n\
+@var{new_decl_p} is always true when the hook is called for a constant.\n\
+\n\
+@cindex @code{SYMBOL_REF_FLAG}, in @code{TARGET_ENCODE_SECTION_INFO}\n\
+The usual thing for this hook to do is to record flags in the\n\
+@code{symbol_ref}, using @code{SYMBOL_REF_FLAG} or @code{SYMBOL_REF_FLAGS}.\n\
+Historically, the name string was modified if it was necessary to\n\
+encode more than one bit of information, but this practice is now\n\
+discouraged; use @code{SYMBOL_REF_FLAGS}.\n\
+\n\
+The default definition of this hook, @code{default_encode_section_info}\n\
+in @file{varasm.c}, sets a number of commonly-useful bits in\n\
+@code{SYMBOL_REF_FLAGS}.  Check whether the default does what you need\n\
+before overriding it.",
+ void, (tree decl, rtx rtl, int new_decl_p),
+ default_encode_section_info)
+
+/* Undo the effects of encode_section_info on the symbol string.  */
+DEFHOOK
+(strip_name_encoding,
+ "Decode @var{name} and return the real name part, sans\n\
+the characters that @code{TARGET_ENCODE_SECTION_INFO}\n\
+may have added.",
+ const char *, (const char *name),
+ default_strip_name_encoding)
+
+/* If shift optabs for MODE are known to always truncate the shift count,
+   return the mask that they apply.  Return 0 otherwise.  */
+DEFHOOK
+(shift_truncation_mask,
+ "This function describes how the standard shift patterns for @var{mode}\n\
+deal with shifts by negative amounts or by more than the width of the mode.\n\
+@xref{shift patterns}.\n\
+\n\
+On many machines, the shift patterns will apply a mask @var{m} to the\n\
+shift count, meaning that a fixed-width shift of @var{x} by @var{y} is\n\
+equivalent to an arbitrary-width shift of @var{x} by @var{y & m}.  If\n\
+this is true for mode @var{mode}, the function should return @var{m},\n\
+otherwise it should return 0.  A return value of 0 indicates that no\n\
+particular behavior is guaranteed.\n\
+\n\
+Note that, unlike @code{SHIFT_COUNT_TRUNCATED}, this function does\n\
+@emph{not} apply to general shift rtxes; it applies only to instructions\n\
+that are generated by the named shift patterns.\n\
+\n\
+The default implementation of this function returns\n\
+@code{GET_MODE_BITSIZE (@var{mode}) - 1} if @code{SHIFT_COUNT_TRUNCATED}\n\
+and 0 otherwise.  This definition is always safe, but if\n\
+@code{SHIFT_COUNT_TRUNCATED} is false, and some shift patterns\n\
+nevertheless truncate the shift count, you may get better code\n\
+by overriding it.",
+ unsigned HOST_WIDE_INT, (machine_mode mode),
+ default_shift_truncation_mask)
+
+/* Return the number of divisions in the given MODE that should be present,
+   so that it is profitable to turn the division into a multiplication by
+   the reciprocal.  */
+DEFHOOK
+(min_divisions_for_recip_mul,
+ "When @option{-ffast-math} is in effect, GCC tries to optimize\n\
+divisions by the same divisor, by turning them into multiplications by\n\
+the reciprocal.  This target hook specifies the minimum number of divisions\n\
+that should be there for GCC to perform the optimization for a variable\n\
+of mode @var{mode}.  The default implementation returns 3 if the machine\n\
+has an instruction for the division, and 2 if it does not.",
+ unsigned int, (machine_mode mode),
+ default_min_divisions_for_recip_mul)
+
+DEFHOOK
+(truly_noop_truncation,
+ "This hook returns true if it is safe to ``convert'' a value of\n\
+@var{inprec} bits to one of @var{outprec} bits (where @var{outprec} is\n\
+smaller than @var{inprec}) by merely operating on it as if it had only\n\
+@var{outprec} bits.  The default returns true unconditionally, which\n\
+is correct for most machines.  When @code{TARGET_TRULY_NOOP_TRUNCATION}\n\
+returns false, the machine description should provide a @code{trunc}\n\
+optab to specify the RTL that performs the required truncation.\n\
+\n\
+If @code{TARGET_MODES_TIEABLE_P} returns false for a pair of modes,\n\
+suboptimal code can result if this hook returns true for the corresponding\n\
+mode sizes.  Making this hook return false in such cases may improve things.",
+ bool, (poly_uint64 outprec, poly_uint64 inprec),
+ hook_bool_puint64_puint64_true)
+
+/* If the representation of integral MODE is such that values are
+   always sign-extended to a wider mode MODE_REP then return
+   SIGN_EXTEND.  Return UNKNOWN otherwise.  */
+/* Note that the return type ought to be RTX_CODE, but that's not
+   necessarily defined at this point.  */
+DEFHOOK
+(mode_rep_extended,
+ "The representation of an integral mode can be such that the values\n\
+are always extended to a wider integral mode.  Return\n\
+@code{SIGN_EXTEND} if values of @var{mode} are represented in\n\
+sign-extended form to @var{rep_mode}.  Return @code{UNKNOWN}\n\
+otherwise.  (Currently, none of the targets use zero-extended\n\
+representation this way so unlike @code{LOAD_EXTEND_OP},\n\
+@code{TARGET_MODE_REP_EXTENDED} is expected to return either\n\
+@code{SIGN_EXTEND} or @code{UNKNOWN}.  Also no target extends\n\
+@var{mode} to @var{rep_mode} so that @var{rep_mode} is not the next\n\
+widest integral mode and currently we take advantage of this fact.)\n\
+\n\
+Similarly to @code{LOAD_EXTEND_OP} you may return a non-@code{UNKNOWN}\n\
+value even if the extension is not performed on certain hard registers\n\
+as long as for the @code{REGNO_REG_CLASS} of these hard registers\n\
+@code{TARGET_CAN_CHANGE_MODE_CLASS} returns false.\n\
+\n\
+Note that @code{TARGET_MODE_REP_EXTENDED} and @code{LOAD_EXTEND_OP}\n\
+describe two related properties.  If you define\n\
+@code{TARGET_MODE_REP_EXTENDED (mode, word_mode)} you probably also want\n\
+to define @code{LOAD_EXTEND_OP (mode)} to return the same type of\n\
+extension.\n\
+\n\
+In order to enforce the representation of @code{mode},\n\
+@code{TARGET_TRULY_NOOP_TRUNCATION} should return false when truncating to\n\
+@code{mode}.",
+ int, (scalar_int_mode mode, scalar_int_mode rep_mode),
+ default_mode_rep_extended)
+
+ DEFHOOK
+(setjmp_preserves_nonvolatile_regs_p,
+ "On some targets, it is assumed that the compiler will spill all pseudos\n\
+  that are live across a call to @code{setjmp}, while other targets treat\n\
+  @code{setjmp} calls as normal function calls.\n\
+  \n\
+  This hook returns false if @code{setjmp} calls do not preserve all\n\
+  non-volatile registers so that gcc that must spill all pseudos that are\n\
+  live across @code{setjmp} calls.  Define this to return true if the\n\
+  target does not need to spill all pseudos live across @code{setjmp} calls.\n\
+  The default implementation conservatively assumes all pseudos must be\n\
+  spilled across @code{setjmp} calls.",
+ bool, (void),
+ hook_bool_void_false)
+
+/* True if MODE is valid for a pointer in __attribute__((mode("MODE"))).  */
+DEFHOOK
+(valid_pointer_mode,
+ "Define this to return nonzero if the port can handle pointers\n\
+with machine mode @var{mode}.  The default version of this\n\
+hook returns true for both @code{ptr_mode} and @code{Pmode}.",
+ bool, (scalar_int_mode mode),
+ default_valid_pointer_mode)
+
+/* Disambiguate with errno.  */
+DEFHOOK
+(ref_may_alias_errno,
+ "Define this to return nonzero if the memory reference @var{ref}\
+  may alias with the system C library errno location.  The default\
+  version of this hook assumes the system C library errno location\
+  is either a declaration of type int or accessed by dereferencing\
+  a pointer to int.",
+ bool, (ao_ref *ref),
+ default_ref_may_alias_errno)
+
+/* Support for named address spaces.  */
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_ADDR_SPACE_"
+HOOK_VECTOR (TARGET_ADDR_SPACE_HOOKS, addr_space)
+
+/* MODE to use for a pointer into another address space.  */
+DEFHOOK
+(pointer_mode,
+ "Define this to return the machine mode to use for pointers to\n\
+@var{address_space} if the target supports named address spaces.\n\
+The default version of this hook returns @code{ptr_mode}.",
+ scalar_int_mode, (addr_space_t address_space),
+ default_addr_space_pointer_mode)
+
+/* MODE to use for an address in another address space.  */
+DEFHOOK
+(address_mode,
+ "Define this to return the machine mode to use for addresses in\n\
+@var{address_space} if the target supports named address spaces.\n\
+The default version of this hook returns @code{Pmode}.",
+ scalar_int_mode, (addr_space_t address_space),
+ default_addr_space_address_mode)
+
+/* True if MODE is valid for a pointer in __attribute__((mode("MODE")))
+   in another address space.  */
+DEFHOOK
+(valid_pointer_mode,
+ "Define this to return nonzero if the port can handle pointers\n\
+with machine mode @var{mode} to address space @var{as}.  This target\n\
+hook is the same as the @code{TARGET_VALID_POINTER_MODE} target hook,\n\
+except that it includes explicit named address space support.  The default\n\
+version of this hook returns true for the modes returned by either the\n\
+@code{TARGET_ADDR_SPACE_POINTER_MODE} or @code{TARGET_ADDR_SPACE_ADDRESS_MODE}\n\
+target hooks for the given address space.",
+ bool, (scalar_int_mode mode, addr_space_t as),
+ default_addr_space_valid_pointer_mode)
+
+/* True if an address is a valid memory address to a given named address
+   space for a given mode.  */
+DEFHOOK
+(legitimate_address_p,
+ "Define this to return true if @var{exp} is a valid address for mode\n\
+@var{mode} in the named address space @var{as}.  The @var{strict}\n\
+parameter says whether strict addressing is in effect after reload has\n\
+finished.  This target hook is the same as the\n\
+@code{TARGET_LEGITIMATE_ADDRESS_P} target hook, except that it includes\n\
+explicit named address space support.",
+ bool, (machine_mode mode, rtx exp, bool strict, addr_space_t as),
+ default_addr_space_legitimate_address_p)
+
+/* Return an updated address to convert an invalid pointer to a named
+   address space to a valid one.  If NULL_RTX is returned use machine
+   independent methods to make the address valid.  */
+DEFHOOK
+(legitimize_address,
+ "Define this to modify an invalid address @var{x} to be a valid address\n\
+with mode @var{mode} in the named address space @var{as}.  This target\n\
+hook is the same as the @code{TARGET_LEGITIMIZE_ADDRESS} target hook,\n\
+except that it includes explicit named address space support.",
+ rtx, (rtx x, rtx oldx, machine_mode mode, addr_space_t as),
+ default_addr_space_legitimize_address)
+
+/* True if one named address space is a subset of another named address. */
+DEFHOOK
+(subset_p,
+ "Define this to return whether the @var{subset} named address space is\n\
+contained within the @var{superset} named address space.  Pointers to\n\
+a named address space that is a subset of another named address space\n\
+will be converted automatically without a cast if used together in\n\
+arithmetic operations.  Pointers to a superset address space can be\n\
+converted to pointers to a subset address space via explicit casts.",
+ bool, (addr_space_t subset, addr_space_t superset),
+ default_addr_space_subset_p)
+
+/* True if 0 is a valid address in the address space, or false if
+   0 is a NULL in the address space.  */
+DEFHOOK
+(zero_address_valid,
+ "Define this to modify the default handling of address 0 for the\n\
+address space.  Return true if 0 should be considered a valid address.",
+ bool, (addr_space_t as),
+ default_addr_space_zero_address_valid)
+
+/* Function to convert an rtl expression from one address space to another.  */
+DEFHOOK
+(convert,
+ "Define this to convert the pointer expression represented by the RTL\n\
+@var{op} with type @var{from_type} that points to a named address\n\
+space to a new pointer expression with type @var{to_type} that points\n\
+to a different named address space.  When this hook it called, it is\n\
+guaranteed that one of the two address spaces is a subset of the other,\n\
+as determined by the @code{TARGET_ADDR_SPACE_SUBSET_P} target hook.",
+ rtx, (rtx op, tree from_type, tree to_type),
+ default_addr_space_convert)
+
+/* Function to encode an address space into dwarf.  */
+DEFHOOK
+(debug,
+ "Define this to define how the address space is encoded in dwarf.\n\
+The result is the value to be used with @code{DW_AT_address_class}.",
+ int, (addr_space_t as),
+ default_addr_space_debug)
+
+/* Function to emit custom diagnostic if an address space is used.  */
+DEFHOOK
+(diagnose_usage,
+ "Define this hook if the availability of an address space depends on\n\
+command line options and some diagnostics should be printed when the\n\
+address space is used.  This hook is called during parsing and allows\n\
+to emit a better diagnostic compared to the case where the address space\n\
+was not registered with @code{c_register_addr_space}.  @var{as} is\n\
+the address space as registered with @code{c_register_addr_space}.\n\
+@var{loc} is the location of the address space qualifier token.\n\
+The default implementation does nothing.",
+ void, (addr_space_t as, location_t loc),
+ default_addr_space_diagnose_usage)
+
+HOOK_VECTOR_END (addr_space)
+
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_"
+
+DEFHOOK
+(lower_local_decl_alignment,
+ "Define this hook to lower alignment of local, parm or result\n\
+decl @samp{(@var{decl})}.",
+ void, (tree decl),
+ hook_void_tree)
+
+DEFHOOK
+(static_rtx_alignment,
+ "This hook returns the preferred alignment in bits for a\n\
+statically-allocated rtx, such as a constant pool entry.  @var{mode}\n\
+is the mode of the rtx.  The default implementation returns\n\
+@samp{GET_MODE_ALIGNMENT (@var{mode})}.",
+ HOST_WIDE_INT, (machine_mode mode),
+ default_static_rtx_alignment)
+
+DEFHOOK
+(constant_alignment,
+ "This hook returns the alignment in bits of a constant that is being\n\
+placed in memory.  @var{constant} is the constant and @var{basic_align}\n\
+is the alignment that the object would ordinarily have.\n\
+\n\
+The default definition just returns @var{basic_align}.\n\
+\n\
+The typical use of this hook is to increase alignment for string\n\
+constants to be word aligned so that @code{strcpy} calls that copy\n\
+constants can be done inline.  The function\n\
+@code{constant_alignment_word_strings} provides such a definition.",
+ HOST_WIDE_INT, (const_tree constant, HOST_WIDE_INT basic_align),
+ default_constant_alignment)
+
+DEFHOOK
+(translate_mode_attribute,
+ "Define this hook if during mode attribute processing, the port should\n\
+translate machine_mode @var{mode} to another mode.  For example, rs6000's\n\
+@code{KFmode}, when it is the same as @code{TFmode}.\n\
+\n\
+The default version of the hook returns that mode that was passed in.",
+ machine_mode, (machine_mode mode),
+ default_translate_mode_attribute)
+
+/* True if MODE is valid for the target.  By "valid", we mean able to
+   be manipulated in non-trivial ways.  In particular, this means all
+   the arithmetic is supported.  */
+DEFHOOK
+(scalar_mode_supported_p,
+ "Define this to return nonzero if the port is prepared to handle\n\
+insns involving scalar mode @var{mode}.  For a scalar mode to be\n\
+considered supported, all the basic arithmetic and comparisons\n\
+must work.\n\
+\n\
+The default version of this hook returns true for any mode\n\
+required to handle the basic C types (as defined by the port).\n\
+Included here are the double-word arithmetic supported by the\n\
+code in @file{optabs.c}.",
+ bool, (scalar_mode mode),
+ default_scalar_mode_supported_p)
+
+/* Similarly for vector modes.  "Supported" here is less strict.  At
+   least some operations are supported; need to check optabs or builtins
+   for further details.  */
+DEFHOOK
+(vector_mode_supported_p,
+ "Define this to return nonzero if the port is prepared to handle\n\
+insns involving vector mode @var{mode}.  At the very least, it\n\
+must have move patterns for this mode.",
+ bool, (machine_mode mode),
+ hook_bool_mode_false)
+
+DEFHOOK
+(compatible_vector_types_p,
+ "Return true if there is no target-specific reason for treating\n\
+vector types @var{type1} and @var{type2} as distinct types.  The caller\n\
+has already checked for target-independent reasons, meaning that the\n\
+types are known to have the same mode, to have the same number of elements,\n\
+and to have what the caller considers to be compatible element types.\n\
+\n\
+The main reason for defining this hook is to reject pairs of types\n\
+that are handled differently by the target's calling convention.\n\
+For example, when a new @var{N}-bit vector architecture is added\n\
+to a target, the target may want to handle normal @var{N}-bit\n\
+@code{VECTOR_TYPE} arguments and return values in the same way as\n\
+before, to maintain backwards compatibility.  However, it may also\n\
+provide new, architecture-specific @code{VECTOR_TYPE}s that are passed\n\
+and returned in a more efficient way.  It is then important to maintain\n\
+a distinction between the ``normal'' @code{VECTOR_TYPE}s and the new\n\
+architecture-specific ones.\n\
+\n\
+The default implementation returns true, which is correct for most targets.",
+ bool, (const_tree type1, const_tree type2),
+ hook_bool_const_tree_const_tree_true)
+
+DEFHOOK
+(vector_alignment,
+ "This hook can be used to define the alignment for a vector of type\n\
+@var{type}, in order to comply with a platform ABI.  The default is to\n\
+require natural alignment for vector types.  The alignment returned by\n\
+this hook must be a power-of-two multiple of the default alignment of\n\
+the vector element type.",
+ HOST_WIDE_INT, (const_tree type),
+ default_vector_alignment)
+
+DEFHOOK
+(array_mode,
+ "Return the mode that GCC should use for an array that has\n\
+@var{nelems} elements, with each element having mode @var{mode}.\n\
+Return no mode if the target has no special requirements.  In the\n\
+latter case, GCC looks for an integer mode of the appropriate size\n\
+if available and uses BLKmode otherwise.  Usually the search for the\n\
+integer mode is limited to @code{MAX_FIXED_MODE_SIZE}, but the\n\
+@code{TARGET_ARRAY_MODE_SUPPORTED_P} hook allows a larger mode to be\n\
+used in specific cases.\n\
+\n\
+The main use of this hook is to specify that an array of vectors should\n\
+also have a vector mode.  The default implementation returns no mode.",
+ opt_machine_mode, (machine_mode mode, unsigned HOST_WIDE_INT nelems),
+ hook_optmode_mode_uhwi_none)
+
+/* True if we should try to use a scalar mode to represent an array,
+   overriding the usual MAX_FIXED_MODE limit.  */
+DEFHOOK
+(array_mode_supported_p,
+ "Return true if GCC should try to use a scalar mode to store an array\n\
+of @var{nelems} elements, given that each element has mode @var{mode}.\n\
+Returning true here overrides the usual @code{MAX_FIXED_MODE} limit\n\
+and allows GCC to use any defined integer mode.\n\
+\n\
+One use of this hook is to support vector load and store operations\n\
+that operate on several homogeneous vectors.  For example, ARM NEON\n\
+has operations like:\n\
+\n\
+@smallexample\n\
+int8x8x3_t vld3_s8 (const int8_t *)\n\
+@end smallexample\n\
+\n\
+where the return type is defined as:\n\
+\n\
+@smallexample\n\
+typedef struct int8x8x3_t\n\
+@{\n\
+  int8x8_t val[3];\n\
+@} int8x8x3_t;\n\
+@end smallexample\n\
+\n\
+If this hook allows @code{val} to have a scalar mode, then\n\
+@code{int8x8x3_t} can have the same mode.  GCC can then store\n\
+@code{int8x8x3_t}s in registers rather than forcing them onto the stack.",
+ bool, (machine_mode mode, unsigned HOST_WIDE_INT nelems),
+ hook_bool_mode_uhwi_false)
+
+DEFHOOK
+(libgcc_floating_mode_supported_p,
+ "Define this to return nonzero if libgcc provides support for the \n\
+floating-point mode @var{mode}, which is known to pass \n\
+@code{TARGET_SCALAR_MODE_SUPPORTED_P}.  The default version of this \n\
+hook returns true for all of @code{SFmode}, @code{DFmode}, \n\
+@code{XFmode} and @code{TFmode}, if such modes exist.",
+ bool, (scalar_float_mode mode),
+ default_libgcc_floating_mode_supported_p)
+
+DEFHOOK
+(floatn_mode,
+ "Define this to return the machine mode to use for the type \n\
+@code{_Float@var{n}}, if @var{extended} is false, or the type \n\
+@code{_Float@var{n}x}, if @var{extended} is true.  If such a type is not\n\
+supported, return @code{opt_scalar_float_mode ()}.  The default version of\n\
+this hook returns @code{SFmode} for @code{_Float32}, @code{DFmode} for\n\
+@code{_Float64} and @code{_Float32x} and @code{TFmode} for \n\
+@code{_Float128}, if those modes exist and satisfy the requirements for \n\
+those types and pass @code{TARGET_SCALAR_MODE_SUPPORTED_P} and \n\
+@code{TARGET_LIBGCC_FLOATING_MODE_SUPPORTED_P}; for @code{_Float64x}, it \n\
+returns the first of @code{XFmode} and @code{TFmode} that exists and \n\
+satisfies the same requirements; for other types, it returns \n\
+@code{opt_scalar_float_mode ()}.  The hook is only called for values\n\
+of @var{n} and @var{extended} that are valid according to\n\
+ISO/IEC TS 18661-3:2015; that is, @var{n} is one of 32, 64, 128, or,\n\
+if @var{extended} is false, 16 or greater than 128 and a multiple of 32.",
+ opt_scalar_float_mode, (int n, bool extended),
+ default_floatn_mode)
+
+DEFHOOK
+(floatn_builtin_p,
+  "Define this to return true if the @code{_Float@var{n}} and\n\
+@code{_Float@var{n}x} built-in functions should implicitly enable the\n\
+built-in function without the @code{__builtin_} prefix in addition to the\n\
+normal built-in function with the @code{__builtin_} prefix.  The default is\n\
+to only enable built-in functions without the @code{__builtin_} prefix for\n\
+the GNU C langauge.  In strict ANSI/ISO mode, the built-in function without\n\
+the @code{__builtin_} prefix is not enabled.  The argument @code{FUNC} is the\n\
+@code{enum built_in_function} id of the function to be enabled.",
+ bool, (int func),
+ default_floatn_builtin_p)
+
+/* Compute cost of moving data from a register of class FROM to one of
+   TO, using MODE.  */
+DEFHOOK
+(register_move_cost,
+ "This target hook should return the cost of moving data of mode @var{mode}\n\
+from a register in class @var{from} to one in class @var{to}.  The classes\n\
+are expressed using the enumeration values such as @code{GENERAL_REGS}.\n\
+A value of 2 is the default; other values are interpreted relative to\n\
+that.\n\
+\n\
+It is not required that the cost always equal 2 when @var{from} is the\n\
+same as @var{to}; on some machines it is expensive to move between\n\
+registers if they are not general registers.\n\
+\n\
+If reload sees an insn consisting of a single @code{set} between two\n\
+hard registers, and if @code{TARGET_REGISTER_MOVE_COST} applied to their\n\
+classes returns a value of 2, reload does not check to ensure that the\n\
+constraints of the insn are met.  Setting a cost of other than 2 will\n\
+allow reload to verify that the constraints are met.  You should do this\n\
+if the @samp{mov@var{m}} pattern's constraints do not allow such copying.\n\
+\n\
+The default version of this function returns 2.",
+ int, (machine_mode mode, reg_class_t from, reg_class_t to),
+ default_register_move_cost)
+
+/* Compute cost of moving registers to/from memory.  */
+/* ??? Documenting the argument types for this hook requires a GFDL
+   license grant.  Also, the documentation uses a different name for RCLASS.  */
+DEFHOOK
+(memory_move_cost,
+ "This target hook should return the cost of moving data of mode @var{mode}\n\
+between a register of class @var{rclass} and memory; @var{in} is @code{false}\n\
+if the value is to be written to memory, @code{true} if it is to be read in.\n\
+This cost is relative to those in @code{TARGET_REGISTER_MOVE_COST}.\n\
+If moving between registers and memory is more expensive than between two\n\
+registers, you should add this target hook to express the relative cost.\n\
+\n\
+If you do not add this target hook, GCC uses a default cost of 4 plus\n\
+the cost of copying via a secondary reload register, if one is\n\
+needed.  If your machine requires a secondary reload register to copy\n\
+between memory and a register of @var{rclass} but the reload mechanism is\n\
+more complex than copying via an intermediate, use this target hook to\n\
+reflect the actual cost of the move.\n\
+\n\
+GCC defines the function @code{memory_move_secondary_cost} if\n\
+secondary reloads are needed.  It computes the costs due to copying via\n\
+a secondary register.  If your machine copies from memory using a\n\
+secondary register in the conventional way but the default base value of\n\
+4 is not correct for your machine, use this target hook to add some other\n\
+value to the result of that function.  The arguments to that function\n\
+are the same as to this target hook.",
+ int, (machine_mode mode, reg_class_t rclass, bool in),
+ default_memory_move_cost)
+
+DEFHOOK
+(use_by_pieces_infrastructure_p,
+ "GCC will attempt several strategies when asked to copy between\n\
+two areas of memory, or to set, clear or store to memory, for example\n\
+when copying a @code{struct}. The @code{by_pieces} infrastructure\n\
+implements such memory operations as a sequence of load, store or move\n\
+insns.  Alternate strategies are to expand the\n\
+@code{cpymem} or @code{setmem} optabs, to emit a library call, or to emit\n\
+unit-by-unit, loop-based operations.\n\
+\n\
+This target hook should return true if, for a memory operation with a\n\
+given @var{size} and @var{alignment}, using the @code{by_pieces}\n\
+infrastructure is expected to result in better code generation.\n\
+Both @var{size} and @var{alignment} are measured in terms of storage\n\
+units.\n\
+\n\
+The parameter @var{op} is one of: @code{CLEAR_BY_PIECES},\n\
+@code{MOVE_BY_PIECES}, @code{SET_BY_PIECES}, @code{STORE_BY_PIECES} or\n\
+@code{COMPARE_BY_PIECES}.  These describe the type of memory operation\n\
+under consideration.\n\
+\n\
+The parameter @var{speed_p} is true if the code is currently being\n\
+optimized for speed rather than size.\n\
+\n\
+Returning true for higher values of @var{size} can improve code generation\n\
+for speed if the target does not provide an implementation of the\n\
+@code{cpymem} or @code{setmem} standard names, if the @code{cpymem} or\n\
+@code{setmem} implementation would be more expensive than a sequence of\n\
+insns, or if the overhead of a library call would dominate that of\n\
+the body of the memory operation.\n\
+\n\
+Returning true for higher values of @code{size} may also cause an increase\n\
+in code size, for example where the number of insns emitted to perform a\n\
+move would be greater than that of a library call.",
+ bool, (unsigned HOST_WIDE_INT size, unsigned int alignment,
+        enum by_pieces_operation op, bool speed_p),
+ default_use_by_pieces_infrastructure_p)
+
+DEFHOOK
+(compare_by_pieces_branch_ratio,
+ "When expanding a block comparison in MODE, gcc can try to reduce the\n\
+number of branches at the expense of more memory operations.  This hook\n\
+allows the target to override the default choice.  It should return the\n\
+factor by which branches should be reduced over the plain expansion with\n\
+one comparison per @var{mode}-sized piece.  A port can also prevent a\n\
+particular mode from being used for block comparisons by returning a\n\
+negative number from this hook.",
+ int, (machine_mode mode),
+ default_compare_by_pieces_branch_ratio)
+
+DEFHOOK
+(slow_unaligned_access,
+ "This hook returns true if memory accesses described by the\n\
+@var{mode} and @var{alignment} parameters have a cost many times greater\n\
+than aligned accesses, for example if they are emulated in a trap handler.\n\
+This hook is invoked only for unaligned accesses, i.e.@: when\n\
+@code{@var{alignment} < GET_MODE_ALIGNMENT (@var{mode})}.\n\
+\n\
+When this hook returns true, the compiler will act as if\n\
+@code{STRICT_ALIGNMENT} were true when generating code for block\n\
+moves.  This can cause significantly more instructions to be produced.\n\
+Therefore, do not make this hook return true if unaligned accesses only\n\
+add a cycle or two to the time for a memory access.\n\
+\n\
+The hook must return true whenever @code{STRICT_ALIGNMENT} is true.\n\
+The default implementation returns @code{STRICT_ALIGNMENT}.",
+ bool, (machine_mode mode, unsigned int align),
+ default_slow_unaligned_access)
+
+DEFHOOK
+(optab_supported_p,
+ "Return true if the optimizers should use optab @var{op} with\n\
+modes @var{mode1} and @var{mode2} for optimization type @var{opt_type}.\n\
+The optab is known to have an associated @file{.md} instruction\n\
+whose C condition is true.  @var{mode2} is only meaningful for conversion\n\
+optabs; for direct optabs it is a copy of @var{mode1}.\n\
+\n\
+For example, when called with @var{op} equal to @code{rint_optab} and\n\
+@var{mode1} equal to @code{DFmode}, the hook should say whether the\n\
+optimizers should use optab @code{rintdf2}.\n\
+\n\
+The default hook returns true for all inputs.",
+ bool, (int op, machine_mode mode1, machine_mode mode2,
+	optimization_type opt_type),
+ default_optab_supported_p)
+
+/* True for MODE if the target expects that registers in this mode will
+   be allocated to registers in a small register class.  The compiler is
+   allowed to use registers explicitly used in the rtl as spill registers
+   but it should prevent extending the lifetime of these registers.  */
+DEFHOOK
+(small_register_classes_for_mode_p,
+ "Define this to return nonzero for machine modes for which the port has\n\
+small register classes.  If this target hook returns nonzero for a given\n\
+@var{mode}, the compiler will try to minimize the lifetime of registers\n\
+in @var{mode}.  The hook may be called with @code{VOIDmode} as argument.\n\
+In this case, the hook is expected to return nonzero if it returns nonzero\n\
+for any mode.\n\
+\n\
+On some machines, it is risky to let hard registers live across arbitrary\n\
+insns.  Typically, these machines have instructions that require values\n\
+to be in specific registers (like an accumulator), and reload will fail\n\
+if the required hard register is used for another purpose across such an\n\
+insn.\n\
+\n\
+Passes before reload do not know which hard registers will be used\n\
+in an instruction, but the machine modes of the registers set or used in\n\
+the instruction are already known.  And for some machines, register\n\
+classes are small for, say, integer registers but not for floating point\n\
+registers.  For example, the AMD x86-64 architecture requires specific\n\
+registers for the legacy x86 integer instructions, but there are many\n\
+SSE registers for floating point operations.  On such targets, a good\n\
+strategy may be to return nonzero from this hook for @code{INTEGRAL_MODE_P}\n\
+machine modes but zero for the SSE register classes.\n\
+\n\
+The default version of this hook returns false for any mode.  It is always\n\
+safe to redefine this hook to return with a nonzero value.  But if you\n\
+unnecessarily define it, you will reduce the amount of optimizations\n\
+that can be performed in some cases.  If you do not define this hook\n\
+to return a nonzero value when it is required, the compiler will run out\n\
+of spill registers and print a fatal error message.",
+ bool, (machine_mode mode),
+ hook_bool_mode_false)
+
+/* Register number for a flags register.  Only needs to be defined if the
+   target is constrainted to use post-reload comparison elimination.  */
+DEFHOOKPOD
+(flags_regnum,
+ "If the target has a dedicated flags register, and it needs to use the\n\
+post-reload comparison elimination pass, or the delay slot filler pass,\n\
+then this value should be set appropriately.",
+unsigned int, INVALID_REGNUM)
+
+/* Compute a (partial) cost for rtx X.  Return true if the complete
+   cost has been computed, and false if subexpressions should be
+   scanned.  In either case, *TOTAL contains the cost result.  */
+/* Note that OUTER_CODE ought to be RTX_CODE, but that's
+   not necessarily defined at this point.  */
+DEFHOOK
+(rtx_costs,
+ "This target hook describes the relative costs of RTL expressions.\n\
+\n\
+The cost may depend on the precise form of the expression, which is\n\
+available for examination in @var{x}, and the fact that @var{x} appears\n\
+as operand @var{opno} of an expression with rtx code @var{outer_code}.\n\
+That is, the hook can assume that there is some rtx @var{y} such\n\
+that @samp{GET_CODE (@var{y}) == @var{outer_code}} and such that\n\
+either (a) @samp{XEXP (@var{y}, @var{opno}) == @var{x}} or\n\
+(b) @samp{XVEC (@var{y}, @var{opno})} contains @var{x}.\n\
+\n\
+@var{mode} is @var{x}'s machine mode, or for cases like @code{const_int} that\n\
+do not have a mode, the mode in which @var{x} is used.\n\
+\n\
+In implementing this hook, you can use the construct\n\
+@code{COSTS_N_INSNS (@var{n})} to specify a cost equal to @var{n} fast\n\
+instructions.\n\
+\n\
+On entry to the hook, @code{*@var{total}} contains a default estimate\n\
+for the cost of the expression.  The hook should modify this value as\n\
+necessary.  Traditionally, the default costs are @code{COSTS_N_INSNS (5)}\n\
+for multiplications, @code{COSTS_N_INSNS (7)} for division and modulus\n\
+operations, and @code{COSTS_N_INSNS (1)} for all other operations.\n\
+\n\
+When optimizing for code size, i.e.@: when @code{speed} is\n\
+false, this target hook should be used to estimate the relative\n\
+size cost of an expression, again relative to @code{COSTS_N_INSNS}.\n\
+\n\
+The hook returns true when all subexpressions of @var{x} have been\n\
+processed, and false when @code{rtx_cost} should recurse.",
+ bool, (rtx x, machine_mode mode, int outer_code, int opno, int *total, bool speed),
+ hook_bool_rtx_mode_int_int_intp_bool_false)
+
+/* Compute the cost of X, used as an address.  Never called with
+   invalid addresses.  */
+DEFHOOK
+(address_cost,
+ "This hook computes the cost of an addressing mode that contains\n\
+@var{address}.  If not defined, the cost is computed from\n\
+the @var{address} expression and the @code{TARGET_RTX_COST} hook.\n\
+\n\
+For most CISC machines, the default cost is a good approximation of the\n\
+true cost of the addressing mode.  However, on RISC machines, all\n\
+instructions normally have the same length and execution time.  Hence\n\
+all addresses will have equal costs.\n\
+\n\
+In cases where more than one form of an address is known, the form with\n\
+the lowest cost will be used.  If multiple forms have the same, lowest,\n\
+cost, the one that is the most complex will be used.\n\
+\n\
+For example, suppose an address that is equal to the sum of a register\n\
+and a constant is used twice in the same basic block.  When this macro\n\
+is not defined, the address will be computed in a register and memory\n\
+references will be indirect through that register.  On machines where\n\
+the cost of the addressing mode containing the sum is no higher than\n\
+that of a simple indirect reference, this will produce an additional\n\
+instruction and possibly require an additional register.  Proper\n\
+specification of this macro eliminates this overhead for such machines.\n\
+\n\
+This hook is never called with an invalid address.\n\
+\n\
+On machines where an address involving more than one register is as\n\
+cheap as an address computation involving only one register, defining\n\
+@code{TARGET_ADDRESS_COST} to reflect this can cause two registers to\n\
+be live over a region of code where only one would have been if\n\
+@code{TARGET_ADDRESS_COST} were not defined in that manner.  This effect\n\
+should be considered in the definition of this macro.  Equivalent costs\n\
+should probably only be given to addresses with different numbers of\n\
+registers on machines with lots of registers.",
+ int, (rtx address, machine_mode mode, addr_space_t as, bool speed),
+ default_address_cost)
+
+/* Compute a cost for INSN.  */
+DEFHOOK
+(insn_cost,
+ "This target hook describes the relative costs of RTL instructions.\n\
+\n\
+In implementing this hook, you can use the construct\n\
+@code{COSTS_N_INSNS (@var{n})} to specify a cost equal to @var{n} fast\n\
+instructions.\n\
+\n\
+When optimizing for code size, i.e.@: when @code{speed} is\n\
+false, this target hook should be used to estimate the relative\n\
+size cost of an expression, again relative to @code{COSTS_N_INSNS}.",
+ int, (rtx_insn *insn, bool speed), NULL)
+
+/* Give a cost, in RTX Costs units, for an edge.  Like BRANCH_COST, but with
+   well defined units.  */
+DEFHOOK
+(max_noce_ifcvt_seq_cost,
+ "This hook returns a value in the same units as @code{TARGET_RTX_COSTS},\n\
+giving the maximum acceptable cost for a sequence generated by the RTL\n\
+if-conversion pass when conditional execution is not available.\n\
+The RTL if-conversion pass attempts to convert conditional operations\n\
+that would require a branch to a series of unconditional operations and\n\
+@code{mov@var{mode}cc} insns.  This hook returns the maximum cost of the\n\
+unconditional instructions and the @code{mov@var{mode}cc} insns.\n\
+RTL if-conversion is cancelled if the cost of the converted sequence\n\
+is greater than the value returned by this hook.\n\
+\n\
+@code{e} is the edge between the basic block containing the conditional\n\
+branch to the basic block which would be executed if the condition\n\
+were true.\n\
+\n\
+The default implementation of this hook uses the\n\
+@code{max-rtl-if-conversion-[un]predictable} parameters if they are set,\n\
+and uses a multiple of @code{BRANCH_COST} otherwise.",
+unsigned int, (edge e),
+default_max_noce_ifcvt_seq_cost)
+
+/* Return true if the given instruction sequence is a good candidate
+   as a replacement for the if-convertible sequence.  */
+DEFHOOK
+(noce_conversion_profitable_p,
+ "This hook returns true if the instruction sequence @code{seq} is a good\n\
+candidate as a replacement for the if-convertible sequence described in\n\
+@code{if_info}.",
+bool, (rtx_insn *seq, struct noce_if_info *if_info),
+default_noce_conversion_profitable_p)
+
+/* Return true if new_addr should be preferred over the existing address used by
+   memref in insn.  */
+DEFHOOK
+(new_address_profitable_p,
+ "Return @code{true} if it is profitable to replace the address in\n\
+@var{memref} with @var{new_addr}.  This allows targets to prevent the\n\
+scheduler from undoing address optimizations.  The instruction containing the\n\
+memref is @var{insn}.  The default implementation returns @code{true}.",
+bool, (rtx memref, rtx_insn * insn, rtx new_addr),
+default_new_address_profitable_p)
+
+DEFHOOK
+(estimated_poly_value,
+ "Return an estimate of the runtime value of @var{val}, for use in\n\
+things like cost calculations or profiling frequencies.  @var{kind} is used\n\
+to ask for the minimum, maximum, and likely estimates of the value through\n\
+the @code{POLY_VALUE_MIN}, @code{POLY_VALUE_MAX} and\n\
+@code{POLY_VALUE_LIKELY} values.  The default\n\
+implementation returns the lowest possible value of @var{val}.",
+ HOST_WIDE_INT, (poly_int64 val, poly_value_estimate_kind kind),
+ default_estimated_poly_value)
+
+/* Permit speculative instructions in delay slots during delayed-branch 
+   scheduling.  */
+DEFHOOK
+(no_speculation_in_delay_slots_p,
+ "This predicate controls the use of the eager delay slot filler to disallow\n\
+speculatively executed instructions being placed in delay slots.  Targets\n\
+such as certain MIPS architectures possess both branches with and without\n\
+delay slots.  As the eager delay slot filler can decrease performance,\n\
+disabling it is beneficial when ordinary branches are available.  Use of\n\
+delay slot branches filled using the basic filler is often still desirable\n\
+as the delay slot can hide a pipeline bubble.", bool, (void),
+  hook_bool_void_false)
+
+/* Return where to allocate pseudo for a given hard register initial value.  */
+DEFHOOK
+(allocate_initial_value,
+ "\n\
+When the initial value of a hard register has been copied in a pseudo\n\
+register, it is often not necessary to actually allocate another register\n\
+to this pseudo register, because the original hard register or a stack slot\n\
+it has been saved into can be used.  @code{TARGET_ALLOCATE_INITIAL_VALUE}\n\
+is called at the start of register allocation once for each hard register\n\
+that had its initial value copied by using\n\
+@code{get_func_hard_reg_initial_val} or @code{get_hard_reg_initial_val}.\n\
+Possible values are @code{NULL_RTX}, if you don't want\n\
+to do any special allocation, a @code{REG} rtx---that would typically be\n\
+the hard register itself, if it is known not to be clobbered---or a\n\
+@code{MEM}.\n\
+If you are returning a @code{MEM}, this is only a hint for the allocator;\n\
+it might decide to use another register anyways.\n\
+You may use @code{current_function_is_leaf} or \n\
+@code{REG_N_SETS} in the hook to determine if the hard\n\
+register in question will not be clobbered.\n\
+The default value of this hook is @code{NULL}, which disables any special\n\
+allocation.",
+ rtx, (rtx hard_reg), NULL)
+
+/* Return nonzero if evaluating UNSPEC X might cause a trap.
+   FLAGS has the same meaning as in rtlanal.c: may_trap_p_1.  */
+DEFHOOK
+(unspec_may_trap_p,
+ "This target hook returns nonzero if @var{x}, an @code{unspec} or\n\
+@code{unspec_volatile} operation, might cause a trap.  Targets can use\n\
+this hook to enhance precision of analysis for @code{unspec} and\n\
+@code{unspec_volatile} operations.  You may call @code{may_trap_p_1}\n\
+to analyze inner elements of @var{x} in which case @var{flags} should be\n\
+passed along.",
+ int, (const_rtx x, unsigned flags),
+ default_unspec_may_trap_p)
+
+/* Given a register, this hook should return a parallel of registers
+   to represent where to find the register pieces.  Define this hook
+   if the register and its mode are represented in Dwarf in
+   non-contiguous locations, or if the register should be
+   represented in more than one register in Dwarf.  Otherwise, this
+   hook should return NULL_RTX.  */
+DEFHOOK
+(dwarf_register_span,
+ "Given a register, this hook should return a parallel of registers to\n\
+represent where to find the register pieces.  Define this hook if the\n\
+register and its mode are represented in Dwarf in non-contiguous\n\
+locations, or if the register should be represented in more than one\n\
+register in Dwarf.  Otherwise, this hook should return @code{NULL_RTX}.\n\
+If not defined, the default is to return @code{NULL_RTX}.",
+ rtx, (rtx reg),
+ hook_rtx_rtx_null)
+
+/* Given a register return the mode of the corresponding DWARF frame
+   register.  */
+DEFHOOK
+(dwarf_frame_reg_mode,
+ "Given a register, this hook should return the mode which the\n\
+corresponding Dwarf frame register should have.  This is normally\n\
+used to return a smaller mode than the raw mode to prevent call\n\
+clobbered parts of a register altering the frame register size",
+ machine_mode, (int regno),
+ default_dwarf_frame_reg_mode)
+
+/* If expand_builtin_init_dwarf_reg_sizes needs to fill in table
+   entries not corresponding directly to registers below
+   FIRST_PSEUDO_REGISTER, this hook should generate the necessary
+   code, given the address of the table.  */
+DEFHOOK
+(init_dwarf_reg_sizes_extra,
+ "If some registers are represented in Dwarf-2 unwind information in\n\
+multiple pieces, define this hook to fill in information about the\n\
+sizes of those pieces in the table used by the unwinder at runtime.\n\
+It will be called by @code{expand_builtin_init_dwarf_reg_sizes} after\n\
+filling in a single size corresponding to each hard register;\n\
+@var{address} is the address of the table.",
+ void, (tree address),
+ hook_void_tree)
+
+/* Fetch the fixed register(s) which hold condition codes, for
+   targets where it makes sense to look for duplicate assignments to
+   the condition codes.  This should return true if there is such a
+   register, false otherwise.  The arguments should be set to the
+   fixed register numbers.  Up to two condition code registers are
+   supported.  If there is only one for this target, the int pointed
+   at by the second argument should be set to -1.  */
+DEFHOOK
+(fixed_condition_code_regs,
+ "On targets which do not use @code{(cc0)}, and which use a hard\n\
+register rather than a pseudo-register to hold condition codes, the\n\
+regular CSE passes are often not able to identify cases in which the\n\
+hard register is set to a common value.  Use this hook to enable a\n\
+small pass which optimizes such cases.  This hook should return true\n\
+to enable this pass, and it should set the integers to which its\n\
+arguments point to the hard register numbers used for condition codes.\n\
+When there is only one such register, as is true on most systems, the\n\
+integer pointed to by @var{p2} should be set to\n\
+@code{INVALID_REGNUM}.\n\
+\n\
+The default version of this hook returns false.",
+ bool, (unsigned int *p1, unsigned int *p2),
+ hook_bool_uintp_uintp_false)
+
+/* If two condition code modes are compatible, return a condition
+     code mode which is compatible with both, such that a comparison
+     done in the returned mode will work for both of the original
+     modes.  If the condition code modes are not compatible, return
+     VOIDmode.  */
+DEFHOOK
+(cc_modes_compatible,
+ "On targets which use multiple condition code modes in class\n\
+@code{MODE_CC}, it is sometimes the case that a comparison can be\n\
+validly done in more than one mode.  On such a system, define this\n\
+target hook to take two mode arguments and to return a mode in which\n\
+both comparisons may be validly done.  If there is no such mode,\n\
+return @code{VOIDmode}.\n\
+\n\
+The default version of this hook checks whether the modes are the\n\
+same.  If they are, it returns that mode.  If they are different, it\n\
+returns @code{VOIDmode}.",
+ machine_mode, (machine_mode m1, machine_mode m2),
+ default_cc_modes_compatible)
+
+/* Do machine-dependent code transformations.  Called just before
+     delayed-branch scheduling.  */
+DEFHOOK
+(machine_dependent_reorg,
+ "If non-null, this hook performs a target-specific pass over the\n\
+instruction stream.  The compiler will run it at all optimization levels,\n\
+just before the point at which it normally does delayed-branch scheduling.\n\
+\n\
+The exact purpose of the hook varies from target to target.  Some use\n\
+it to do transformations that are necessary for correctness, such as\n\
+laying out in-function constant pools or avoiding hardware hazards.\n\
+Others use it as an opportunity to do some machine-dependent optimizations.\n\
+\n\
+You need not implement the hook if it has nothing to do.  The default\n\
+definition is null.",
+ void, (void), NULL)
+
+/* Create the __builtin_va_list type.  */
+DEFHOOK
+(build_builtin_va_list,
+ "This hook returns a type node for @code{va_list} for the target.\n\
+The default version of the hook returns @code{void*}.",
+ tree, (void),
+ std_build_builtin_va_list)
+
+/* Enumerate the va list variants.  */
+DEFHOOK
+(enum_va_list_p,
+ "This target hook is used in function @code{c_common_nodes_and_builtins}\n\
+to iterate through the target specific builtin types for va_list. The\n\
+variable @var{idx} is used as iterator. @var{pname} has to be a pointer\n\
+to a @code{const char *} and @var{ptree} a pointer to a @code{tree} typed\n\
+variable.\n\
+The arguments @var{pname} and @var{ptree} are used to store the result of\n\
+this macro and are set to the name of the va_list builtin type and its\n\
+internal type.\n\
+If the return value of this macro is zero, then there is no more element.\n\
+Otherwise the @var{IDX} should be increased for the next call of this\n\
+macro to iterate through all types.",
+ int, (int idx, const char **pname, tree *ptree),
+ NULL)
+
+/* Get the cfun/fndecl calling abi __builtin_va_list type.  */
+DEFHOOK
+(fn_abi_va_list,
+ "This hook returns the va_list type of the calling convention specified by\n\
+@var{fndecl}.\n\
+The default version of this hook returns @code{va_list_type_node}.",
+ tree, (tree fndecl),
+ std_fn_abi_va_list)
+
+/* Get the __builtin_va_list type dependent on input type.  */
+DEFHOOK
+(canonical_va_list_type,
+ "This hook returns the va_list type of the calling convention specified by the\n\
+type of @var{type}. If @var{type} is not a valid va_list type, it returns\n\
+@code{NULL_TREE}.",
+ tree, (tree type),
+ std_canonical_va_list_type)
+
+/* ??? Documenting this hook requires a GFDL license grant.  */
+DEFHOOK_UNDOC
+(expand_builtin_va_start,
+"Expand the @code{__builtin_va_start} builtin.",
+ void, (tree valist, rtx nextarg), NULL)
+
+/* Gimplifies a VA_ARG_EXPR.  */
+DEFHOOK
+(gimplify_va_arg_expr,
+ "This hook performs target-specific gimplification of\n\
+@code{VA_ARG_EXPR}.  The first two parameters correspond to the\n\
+arguments to @code{va_arg}; the latter two are as in\n\
+@code{gimplify.c:gimplify_expr}.",
+ tree, (tree valist, tree type, gimple_seq *pre_p, gimple_seq *post_p),
+ std_gimplify_va_arg_expr)
+
+/* Validity-checking routines for PCH files, target-specific.
+   get_pch_validity returns a pointer to the data to be stored,
+   and stores the size in its argument.  pch_valid_p gets the same
+   information back and returns NULL if the PCH is valid,
+   or an error message if not.  */
+DEFHOOK
+(get_pch_validity,
+ "This hook returns a pointer to the data needed by\n\
+@code{TARGET_PCH_VALID_P} and sets\n\
+@samp{*@var{sz}} to the size of the data in bytes.",
+ void *, (size_t *sz),
+ default_get_pch_validity)
+
+DEFHOOK
+(pch_valid_p,
+ "This hook checks whether the options used to create a PCH file are\n\
+compatible with the current settings.  It returns @code{NULL}\n\
+if so and a suitable error message if not.  Error messages will\n\
+be presented to the user and must be localized using @samp{_(@var{msg})}.\n\
+\n\
+@var{data} is the data that was returned by @code{TARGET_GET_PCH_VALIDITY}\n\
+when the PCH file was created and @var{sz} is the size of that data in bytes.\n\
+It's safe to assume that the data was created by the same version of the\n\
+compiler, so no format checking is needed.\n\
+\n\
+The default definition of @code{default_pch_valid_p} should be\n\
+suitable for most targets.",
+ const char *, (const void *data, size_t sz),
+ default_pch_valid_p)
+
+DEFHOOK
+(prepare_pch_save,
+ "Called before writing out a PCH file.  If the target has some\n\
+garbage-collected data that needs to be in a particular state on PCH loads,\n\
+it can use this hook to enforce that state.  Very few targets need\n\
+to do anything here.",
+ void, (void),
+ hook_void_void)
+
+/* If nonnull, this function checks whether a PCH file with the
+   given set of target flags can be used.  It returns NULL if so,
+   otherwise it returns an error message.  */
+DEFHOOK
+(check_pch_target_flags,
+ "If this hook is nonnull, the default implementation of\n\
+@code{TARGET_PCH_VALID_P} will use it to check for compatible values\n\
+of @code{target_flags}.  @var{pch_flags} specifies the value that\n\
+@code{target_flags} had when the PCH file was created.  The return\n\
+value is the same as for @code{TARGET_PCH_VALID_P}.",
+ const char *, (int pch_flags), NULL)
+
+/* True if the compiler should give an enum type only as many
+   bytes as it takes to represent the range of possible values of
+   that type.  */
+DEFHOOK
+(default_short_enums,
+ "This target hook should return true if the compiler should give an\n\
+@code{enum} type only as many bytes as it takes to represent the range\n\
+of possible values of that type.  It should return false if all\n\
+@code{enum} types should be allocated like @code{int}.\n\
+\n\
+The default is to return false.",
+ bool, (void),
+ hook_bool_void_false)
+
+/* This target hook returns an rtx that is used to store the address
+   of the current frame into the built-in setjmp buffer.  */
+DEFHOOK
+(builtin_setjmp_frame_value,
+ "This target hook should return an rtx that is used to store\n\
+the address of the current frame into the built in @code{setjmp} buffer.\n\
+The default value, @code{virtual_stack_vars_rtx}, is correct for most\n\
+machines.  One reason you may need to define this target hook is if\n\
+@code{hard_frame_pointer_rtx} is the appropriate value on your machine.",
+ rtx, (void),
+ default_builtin_setjmp_frame_value)
+
+/* This target hook should manipulate the outputs, inputs, constraints,
+   and clobbers the port wishes for pre-processing the asm.  */
+DEFHOOK
+(md_asm_adjust,
+ "This target hook may add @dfn{clobbers} to @var{clobbers} and\n\
+@var{clobbered_regs} for any hard regs the port wishes to automatically\n\
+clobber for an asm.  The @var{outputs} and @var{inputs} may be inspected\n\
+to avoid clobbering a register that is already used by the asm.\n\
+\n\
+It may modify the @var{outputs}, @var{inputs}, @var{input_modes}, and\n\
+@var{constraints} as necessary for other pre-processing.  In this case the\n\
+return value is a sequence of insns to emit after the asm.  Note that\n\
+changes to @var{inputs} must be accompanied by the corresponding changes\n\
+to @var{input_modes}.",
+ rtx_insn *,
+ (vec<rtx>& outputs, vec<rtx>& inputs, vec<machine_mode>& input_modes,
+  vec<const char *>& constraints, vec<rtx>& clobbers,
+  HARD_REG_SET& clobbered_regs),
+ NULL)
+
+/* This target hook allows the backend to specify a calling convention
+   in the debug information.  This function actually returns an
+   enum dwarf_calling_convention, but because of forward declarations
+   and not wanting to include dwarf2.h everywhere target.h is included
+   the function is being declared as an int.  */
+DEFHOOK
+(dwarf_calling_convention,
+ "Define this to enable the dwarf attribute @code{DW_AT_calling_convention} to\n\
+be emitted for each function.  Instead of an integer return the enum\n\
+value for the @code{DW_CC_} tag.",
+ int, (const_tree function),
+ hook_int_const_tree_0)
+
+/* This target hook allows the backend to emit frame-related insns that
+   contain UNSPECs or UNSPEC_VOLATILEs.  The call frame debugging info
+   engine will invoke it on insns of the form
+     (set (reg) (unspec [...] UNSPEC_INDEX))
+   and
+     (set (reg) (unspec_volatile [...] UNSPECV_INDEX))
+   to let the backend emit the call frame instructions.  */
+DEFHOOK
+(dwarf_handle_frame_unspec,
+ "This target hook allows the backend to emit frame-related insns that\n\
+contain UNSPECs or UNSPEC_VOLATILEs.  The DWARF 2 call frame debugging\n\
+info engine will invoke it on insns of the form\n\
+@smallexample\n\
+(set (reg) (unspec [@dots{}] UNSPEC_INDEX))\n\
+@end smallexample\n\
+and\n\
+@smallexample\n\
+(set (reg) (unspec_volatile [@dots{}] UNSPECV_INDEX)).\n\
+@end smallexample\n\
+to let the backend emit the call frame instructions.  @var{label} is\n\
+the CFI label attached to the insn, @var{pattern} is the pattern of\n\
+the insn and @var{index} is @code{UNSPEC_INDEX} or @code{UNSPECV_INDEX}.",
+ void, (const char *label, rtx pattern, int index), NULL)
+
+DEFHOOK
+(dwarf_poly_indeterminate_value,
+ "Express the value of @code{poly_int} indeterminate @var{i} as a DWARF\n\
+expression, with @var{i} counting from 1.  Return the number of a DWARF\n\
+register @var{R} and set @samp{*@var{factor}} and @samp{*@var{offset}} such\n\
+that the value of the indeterminate is:\n\
+@smallexample\n\
+value_of(@var{R}) / @var{factor} - @var{offset}\n\
+@end smallexample\n\
+\n\
+A target only needs to define this hook if it sets\n\
+@samp{NUM_POLY_INT_COEFFS} to a value greater than 1.",
+ unsigned int, (unsigned int i, unsigned int *factor, int *offset),
+ default_dwarf_poly_indeterminate_value)
+
+/* ??? Documenting this hook requires a GFDL license grant.  */
+DEFHOOK_UNDOC
+(stdarg_optimize_hook,
+"Perform architecture specific checking of statements gimplified\
+ from @code{VA_ARG_EXPR}.  @var{stmt} is the statement.  Returns true if\
+ the statement doesn't need to be checked for @code{va_list} references.",
+ bool, (struct stdarg_info *ai, const gimple *stmt), NULL)
+
+/* This target hook allows the operating system to override the DECL
+   that represents the external variable that contains the stack
+   protection guard variable.  The type of this DECL is ptr_type_node.  */
+DEFHOOK
+(stack_protect_guard,
+ "This hook returns a @code{DECL} node for the external variable to use\n\
+for the stack protection guard.  This variable is initialized by the\n\
+runtime to some random value and is used to initialize the guard value\n\
+that is placed at the top of the local stack frame.  The type of this\n\
+variable must be @code{ptr_type_node}.\n\
+\n\
+The default version of this hook creates a variable called\n\
+@samp{__stack_chk_guard}, which is normally defined in @file{libgcc2.c}.",
+ tree, (void),
+ default_stack_protect_guard)
+
+/* This target hook allows the operating system to override the CALL_EXPR
+   that is invoked when a check vs the guard variable fails.  */
+DEFHOOK
+(stack_protect_fail,
+ "This hook returns a @code{CALL_EXPR} that alerts the runtime that the\n\
+stack protect guard variable has been modified.  This expression should\n\
+involve a call to a @code{noreturn} function.\n\
+\n\
+The default version of this hook invokes a function called\n\
+@samp{__stack_chk_fail}, taking no arguments.  This function is\n\
+normally defined in @file{libgcc2.c}.",
+ tree, (void),
+ default_external_stack_protect_fail)
+
+/* This target hook allows the operating system to disable the default stack
+   protector runtime support.  */
+DEFHOOK
+(stack_protect_runtime_enabled_p,
+ "Returns true if the target wants GCC's default stack protect runtime support,\
+ otherwise return false.  The default implementation always returns true.",
+ bool, (void),
+ hook_bool_void_true)
+
+DEFHOOK
+(have_speculation_safe_value,
+"This hook is used to determine the level of target support for\n\
+ @code{__builtin_speculation_safe_value}.  If called with an argument\n\
+ of false, it returns true if the target has been modified to support\n\
+ this builtin.  If called with an argument of true, it returns true\n\
+ if the target requires active mitigation execution might be speculative.\n\
+ \n\
+ The default implementation returns false if the target does not define\n\
+ a pattern named @code{speculation_barrier}.  Else it returns true\n\
+ for the first case and whether the pattern is enabled for the current\n\
+ compilation for the second case.\n\
+ \n\
+ For targets that have no processors that can execute instructions\n\
+ speculatively an alternative implemenation of this hook is available:\n\
+ simply redefine this hook to @code{speculation_safe_value_not_needed}\n\
+ along with your other target hooks.",
+bool, (bool active), default_have_speculation_safe_value)
+
+DEFHOOK
+(speculation_safe_value,
+"This target hook can be used to generate a target-specific code\n\
+ sequence that implements the @code{__builtin_speculation_safe_value}\n\
+ built-in function.  The function must always return @var{val} in\n\
+ @var{result} in mode @var{mode} when the cpu is not executing\n\
+ speculatively, but must never return that when speculating until it\n\
+ is known that the speculation will not be unwound.  The hook supports\n\
+ two primary mechanisms for implementing the requirements.  The first\n\
+ is to emit a speculation barrier which forces the processor to wait\n\
+ until all prior speculative operations have been resolved; the second\n\
+ is to use a target-specific mechanism that can track the speculation\n\
+ state and to return @var{failval} if it can determine that\n\
+ speculation must be unwound at a later time.\n\
+ \n\
+ The default implementation simply copies @var{val} to @var{result} and\n\
+ emits a @code{speculation_barrier} instruction if that is defined.",
+rtx, (machine_mode mode, rtx result, rtx val, rtx failval),
+ default_speculation_safe_value)
+ 
+DEFHOOK
+(predict_doloop_p,
+ "Return true if we can predict it is possible to use a low-overhead loop\n\
+for a particular loop.  The parameter @var{loop} is a pointer to the loop.\n\
+This target hook is required only when the target supports low-overhead\n\
+loops, and will help ivopts to make some decisions.\n\
+The default version of this hook returns false.",
+ bool, (class loop *loop),
+ default_predict_doloop_p)
+
+DEFHOOKPOD
+(have_count_reg_decr_p,
+ "Return true if the target supports hardware count register for decrement\n\
+and branch.\n\
+The default value is false.",
+ bool, false)
+
+DEFHOOKPOD
+(doloop_cost_for_generic,
+ "One IV candidate dedicated for doloop is introduced in IVOPTs, we can\n\
+calculate the computation cost of adopting it to any generic IV use by\n\
+function get_computation_cost as before.  But for targets which have\n\
+hardware count register support for decrement and branch, it may have to\n\
+move IV value from hardware count register to general purpose register\n\
+while doloop IV candidate is used for generic IV uses.  It probably takes\n\
+expensive penalty.  This hook allows target owners to define the cost for\n\
+this especially for generic IV uses.\n\
+The default value is zero.",
+ int64_t, 0)
+
+DEFHOOKPOD
+(doloop_cost_for_address,
+ "One IV candidate dedicated for doloop is introduced in IVOPTs, we can\n\
+calculate the computation cost of adopting it to any address IV use by\n\
+function get_computation_cost as before.  But for targets which have\n\
+hardware count register support for decrement and branch, it may have to\n\
+move IV value from hardware count register to general purpose register\n\
+while doloop IV candidate is used for address IV uses.  It probably takes\n\
+expensive penalty.  This hook allows target owners to define the cost for\n\
+this escpecially for address IV uses.\n\
+The default value is zero.",
+ int64_t, 0)
+
+DEFHOOK
+(can_use_doloop_p,
+ "Return true if it is possible to use low-overhead loops (@code{doloop_end}\n\
+and @code{doloop_begin}) for a particular loop.  @var{iterations} gives the\n\
+exact number of iterations, or 0 if not known.  @var{iterations_max} gives\n\
+the maximum number of iterations, or 0 if not known.  @var{loop_depth} is\n\
+the nesting depth of the loop, with 1 for innermost loops, 2 for loops that\n\
+contain innermost loops, and so on.  @var{entered_at_top} is true if the\n\
+loop is only entered from the top.\n\
+\n\
+This hook is only used if @code{doloop_end} is available.  The default\n\
+implementation returns true.  You can use @code{can_use_doloop_if_innermost}\n\
+if the loop must be the innermost, and if there are no other restrictions.",
+ bool, (const widest_int &iterations, const widest_int &iterations_max,
+	unsigned int loop_depth, bool entered_at_top),
+ hook_bool_wint_wint_uint_bool_true)
+
+/* Returns NULL if target supports the insn within a doloop block,
+   otherwise it returns an error message.  */
+DEFHOOK
+(invalid_within_doloop,
+ "\n\
+Take an instruction in @var{insn} and return NULL if it is valid within a\n\
+low-overhead loop, otherwise return a string explaining why doloop\n\
+could not be applied.\n\
+\n\
+Many targets use special registers for low-overhead looping. For any\n\
+instruction that clobbers these this function should return a string indicating\n\
+the reason why the doloop could not be applied.\n\
+By default, the RTL loop optimizer does not use a present doloop pattern for\n\
+loops containing function calls or branch on table instructions.",
+ const char *, (const rtx_insn *insn),
+ default_invalid_within_doloop)
+
+/* Returns true for a legitimate combined insn.  */
+DEFHOOK
+(legitimate_combined_insn,
+"Take an instruction in @var{insn} and return @code{false} if the instruction\
+ is not appropriate as a combination of two or more instructions.  The\
+ default is to accept all instructions.",
+ bool, (rtx_insn *insn),
+ hook_bool_rtx_insn_true)
+
+DEFHOOK
+(valid_dllimport_attribute_p,
+"@var{decl} is a variable or function with @code{__attribute__((dllimport))}\
+ specified.  Use this hook if the target needs to add extra validation\
+ checks to @code{handle_dll_attribute}.",
+ bool, (const_tree decl),
+ hook_bool_const_tree_true)
+
+/* If non-zero, align constant anchors in CSE to a multiple of this
+   value.  */
+DEFHOOKPOD
+(const_anchor,
+ "On some architectures it can take multiple instructions to synthesize\n\
+a constant.  If there is another constant already in a register that\n\
+is close enough in value then it is preferable that the new constant\n\
+is computed from this register using immediate addition or\n\
+subtraction.  We accomplish this through CSE.  Besides the value of\n\
+the constant we also add a lower and an upper constant anchor to the\n\
+available expressions.  These are then queried when encountering new\n\
+constants.  The anchors are computed by rounding the constant up and\n\
+down to a multiple of the value of @code{TARGET_CONST_ANCHOR}.\n\
+@code{TARGET_CONST_ANCHOR} should be the maximum positive value\n\
+accepted by immediate-add plus one.  We currently assume that the\n\
+value of @code{TARGET_CONST_ANCHOR} is a power of 2.  For example, on\n\
+MIPS, where add-immediate takes a 16-bit signed value,\n\
+@code{TARGET_CONST_ANCHOR} is set to @samp{0x8000}.  The default value\n\
+is zero, which disables this optimization.",
+ unsigned HOST_WIDE_INT, 0)
+
+/* Defines, which target-dependent bits (upper 16) are used by port  */
+DEFHOOK
+(memmodel_check,
+ "Validate target specific memory model mask bits. When NULL no target specific\n\
+memory model bits are allowed.",
+ unsigned HOST_WIDE_INT, (unsigned HOST_WIDE_INT val), NULL)
+
+/* Defines an offset bitwise ored into shifted address to get corresponding
+   Address Sanitizer shadow address, or -1 if Address Sanitizer is not
+   supported by the target.  */
+DEFHOOK
+(asan_shadow_offset,
+ "Return the offset bitwise ored into shifted address to get corresponding\n\
+Address Sanitizer shadow memory address.  NULL if Address Sanitizer is not\n\
+supported by the target.  May return 0 if Address Sanitizer is not supported\n\
+by a subtarget.",
+ unsigned HOST_WIDE_INT, (void),
+ NULL)
+
+/* Functions relating to calls - argument passing, returns, etc.  */
+/* Members of struct call have no special macro prefix.  */
+HOOK_VECTOR (TARGET_CALLS, calls)
+
+DEFHOOK
+(promote_function_mode,
+ "Like @code{PROMOTE_MODE}, but it is applied to outgoing function arguments or\n\
+function return values.  The target hook should return the new mode\n\
+and possibly change @code{*@var{punsignedp}} if the promotion should\n\
+change signedness.  This function is called only for scalar @emph{or\n\
+pointer} types.\n\
+\n\
+@var{for_return} allows to distinguish the promotion of arguments and\n\
+return values.  If it is @code{1}, a return value is being promoted and\n\
+@code{TARGET_FUNCTION_VALUE} must perform the same promotions done here.\n\
+If it is @code{2}, the returned mode should be that of the register in\n\
+which an incoming parameter is copied, or the outgoing result is computed;\n\
+then the hook should return the same mode as @code{promote_mode}, though\n\
+the signedness may be different.\n\
+\n\
+@var{type} can be NULL when promoting function arguments of libcalls.\n\
+\n\
+The default is to not promote arguments and return values.  You can\n\
+also define the hook to @code{default_promote_function_mode_always_promote}\n\
+if you would like to apply the same rules given by @code{PROMOTE_MODE}.",
+ machine_mode, (const_tree type, machine_mode mode, int *punsignedp,
+		     const_tree funtype, int for_return),
+ default_promote_function_mode)
+
+DEFHOOK
+(promote_prototypes,
+ "This target hook returns @code{true} if an argument declared in a\n\
+prototype as an integral type smaller than @code{int} should actually be\n\
+passed as an @code{int}.  In addition to avoiding errors in certain\n\
+cases of mismatch, it also makes for better code on certain machines.\n\
+The default is to not promote prototypes.",
+ bool, (const_tree fntype),
+ hook_bool_const_tree_false)
+
+DEFHOOK
+(struct_value_rtx,
+ "This target hook should return the location of the structure value\n\
+address (normally a @code{mem} or @code{reg}), or 0 if the address is\n\
+passed as an ``invisible'' first argument.  Note that @var{fndecl} may\n\
+be @code{NULL}, for libcalls.  You do not need to define this target\n\
+hook if the address is always passed as an ``invisible'' first\n\
+argument.\n\
+\n\
+On some architectures the place where the structure value address\n\
+is found by the called function is not the same place that the\n\
+caller put it.  This can be due to register windows, or it could\n\
+be because the function prologue moves it to a different place.\n\
+@var{incoming} is @code{1} or @code{2} when the location is needed in\n\
+the context of the called function, and @code{0} in the context of\n\
+the caller.\n\
+\n\
+If @var{incoming} is nonzero and the address is to be found on the\n\
+stack, return a @code{mem} which refers to the frame pointer. If\n\
+@var{incoming} is @code{2}, the result is being used to fetch the\n\
+structure value address at the beginning of a function.  If you need\n\
+to emit adjusting code, you should do it at this point.",
+ rtx, (tree fndecl, int incoming),
+ hook_rtx_tree_int_null)
+
+DEFHOOKPOD
+(omit_struct_return_reg,
+ "Normally, when a function returns a structure by memory, the address\n\
+is passed as an invisible pointer argument, but the compiler also\n\
+arranges to return the address from the function like it would a normal\n\
+pointer return value.  Define this to true if that behavior is\n\
+undesirable on your target.",
+ bool, false)
+
+DEFHOOK
+(return_in_memory,
+ "This target hook should return a nonzero value to say to return the\n\
+function value in memory, just as large structures are always returned.\n\
+Here @var{type} will be the data type of the value, and @var{fntype}\n\
+will be the type of the function doing the returning, or @code{NULL} for\n\
+libcalls.\n\
+\n\
+Note that values of mode @code{BLKmode} must be explicitly handled\n\
+by this function.  Also, the option @option{-fpcc-struct-return}\n\
+takes effect regardless of this macro.  On most systems, it is\n\
+possible to leave the hook undefined; this causes a default\n\
+definition to be used, whose value is the constant 1 for @code{BLKmode}\n\
+values, and 0 otherwise.\n\
+\n\
+Do not use this hook to indicate that structures and unions should always\n\
+be returned in memory.  You should instead use @code{DEFAULT_PCC_STRUCT_RETURN}\n\
+to indicate this.",
+ bool, (const_tree type, const_tree fntype),
+ default_return_in_memory)
+
+DEFHOOK
+(return_in_msb,
+ "This hook should return true if values of type @var{type} are returned\n\
+at the most significant end of a register (in other words, if they are\n\
+padded at the least significant end).  You can assume that @var{type}\n\
+is returned in a register; the caller is required to check this.\n\
+\n\
+Note that the register provided by @code{TARGET_FUNCTION_VALUE} must\n\
+be able to hold the complete return value.  For example, if a 1-, 2-\n\
+or 3-byte structure is returned at the most significant end of a\n\
+4-byte register, @code{TARGET_FUNCTION_VALUE} should provide an\n\
+@code{SImode} rtx.",
+ bool, (const_tree type),
+ hook_bool_const_tree_false)
+
+/* Return true if a parameter must be passed by reference.  TYPE may
+   be null if this is a libcall.  CA may be null if this query is
+   from __builtin_va_arg.  */
+DEFHOOK
+(pass_by_reference,
+ "This target hook should return @code{true} if argument @var{arg} at the\n\
+position indicated by @var{cum} should be passed by reference.  This\n\
+predicate is queried after target independent reasons for being\n\
+passed by reference, such as @code{TREE_ADDRESSABLE (@var{arg}.type)}.\n\
+\n\
+If the hook returns true, a copy of that argument is made in memory and a\n\
+pointer to the argument is passed instead of the argument itself.\n\
+The pointer is passed in whatever way is appropriate for passing a pointer\n\
+to that type.",
+ bool,
+ (cumulative_args_t cum, const function_arg_info &arg),
+ hook_bool_CUMULATIVE_ARGS_arg_info_false)
+
+DEFHOOK
+(expand_builtin_saveregs,
+ "If defined, this hook produces the machine-specific code for a call to\n\
+@code{__builtin_saveregs}.  This code will be moved to the very\n\
+beginning of the function, before any parameter access are made.  The\n\
+return value of this function should be an RTX that contains the value\n\
+to use as the return of @code{__builtin_saveregs}.",
+ rtx, (void),
+ default_expand_builtin_saveregs)
+
+/* Returns pretend_argument_size.  */
+DEFHOOK
+(setup_incoming_varargs,
+ "This target hook offers an alternative to using\n\
+@code{__builtin_saveregs} and defining the hook\n\
+@code{TARGET_EXPAND_BUILTIN_SAVEREGS}.  Use it to store the anonymous\n\
+register arguments into the stack so that all the arguments appear to\n\
+have been passed consecutively on the stack.  Once this is done, you can\n\
+use the standard implementation of varargs that works for machines that\n\
+pass all their arguments on the stack.\n\
+\n\
+The argument @var{args_so_far} points to the @code{CUMULATIVE_ARGS} data\n\
+structure, containing the values that are obtained after processing the\n\
+named arguments.  The argument @var{arg} describes the last of these named\n\
+arguments.\n\
+\n\
+The target hook should do two things: first, push onto the stack all the\n\
+argument registers @emph{not} used for the named arguments, and second,\n\
+store the size of the data thus pushed into the @code{int}-valued\n\
+variable pointed to by @var{pretend_args_size}.  The value that you\n\
+store here will serve as additional offset for setting up the stack\n\
+frame.\n\
+\n\
+Because you must generate code to push the anonymous arguments at\n\
+compile time without knowing their data types,\n\
+@code{TARGET_SETUP_INCOMING_VARARGS} is only useful on machines that\n\
+have just a single category of argument register and use it uniformly\n\
+for all data types.\n\
+\n\
+If the argument @var{second_time} is nonzero, it means that the\n\
+arguments of the function are being analyzed for the second time.  This\n\
+happens for an inline function, which is not actually compiled until the\n\
+end of the source file.  The hook @code{TARGET_SETUP_INCOMING_VARARGS} should\n\
+not generate any instructions in this case.",
+ void, (cumulative_args_t args_so_far, const function_arg_info &arg,
+	int *pretend_args_size, int second_time),
+ default_setup_incoming_varargs)
+
+DEFHOOK
+(load_bounds_for_arg,
+ "This hook is used by expand pass to emit insn to load bounds of\n\
+@var{arg} passed in @var{slot}.  Expand pass uses this hook in case\n\
+bounds of @var{arg} are not passed in register.  If @var{slot} is a\n\
+memory, then bounds are loaded as for regular pointer loaded from\n\
+memory.  If @var{slot} is not a memory then @var{slot_no} is an integer\n\
+constant holding number of the target dependent special slot which\n\
+should be used to obtain bounds.  Hook returns RTX holding loaded bounds.",
+ rtx, (rtx slot, rtx arg, rtx slot_no),
+ default_load_bounds_for_arg)
+
+DEFHOOK
+(store_bounds_for_arg,
+ "This hook is used by expand pass to emit insns to store @var{bounds} of\n\
+@var{arg} passed in @var{slot}.  Expand pass uses this hook in case\n\
+@var{bounds} of @var{arg} are not passed in register.  If @var{slot} is a\n\
+memory, then @var{bounds} are stored as for regular pointer stored in\n\
+memory.  If @var{slot} is not a memory then @var{slot_no} is an integer\n\
+constant holding number of the target dependent special slot which\n\
+should be used to store @var{bounds}.",
+ void, (rtx arg, rtx slot, rtx bounds, rtx slot_no),
+ default_store_bounds_for_arg)
+
+DEFHOOK
+(load_returned_bounds,
+ "This hook is used by expand pass to emit insn to load bounds\n\
+returned by function call in @var{slot}.  Hook returns RTX holding\n\
+loaded bounds.",
+ rtx, (rtx slot),
+ default_load_returned_bounds)
+
+DEFHOOK
+(store_returned_bounds,
+ "This hook is used by expand pass to emit insn to store @var{bounds}\n\
+returned by function call into @var{slot}.",
+ void, (rtx slot, rtx bounds),
+ default_store_returned_bounds)
+
+DEFHOOK
+(call_args,
+ "While generating RTL for a function call, this target hook is invoked once\n\
+for each argument passed to the function, either a register returned by\n\
+@code{TARGET_FUNCTION_ARG} or a memory location.  It is called just\n\
+before the point where argument registers are stored.  The type of the\n\
+function to be called is also passed as the second argument; it is\n\
+@code{NULL_TREE} for libcalls.  The @code{TARGET_END_CALL_ARGS} hook is\n\
+invoked just after the code to copy the return reg has been emitted.\n\
+This functionality can be used to perform special setup of call argument\n\
+registers if a target needs it.\n\
+For functions without arguments, the hook is called once with @code{pc_rtx}\n\
+passed instead of an argument register.\n\
+Most ports do not need to implement anything for this hook.",
+ void, (rtx, tree),
+ hook_void_rtx_tree)
+
+DEFHOOK
+(end_call_args,
+ "This target hook is invoked while generating RTL for a function call,\n\
+just after the point where the return reg is copied into a pseudo.  It\n\
+signals that all the call argument and return registers for the just\n\
+emitted call are now no longer in use.\n\
+Most ports do not need to implement anything for this hook.",
+ void, (void),
+ hook_void_void)
+
+DEFHOOK
+(strict_argument_naming,
+ "Define this hook to return @code{true} if the location where a function\n\
+argument is passed depends on whether or not it is a named argument.\n\
+\n\
+This hook controls how the @var{named} argument to @code{TARGET_FUNCTION_ARG}\n\
+is set for varargs and stdarg functions.  If this hook returns\n\
+@code{true}, the @var{named} argument is always true for named\n\
+arguments, and false for unnamed arguments.  If it returns @code{false},\n\
+but @code{TARGET_PRETEND_OUTGOING_VARARGS_NAMED} returns @code{true},\n\
+then all arguments are treated as named.  Otherwise, all named arguments\n\
+except the last are treated as named.\n\
+\n\
+You need not define this hook if it always returns @code{false}.",
+ bool, (cumulative_args_t ca),
+ hook_bool_CUMULATIVE_ARGS_false)
+
+/* Returns true if we should use
+   targetm.calls.setup_incoming_varargs() and/or
+   targetm.calls.strict_argument_naming().  */
+DEFHOOK
+(pretend_outgoing_varargs_named,
+ "If you need to conditionally change ABIs so that one works with\n\
+@code{TARGET_SETUP_INCOMING_VARARGS}, but the other works like neither\n\
+@code{TARGET_SETUP_INCOMING_VARARGS} nor @code{TARGET_STRICT_ARGUMENT_NAMING} was\n\
+defined, then define this hook to return @code{true} if\n\
+@code{TARGET_SETUP_INCOMING_VARARGS} is used, @code{false} otherwise.\n\
+Otherwise, you should not define this hook.",
+ bool, (cumulative_args_t ca),
+ default_pretend_outgoing_varargs_named)
+
+/* Given a complex type T, return true if a parameter of type T
+   should be passed as two scalars.  */
+DEFHOOK
+(split_complex_arg,
+ "This hook should return true if parameter of type @var{type} are passed\n\
+as two scalar parameters.  By default, GCC will attempt to pack complex\n\
+arguments into the target's word size.  Some ABIs require complex arguments\n\
+to be split and treated as their individual components.  For example, on\n\
+AIX64, complex floats should be passed in a pair of floating point\n\
+registers, even though a complex float would fit in one 64-bit floating\n\
+point register.\n\
+\n\
+The default value of this hook is @code{NULL}, which is treated as always\n\
+false.",
+ bool, (const_tree type), NULL)
+
+/* Return true if type T, mode MODE, may not be passed in registers,
+   but must be passed on the stack.  */
+/* ??? This predicate should be applied strictly after pass-by-reference.
+   Need audit to verify that this is the case.  */
+DEFHOOK
+(must_pass_in_stack,
+ "This target hook should return @code{true} if we should not pass @var{arg}\n\
+solely in registers.  The file @file{expr.h} defines a\n\
+definition that is usually appropriate, refer to @file{expr.h} for additional\n\
+documentation.",
+ bool, (const function_arg_info &arg),
+ must_pass_in_stack_var_size_or_pad)
+
+/* Return true if type TYPE, mode MODE, which is passed by reference,
+   should have the object copy generated by the callee rather than
+   the caller.  It is never called for TYPE requiring constructors.  */
+DEFHOOK
+(callee_copies,
+ "The function argument described by the parameters to this hook is\n\
+known to be passed by reference.  The hook should return true if the\n\
+function argument should be copied by the callee instead of copied\n\
+by the caller.\n\
+\n\
+For any argument for which the hook returns true, if it can be\n\
+determined that the argument is not modified, then a copy need\n\
+not be generated.\n\
+\n\
+The default version of this hook always returns false.",
+ bool,
+ (cumulative_args_t cum, const function_arg_info &arg),
+ hook_bool_CUMULATIVE_ARGS_arg_info_false)
+
+/* Return zero for arguments passed entirely on the stack or entirely
+   in registers.  If passed in both, return the number of bytes passed
+   in registers; the balance is therefore passed on the stack.  */
+DEFHOOK
+(arg_partial_bytes,
+ "This target hook returns the number of bytes at the beginning of an\n\
+argument that must be put in registers.  The value must be zero for\n\
+arguments that are passed entirely in registers or that are entirely\n\
+pushed on the stack.\n\
+\n\
+On some machines, certain arguments must be passed partially in\n\
+registers and partially in memory.  On these machines, typically the\n\
+first few words of arguments are passed in registers, and the rest\n\
+on the stack.  If a multi-word argument (a @code{double} or a\n\
+structure) crosses that boundary, its first few words must be passed\n\
+in registers and the rest must be pushed.  This macro tells the\n\
+compiler when this occurs, and how many bytes should go in registers.\n\
+\n\
+@code{TARGET_FUNCTION_ARG} for these arguments should return the first\n\
+register to be used by the caller for this argument; likewise\n\
+@code{TARGET_FUNCTION_INCOMING_ARG}, for the called function.",
+ int, (cumulative_args_t cum, const function_arg_info &arg),
+ hook_int_CUMULATIVE_ARGS_arg_info_0)
+
+/* Update the state in CA to advance past an argument in the
+   argument list.  The values MODE, TYPE, and NAMED describe that
+   argument.  */
+DEFHOOK
+(function_arg_advance,
+ "This hook updates the summarizer variable pointed to by @var{ca} to\n\
+advance past argument @var{arg} in the argument list.  Once this is done,\n\
+the variable @var{cum} is suitable for analyzing the @emph{following}\n\
+argument with @code{TARGET_FUNCTION_ARG}, etc.\n\
+\n\
+This hook need not do anything if the argument in question was passed\n\
+on the stack.  The compiler knows how to track the amount of stack space\n\
+used for arguments without any special help.",
+ void,
+ (cumulative_args_t ca, const function_arg_info &arg),
+ default_function_arg_advance)
+
+DEFHOOK
+(function_arg_offset,
+ "This hook returns the number of bytes to add to the offset of an\n\
+argument of type @var{type} and mode @var{mode} when passed in memory.\n\
+This is needed for the SPU, which passes @code{char} and @code{short}\n\
+arguments in the preferred slot that is in the middle of the quad word\n\
+instead of starting at the top.  The default implementation returns 0.",
+ HOST_WIDE_INT, (machine_mode mode, const_tree type),
+ default_function_arg_offset)
+
+DEFHOOK
+(function_arg_padding,
+ "This hook determines whether, and in which direction, to pad out\n\
+an argument of mode @var{mode} and type @var{type}.  It returns\n\
+@code{PAD_UPWARD} to insert padding above the argument, @code{PAD_DOWNWARD}\n\
+to insert padding below the argument, or @code{PAD_NONE} to inhibit padding.\n\
+\n\
+The @emph{amount} of padding is not controlled by this hook, but by\n\
+@code{TARGET_FUNCTION_ARG_ROUND_BOUNDARY}.  It is always just enough\n\
+to reach the next multiple of that boundary.\n\
+\n\
+This hook has a default definition that is right for most systems.\n\
+For little-endian machines, the default is to pad upward.  For\n\
+big-endian machines, the default is to pad downward for an argument of\n\
+constant size shorter than an @code{int}, and upward otherwise.",
+ pad_direction, (machine_mode mode, const_tree type),
+ default_function_arg_padding)
+
+/* Return zero if the argument described by the state of CA should
+   be placed on a stack, or a hard register in which to store the
+   argument.  The values MODE, TYPE, and NAMED describe that
+   argument.  */
+DEFHOOK
+(function_arg,
+ "Return an RTX indicating whether function argument @var{arg} is passed\n\
+in a register and if so, which register.  Argument @var{ca} summarizes all\n\
+the previous arguments.\n\
+\n\
+The return value is usually either a @code{reg} RTX for the hard\n\
+register in which to pass the argument, or zero to pass the argument\n\
+on the stack.\n\
+\n\
+The return value can be a @code{const_int} which means argument is\n\
+passed in a target specific slot with specified number.  Target hooks\n\
+should be used to store or load argument in such case.  See\n\
+@code{TARGET_STORE_BOUNDS_FOR_ARG} and @code{TARGET_LOAD_BOUNDS_FOR_ARG}\n\
+for more information.\n\
+\n\
+The value of the expression can also be a @code{parallel} RTX@.  This is\n\
+used when an argument is passed in multiple locations.  The mode of the\n\
+@code{parallel} should be the mode of the entire argument.  The\n\
+@code{parallel} holds any number of @code{expr_list} pairs; each one\n\
+describes where part of the argument is passed.  In each\n\
+@code{expr_list} the first operand must be a @code{reg} RTX for the hard\n\
+register in which to pass this part of the argument, and the mode of the\n\
+register RTX indicates how large this part of the argument is.  The\n\
+second operand of the @code{expr_list} is a @code{const_int} which gives\n\
+the offset in bytes into the entire argument of where this part starts.\n\
+As a special exception the first @code{expr_list} in the @code{parallel}\n\
+RTX may have a first operand of zero.  This indicates that the entire\n\
+argument is also stored on the stack.\n\
+\n\
+The last time this hook is called, it is called with @code{MODE ==\n\
+VOIDmode}, and its result is passed to the @code{call} or @code{call_value}\n\
+pattern as operands 2 and 3 respectively.\n\
+\n\
+@cindex @file{stdarg.h} and register arguments\n\
+The usual way to make the ISO library @file{stdarg.h} work on a\n\
+machine where some arguments are usually passed in registers, is to\n\
+cause nameless arguments to be passed on the stack instead.  This is\n\
+done by making @code{TARGET_FUNCTION_ARG} return 0 whenever\n\
+@var{named} is @code{false}.\n\
+\n\
+@cindex @code{TARGET_MUST_PASS_IN_STACK}, and @code{TARGET_FUNCTION_ARG}\n\
+@cindex @code{REG_PARM_STACK_SPACE}, and @code{TARGET_FUNCTION_ARG}\n\
+You may use the hook @code{targetm.calls.must_pass_in_stack}\n\
+in the definition of this macro to determine if this argument is of a\n\
+type that must be passed in the stack.  If @code{REG_PARM_STACK_SPACE}\n\
+is not defined and @code{TARGET_FUNCTION_ARG} returns nonzero for such an\n\
+argument, the compiler will abort.  If @code{REG_PARM_STACK_SPACE} is\n\
+defined, the argument will be computed in the stack and then loaded into\n\
+a register.",
+ rtx, (cumulative_args_t ca, const function_arg_info &arg),
+ default_function_arg)
+
+DEFHOOK
+(function_incoming_arg,
+ "Define this hook if the caller and callee on the target have different\n\
+views of where arguments are passed.  Also define this hook if there are\n\
+functions that are never directly called, but are invoked by the hardware\n\
+and which have nonstandard calling conventions.\n\
+\n\
+In this case @code{TARGET_FUNCTION_ARG} computes the register in\n\
+which the caller passes the value, and\n\
+@code{TARGET_FUNCTION_INCOMING_ARG} should be defined in a similar\n\
+fashion to tell the function being called where the arguments will\n\
+arrive.\n\
+\n\
+@code{TARGET_FUNCTION_INCOMING_ARG} can also return arbitrary address\n\
+computation using hard register, which can be forced into a register,\n\
+so that it can be used to pass special arguments.\n\
+\n\
+If @code{TARGET_FUNCTION_INCOMING_ARG} is not defined,\n\
+@code{TARGET_FUNCTION_ARG} serves both purposes.",
+ rtx, (cumulative_args_t ca, const function_arg_info &arg),
+ default_function_incoming_arg)
+
+DEFHOOK
+(function_arg_boundary,
+ "This hook returns the alignment boundary, in bits, of an argument\n\
+with the specified mode and type.  The default hook returns\n\
+@code{PARM_BOUNDARY} for all arguments.",
+ unsigned int, (machine_mode mode, const_tree type),
+ default_function_arg_boundary)
+
+DEFHOOK
+(function_arg_round_boundary,
+ "Normally, the size of an argument is rounded up to @code{PARM_BOUNDARY},\n\
+which is the default value for this hook.  You can define this hook to\n\
+return a different value if an argument size must be rounded to a larger\n\
+value.",
+ unsigned int, (machine_mode mode, const_tree type),
+ default_function_arg_round_boundary)
+
+/* Return the diagnostic message string if function without a prototype
+   is not allowed for this 'val' argument; NULL otherwise. */
+DEFHOOK
+(invalid_arg_for_unprototyped_fn,
+ "If defined, this macro returns the diagnostic message when it is\n\
+illegal to pass argument @var{val} to function @var{funcdecl}\n\
+with prototype @var{typelist}.",
+ const char *, (const_tree typelist, const_tree funcdecl, const_tree val),
+ hook_invalid_arg_for_unprototyped_fn)
+
+/* Return an rtx for the return value location of the function
+   specified by FN_DECL_OR_TYPE with a return type of RET_TYPE.  */
+DEFHOOK
+(function_value,
+ "\n\
+Define this to return an RTX representing the place where a function\n\
+returns or receives a value of data type @var{ret_type}, a tree node\n\
+representing a data type.  @var{fn_decl_or_type} is a tree node\n\
+representing @code{FUNCTION_DECL} or @code{FUNCTION_TYPE} of a\n\
+function being called.  If @var{outgoing} is false, the hook should\n\
+compute the register in which the caller will see the return value.\n\
+Otherwise, the hook should return an RTX representing the place where\n\
+a function returns a value.\n\
+\n\
+On many machines, only @code{TYPE_MODE (@var{ret_type})} is relevant.\n\
+(Actually, on most machines, scalar values are returned in the same\n\
+place regardless of mode.)  The value of the expression is usually a\n\
+@code{reg} RTX for the hard register where the return value is stored.\n\
+The value can also be a @code{parallel} RTX, if the return value is in\n\
+multiple places.  See @code{TARGET_FUNCTION_ARG} for an explanation of the\n\
+@code{parallel} form.   Note that the callee will populate every\n\
+location specified in the @code{parallel}, but if the first element of\n\
+the @code{parallel} contains the whole return value, callers will use\n\
+that element as the canonical location and ignore the others.  The m68k\n\
+port uses this type of @code{parallel} to return pointers in both\n\
+@samp{%a0} (the canonical location) and @samp{%d0}.\n\
+\n\
+If @code{TARGET_PROMOTE_FUNCTION_RETURN} returns true, you must apply\n\
+the same promotion rules specified in @code{PROMOTE_MODE} if\n\
+@var{valtype} is a scalar type.\n\
+\n\
+If the precise function being called is known, @var{func} is a tree\n\
+node (@code{FUNCTION_DECL}) for it; otherwise, @var{func} is a null\n\
+pointer.  This makes it possible to use a different value-returning\n\
+convention for specific functions when all their calls are\n\
+known.\n\
+\n\
+Some target machines have ``register windows'' so that the register in\n\
+which a function returns its value is not the same as the one in which\n\
+the caller sees the value.  For such machines, you should return\n\
+different RTX depending on @var{outgoing}.\n\
+\n\
+@code{TARGET_FUNCTION_VALUE} is not used for return values with\n\
+aggregate data types, because these are returned in another way.  See\n\
+@code{TARGET_STRUCT_VALUE_RTX} and related macros, below.",
+ rtx, (const_tree ret_type, const_tree fn_decl_or_type, bool outgoing),
+ default_function_value)
+
+/* Return the rtx for the result of a libcall of mode MODE,
+   calling the function FN_NAME.  */
+DEFHOOK
+(libcall_value,
+ "Define this hook if the back-end needs to know the name of the libcall\n\
+function in order to determine where the result should be returned.\n\
+\n\
+The mode of the result is given by @var{mode} and the name of the called\n\
+library function is given by @var{fun}.  The hook should return an RTX\n\
+representing the place where the library function result will be returned.\n\
+\n\
+If this hook is not defined, then LIBCALL_VALUE will be used.",
+ rtx, (machine_mode mode, const_rtx fun),
+ default_libcall_value)
+
+/* Return true if REGNO is a possible register number for
+   a function value as seen by the caller.  */
+DEFHOOK
+(function_value_regno_p,
+ "A target hook that return @code{true} if @var{regno} is the number of a hard\n\
+register in which the values of called function may come back.\n\
+\n\
+A register whose use for returning values is limited to serving as the\n\
+second of a pair (for a value of type @code{double}, say) need not be\n\
+recognized by this target hook.\n\
+\n\
+If the machine has register windows, so that the caller and the called\n\
+function use different registers for the return value, this target hook\n\
+should recognize only the caller's register numbers.\n\
+\n\
+If this hook is not defined, then FUNCTION_VALUE_REGNO_P will be used.",
+ bool, (const unsigned int regno),
+ default_function_value_regno_p)
+
+DEFHOOK
+(fntype_abi,
+ "Return the ABI used by a function with type @var{type}; see the\n\
+definition of @code{predefined_function_abi} for details of the ABI\n\
+descriptor.  Targets only need to define this hook if they support\n\
+interoperability between several ABIs in the same translation unit.",
+ const predefined_function_abi &, (const_tree type),
+ NULL)
+
+DEFHOOK
+(insn_callee_abi,
+ "This hook returns a description of the ABI used by the target of\n\
+call instruction @var{insn}; see the definition of\n\
+@code{predefined_function_abi} for details of the ABI descriptor.\n\
+Only the global function @code{insn_callee_abi} should call this hook\n\
+directly.\n\
+\n\
+Targets only need to define this hook if they support\n\
+interoperability between several ABIs in the same translation unit.",
+ const predefined_function_abi &, (const rtx_insn *insn),
+ NULL)
+
+/* ??? Documenting this hook requires a GFDL license grant.  */
+DEFHOOK_UNDOC
+(internal_arg_pointer,
+"Return an rtx for the argument pointer incoming to the\
+ current function.",
+ rtx, (void),
+ default_internal_arg_pointer)
+
+/* Update the current function stack boundary if needed.  */
+DEFHOOK
+(update_stack_boundary,
+ "Define this macro to update the current function stack boundary if\n\
+necessary.",
+ void, (void), NULL)
+
+/* Handle stack alignment and return an rtx for Dynamic Realign
+   Argument Pointer if necessary.  */
+DEFHOOK
+(get_drap_rtx,
+ "This hook should return an rtx for Dynamic Realign Argument Pointer (DRAP) if a\n\
+different argument pointer register is needed to access the function's\n\
+argument list due to stack realignment.  Return @code{NULL} if no DRAP\n\
+is needed.",
+ rtx, (void), NULL)
+
+/* Generate instruction sequence to zero call used registers.  */
+DEFHOOK
+(zero_call_used_regs,
+ "This target hook emits instructions to zero the subset of @var{selected_regs}\n\
+that could conceivably contain values that are useful to an attacker.\n\
+Return the set of registers that were actually cleared.\n\
+\n\
+The default implementation uses normal move instructions to zero\n\
+all the registers in @var{selected_regs}.  Define this hook if the\n\
+target has more efficient ways of zeroing certain registers,\n\
+or if you believe that certain registers would never contain\n\
+values that are useful to an attacker.",
+ HARD_REG_SET, (HARD_REG_SET selected_regs),
+default_zero_call_used_regs)
+
+/* Return true if all function parameters should be spilled to the
+   stack.  */
+DEFHOOK
+(allocate_stack_slots_for_args,
+ "When optimization is disabled, this hook indicates whether or not\n\
+arguments should be allocated to stack slots.  Normally, GCC allocates\n\
+stacks slots for arguments when not optimizing in order to make\n\
+debugging easier.  However, when a function is declared with\n\
+@code{__attribute__((naked))}, there is no stack frame, and the compiler\n\
+cannot safely move arguments from the registers in which they are passed\n\
+to the stack.  Therefore, this hook should return true in general, but\n\
+false for naked functions.  The default implementation always returns true.",
+ bool, (void),
+ hook_bool_void_true)
+
+/* Return an rtx for the static chain for FNDECL_OR_TYPE.  If INCOMING_P
+   is true, then it should be for the callee; otherwise for the caller.  */
+DEFHOOK
+(static_chain,
+ "This hook replaces the use of @code{STATIC_CHAIN_REGNUM} et al for\n\
+targets that may use different static chain locations for different\n\
+nested functions.  This may be required if the target has function\n\
+attributes that affect the calling conventions of the function and\n\
+those calling conventions use different static chain locations.\n\
+\n\
+The default version of this hook uses @code{STATIC_CHAIN_REGNUM} et al.\n\
+\n\
+If the static chain is passed in memory, this hook should be used to\n\
+provide rtx giving @code{mem} expressions that denote where they are stored.\n\
+Often the @code{mem} expression as seen by the caller will be at an offset\n\
+from the stack pointer and the @code{mem} expression as seen by the callee\n\
+will be at an offset from the frame pointer.\n\
+@findex stack_pointer_rtx\n\
+@findex frame_pointer_rtx\n\
+@findex arg_pointer_rtx\n\
+The variables @code{stack_pointer_rtx}, @code{frame_pointer_rtx}, and\n\
+@code{arg_pointer_rtx} will have been initialized and should be used\n\
+to refer to those items.",
+ rtx, (const_tree fndecl_or_type, bool incoming_p),
+ default_static_chain)
+
+/* Fill in the trampoline at MEM with a call to FNDECL and a
+   static chain value of CHAIN.  */
+DEFHOOK
+(trampoline_init,
+ "This hook is called to initialize a trampoline.\n\
+@var{m_tramp} is an RTX for the memory block for the trampoline; @var{fndecl}\n\
+is the @code{FUNCTION_DECL} for the nested function; @var{static_chain} is an\n\
+RTX for the static chain value that should be passed to the function\n\
+when it is called.\n\
+\n\
+If the target defines @code{TARGET_ASM_TRAMPOLINE_TEMPLATE}, then the\n\
+first thing this hook should do is emit a block move into @var{m_tramp}\n\
+from the memory block returned by @code{assemble_trampoline_template}.\n\
+Note that the block move need only cover the constant parts of the\n\
+trampoline.  If the target isolates the variable parts of the trampoline\n\
+to the end, not all @code{TRAMPOLINE_SIZE} bytes need be copied.\n\
+\n\
+If the target requires any other actions, such as flushing caches\n\
+(possibly calling function maybe_emit_call_builtin___clear_cache) or\n\
+enabling stack execution, these actions should be performed after\n\
+initializing the trampoline proper.",
+ void, (rtx m_tramp, tree fndecl, rtx static_chain),
+ default_trampoline_init)
+
+/* Emit a call to a function to clear the instruction cache.  */
+DEFHOOK
+(emit_call_builtin___clear_cache,
+ "On targets that do not define a @code{clear_cache} insn expander,\n\
+but that define the @code{CLEAR_CACHE_INSN} macro,\n\
+maybe_emit_call_builtin___clear_cache relies on this target hook\n\
+to clear an address range in the instruction cache.\n\
+\n\
+The default implementation calls the @code{__clear_cache} builtin,\n\
+taking the assembler name from the builtin declaration.  Overriding\n\
+definitions may call alternate functions, with alternate calling\n\
+conventions, or emit alternate RTX to perform the job.",
+ void, (rtx begin, rtx end),
+ default_emit_call_builtin___clear_cache)
+
+/* Adjust the address of the trampoline in a target-specific way.  */
+DEFHOOK
+(trampoline_adjust_address,
+ "This hook should perform any machine-specific adjustment in\n\
+the address of the trampoline.  Its argument contains the address of the\n\
+memory block that was passed to @code{TARGET_TRAMPOLINE_INIT}.  In case\n\
+the address to be used for a function call should be different from the\n\
+address at which the template was stored, the different address should\n\
+be returned; otherwise @var{addr} should be returned unchanged.\n\
+If this hook is not defined, @var{addr} will be used for function calls.",
+ rtx, (rtx addr), NULL)
+
+DEFHOOKPOD
+(custom_function_descriptors,
+ "If the target can use GCC's generic descriptor mechanism for nested\n\
+functions, define this hook to a power of 2 representing an unused bit\n\
+in function pointers which can be used to differentiate descriptors at\n\
+run time.  This value gives the number of bytes by which descriptor\n\
+pointers are misaligned compared to function pointers.  For example, on\n\
+targets that require functions to be aligned to a 4-byte boundary, a\n\
+value of either 1 or 2 is appropriate unless the architecture already\n\
+reserves the bit for another purpose, such as on ARM.\n\
+\n\
+Define this hook to 0 if the target implements ABI support for\n\
+function descriptors in its standard calling sequence, like for example\n\
+HPPA or IA-64.\n\
+\n\
+Using descriptors for nested functions\n\
+eliminates the need for trampolines that reside on the stack and require\n\
+it to be made executable.",\
+ int, -1)
+
+/* Return the number of bytes of its own arguments that a function
+   pops on returning, or 0 if the function pops no arguments and the
+   caller must therefore pop them all after the function returns.  */
+/* ??? tm.texi has no types for the parameters.  */
+DEFHOOK
+(return_pops_args,
+ "This target hook returns the number of bytes of its own arguments that\n\
+a function pops on returning, or 0 if the function pops no arguments\n\
+and the caller must therefore pop them all after the function returns.\n\
+\n\
+@var{fundecl} is a C variable whose value is a tree node that describes\n\
+the function in question.  Normally it is a node of type\n\
+@code{FUNCTION_DECL} that describes the declaration of the function.\n\
+From this you can obtain the @code{DECL_ATTRIBUTES} of the function.\n\
+\n\
+@var{funtype} is a C variable whose value is a tree node that\n\
+describes the function in question.  Normally it is a node of type\n\
+@code{FUNCTION_TYPE} that describes the data type of the function.\n\
+From this it is possible to obtain the data types of the value and\n\
+arguments (if known).\n\
+\n\
+When a call to a library function is being considered, @var{fundecl}\n\
+will contain an identifier node for the library function.  Thus, if\n\
+you need to distinguish among various library functions, you can do so\n\
+by their names.  Note that ``library function'' in this context means\n\
+a function used to perform arithmetic, whose name is known specially\n\
+in the compiler and was not mentioned in the C code being compiled.\n\
+\n\
+@var{size} is the number of bytes of arguments passed on the\n\
+stack.  If a variable number of bytes is passed, it is zero, and\n\
+argument popping will always be the responsibility of the calling function.\n\
+\n\
+On the VAX, all functions always pop their arguments, so the definition\n\
+of this macro is @var{size}.  On the 68000, using the standard\n\
+calling convention, no functions pop their arguments, so the value of\n\
+the macro is always 0 in this case.  But an alternative calling\n\
+convention is available in which functions that take a fixed number of\n\
+arguments pop them but other functions (such as @code{printf}) pop\n\
+nothing (the caller pops all).  When this convention is in use,\n\
+@var{funtype} is examined to determine whether a function takes a fixed\n\
+number of arguments.",
+ poly_int64, (tree fundecl, tree funtype, poly_int64 size),
+ default_return_pops_args)
+
+/* Return a mode wide enough to copy any function value that might be
+   returned.  */
+DEFHOOK
+(get_raw_result_mode,
+ "This target hook returns the mode to be used when accessing raw return\
+ registers in @code{__builtin_return}.  Define this macro if the value\
+ in @var{reg_raw_mode} is not correct.",
+ fixed_size_mode, (int regno),
+ default_get_reg_raw_mode)
+
+/* Return a mode wide enough to copy any argument value that might be
+   passed.  */
+DEFHOOK
+(get_raw_arg_mode,
+ "This target hook returns the mode to be used when accessing raw argument\
+ registers in @code{__builtin_apply_args}.  Define this macro if the value\
+ in @var{reg_raw_mode} is not correct.",
+ fixed_size_mode, (int regno),
+ default_get_reg_raw_mode)
+
+/* Return true if a type is an empty record.  */
+DEFHOOK
+(empty_record_p,
+ "This target hook returns true if the type is an empty record.  The default\n\
+is to return @code{false}.",
+ bool, (const_tree type),
+ hook_bool_const_tree_false)
+
+/* Warn about the change in empty class parameter passing ABI.  */
+DEFHOOK
+(warn_parameter_passing_abi,
+ "This target hook warns about the change in empty class parameter passing\n\
+ABI.",
+ void, (cumulative_args_t ca, tree type),
+ hook_void_CUMULATIVE_ARGS_tree)
+
+HOOK_VECTOR_END (calls)
+
+DEFHOOK
+(use_pseudo_pic_reg,
+ "This hook should return 1 in case pseudo register should be created\n\
+for pic_offset_table_rtx during function expand.",
+ bool, (void),
+ hook_bool_void_false)
+
+DEFHOOK
+(init_pic_reg,
+ "Perform a target dependent initialization of pic_offset_table_rtx.\n\
+This hook is called at the start of register allocation.",
+ void, (void),
+ hook_void_void)
+
+/* Return the diagnostic message string if conversion from FROMTYPE
+   to TOTYPE is not allowed, NULL otherwise.  */
+DEFHOOK
+(invalid_conversion,
+ "If defined, this macro returns the diagnostic message when it is\n\
+invalid to convert from @var{fromtype} to @var{totype}, or @code{NULL}\n\
+if validity should be determined by the front end.",
+ const char *, (const_tree fromtype, const_tree totype),
+ hook_constcharptr_const_tree_const_tree_null)
+
+/* Return the diagnostic message string if the unary operation OP is
+   not permitted on TYPE, NULL otherwise.  */
+DEFHOOK
+(invalid_unary_op,
+ "If defined, this macro returns the diagnostic message when it is\n\
+invalid to apply operation @var{op} (where unary plus is denoted by\n\
+@code{CONVERT_EXPR}) to an operand of type @var{type}, or @code{NULL}\n\
+if validity should be determined by the front end.",
+ const char *, (int op, const_tree type),
+ hook_constcharptr_int_const_tree_null)
+
+/* Return the diagnostic message string if the binary operation OP
+   is not permitted on TYPE1 and TYPE2, NULL otherwise.  */
+DEFHOOK
+(invalid_binary_op,
+ "If defined, this macro returns the diagnostic message when it is\n\
+invalid to apply operation @var{op} to operands of types @var{type1}\n\
+and @var{type2}, or @code{NULL} if validity should be determined by\n\
+the front end.",
+ const char *, (int op, const_tree type1, const_tree type2),
+ hook_constcharptr_int_const_tree_const_tree_null)
+
+/* If values of TYPE are promoted to some other type when used in
+   expressions (analogous to the integer promotions), return that type,
+   or NULL_TREE otherwise.  */
+DEFHOOK
+(promoted_type,
+ "If defined, this target hook returns the type to which values of\n\
+@var{type} should be promoted when they appear in expressions,\n\
+analogous to the integer promotions, or @code{NULL_TREE} to use the\n\
+front end's normal promotion rules.  This hook is useful when there are\n\
+target-specific types with special promotion rules.\n\
+This is currently used only by the C and C++ front ends.",
+ tree, (const_tree type),
+ hook_tree_const_tree_null)
+
+/* Convert EXPR to TYPE, if target-specific types with special conversion
+   rules are involved.  Return the converted expression, or NULL to apply
+   the standard conversion rules.  */
+DEFHOOK
+(convert_to_type,
+ "If defined, this hook returns the result of converting @var{expr} to\n\
+@var{type}.  It should return the converted expression,\n\
+or @code{NULL_TREE} to apply the front end's normal conversion rules.\n\
+This hook is useful when there are target-specific types with special\n\
+conversion rules.\n\
+This is currently used only by the C and C++ front ends.",
+ tree, (tree type, tree expr),
+ hook_tree_tree_tree_null)
+
+DEFHOOK
+(verify_type_context,
+ "If defined, this hook returns false if there is a target-specific reason\n\
+why type @var{type} cannot be used in the source language context described\n\
+by @var{context}.  When @var{silent_p} is false, the hook also reports an\n\
+error against @var{loc} for invalid uses of @var{type}.\n\
+\n\
+Calls to this hook should be made through the global function\n\
+@code{verify_type_context}, which makes the @var{silent_p} parameter\n\
+default to false and also handles @code{error_mark_node}.\n\
+\n\
+The default implementation always returns true.",
+ bool, (location_t loc, type_context_kind context, const_tree type,
+	bool silent_p),
+ NULL)
+
+DEFHOOK
+(can_change_mode_class,
+ "This hook returns true if it is possible to bitcast values held in\n\
+registers of class @var{rclass} from mode @var{from} to mode @var{to}\n\
+and if doing so preserves the low-order bits that are common to both modes.\n\
+The result is only meaningful if @var{rclass} has registers that can hold\n\
+both @code{from} and @code{to}.  The default implementation returns true.\n\
+\n\
+As an example of when such bitcasting is invalid, loading 32-bit integer or\n\
+floating-point objects into floating-point registers on Alpha extends them\n\
+to 64 bits.  Therefore loading a 64-bit object and then storing it as a\n\
+32-bit object does not store the low-order 32 bits, as would be the case\n\
+for a normal register.  Therefore, @file{alpha.h} defines\n\
+@code{TARGET_CAN_CHANGE_MODE_CLASS} to return:\n\
+\n\
+@smallexample\n\
+(GET_MODE_SIZE (from) == GET_MODE_SIZE (to)\n\
+ || !reg_classes_intersect_p (FLOAT_REGS, rclass))\n\
+@end smallexample\n\
+\n\
+Even if storing from a register in mode @var{to} would be valid,\n\
+if both @var{from} and @code{raw_reg_mode} for @var{rclass} are wider\n\
+than @code{word_mode}, then we must prevent @var{to} narrowing the\n\
+mode.  This happens when the middle-end assumes that it can load\n\
+or store pieces of an @var{N}-word pseudo, and that the pseudo will\n\
+eventually be allocated to @var{N} @code{word_mode} hard registers.\n\
+Failure to prevent this kind of mode change will result in the\n\
+entire @code{raw_reg_mode} being modified instead of the partial\n\
+value that the middle-end intended.",
+ bool, (machine_mode from, machine_mode to, reg_class_t rclass),
+ hook_bool_mode_mode_reg_class_t_true)
+
+/* Change pseudo allocno class calculated by IRA.  */
+DEFHOOK
+(ira_change_pseudo_allocno_class,
+ "A target hook which can change allocno class for given pseudo from\n\
+  allocno and best class calculated by IRA.\n\
+  \n\
+  The default version of this target hook always returns given class.",
+ reg_class_t, (int, reg_class_t, reg_class_t),
+ default_ira_change_pseudo_allocno_class)
+
+/* Return true if we use LRA instead of reload.  */
+DEFHOOK
+(lra_p,
+ "A target hook which returns true if we use LRA instead of reload pass.\
+  \
+  The default version of this target hook returns true.  New ports\
+  should use LRA, and existing ports are encouraged to convert.",
+ bool, (void),
+ default_lra_p)
+
+/* Return register priority of given hard regno for the current target.  */
+DEFHOOK
+(register_priority,
+ "A target hook which returns the register priority number to which the\
+  register @var{hard_regno} belongs to.  The bigger the number, the\
+  more preferable the hard register usage (when all other conditions are\
+  the same).  This hook can be used to prefer some hard register over\
+  others in LRA.  For example, some x86-64 register usage needs\
+  additional prefix which makes instructions longer.  The hook can\
+  return lower priority number for such registers make them less favorable\
+  and as result making the generated code smaller.\
+  \
+  The default version of this target hook returns always zero.",
+ int, (int),
+ default_register_priority)
+
+/* Return true if we need register usage leveling.  */
+DEFHOOK
+(register_usage_leveling_p,
+ "A target hook which returns true if we need register usage leveling.\
+  That means if a few hard registers are equally good for the\
+  assignment, we choose the least used hard register.  The register\
+  usage leveling may be profitable for some targets.  Don't use the\
+  usage leveling for targets with conditional execution or targets\
+  with big register files as it hurts if-conversion and cross-jumping\
+  optimizations.\
+  \
+  The default version of this target hook returns always false.",
+ bool, (void),
+ default_register_usage_leveling_p)
+
+/* Return true if maximal address displacement can be different.  */
+DEFHOOK
+(different_addr_displacement_p,
+ "A target hook which returns true if an address with the same structure\
+  can have different maximal legitimate displacement.  For example, the\
+  displacement can depend on memory mode or on operand combinations in\
+  the insn.\
+  \
+  The default version of this target hook returns always false.",
+ bool, (void),
+ default_different_addr_displacement_p)
+
+/* Determine class for spilling pseudos of given mode into registers
+   instead of memory.  */
+DEFHOOK
+(spill_class,
+ "This hook defines a class of registers which could be used for spilling\
+  pseudos of the given mode and class, or @code{NO_REGS} if only memory\
+  should be used.  Not defining this hook is equivalent to returning\
+  @code{NO_REGS} for all inputs.",
+ reg_class_t, (reg_class_t, machine_mode),
+ NULL)
+
+/* Determine an additional allocno class.  */
+DEFHOOK
+(additional_allocno_class_p,
+ "This hook should return @code{true} if given class of registers should\
+  be an allocno class in any way.  Usually RA uses only one register\
+  class from all classes containing the same register set.  In some\
+  complicated cases, you need to have two or more such classes as\
+  allocno ones for RA correct work.  Not defining this hook is\
+  equivalent to returning @code{false} for all inputs.",
+ bool, (reg_class_t),
+ hook_bool_reg_class_t_false)
+
+DEFHOOK
+(cstore_mode,
+ "This hook defines the machine mode to use for the boolean result of\
+  conditional store patterns.  The ICODE argument is the instruction code\
+  for the cstore being performed.  Not definiting this hook is the same\
+  as accepting the mode encoded into operand 0 of the cstore expander\
+  patterns.",
+  scalar_int_mode, (enum insn_code icode),
+  default_cstore_mode)
+
+/* This target hook allows the backend to compute the register pressure
+   classes to use.  */
+DEFHOOK
+(compute_pressure_classes,
+ "A target hook which lets a backend compute the set of pressure classes to\
+  be used by those optimization passes which take register pressure into\
+  account, as opposed to letting IRA compute them.  It returns the number of\
+  register classes stored in the array @var{pressure_classes}.",
+ int, (enum reg_class *pressure_classes), NULL)
+
+/* True if a structure, union or array with MODE containing FIELD should
+   be accessed using BLKmode.  */
+DEFHOOK
+(member_type_forces_blk,
+ "Return true if a structure, union or array containing @var{field} should\n\
+be accessed using @code{BLKMODE}.\n\
+\n\
+If @var{field} is the only field in the structure, @var{mode} is its\n\
+mode, otherwise @var{mode} is VOIDmode.  @var{mode} is provided in the\n\
+case where structures of one field would require the structure's mode to\n\
+retain the field's mode.\n\
+\n\
+Normally, this is not needed.",
+ bool, (const_tree field, machine_mode mode),
+ default_member_type_forces_blk)
+
+/* See tree-ssa-math-opts.c:divmod_candidate_p for conditions
+   that gate the divod transform.  */
+DEFHOOK
+(expand_divmod_libfunc,
+ "Define this hook for enabling divmod transform if the port does not have\n\
+hardware divmod insn but defines target-specific divmod libfuncs.", 
+ void, (rtx libfunc, machine_mode mode, rtx op0, rtx op1, rtx *quot, rtx *rem),
+ NULL)
+
+/* Return the class for a secondary reload, and fill in extra information.  */
+DEFHOOK
+(secondary_reload,
+ "Many machines have some registers that cannot be copied directly to or\n\
+from memory or even from other types of registers.  An example is the\n\
+@samp{MQ} register, which on most machines, can only be copied to or\n\
+from general registers, but not memory.  Below, we shall be using the\n\
+term 'intermediate register' when a move operation cannot be performed\n\
+directly, but has to be done by copying the source into the intermediate\n\
+register first, and then copying the intermediate register to the\n\
+destination.  An intermediate register always has the same mode as\n\
+source and destination.  Since it holds the actual value being copied,\n\
+reload might apply optimizations to re-use an intermediate register\n\
+and eliding the copy from the source when it can determine that the\n\
+intermediate register still holds the required value.\n\
+\n\
+Another kind of secondary reload is required on some machines which\n\
+allow copying all registers to and from memory, but require a scratch\n\
+register for stores to some memory locations (e.g., those with symbolic\n\
+address on the RT, and those with certain symbolic address on the SPARC\n\
+when compiling PIC)@.  Scratch registers need not have the same mode\n\
+as the value being copied, and usually hold a different value than\n\
+that being copied.  Special patterns in the md file are needed to\n\
+describe how the copy is performed with the help of the scratch register;\n\
+these patterns also describe the number, register class(es) and mode(s)\n\
+of the scratch register(s).\n\
+\n\
+In some cases, both an intermediate and a scratch register are required.\n\
+\n\
+For input reloads, this target hook is called with nonzero @var{in_p},\n\
+and @var{x} is an rtx that needs to be copied to a register of class\n\
+@var{reload_class} in @var{reload_mode}.  For output reloads, this target\n\
+hook is called with zero @var{in_p}, and a register of class @var{reload_class}\n\
+needs to be copied to rtx @var{x} in @var{reload_mode}.\n\
+\n\
+If copying a register of @var{reload_class} from/to @var{x} requires\n\
+an intermediate register, the hook @code{secondary_reload} should\n\
+return the register class required for this intermediate register.\n\
+If no intermediate register is required, it should return NO_REGS.\n\
+If more than one intermediate register is required, describe the one\n\
+that is closest in the copy chain to the reload register.\n\
+\n\
+If scratch registers are needed, you also have to describe how to\n\
+perform the copy from/to the reload register to/from this\n\
+closest intermediate register.  Or if no intermediate register is\n\
+required, but still a scratch register is needed, describe the\n\
+copy  from/to the reload register to/from the reload operand @var{x}.\n\
+\n\
+You do this by setting @code{sri->icode} to the instruction code of a pattern\n\
+in the md file which performs the move.  Operands 0 and 1 are the output\n\
+and input of this copy, respectively.  Operands from operand 2 onward are\n\
+for scratch operands.  These scratch operands must have a mode, and a\n\
+single-register-class\n\
+@c [later: or memory]\n\
+output constraint.\n\
+\n\
+When an intermediate register is used, the @code{secondary_reload}\n\
+hook will be called again to determine how to copy the intermediate\n\
+register to/from the reload operand @var{x}, so your hook must also\n\
+have code to handle the register class of the intermediate operand.\n\
+\n\
+@c [For later: maybe we'll allow multi-alternative reload patterns -\n\
+@c   the port maintainer could name a mov<mode> pattern that has clobbers -\n\
+@c   and match the constraints of input and output to determine the required\n\
+@c   alternative.  A restriction would be that constraints used to match\n\
+@c   against reloads registers would have to be written as register class\n\
+@c   constraints, or we need a new target macro / hook that tells us if an\n\
+@c   arbitrary constraint can match an unknown register of a given class.\n\
+@c   Such a macro / hook would also be useful in other places.]\n\
+\n\
+\n\
+@var{x} might be a pseudo-register or a @code{subreg} of a\n\
+pseudo-register, which could either be in a hard register or in memory.\n\
+Use @code{true_regnum} to find out; it will return @minus{}1 if the pseudo is\n\
+in memory and the hard register number if it is in a register.\n\
+\n\
+Scratch operands in memory (constraint @code{\"=m\"} / @code{\"=&m\"}) are\n\
+currently not supported.  For the time being, you will have to continue\n\
+to use @code{TARGET_SECONDARY_MEMORY_NEEDED} for that purpose.\n\
+\n\
+@code{copy_cost} also uses this target hook to find out how values are\n\
+copied.  If you want it to include some extra cost for the need to allocate\n\
+(a) scratch register(s), set @code{sri->extra_cost} to the additional cost.\n\
+Or if two dependent moves are supposed to have a lower cost than the sum\n\
+of the individual moves due to expected fortuitous scheduling and/or special\n\
+forwarding logic, you can set @code{sri->extra_cost} to a negative amount.",
+ reg_class_t,
+ (bool in_p, rtx x, reg_class_t reload_class, machine_mode reload_mode,
+  secondary_reload_info *sri),
+ default_secondary_reload)
+
+DEFHOOK
+(secondary_memory_needed,
+ "Certain machines have the property that some registers cannot be copied\n\
+to some other registers without using memory.  Define this hook on\n\
+those machines to return true if objects of mode @var{m} in registers\n\
+of @var{class1} can only be copied to registers of class @var{class2} by\n\
+ storing a register of @var{class1} into memory and loading that memory\n\
+location into a register of @var{class2}.  The default definition returns\n\
+false for all inputs.",
+ bool, (machine_mode mode, reg_class_t class1, reg_class_t class2),
+ hook_bool_mode_reg_class_t_reg_class_t_false)
+
+DEFHOOK
+(secondary_memory_needed_mode,
+ "If @code{TARGET_SECONDARY_MEMORY_NEEDED} tells the compiler to use memory\n\
+when moving between two particular registers of mode @var{mode},\n\
+this hook specifies the mode that the memory should have.\n\
+\n\
+The default depends on @code{TARGET_LRA_P}.  Without LRA, the default\n\
+is to use a word-sized mode for integral modes that are smaller than a\n\
+a word.  This is right thing to do on most machines because it ensures\n\
+that all bits of the register are copied and prevents accesses to the\n\
+registers in a narrower mode, which some machines prohibit for\n\
+floating-point registers.\n\
+\n\
+However, this default behavior is not correct on some machines, such as\n\
+the DEC Alpha, that store short integers in floating-point registers\n\
+differently than in integer registers.  On those machines, the default\n\
+widening will not work correctly and you must define this hook to\n\
+suppress that widening in some cases.  See the file @file{alpha.c} for\n\
+details.\n\
+\n\
+With LRA, the default is to use @var{mode} unmodified.",
+ machine_mode, (machine_mode mode),
+ default_secondary_memory_needed_mode)
+
+/* Given an rtx X being reloaded into a reg required to be in class CLASS,
+   return the class of reg to actually use.  */
+DEFHOOK
+(preferred_reload_class,
+ "A target hook that places additional restrictions on the register class\n\
+to use when it is necessary to copy value @var{x} into a register in class\n\
+@var{rclass}.  The value is a register class; perhaps @var{rclass}, or perhaps\n\
+another, smaller class.\n\
+\n\
+The default version of this hook always returns value of @code{rclass} argument.\n\
+\n\
+Sometimes returning a more restrictive class makes better code.  For\n\
+example, on the 68000, when @var{x} is an integer constant that is in range\n\
+for a @samp{moveq} instruction, the value of this macro is always\n\
+@code{DATA_REGS} as long as @var{rclass} includes the data registers.\n\
+Requiring a data register guarantees that a @samp{moveq} will be used.\n\
+\n\
+One case where @code{TARGET_PREFERRED_RELOAD_CLASS} must not return\n\
+@var{rclass} is if @var{x} is a legitimate constant which cannot be\n\
+loaded into some register class.  By returning @code{NO_REGS} you can\n\
+force @var{x} into a memory location.  For example, rs6000 can load\n\
+immediate values into general-purpose registers, but does not have an\n\
+instruction for loading an immediate value into a floating-point\n\
+register, so @code{TARGET_PREFERRED_RELOAD_CLASS} returns @code{NO_REGS} when\n\
+@var{x} is a floating-point constant.  If the constant can't be loaded\n\
+into any kind of register, code generation will be better if\n\
+@code{TARGET_LEGITIMATE_CONSTANT_P} makes the constant illegitimate instead\n\
+of using @code{TARGET_PREFERRED_RELOAD_CLASS}.\n\
+\n\
+If an insn has pseudos in it after register allocation, reload will go\n\
+through the alternatives and call repeatedly @code{TARGET_PREFERRED_RELOAD_CLASS}\n\
+to find the best one.  Returning @code{NO_REGS}, in this case, makes\n\
+reload add a @code{!} in front of the constraint: the x86 back-end uses\n\
+this feature to discourage usage of 387 registers when math is done in\n\
+the SSE registers (and vice versa).",
+ reg_class_t,
+ (rtx x, reg_class_t rclass),
+ default_preferred_reload_class)
+
+/* Like TARGET_PREFERRED_RELOAD_CLASS, but for output reloads instead of
+   input reloads.  */
+DEFHOOK
+(preferred_output_reload_class,
+ "Like @code{TARGET_PREFERRED_RELOAD_CLASS}, but for output reloads instead of\n\
+input reloads.\n\
+\n\
+The default version of this hook always returns value of @code{rclass}\n\
+argument.\n\
+\n\
+You can also use @code{TARGET_PREFERRED_OUTPUT_RELOAD_CLASS} to discourage\n\
+reload from using some alternatives, like @code{TARGET_PREFERRED_RELOAD_CLASS}.",
+ reg_class_t,
+ (rtx x, reg_class_t rclass),
+ default_preferred_output_reload_class)
+
+DEFHOOK
+(select_early_remat_modes,
+ "On some targets, certain modes cannot be held in registers around a\n\
+standard ABI call and are relatively expensive to spill to the stack.\n\
+The early rematerialization pass can help in such cases by aggressively\n\
+recomputing values after calls, so that they don't need to be spilled.\n\
+\n\
+This hook returns the set of such modes by setting the associated bits\n\
+in @var{modes}.  The default implementation selects no modes, which has\n\
+the effect of disabling the early rematerialization pass.",
+ void, (sbitmap modes),
+ default_select_early_remat_modes)
+
+DEFHOOK
+(class_likely_spilled_p,
+ "A target hook which returns @code{true} if pseudos that have been assigned\n\
+to registers of class @var{rclass} would likely be spilled because\n\
+registers of @var{rclass} are needed for spill registers.\n\
+\n\
+The default version of this target hook returns @code{true} if @var{rclass}\n\
+has exactly one register and @code{false} otherwise.  On most machines, this\n\
+default should be used.  For generally register-starved machines, such as\n\
+i386, or machines with right register constraints, such as SH, this hook\n\
+can be used to avoid excessive spilling.\n\
+\n\
+This hook is also used by some of the global intra-procedural code\n\
+transformations to throtle code motion, to avoid increasing register\n\
+pressure.",
+ bool, (reg_class_t rclass),
+ default_class_likely_spilled_p)
+
+/* Return the maximum number of consecutive registers
+   needed to represent mode MODE in a register of class RCLASS.  */
+DEFHOOK
+(class_max_nregs,
+ "A target hook returns the maximum number of consecutive registers\n\
+of class @var{rclass} needed to hold a value of mode @var{mode}.\n\
+\n\
+This is closely related to the macro @code{TARGET_HARD_REGNO_NREGS}.\n\
+In fact, the value returned by @code{TARGET_CLASS_MAX_NREGS (@var{rclass},\n\
+@var{mode})} target hook should be the maximum value of\n\
+@code{TARGET_HARD_REGNO_NREGS (@var{regno}, @var{mode})} for all @var{regno}\n\
+values in the class @var{rclass}.\n\
+\n\
+This target hook helps control the handling of multiple-word values\n\
+in the reload pass.\n\
+\n\
+The default version of this target hook returns the size of @var{mode}\n\
+in words.",
+ unsigned char, (reg_class_t rclass, machine_mode mode),
+ default_class_max_nregs)
+
+DEFHOOK
+(preferred_rename_class,
+ "A target hook that places additional preference on the register\
+ class to use when it is necessary to rename a register in class\
+ @var{rclass} to another class, or perhaps @var{NO_REGS}, if no\
+ preferred register class is found or hook @code{preferred_rename_class}\
+ is not implemented.\
+ Sometimes returning a more restrictive class makes better code.  For\
+ example, on ARM, thumb-2 instructions using @code{LO_REGS} may be\
+ smaller than instructions using @code{GENERIC_REGS}.  By returning\
+ @code{LO_REGS} from @code{preferred_rename_class}, code size can\
+ be reduced.",
+ reg_class_t, (reg_class_t rclass),
+ default_preferred_rename_class)
+
+/* This target hook allows the backend to avoid unsafe substitution
+   during register allocation.  */
+DEFHOOK
+(cannot_substitute_mem_equiv_p,
+ "A target hook which returns @code{true} if @var{subst} can't\n\
+substitute safely pseudos with equivalent memory values during\n\
+register allocation.\n\
+The default version of this target hook returns @code{false}.\n\
+On most machines, this default should be used.  For generally\n\
+machines with non orthogonal register usage for addressing, such\n\
+as SH, this hook can be used to avoid excessive spilling.",
+ bool, (rtx subst),
+ hook_bool_rtx_false)
+
+/* This target hook allows the backend to legitimize base plus
+   displacement addressing.  */
+DEFHOOK
+(legitimize_address_displacement,
+ "This hook tries to split address offset @var{orig_offset} into\n\
+two parts: one that should be added to the base address to create\n\
+a local anchor point, and an additional offset that can be applied\n\
+to the anchor to address a value of mode @var{mode}.  The idea is that\n\
+the local anchor could be shared by other accesses to nearby locations.\n\
+\n\
+The hook returns true if it succeeds, storing the offset of the\n\
+anchor from the base in @var{offset1} and the offset of the final address\n\
+from the anchor in @var{offset2}.  The default implementation returns false.",
+ bool, (rtx *offset1, rtx *offset2, poly_int64 orig_offset, machine_mode mode),
+ default_legitimize_address_displacement)
+
+/* This target hook allows the backend to perform additional
+   processing while initializing for variable expansion.  */
+DEFHOOK
+(expand_to_rtl_hook,
+ "This hook is called just before expansion into rtl, allowing the target\n\
+to perform additional initializations or analysis before the expansion.\n\
+For example, the rs6000 port uses it to allocate a scratch stack slot\n\
+for use in copying SDmode values between memory and floating point\n\
+registers whenever the function being expanded has any SDmode\n\
+usage.",
+ void, (void),
+ hook_void_void)
+
+/* This target hook allows the backend to perform additional
+   instantiations on rtx that are not actually in insns yet,
+   but will be later.  */
+DEFHOOK
+(instantiate_decls,
+ "This hook allows the backend to perform additional instantiations on rtl\n\
+that are not actually in any insns yet, but will be later.",
+ void, (void),
+ hook_void_void)
+
+DEFHOOK
+(hard_regno_nregs,
+ "This hook returns the number of consecutive hard registers, starting\n\
+at register number @var{regno}, required to hold a value of mode\n\
+@var{mode}.  This hook must never return zero, even if a register\n\
+cannot hold the requested mode - indicate that with\n\
+@code{TARGET_HARD_REGNO_MODE_OK} and/or\n\
+@code{TARGET_CAN_CHANGE_MODE_CLASS} instead.\n\
+\n\
+The default definition returns the number of words in @var{mode}.",
+ unsigned int, (unsigned int regno, machine_mode mode),
+ default_hard_regno_nregs)
+
+DEFHOOK
+(hard_regno_mode_ok,
+ "This hook returns true if it is permissible to store a value\n\
+of mode @var{mode} in hard register number @var{regno} (or in several\n\
+registers starting with that one).  The default definition returns true\n\
+unconditionally.\n\
+\n\
+You need not include code to check for the numbers of fixed registers,\n\
+because the allocation mechanism considers them to be always occupied.\n\
+\n\
+@cindex register pairs\n\
+On some machines, double-precision values must be kept in even/odd\n\
+register pairs.  You can implement that by defining this hook to reject\n\
+odd register numbers for such modes.\n\
+\n\
+The minimum requirement for a mode to be OK in a register is that the\n\
+@samp{mov@var{mode}} instruction pattern support moves between the\n\
+register and other hard register in the same class and that moving a\n\
+value into the register and back out not alter it.\n\
+\n\
+Since the same instruction used to move @code{word_mode} will work for\n\
+all narrower integer modes, it is not necessary on any machine for\n\
+this hook to distinguish between these modes, provided you define\n\
+patterns @samp{movhi}, etc., to take advantage of this.  This is\n\
+useful because of the interaction between @code{TARGET_HARD_REGNO_MODE_OK}\n\
+and @code{TARGET_MODES_TIEABLE_P}; it is very desirable for all integer\n\
+modes to be tieable.\n\
+\n\
+Many machines have special registers for floating point arithmetic.\n\
+Often people assume that floating point machine modes are allowed only\n\
+in floating point registers.  This is not true.  Any registers that\n\
+can hold integers can safely @emph{hold} a floating point machine\n\
+mode, whether or not floating arithmetic can be done on it in those\n\
+registers.  Integer move instructions can be used to move the values.\n\
+\n\
+On some machines, though, the converse is true: fixed-point machine\n\
+modes may not go in floating registers.  This is true if the floating\n\
+registers normalize any value stored in them, because storing a\n\
+non-floating value there would garble it.  In this case,\n\
+@code{TARGET_HARD_REGNO_MODE_OK} should reject fixed-point machine modes in\n\
+floating registers.  But if the floating registers do not automatically\n\
+normalize, if you can store any bit pattern in one and retrieve it\n\
+unchanged without a trap, then any machine mode may go in a floating\n\
+register, so you can define this hook to say so.\n\
+\n\
+The primary significance of special floating registers is rather that\n\
+they are the registers acceptable in floating point arithmetic\n\
+instructions.  However, this is of no concern to\n\
+@code{TARGET_HARD_REGNO_MODE_OK}.  You handle it by writing the proper\n\
+constraints for those instructions.\n\
+\n\
+On some machines, the floating registers are especially slow to access,\n\
+so that it is better to store a value in a stack frame than in such a\n\
+register if floating point arithmetic is not being done.  As long as the\n\
+floating registers are not in class @code{GENERAL_REGS}, they will not\n\
+be used unless some pattern's constraint asks for one.",
+ bool, (unsigned int regno, machine_mode mode),
+ hook_bool_uint_mode_true)
+
+DEFHOOK
+(modes_tieable_p,
+ "This hook returns true if a value of mode @var{mode1} is accessible\n\
+in mode @var{mode2} without copying.\n\
+\n\
+If @code{TARGET_HARD_REGNO_MODE_OK (@var{r}, @var{mode1})} and\n\
+@code{TARGET_HARD_REGNO_MODE_OK (@var{r}, @var{mode2})} are always\n\
+the same for any @var{r}, then\n\
+@code{TARGET_MODES_TIEABLE_P (@var{mode1}, @var{mode2})}\n\
+should be true.  If they differ for any @var{r}, you should define\n\
+this hook to return false unless some other mechanism ensures the\n\
+accessibility of the value in a narrower mode.\n\
+\n\
+You should define this hook to return true in as many cases as\n\
+possible since doing so will allow GCC to perform better register\n\
+allocation.  The default definition returns true unconditionally.",
+ bool, (machine_mode mode1, machine_mode mode2),
+ hook_bool_mode_mode_true)
+
+/* Return true if is OK to use a hard register REGNO as scratch register
+   in peephole2.  */
+DEFHOOK
+(hard_regno_scratch_ok,
+ "This target hook should return @code{true} if it is OK to use a hard register\n\
+@var{regno} as scratch reg in peephole2.\n\
+\n\
+One common use of this macro is to prevent using of a register that\n\
+is not saved by a prologue in an interrupt handler.\n\
+\n\
+The default version of this hook always returns @code{true}.",
+ bool, (unsigned int regno),
+ default_hard_regno_scratch_ok)
+
+DEFHOOK
+(hard_regno_call_part_clobbered,
+ "ABIs usually specify that calls must preserve the full contents\n\
+of a particular register, or that calls can alter any part of a\n\
+particular register.  This information is captured by the target macro\n\
+@code{CALL_REALLY_USED_REGISTERS}.  However, some ABIs specify that calls\n\
+must preserve certain bits of a particular register but can alter others.\n\
+This hook should return true if this applies to at least one of the\n\
+registers in @samp{(reg:@var{mode} @var{regno})}, and if as a result the\n\
+call would alter part of the @var{mode} value.  For example, if a call\n\
+preserves the low 32 bits of a 64-bit hard register @var{regno} but can\n\
+clobber the upper 32 bits, this hook should return true for a 64-bit mode\n\
+but false for a 32-bit mode.\n\
+\n\
+The value of @var{abi_id} comes from the @code{predefined_function_abi}\n\
+structure that describes the ABI of the call; see the definition of the\n\
+structure for more details.  If (as is usual) the target uses the same ABI\n\
+for all functions in a translation unit, @var{abi_id} is always 0.\n\
+\n\
+The default implementation returns false, which is correct\n\
+for targets that don't have partly call-clobbered registers.",
+ bool, (unsigned int abi_id, unsigned int regno, machine_mode mode),
+ hook_bool_uint_uint_mode_false)
+
+DEFHOOK
+(get_multilib_abi_name,
+ "This hook returns name of multilib ABI name.",
+ const char *, (void),
+ hook_constcharptr_void_null)
+
+/* Return the smallest number of different values for which it is best to
+   use a jump-table instead of a tree of conditional branches.  */
+DEFHOOK
+(case_values_threshold,
+ "This function return the smallest number of different values for which it\n\
+is best to use a jump-table instead of a tree of conditional branches.\n\
+The default is four for machines with a @code{casesi} instruction and\n\
+five otherwise.  This is best for most machines.",
+ unsigned int, (void),
+ default_case_values_threshold)
+
+DEFHOOK
+(starting_frame_offset,
+ "This hook returns the offset from the frame pointer to the first local\n\
+variable slot to be allocated.  If @code{FRAME_GROWS_DOWNWARD}, it is the\n\
+offset to @emph{end} of the first slot allocated, otherwise it is the\n\
+offset to @emph{beginning} of the first slot allocated.  The default\n\
+implementation returns 0.",
+ HOST_WIDE_INT, (void),
+ hook_hwi_void_0)
+
+/* Optional callback to advise the target to compute the frame layout.  */
+DEFHOOK
+(compute_frame_layout,
+ "This target hook is called once each time the frame layout needs to be\n\
+recalculated.  The calculations can be cached by the target and can then\n\
+be used by @code{INITIAL_ELIMINATION_OFFSET} instead of re-computing the\n\
+layout on every invocation of that hook.  This is particularly useful\n\
+for targets that have an expensive frame layout function.  Implementing\n\
+this callback is optional.",
+ void, (void),
+ hook_void_void)
+
+/* Return true if a function must have and use a frame pointer.  */
+DEFHOOK
+(frame_pointer_required,
+ "This target hook should return @code{true} if a function must have and use\n\
+a frame pointer.  This target hook is called in the reload pass.  If its return\n\
+value is @code{true} the function will have a frame pointer.\n\
+\n\
+This target hook can in principle examine the current function and decide\n\
+according to the facts, but on most machines the constant @code{false} or the\n\
+constant @code{true} suffices.  Use @code{false} when the machine allows code\n\
+to be generated with no frame pointer, and doing so saves some time or space.\n\
+Use @code{true} when there is no possible advantage to avoiding a frame\n\
+pointer.\n\
+\n\
+In certain cases, the compiler does not know how to produce valid code\n\
+without a frame pointer.  The compiler recognizes those cases and\n\
+automatically gives the function a frame pointer regardless of what\n\
+@code{targetm.frame_pointer_required} returns.  You don't need to worry about\n\
+them.\n\
+\n\
+In a function that does not require a frame pointer, the frame pointer\n\
+register can be allocated for ordinary usage, unless you mark it as a\n\
+fixed register.  See @code{FIXED_REGISTERS} for more information.\n\
+\n\
+Default return value is @code{false}.",
+ bool, (void),
+ hook_bool_void_false)
+
+/* Returns true if the compiler is allowed to try to replace register number
+   from-reg with register number to-reg.  */
+DEFHOOK
+(can_eliminate,
+ "This target hook should return @code{true} if the compiler is allowed to\n\
+try to replace register number @var{from_reg} with register number\n\
+@var{to_reg}.  This target hook will usually be @code{true}, since most of the\n\
+cases preventing register elimination are things that the compiler already\n\
+knows about.\n\
+\n\
+Default return value is @code{true}.",
+ bool, (const int from_reg, const int to_reg),
+ hook_bool_const_int_const_int_true)
+
+/* Modify any or all of fixed_regs, call_used_regs, global_regs,
+   reg_names, and reg_class_contents to account of the vagaries of the
+   target.  */
+DEFHOOK
+(conditional_register_usage,
+ "This hook may conditionally modify five variables\n\
+@code{fixed_regs}, @code{call_used_regs}, @code{global_regs},\n\
+@code{reg_names}, and @code{reg_class_contents}, to take into account\n\
+any dependence of these register sets on target flags.  The first three\n\
+of these are of type @code{char []} (interpreted as boolean vectors).\n\
+@code{global_regs} is a @code{const char *[]}, and\n\
+@code{reg_class_contents} is a @code{HARD_REG_SET}.  Before the macro is\n\
+called, @code{fixed_regs}, @code{call_used_regs},\n\
+@code{reg_class_contents}, and @code{reg_names} have been initialized\n\
+from @code{FIXED_REGISTERS}, @code{CALL_USED_REGISTERS},\n\
+@code{REG_CLASS_CONTENTS}, and @code{REGISTER_NAMES}, respectively.\n\
+@code{global_regs} has been cleared, and any @option{-ffixed-@var{reg}},\n\
+@option{-fcall-used-@var{reg}} and @option{-fcall-saved-@var{reg}}\n\
+command options have been applied.\n\
+\n\
+@cindex disabling certain registers\n\
+@cindex controlling register usage\n\
+If the usage of an entire class of registers depends on the target\n\
+flags, you may indicate this to GCC by using this macro to modify\n\
+@code{fixed_regs} and @code{call_used_regs} to 1 for each of the\n\
+registers in the classes which should not be used by GCC@.  Also make\n\
+@code{define_register_constraint}s return @code{NO_REGS} for constraints\n\
+that shouldn't be used.\n\
+\n\
+(However, if this class is not included in @code{GENERAL_REGS} and all\n\
+of the insn patterns whose constraints permit this class are\n\
+controlled by target switches, then GCC will automatically avoid using\n\
+these registers when the target switches are opposed to them.)",
+ void, (void),
+ hook_void_void)
+
+DEFHOOK
+(stack_clash_protection_alloca_probe_range,
+ "Some targets have an ABI defined interval for which no probing needs to be done.\n\
+When a probe does need to be done this same interval is used as the probe distance \
+up when doing stack clash protection for alloca.\n\
+On such targets this value can be set to override the default probing up interval.\n\
+Define this variable to return nonzero if such a probe range is required or zero otherwise.  \
+Defining this hook also requires your functions which make use of alloca to have at least 8 byes\
+of outgoing arguments.  If this is not the case the stack will be corrupted.\n\
+You need not define this macro if it would always have the value zero.",
+ HOST_WIDE_INT, (void),
+ default_stack_clash_protection_alloca_probe_range)
+
+
+/* Functions specific to the C family of frontends.  */
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_C_"
+HOOK_VECTOR (TARGET_C, c)
+
+/* ??? Documenting this hook requires a GFDL license grant.  */
+DEFHOOK_UNDOC
+(mode_for_suffix,
+"Return machine mode for non-standard constant literal suffix @var{c},\
+ or VOIDmode if non-standard suffixes are unsupported.",
+ machine_mode, (char c),
+ default_mode_for_suffix)
+
+DEFHOOK
+(excess_precision,
+ "Return a value, with the same meaning as the C99 macro\
+ @code{FLT_EVAL_METHOD} that describes which excess precision should be\
+ applied.  @var{type} is either @code{EXCESS_PRECISION_TYPE_IMPLICIT},\
+ @code{EXCESS_PRECISION_TYPE_FAST}, or\
+ @code{EXCESS_PRECISION_TYPE_STANDARD}.  For\
+ @code{EXCESS_PRECISION_TYPE_IMPLICIT}, the target should return which\
+ precision and range operations will be implictly evaluated in regardless\
+ of the excess precision explicitly added.  For\
+ @code{EXCESS_PRECISION_TYPE_STANDARD} and\
+ @code{EXCESS_PRECISION_TYPE_FAST}, the target should return the\
+ explicit excess precision that should be added depending on the\
+ value set for @option{-fexcess-precision=@r{[}standard@r{|}fast@r{]}}.\
+ Note that unpredictable explicit excess precision does not make sense,\
+ so a target should never return @code{FLT_EVAL_METHOD_UNPREDICTABLE}\
+ when @var{type} is @code{EXCESS_PRECISION_TYPE_STANDARD} or\
+ @code{EXCESS_PRECISION_TYPE_FAST}.",
+ enum flt_eval_method, (enum excess_precision_type type),
+ default_excess_precision)
+
+HOOK_VECTOR_END (c)
+
+/* Functions specific to the C++ frontend.  */
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_CXX_"
+HOOK_VECTOR (TARGET_CXX, cxx)
+
+/* Return the integer type used for guard variables.  */
+DEFHOOK
+(guard_type,
+ "Define this hook to override the integer type used for guard variables.\n\
+These are used to implement one-time construction of static objects.  The\n\
+default is long_long_integer_type_node.",
+ tree, (void),
+ default_cxx_guard_type)
+
+/* Return true if only the low bit of the guard should be tested.  */
+DEFHOOK
+(guard_mask_bit,
+ "This hook determines how guard variables are used.  It should return\n\
+@code{false} (the default) if the first byte should be used.  A return value of\n\
+@code{true} indicates that only the least significant bit should be used.",
+ bool, (void),
+ hook_bool_void_false)
+
+/* Returns the size of the array cookie for an array of type.  */
+DEFHOOK
+(get_cookie_size,
+ "This hook returns the size of the cookie to use when allocating an array\n\
+whose elements have the indicated @var{type}.  Assumes that it is already\n\
+known that a cookie is needed.  The default is\n\
+@code{max(sizeof (size_t), alignof(type))}, as defined in section 2.7 of the\n\
+IA64/Generic C++ ABI@.",
+ tree, (tree type),
+ default_cxx_get_cookie_size)
+
+/* Returns true if the element size should be stored in the array cookie.  */
+DEFHOOK
+(cookie_has_size,
+ "This hook should return @code{true} if the element size should be stored in\n\
+array cookies.  The default is to return @code{false}.",
+ bool, (void),
+ hook_bool_void_false)
+
+/* Allows backends to perform additional processing when
+   deciding if a class should be exported or imported.  */
+DEFHOOK
+(import_export_class,
+ "If defined by a backend this hook allows the decision made to export\n\
+class @var{type} to be overruled.  Upon entry @var{import_export}\n\
+will contain 1 if the class is going to be exported, @minus{}1 if it is going\n\
+to be imported and 0 otherwise.  This function should return the\n\
+modified value and perform any other actions necessary to support the\n\
+backend's targeted operating system.",
+ int, (tree type, int import_export), NULL)
+
+/* Returns true if constructors and destructors return "this".  */
+DEFHOOK
+(cdtor_returns_this,
+ "This hook should return @code{true} if constructors and destructors return\n\
+the address of the object created/destroyed.  The default is to return\n\
+@code{false}.",
+ bool, (void),
+ hook_bool_void_false)
+
+/* Returns true if the key method for a class can be an inline
+   function, so long as it is not declared inline in the class
+   itself.  Returning true is the behavior required by the Itanium C++ ABI.  */
+DEFHOOK
+(key_method_may_be_inline,
+ "This hook returns true if the key method for a class (i.e., the method\n\
+which, if defined in the current translation unit, causes the virtual\n\
+table to be emitted) may be an inline function.  Under the standard\n\
+Itanium C++ ABI the key method may be an inline function so long as\n\
+the function is not declared inline in the class definition.  Under\n\
+some variants of the ABI, an inline function can never be the key\n\
+method.  The default is to return @code{true}.",
+ bool, (void),
+ hook_bool_void_true)
+
+DEFHOOK
+(determine_class_data_visibility,
+"@var{decl} is a virtual table, virtual table table, typeinfo object,\
+ or other similar implicit class data object that will be emitted with\
+ external linkage in this translation unit.  No ELF visibility has been\
+ explicitly specified.  If the target needs to specify a visibility\
+ other than that of the containing class, use this hook to set\
+ @code{DECL_VISIBILITY} and @code{DECL_VISIBILITY_SPECIFIED}.",
+ void, (tree decl),
+ hook_void_tree)
+
+/* Returns true (the default) if virtual tables and other
+   similar implicit class data objects are always COMDAT if they
+   have external linkage.  If this hook returns false, then
+   class data for classes whose virtual table will be emitted in
+   only one translation unit will not be COMDAT.  */
+DEFHOOK
+(class_data_always_comdat,
+ "This hook returns true (the default) if virtual tables and other\n\
+similar implicit class data objects are always COMDAT if they have\n\
+external linkage.  If this hook returns false, then class data for\n\
+classes whose virtual table will be emitted in only one translation\n\
+unit will not be COMDAT.",
+ bool, (void),
+ hook_bool_void_true)
+
+/* Returns true (the default) if the RTTI for the basic types,
+   which is always defined in the C++ runtime, should be COMDAT;
+   false if it should not be COMDAT.  */
+DEFHOOK
+(library_rtti_comdat,
+ "This hook returns true (the default) if the RTTI information for\n\
+the basic types which is defined in the C++ runtime should always\n\
+be COMDAT, false if it should not be COMDAT.",
+ bool, (void),
+ hook_bool_void_true)
+
+/* Returns true if __aeabi_atexit should be used to register static
+   destructors.  */
+DEFHOOK
+(use_aeabi_atexit,
+ "This hook returns true if @code{__aeabi_atexit} (as defined by the ARM EABI)\n\
+should be used to register static destructors when @option{-fuse-cxa-atexit}\n\
+is in effect.  The default is to return false to use @code{__cxa_atexit}.",
+ bool, (void),
+ hook_bool_void_false)
+
+/* Returns true if target may use atexit in the same manner as
+   __cxa_atexit  to register static destructors.  */
+DEFHOOK
+(use_atexit_for_cxa_atexit,
+ "This hook returns true if the target @code{atexit} function can be used\n\
+in the same manner as @code{__cxa_atexit} to register C++ static\n\
+destructors. This requires that @code{atexit}-registered functions in\n\
+shared libraries are run in the correct order when the libraries are\n\
+unloaded. The default is to return false.",
+ bool, (void),
+ hook_bool_void_false)
+
+DEFHOOK
+(adjust_class_at_definition,
+"@var{type} is a C++ class (i.e., RECORD_TYPE or UNION_TYPE) that has just\
+ been defined.  Use this hook to make adjustments to the class (eg, tweak\
+ visibility or perform any other required target modifications).",
+ void, (tree type),
+ hook_void_tree)
+
+DEFHOOK
+(decl_mangling_context,
+ "Return target-specific mangling context of @var{decl} or @code{NULL_TREE}.",
+ tree, (const_tree decl),
+ hook_tree_const_tree_null)
+
+HOOK_VECTOR_END (cxx)
+
+/* Functions and data for emulated TLS support.  */
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_EMUTLS_"
+HOOK_VECTOR (TARGET_EMUTLS, emutls)
+
+/* Name of the address and common functions.  */
+DEFHOOKPOD
+(get_address,
+ "Contains the name of the helper function that uses a TLS control\n\
+object to locate a TLS instance.  The default causes libgcc's\n\
+emulated TLS helper function to be used.",
+ const char *, "__builtin___emutls_get_address")
+
+DEFHOOKPOD
+(register_common,
+ "Contains the name of the helper function that should be used at\n\
+program startup to register TLS objects that are implicitly\n\
+initialized to zero.  If this is @code{NULL}, all TLS objects will\n\
+have explicit initializers.  The default causes libgcc's emulated TLS\n\
+registration function to be used.",
+ const char *, "__builtin___emutls_register_common")
+
+/* Prefixes for proxy variable and template.  */
+DEFHOOKPOD
+(var_section,
+ "Contains the name of the section in which TLS control variables should\n\
+be placed.  The default of @code{NULL} allows these to be placed in\n\
+any section.",
+ const char *, NULL)
+
+DEFHOOKPOD
+(tmpl_section,
+ "Contains the name of the section in which TLS initializers should be\n\
+placed.  The default of @code{NULL} allows these to be placed in any\n\
+section.",
+ const char *, NULL)
+
+/* Prefixes for proxy variable and template.  */
+DEFHOOKPOD
+(var_prefix,
+ "Contains the prefix to be prepended to TLS control variable names.\n\
+The default of @code{NULL} uses a target-specific prefix.",
+ const char *, NULL)
+
+DEFHOOKPOD
+(tmpl_prefix,
+ "Contains the prefix to be prepended to TLS initializer objects.  The\n\
+default of @code{NULL} uses a target-specific prefix.",
+ const char *, NULL)
+
+/* Function to generate field definitions of the proxy variable.  */
+DEFHOOK
+(var_fields,
+ "Specifies a function that generates the FIELD_DECLs for a TLS control\n\
+object type.  @var{type} is the RECORD_TYPE the fields are for and\n\
+@var{name} should be filled with the structure tag, if the default of\n\
+@code{__emutls_object} is unsuitable.  The default creates a type suitable\n\
+for libgcc's emulated TLS function.",
+ tree, (tree type, tree *name),
+ default_emutls_var_fields)
+
+/* Function to initialize a proxy variable.  */
+DEFHOOK
+(var_init,
+ "Specifies a function that generates the CONSTRUCTOR to initialize a\n\
+TLS control object.  @var{var} is the TLS control object, @var{decl}\n\
+is the TLS object and @var{tmpl_addr} is the address of the\n\
+initializer.  The default initializes libgcc's emulated TLS control object.",
+ tree, (tree var, tree decl, tree tmpl_addr),
+ default_emutls_var_init)
+
+/* Whether we are allowed to alter the usual alignment of the
+   proxy variable.  */
+DEFHOOKPOD
+(var_align_fixed,
+ "Specifies whether the alignment of TLS control variable objects is\n\
+fixed and should not be increased as some backends may do to optimize\n\
+single objects.  The default is false.",
+ bool, false)
+
+/* Whether we can emit debug information for TLS vars.  */
+DEFHOOKPOD
+(debug_form_tls_address,
+ "Specifies whether a DWARF @code{DW_OP_form_tls_address} location descriptor\n\
+may be used to describe emulated TLS control objects.",
+ bool, false)
+
+HOOK_VECTOR_END (emutls)
+
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_OPTION_"
+HOOK_VECTOR (TARGET_OPTION_HOOKS, target_option_hooks)
+
+/* Function to validate the attribute((target(...))) strings.  If
+   the option is validated, the hook should also fill in
+   DECL_FUNCTION_SPECIFIC_TARGET in the function decl node.  */
+DEFHOOK
+(valid_attribute_p,
+ "This hook is called to parse @code{attribute(target(\"...\"))}, which\n\
+allows setting target-specific options on individual functions.\n\
+These function-specific options may differ\n\
+from the options specified on the command line.  The hook should return\n\
+@code{true} if the options are valid.\n\
+\n\
+The hook should set the @code{DECL_FUNCTION_SPECIFIC_TARGET} field in\n\
+the function declaration to hold a pointer to a target-specific\n\
+@code{struct cl_target_option} structure.",
+ bool, (tree fndecl, tree name, tree args, int flags),
+ default_target_option_valid_attribute_p)
+
+/* Function to save any extra target state in the target options structure.  */
+DEFHOOK
+(save,
+ "This hook is called to save any additional target-specific information\n\
+in the @code{struct cl_target_option} structure for function-specific\n\
+options from the @code{struct gcc_options} structure.\n\
+@xref{Option file format}.",
+ void, (struct cl_target_option *ptr, struct gcc_options *opts,
+	struct gcc_options *opts_set), NULL)
+
+/* Function to restore any extra target state from the target options
+   structure.  */
+DEFHOOK
+(restore,
+ "This hook is called to restore any additional target-specific\n\
+information in the @code{struct cl_target_option} structure for\n\
+function-specific options to the @code{struct gcc_options} structure.",
+ void, (struct gcc_options *opts, struct gcc_options *opts_set,
+	struct cl_target_option *ptr), NULL)
+
+/* Function to update target-specific option information after being
+   streamed in.  */
+DEFHOOK
+(post_stream_in,
+ "This hook is called to update target-specific information in the\n\
+@code{struct cl_target_option} structure after it is streamed in from\n\
+LTO bytecode.",
+ void, (struct cl_target_option *ptr), NULL)
+
+/* Function to print any extra target state from the target options
+   structure.  */
+DEFHOOK
+(print,
+ "This hook is called to print any additional target-specific\n\
+information in the @code{struct cl_target_option} structure for\n\
+function-specific options.",
+ void, (FILE *file, int indent, struct cl_target_option *ptr), NULL)
+
+/* Function to parse arguments to be validated for #pragma target, and to
+   change the state if the options are valid.  If the first argument is
+   NULL, the second argument specifies the default options to use.  Return
+   true if the options are valid, and set the current state.  */
+DEFHOOK
+(pragma_parse,
+ "This target hook parses the options for @code{#pragma GCC target}, which\n\
+sets the target-specific options for functions that occur later in the\n\
+input stream.  The options accepted should be the same as those handled by the\n\
+@code{TARGET_OPTION_VALID_ATTRIBUTE_P} hook.",
+ bool, (tree args, tree pop_target),
+ default_target_option_pragma_parse)
+
+/* Do option overrides for the target.  */
+DEFHOOK
+(override,
+ "Sometimes certain combinations of command options do not make sense on\n\
+a particular target machine.  You can override the hook\n\
+@code{TARGET_OPTION_OVERRIDE} to take account of this.  This hooks is called\n\
+once just after all the command options have been parsed.\n\
+\n\
+Don't use this hook to turn on various extra optimizations for\n\
+@option{-O}.  That is what @code{TARGET_OPTION_OPTIMIZATION} is for.\n\
+\n\
+If you need to do something whenever the optimization level is\n\
+changed via the optimize attribute or pragma, see\n\
+@code{TARGET_OVERRIDE_OPTIONS_AFTER_CHANGE}",
+ void, (void),
+ hook_void_void)
+
+/* This function returns true if DECL1 and DECL2 are versions of the same
+   function.  DECL1 and DECL2 are function versions if and only if they
+   have the same function signature and different target specific attributes,
+   that is, they are compiled for different target machines.  */
+DEFHOOK
+(function_versions,
+ "This target hook returns @code{true} if @var{DECL1} and @var{DECL2} are\n\
+versions of the same function.  @var{DECL1} and @var{DECL2} are function\n\
+versions if and only if they have the same function signature and\n\
+different target specific attributes, that is, they are compiled for\n\
+different target machines.",
+ bool, (tree decl1, tree decl2),
+ hook_bool_tree_tree_false)
+
+/* Function to determine if one function can inline another function.  */
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_"
+DEFHOOK
+(can_inline_p,
+ "This target hook returns @code{false} if the @var{caller} function\n\
+cannot inline @var{callee}, based on target specific information.  By\n\
+default, inlining is not allowed if the callee function has function\n\
+specific target options and the caller does not use the same options.",
+ bool, (tree caller, tree callee),
+ default_target_can_inline_p)
+
+DEFHOOK
+(relayout_function,
+"This target hook fixes function @var{fndecl} after attributes are processed. Default does nothing. On ARM, the default function's alignment is updated with the attribute target.",
+ void, (tree fndecl),
+ hook_void_tree)
+
+HOOK_VECTOR_END (target_option)
+
+/* For targets that need to mark extra registers as live on entry to
+   the function, they should define this target hook and set their
+   bits in the bitmap passed in. */
+DEFHOOK
+(extra_live_on_entry,
+ "Add any hard registers to @var{regs} that are live on entry to the\n\
+function.  This hook only needs to be defined to provide registers that\n\
+cannot be found by examination of FUNCTION_ARG_REGNO_P, the callee saved\n\
+registers, STATIC_CHAIN_INCOMING_REGNUM, STATIC_CHAIN_REGNUM,\n\
+TARGET_STRUCT_VALUE_RTX, FRAME_POINTER_REGNUM, EH_USES,\n\
+FRAME_POINTER_REGNUM, ARG_POINTER_REGNUM, and the PIC_OFFSET_TABLE_REGNUM.",
+ void, (bitmap regs),
+ hook_void_bitmap)
+
+/* Targets should define this target hook to mark that non-callee clobbers are
+   present in CALL_INSN_FUNCTION_USAGE for all the calls that bind to a local
+   definition.  */
+DEFHOOKPOD
+(call_fusage_contains_non_callee_clobbers,
+ "Set to true if each call that binds to a local definition explicitly\n\
+clobbers or sets all non-fixed registers modified by performing the call.\n\
+That is, by the call pattern itself, or by code that might be inserted by the\n\
+linker (e.g.@: stubs, veneers, branch islands), but not including those\n\
+modifiable by the callee.  The affected registers may be mentioned explicitly\n\
+in the call pattern, or included as clobbers in CALL_INSN_FUNCTION_USAGE.\n\
+The default version of this hook is set to false.  The purpose of this hook\n\
+is to enable the fipa-ra optimization.",
+ bool, 
+ false)
+
+/* Fill in additional registers set up by prologue into a regset.  */
+DEFHOOK
+(set_up_by_prologue,
+ "This hook should add additional registers that are computed by the prologue\
+ to the hard regset for shrink-wrapping optimization purposes.",
+ void, (struct hard_reg_set_container *),
+ NULL)
+
+/* For targets that have attributes that can affect whether a
+   function's return statements need checking.  For instance a 'naked'
+   function attribute.  */
+DEFHOOK
+(warn_func_return,
+ "True if a function's return statements should be checked for matching the function's return type.  This includes checking for falling off the end of a non-void function.  Return false if no such check should be made.",
+ bool, (tree),
+ hook_bool_tree_true)
+
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_SHRINK_WRAP_"
+HOOK_VECTOR (TARGET_SHRINK_WRAP_HOOKS, shrink_wrap)
+
+DEFHOOK
+(get_separate_components,
+ "This hook should return an @code{sbitmap} with the bits set for those\n\
+components that can be separately shrink-wrapped in the current function.\n\
+Return @code{NULL} if the current function should not get any separate\n\
+shrink-wrapping.\n\
+Don't define this hook if it would always return @code{NULL}.\n\
+If it is defined, the other hooks in this group have to be defined as well.",
+ sbitmap, (void),
+ NULL)
+
+DEFHOOK
+(components_for_bb,
+ "This hook should return an @code{sbitmap} with the bits set for those\n\
+components where either the prologue component has to be executed before\n\
+the @code{basic_block}, or the epilogue component after it, or both.",
+ sbitmap, (basic_block),
+ NULL)
+
+DEFHOOK
+(disqualify_components,
+ "This hook should clear the bits in the @var{components} bitmap for those\n\
+components in @var{edge_components} that the target cannot handle on edge\n\
+@var{e}, where @var{is_prologue} says if this is for a prologue or an\n\
+epilogue instead.",
+ void, (sbitmap components, edge e, sbitmap edge_components, bool is_prologue),
+ NULL)
+
+DEFHOOK
+(emit_prologue_components,
+ "Emit prologue insns for the components indicated by the parameter.",
+ void, (sbitmap),
+ NULL)
+
+DEFHOOK
+(emit_epilogue_components,
+ "Emit epilogue insns for the components indicated by the parameter.",
+ void, (sbitmap),
+ NULL)
+
+DEFHOOK
+(set_handled_components,
+ "Mark the components in the parameter as handled, so that the\n\
+@code{prologue} and @code{epilogue} named patterns know to ignore those\n\
+components.  The target code should not hang on to the @code{sbitmap}, it\n\
+will be deleted after this call.",
+ void, (sbitmap),
+ NULL)
+
+HOOK_VECTOR_END (shrink_wrap)
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_"
+
+/* Determine the type of unwind info to emit for debugging.  */
+DEFHOOK
+(debug_unwind_info,
+ "This hook defines the mechanism that will be used for describing frame\n\
+unwind information to the debugger.  Normally the hook will return\n\
+@code{UI_DWARF2} if DWARF 2 debug information is enabled, and\n\
+return @code{UI_NONE} otherwise.\n\
+\n\
+A target may return @code{UI_DWARF2} even when DWARF 2 debug information\n\
+is disabled in order to always output DWARF 2 frame information.\n\
+\n\
+A target may return @code{UI_TARGET} if it has ABI specified unwind tables.\n\
+This will suppress generation of the normal debug frame unwind information.",
+ enum unwind_info_type, (void),
+ default_debug_unwind_info)
+
+DEFHOOK
+(reset_location_view, "\
+This hook, if defined, enables -ginternal-reset-location-views, and\n\
+uses its result to override cases in which the estimated min insn\n\
+length might be nonzero even when a PC advance (i.e., a view reset)\n\
+cannot be taken for granted.\n\
+\n\
+If the hook is defined, it must return a positive value to indicate\n\
+the insn definitely advances the PC, and so the view number can be\n\
+safely assumed to be reset; a negative value to mean the insn\n\
+definitely does not advance the PC, and os the view number must not\n\
+be reset; or zero to decide based on the estimated insn length.\n\
+\n\
+If insn length is to be regarded as reliable, set the hook to\n\
+@code{hook_int_rtx_insn_0}.",
+ int, (rtx_insn *), NULL)
+
+/* The code parameter should be of type enum rtx_code but this is not
+   defined at this time.  */
+DEFHOOK
+(canonicalize_comparison,
+ "On some machines not all possible comparisons are defined, but you can\n\
+convert an invalid comparison into a valid one.  For example, the Alpha\n\
+does not have a @code{GT} comparison, but you can use an @code{LT}\n\
+comparison instead and swap the order of the operands.\n\
+\n\
+On such machines, implement this hook to do any required conversions.\n\
+@var{code} is the initial comparison code and @var{op0} and @var{op1}\n\
+are the left and right operands of the comparison, respectively.  If\n\
+@var{op0_preserve_value} is @code{true} the implementation is not\n\
+allowed to change the value of @var{op0} since the value might be used\n\
+in RTXs which aren't comparisons.  E.g. the implementation is not\n\
+allowed to swap operands in that case.\n\
+\n\
+GCC will not assume that the comparison resulting from this macro is\n\
+valid but will see if the resulting insn matches a pattern in the\n\
+@file{md} file.\n\
+\n\
+You need not to implement this hook if it would never change the\n\
+comparison code or operands.",
+ void, (int *code, rtx *op0, rtx *op1, bool op0_preserve_value),
+ default_canonicalize_comparison)
+
+DEFHOOK
+(min_arithmetic_precision,
+ "On some RISC architectures with 64-bit registers, the processor also\n\
+maintains 32-bit condition codes that make it possible to do real 32-bit\n\
+arithmetic, although the operations are performed on the full registers.\n\
+\n\
+On such architectures, defining this hook to 32 tells the compiler to try\n\
+using 32-bit arithmetical operations setting the condition codes instead\n\
+of doing full 64-bit arithmetic.\n\
+\n\
+More generally, define this hook on RISC architectures if you want the\n\
+compiler to try using arithmetical operations setting the condition codes\n\
+with a precision lower than the word precision.\n\
+\n\
+You need not define this hook if @code{WORD_REGISTER_OPERATIONS} is not\n\
+defined to 1.",
+ unsigned int, (void), default_min_arithmetic_precision)
+
+DEFHOOKPOD
+(atomic_test_and_set_trueval,
+ "This value should be set if the result written by\
+ @code{atomic_test_and_set} is not exactly 1, i.e.@: the\
+ @code{bool} @code{true}.",
+ unsigned char, 1)
+
+/* Return an unsigned int representing the alignment (in bits) of the atomic
+   type which maps to machine MODE.  This allows alignment to be overridden
+   as needed.  */
+DEFHOOK
+(atomic_align_for_mode,
+"If defined, this function returns an appropriate alignment in bits for an\
+ atomic object of machine_mode @var{mode}.  If 0 is returned then the\
+ default alignment for the specified mode is used. ",
+ unsigned int, (machine_mode mode),
+ hook_uint_mode_0)
+
+DEFHOOK
+(atomic_assign_expand_fenv,
+"ISO C11 requires atomic compound assignments that may raise floating-point\
+ exceptions to raise exceptions corresponding to the arithmetic operation\
+ whose result was successfully stored in a compare-and-exchange sequence. \
+ This requires code equivalent to calls to @code{feholdexcept},\
+ @code{feclearexcept} and @code{feupdateenv} to be generated at\
+ appropriate points in the compare-and-exchange sequence.  This hook should\
+ set @code{*@var{hold}} to an expression equivalent to the call to\
+ @code{feholdexcept}, @code{*@var{clear}} to an expression equivalent to\
+ the call to @code{feclearexcept} and @code{*@var{update}} to an expression\
+ equivalent to the call to @code{feupdateenv}.  The three expressions are\
+ @code{NULL_TREE} on entry to the hook and may be left as @code{NULL_TREE}\
+ if no code is required in a particular place.  The default implementation\
+ leaves all three expressions as @code{NULL_TREE}.  The\
+ @code{__atomic_feraiseexcept} function from @code{libatomic} may be of use\
+ as part of the code generated in @code{*@var{update}}.",
+ void, (tree *hold, tree *clear, tree *update),
+ default_atomic_assign_expand_fenv)
+
+/* Leave the boolean fields at the end.  */
+
+/* True if we can create zeroed data by switching to a BSS section
+   and then using ASM_OUTPUT_SKIP to allocate the space.  */
+DEFHOOKPOD
+(have_switchable_bss_sections,
+ "This flag is true if we can create zeroed data by switching to a BSS\n\
+section and then using @code{ASM_OUTPUT_SKIP} to allocate the space.\n\
+This is true on most ELF targets.",
+ bool, false)
+
+/* True if "native" constructors and destructors are supported,
+   false if we're using collect2 for the job.  */
+DEFHOOKPOD
+(have_ctors_dtors,
+ "This value is true if the target supports some ``native'' method of\n\
+collecting constructors and destructors to be run at startup and exit.\n\
+It is false if we must use @command{collect2}.",
+ bool, false)
+
+/* True if thread-local storage is supported.  */
+DEFHOOKPOD
+(have_tls,
+ "Contains the value true if the target supports thread-local storage.\n\
+The default value is false.",
+ bool, false)
+
+/* True if a small readonly data section is supported.  */
+DEFHOOKPOD
+(have_srodata_section,
+ "Contains the value true if the target places read-only\n\
+``small data'' into a separate section.  The default value is false.",
+ bool, false)
+
+/* True if EH frame info sections should be zero-terminated.  */
+DEFHOOKPOD
+(terminate_dw2_eh_frame_info,
+ "Contains the value true if the target should add a zero word onto the\n\
+end of a Dwarf-2 frame info section when used for exception handling.\n\
+Default value is false if @code{EH_FRAME_SECTION_NAME} is defined, and\n\
+true otherwise.",
+ bool, true)
+
+/* True if #NO_APP should be emitted at the beginning of assembly output.  */
+DEFHOOKPOD
+(asm_file_start_app_off,
+ "If this flag is true, the text of the macro @code{ASM_APP_OFF} will be\n\
+printed as the very first line in the assembly file, unless\n\
+@option{-fverbose-asm} is in effect.  (If that macro has been defined\n\
+to the empty string, this variable has no effect.)  With the normal\n\
+definition of @code{ASM_APP_OFF}, the effect is to notify the GNU\n\
+assembler that it need not bother stripping comments or extra\n\
+whitespace from its input.  This allows it to work a bit faster.\n\
+\n\
+The default is false.  You should not set it to true unless you have\n\
+verified that your port does not generate any extra whitespace or\n\
+comments that will cause GAS to issue errors in NO_APP mode.",
+ bool, false)
+
+/* True if output_file_directive should be called for main_input_filename
+   at the beginning of assembly output.  */
+DEFHOOKPOD
+(asm_file_start_file_directive,
+ "If this flag is true, @code{output_file_directive} will be called\n\
+for the primary source file, immediately after printing\n\
+@code{ASM_APP_OFF} (if that is enabled).  Most ELF assemblers expect\n\
+this to be done.  The default is false.",
+ bool, false)
+
+/* Returns true if we should generate exception tables for use with the
+   ARM EABI.  The effects the encoding of function exception specifications.  */
+DEFHOOKPOD
+(arm_eabi_unwinder,
+ "This flag should be set to @code{true} on targets that use an ARM EABI\n\
+based unwinding library, and @code{false} on other targets.  This effects\n\
+the format of unwinding tables, and how the unwinder in entered after\n\
+running a cleanup.  The default is @code{false}.",
+ bool, false)
+
+DEFHOOKPOD
+(want_debug_pub_sections,
+ "True if the @code{.debug_pubtypes} and @code{.debug_pubnames} sections\
+ should be emitted.  These sections are not used on most platforms, and\
+ in particular GDB does not use them.",
+ bool, false)
+
+DEFHOOKPOD
+(delay_sched2, "True if sched2 is not to be run at its normal place.\n\
+This usually means it will be run as part of machine-specific reorg.",
+bool, false)
+
+DEFHOOKPOD
+(delay_vartrack, "True if vartrack is not to be run at its normal place.\n\
+This usually means it will be run as part of machine-specific reorg.",
+bool, false)
+
+DEFHOOKPOD
+(no_register_allocation, "True if register allocation and the passes\n\
+following it should not be run.  Usually true only for virtual assembler\n\
+targets.",
+bool, false)
+
+/* Leave the boolean fields at the end.  */
+
+/* Functions related to mode switching.  */
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_MODE_"
+HOOK_VECTOR (TARGET_TOGGLE_, mode_switching)
+
+DEFHOOK
+(emit,
+ "Generate one or more insns to set @var{entity} to @var{mode}. @var{hard_reg_live} is the set of hard registers live at the point where the insn(s) are to be inserted. @var{prev_moxde} indicates the mode to switch from. Sets of a lower numbered entity will be emitted before sets of a higher numbered entity to a mode of the same or lower priority.",
+ void, (int entity, int mode, int prev_mode, HARD_REG_SET regs_live), NULL)
+
+DEFHOOK
+(needed,
+ "@var{entity} is an integer specifying a mode-switched entity.  If @code{OPTIMIZE_MODE_SWITCHING} is defined, you must define this macro to return an integer value not larger than the corresponding element in @code{NUM_MODES_FOR_MODE_SWITCHING}, to denote the mode that @var{entity} must be switched into prior to the execution of @var{insn}.",
+ int, (int entity, rtx_insn *insn), NULL)
+
+DEFHOOK
+(after,
+ "@var{entity} is an integer specifying a mode-switched entity.  If this macro is defined, it is evaluated for every @var{insn} during mode switching.  It determines the mode that an insn results in (if different from the incoming mode).",
+ int, (int entity, int mode, rtx_insn *insn), NULL)
+
+DEFHOOK
+(entry,
+ "If this macro is defined, it is evaluated for every @var{entity} that needs mode switching.  It should evaluate to an integer, which is a mode that @var{entity} is assumed to be switched to at function entry.  If @code{TARGET_MODE_ENTRY} is defined then @code{TARGET_MODE_EXIT} must be defined.",
+ int, (int entity), NULL)
+
+DEFHOOK
+(exit,
+ "If this macro is defined, it is evaluated for every @var{entity} that needs mode switching.  It should evaluate to an integer, which is a mode that @var{entity} is assumed to be switched to at function exit.  If @code{TARGET_MODE_EXIT} is defined then @code{TARGET_MODE_ENTRY} must be defined.",
+ int, (int entity), NULL)
+
+DEFHOOK
+(priority,
+ "This macro specifies the order in which modes for @var{entity} are processed. 0 is the highest priority, @code{NUM_MODES_FOR_MODE_SWITCHING[@var{entity}] - 1} the lowest.  The value of the macro should be an integer designating a mode for @var{entity}.  For any fixed @var{entity}, @code{mode_priority} (@var{entity}, @var{n}) shall be a bijection in 0 @dots{} @code{num_modes_for_mode_switching[@var{entity}] - 1}.",
+ int, (int entity, int n), NULL)
+
+HOOK_VECTOR_END (mode_switching)
+
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_MEMTAG_"
+HOOK_VECTOR (TARGET_MEMTAG_, memtag)
+
+DEFHOOK
+(can_tag_addresses,
+ "True if the backend architecture naturally supports ignoring some region\n\
+of pointers.  This feature means that @option{-fsanitize=hwaddress} can\n\
+work.\n\
+\n\
+At preset, this feature does not support address spaces.  It also requires\n\
+@code{Pmode} to be the same as @code{ptr_mode}.",
+ bool, (), default_memtag_can_tag_addresses)
+
+DEFHOOK
+(tag_size,
+ "Return the size of a tag (in bits) for this platform.\n\
+\n\
+The default returns 8.",
+  uint8_t, (), default_memtag_tag_size)
+
+DEFHOOK
+(granule_size,
+ "Return the size in real memory that each byte in shadow memory refers to.\n\
+I.e. if a variable is @var{X} bytes long in memory, then this hook should\n\
+return the value @var{Y} such that the tag in shadow memory spans\n\
+@var{X}/@var{Y} bytes.\n\
+\n\
+Most variables will need to be aligned to this amount since two variables\n\
+that are neighbors in memory and share a tag granule would need to share\n\
+the same tag.\n\
+\n\
+The default returns 16.",
+  uint8_t, (), default_memtag_granule_size)
+
+DEFHOOK
+(insert_random_tag,
+ "Return an RTX representing the value of @var{untagged} but with a\n\
+(possibly) random tag in it.\n\
+Put that value into @var{target} if it is convenient to do so.\n\
+This function is used to generate a tagged base for the current stack frame.",
+  rtx, (rtx untagged, rtx target), default_memtag_insert_random_tag)
+
+DEFHOOK
+(add_tag,
+ "Return an RTX that represents the result of adding @var{addr_offset} to\n\
+the address in pointer @var{base} and @var{tag_offset} to the tag in pointer\n\
+@var{base}.\n\
+The resulting RTX must either be a valid memory address or be able to get\n\
+put into an operand with @code{force_operand}.\n\
+\n\
+Unlike other memtag hooks, this must return an expression and not emit any\n\
+RTL.",
+  rtx, (rtx base, poly_int64 addr_offset, uint8_t tag_offset),
+  default_memtag_add_tag)
+
+DEFHOOK
+(set_tag,
+ "Return an RTX representing @var{untagged_base} but with the tag @var{tag}.\n\
+Try and store this in @var{target} if convenient.\n\
+@var{untagged_base} is required to have a zero tag when this hook is called.\n\
+The default of this hook is to set the top byte of @var{untagged_base} to\n\
+@var{tag}.",
+  rtx, (rtx untagged_base, rtx tag, rtx target), default_memtag_set_tag)
+
+DEFHOOK
+(extract_tag,
+ "Return an RTX representing the tag stored in @var{tagged_pointer}.\n\
+Store the result in @var{target} if it is convenient.\n\
+The default represents the top byte of the original pointer.",
+  rtx, (rtx tagged_pointer, rtx target), default_memtag_extract_tag)
+
+DEFHOOK
+(untagged_pointer,
+ "Return an RTX representing @var{tagged_pointer} with its tag set to zero.\n\
+Store the result in @var{target} if convenient.\n\
+The default clears the top byte of the original pointer.",
+  rtx, (rtx tagged_pointer, rtx target), default_memtag_untagged_pointer)
+
+HOOK_VECTOR_END (memtag)
+#undef HOOK_PREFIX
+#define HOOK_PREFIX "TARGET_"
+
+#define DEF_TARGET_INSN(NAME, PROTO) \
+  DEFHOOK_UNDOC (have_##NAME, "", bool, (void), false)
+#include "target-insns.def"
+#undef DEF_TARGET_INSN
+
+#define DEF_TARGET_INSN(NAME, PROTO) \
+  DEFHOOK_UNDOC (gen_##NAME, "", rtx_insn *, PROTO, NULL)
+#include "target-insns.def"
+#undef DEF_TARGET_INSN
+
+#define DEF_TARGET_INSN(NAME, PROTO) \
+  DEFHOOKPOD (code_for_##NAME, "*", enum insn_code, CODE_FOR_nothing)
+#include "target-insns.def"
+#undef DEF_TARGET_INSN
+
+DEFHOOK
+(run_target_selftests,
+ "If selftests are enabled, run any selftests for this target.",
+ void, (void),
+ NULL)
+
+/* Close the 'struct gcc_target' definition.  */
+HOOK_VECTOR_END (C90_EMPTY_HACK)
+
diff -uNr gcc-11.2.0.orig/gcc/targhooks.c gcc-11.2.0/gcc/targhooks.c
--- gcc-11.2.0.orig/gcc/targhooks.c	2021-07-28 01:55:07.504290371 -0500
+++ gcc-11.2.0/gcc/targhooks.c	2021-08-06 00:34:01.884709574 -0500
@@ -1822,6 +1822,14 @@
   return by_pieces_ninsns (size, alignment, max_size + 1, op) < ratio;
 }
 
+/* This hook returns an rtx that is used to clear memory.  */
+
+rtx
+default_clear_by_pieces (void *, void *, HOST_WIDE_INT, scalar_int_mode)
+{
+  return const0_rtx;
+}
+
 /* This hook controls code generation for expanding a memcmp operation by
    pieces.  Return 1 for the normal pattern of compare/jump after each pair
    of loads, or a higher number to reduce the number of branches.  */
diff -uNr gcc-11.2.0.orig/gcc/targhooks.c.orig gcc-11.2.0/gcc/targhooks.c.orig
--- gcc-11.2.0.orig/gcc/targhooks.c.orig	1969-12-31 18:00:00.000000000 -0600
+++ gcc-11.2.0/gcc/targhooks.c.orig	2021-07-28 01:55:07.504290371 -0500
@@ -0,0 +1,2550 @@
+/* Default target hook functions.
+   Copyright (C) 2003-2021 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 3, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING3.  If not see
+<http://www.gnu.org/licenses/>.  */
+
+/* The migration of target macros to target hooks works as follows:
+
+   1. Create a target hook that uses the existing target macros to
+      implement the same functionality.
+
+   2. Convert all the MI files to use the hook instead of the macro.
+
+   3. Repeat for a majority of the remaining target macros.  This will
+      take some time.
+
+   4. Tell target maintainers to start migrating.
+
+   5. Eventually convert the backends to override the hook instead of
+      defining the macros.  This will take some time too.
+
+   6. TBD when, poison the macros.  Unmigrated targets will break at
+      this point.
+
+   Note that we expect steps 1-3 to be done by the people that
+   understand what the MI does with each macro, and step 5 to be done
+   by the target maintainers for their respective targets.
+
+   Note that steps 1 and 2 don't have to be done together, but no
+   target can override the new hook until step 2 is complete for it.
+
+   Once the macros are poisoned, we will revert to the old migration
+   rules - migrate the macro, callers, and targets all at once.  This
+   comment can thus be removed at that point.  */
+
+#include "config.h"
+#include "system.h"
+#include "coretypes.h"
+#include "target.h"
+#include "function.h"
+#include "rtl.h"
+#include "tree.h"
+#include "tree-ssa-alias.h"
+#include "gimple-expr.h"
+#include "memmodel.h"
+#include "backend.h"
+#include "emit-rtl.h"
+#include "df.h"
+#include "tm_p.h"
+#include "stringpool.h"
+#include "tree-vrp.h"
+#include "tree-ssanames.h"
+#include "profile-count.h"
+#include "optabs.h"
+#include "regs.h"
+#include "recog.h"
+#include "diagnostic-core.h"
+#include "fold-const.h"
+#include "stor-layout.h"
+#include "varasm.h"
+#include "flags.h"
+#include "explow.h"
+#include "expmed.h"
+#include "calls.h"
+#include "expr.h"
+#include "output.h"
+#include "common/common-target.h"
+#include "reload.h"
+#include "intl.h"
+#include "opts.h"
+#include "gimplify.h"
+#include "predict.h"
+#include "real.h"
+#include "langhooks.h"
+#include "sbitmap.h"
+#include "function-abi.h"
+#include "attribs.h"
+#include "asan.h"
+#include "emit-rtl.h"
+
+bool
+default_legitimate_address_p (machine_mode mode ATTRIBUTE_UNUSED,
+			      rtx addr ATTRIBUTE_UNUSED,
+			      bool strict ATTRIBUTE_UNUSED)
+{
+#ifdef GO_IF_LEGITIMATE_ADDRESS
+  /* Defer to the old implementation using a goto.  */
+  if (strict)
+    return strict_memory_address_p (mode, addr);
+  else
+    return memory_address_p (mode, addr);
+#else
+  gcc_unreachable ();
+#endif
+}
+
+void
+default_external_libcall (rtx fun ATTRIBUTE_UNUSED)
+{
+#ifdef ASM_OUTPUT_EXTERNAL_LIBCALL
+  ASM_OUTPUT_EXTERNAL_LIBCALL (asm_out_file, fun);
+#endif
+}
+
+int
+default_unspec_may_trap_p (const_rtx x, unsigned flags)
+{
+  int i;
+
+  /* Any floating arithmetic may trap.  */
+  if ((SCALAR_FLOAT_MODE_P (GET_MODE (x)) && flag_trapping_math))
+    return 1;
+
+  for (i = 0; i < XVECLEN (x, 0); ++i)
+    {
+      if (may_trap_p_1 (XVECEXP (x, 0, i), flags))
+	return 1;
+    }
+
+  return 0;
+}
+
+machine_mode
+default_promote_function_mode (const_tree type ATTRIBUTE_UNUSED,
+			       machine_mode mode,
+			       int *punsignedp ATTRIBUTE_UNUSED,
+			       const_tree funtype ATTRIBUTE_UNUSED,
+			       int for_return ATTRIBUTE_UNUSED)
+{
+  if (type != NULL_TREE && for_return == 2)
+    return promote_mode (type, mode, punsignedp);
+  return mode;
+}
+
+machine_mode
+default_promote_function_mode_always_promote (const_tree type,
+					      machine_mode mode,
+					      int *punsignedp,
+					      const_tree funtype ATTRIBUTE_UNUSED,
+					      int for_return ATTRIBUTE_UNUSED)
+{
+  return promote_mode (type, mode, punsignedp);
+}
+
+machine_mode
+default_cc_modes_compatible (machine_mode m1, machine_mode m2)
+{
+  if (m1 == m2)
+    return m1;
+  return VOIDmode;
+}
+
+bool
+default_return_in_memory (const_tree type,
+			  const_tree fntype ATTRIBUTE_UNUSED)
+{
+  return (TYPE_MODE (type) == BLKmode);
+}
+
+rtx
+default_legitimize_address (rtx x, rtx orig_x ATTRIBUTE_UNUSED,
+			    machine_mode mode ATTRIBUTE_UNUSED)
+{
+  return x;
+}
+
+bool
+default_legitimize_address_displacement (rtx *, rtx *, poly_int64,
+					 machine_mode)
+{
+  return false;
+}
+
+bool
+default_const_not_ok_for_debug_p (rtx x)
+{
+  if (GET_CODE (x) == UNSPEC)
+    return true;
+  return false;
+}
+
+rtx
+default_expand_builtin_saveregs (void)
+{
+  error ("%<__builtin_saveregs%> not supported by this target");
+  return const0_rtx;
+}
+
+void
+default_setup_incoming_varargs (cumulative_args_t,
+				const function_arg_info &, int *, int)
+{
+}
+
+/* The default implementation of TARGET_BUILTIN_SETJMP_FRAME_VALUE.  */
+
+rtx
+default_builtin_setjmp_frame_value (void)
+{
+  return virtual_stack_vars_rtx;
+}
+
+/* Generic hook that takes a CUMULATIVE_ARGS pointer and returns false.  */
+
+bool
+hook_bool_CUMULATIVE_ARGS_false (cumulative_args_t ca ATTRIBUTE_UNUSED)
+{
+  return false;
+}
+
+bool
+default_pretend_outgoing_varargs_named (cumulative_args_t ca ATTRIBUTE_UNUSED)
+{
+  return (targetm.calls.setup_incoming_varargs
+	  != default_setup_incoming_varargs);
+}
+
+scalar_int_mode
+default_eh_return_filter_mode (void)
+{
+  return targetm.unwind_word_mode ();
+}
+
+scalar_int_mode
+default_libgcc_cmp_return_mode (void)
+{
+  return word_mode;
+}
+
+scalar_int_mode
+default_libgcc_shift_count_mode (void)
+{
+  return word_mode;
+}
+
+scalar_int_mode
+default_unwind_word_mode (void)
+{
+  return word_mode;
+}
+
+/* The default implementation of TARGET_SHIFT_TRUNCATION_MASK.  */
+
+unsigned HOST_WIDE_INT
+default_shift_truncation_mask (machine_mode mode)
+{
+  return SHIFT_COUNT_TRUNCATED ? GET_MODE_UNIT_BITSIZE (mode) - 1 : 0;
+}
+
+/* The default implementation of TARGET_MIN_DIVISIONS_FOR_RECIP_MUL.  */
+
+unsigned int
+default_min_divisions_for_recip_mul (machine_mode mode ATTRIBUTE_UNUSED)
+{
+  return have_insn_for (DIV, mode) ? 3 : 2;
+}
+
+/* The default implementation of TARGET_MODE_REP_EXTENDED.  */
+
+int
+default_mode_rep_extended (scalar_int_mode, scalar_int_mode)
+{
+  return UNKNOWN;
+}
+
+/* Generic hook that takes a CUMULATIVE_ARGS pointer and returns true.  */
+
+bool
+hook_bool_CUMULATIVE_ARGS_true (cumulative_args_t a ATTRIBUTE_UNUSED)
+{
+  return true;
+}
+
+/* Return machine mode for non-standard suffix
+   or VOIDmode if non-standard suffixes are unsupported.  */
+machine_mode
+default_mode_for_suffix (char suffix ATTRIBUTE_UNUSED)
+{
+  return VOIDmode;
+}
+
+/* The generic C++ ABI specifies this is a 64-bit value.  */
+tree
+default_cxx_guard_type (void)
+{
+  return long_long_integer_type_node;
+}
+
+/* Returns the size of the cookie to use when allocating an array
+   whose elements have the indicated TYPE.  Assumes that it is already
+   known that a cookie is needed.  */
+
+tree
+default_cxx_get_cookie_size (tree type)
+{
+  tree cookie_size;
+
+  /* We need to allocate an additional max (sizeof (size_t), alignof
+     (true_type)) bytes.  */
+  tree sizetype_size;
+  tree type_align;
+
+  sizetype_size = size_in_bytes (sizetype);
+  type_align = size_int (TYPE_ALIGN_UNIT (type));
+  if (tree_int_cst_lt (type_align, sizetype_size))
+    cookie_size = sizetype_size;
+  else
+    cookie_size = type_align;
+
+  return cookie_size;
+}
+
+/* Return true if a parameter must be passed by reference.  This version
+   of the TARGET_PASS_BY_REFERENCE hook uses just MUST_PASS_IN_STACK.  */
+
+bool
+hook_pass_by_reference_must_pass_in_stack (cumulative_args_t,
+					   const function_arg_info &arg)
+{
+  return targetm.calls.must_pass_in_stack (arg);
+}
+
+/* Return true if a parameter follows callee copies conventions.  This
+   version of the hook is true for all named arguments.  */
+
+bool
+hook_callee_copies_named (cumulative_args_t, const function_arg_info &arg)
+{
+  return arg.named;
+}
+
+/* Emit to STREAM the assembler syntax for insn operand X.  */
+
+void
+default_print_operand (FILE *stream ATTRIBUTE_UNUSED, rtx x ATTRIBUTE_UNUSED,
+		       int code ATTRIBUTE_UNUSED)
+{
+#ifdef PRINT_OPERAND
+  PRINT_OPERAND (stream, x, code);
+#else
+  gcc_unreachable ();
+#endif
+}
+
+/* Emit to STREAM the assembler syntax for an insn operand whose memory
+   address is X.  */
+
+void
+default_print_operand_address (FILE *stream ATTRIBUTE_UNUSED,
+			       machine_mode /*mode*/,
+			       rtx x ATTRIBUTE_UNUSED)
+{
+#ifdef PRINT_OPERAND_ADDRESS
+  PRINT_OPERAND_ADDRESS (stream, x);
+#else
+  gcc_unreachable ();
+#endif
+}
+
+/* Return true if CODE is a valid punctuation character for the
+   `print_operand' hook.  */
+
+bool
+default_print_operand_punct_valid_p (unsigned char code ATTRIBUTE_UNUSED)
+{
+#ifdef PRINT_OPERAND_PUNCT_VALID_P
+  return PRINT_OPERAND_PUNCT_VALID_P (code);
+#else
+  return false;
+#endif
+}
+
+/* The default implementation of TARGET_MANGLE_ASSEMBLER_NAME.  */
+tree
+default_mangle_assembler_name (const char *name ATTRIBUTE_UNUSED)
+{
+  const char *skipped = name + (*name == '*' ? 1 : 0);
+  const char *stripped = targetm.strip_name_encoding (skipped);
+  if (*name != '*' && user_label_prefix[0])
+    stripped = ACONCAT ((user_label_prefix, stripped, NULL));
+  return get_identifier (stripped);
+}
+
+/* The default implementation of TARGET_TRANSLATE_MODE_ATTRIBUTE.  */
+
+machine_mode
+default_translate_mode_attribute (machine_mode mode)
+{
+  return mode;
+}
+
+/* True if MODE is valid for the target.  By "valid", we mean able to
+   be manipulated in non-trivial ways.  In particular, this means all
+   the arithmetic is supported.
+
+   By default we guess this means that any C type is supported.  If
+   we can't map the mode back to a type that would be available in C,
+   then reject it.  Special case, here, is the double-word arithmetic
+   supported by optabs.c.  */
+
+bool
+default_scalar_mode_supported_p (scalar_mode mode)
+{
+  int precision = GET_MODE_PRECISION (mode);
+
+  switch (GET_MODE_CLASS (mode))
+    {
+    case MODE_PARTIAL_INT:
+    case MODE_INT:
+      if (precision == CHAR_TYPE_SIZE)
+	return true;
+      if (precision == SHORT_TYPE_SIZE)
+	return true;
+      if (precision == INT_TYPE_SIZE)
+	return true;
+      if (precision == LONG_TYPE_SIZE)
+	return true;
+      if (precision == LONG_LONG_TYPE_SIZE)
+	return true;
+      if (precision == 2 * BITS_PER_WORD)
+	return true;
+      return false;
+
+    case MODE_FLOAT:
+      if (precision == FLOAT_TYPE_SIZE)
+	return true;
+      if (precision == DOUBLE_TYPE_SIZE)
+	return true;
+      if (precision == LONG_DOUBLE_TYPE_SIZE)
+	return true;
+      return false;
+
+    case MODE_DECIMAL_FLOAT:
+    case MODE_FRACT:
+    case MODE_UFRACT:
+    case MODE_ACCUM:
+    case MODE_UACCUM:
+      return false;
+
+    default:
+      gcc_unreachable ();
+    }
+}
+
+/* Return true if libgcc supports floating-point mode MODE (known to
+   be supported as a scalar mode).  */
+
+bool
+default_libgcc_floating_mode_supported_p (scalar_float_mode mode)
+{
+  switch (mode)
+    {
+#ifdef HAVE_SFmode
+    case E_SFmode:
+#endif
+#ifdef HAVE_DFmode
+    case E_DFmode:
+#endif
+#ifdef HAVE_XFmode
+    case E_XFmode:
+#endif
+#ifdef HAVE_TFmode
+    case E_TFmode:
+#endif
+      return true;
+
+    default:
+      return false;
+    }
+}
+
+/* Return the machine mode to use for the type _FloatN, if EXTENDED is
+   false, or _FloatNx, if EXTENDED is true, or VOIDmode if not
+   supported.  */
+opt_scalar_float_mode
+default_floatn_mode (int n, bool extended)
+{
+  if (extended)
+    {
+      opt_scalar_float_mode cand1, cand2;
+      scalar_float_mode mode;
+      switch (n)
+	{
+	case 32:
+#ifdef HAVE_DFmode
+	  cand1 = DFmode;
+#endif
+	  break;
+
+	case 64:
+#ifdef HAVE_XFmode
+	  cand1 = XFmode;
+#endif
+#ifdef HAVE_TFmode
+	  cand2 = TFmode;
+#endif
+	  break;
+
+	case 128:
+	  break;
+
+	default:
+	  /* Those are the only valid _FloatNx types.  */
+	  gcc_unreachable ();
+	}
+      if (cand1.exists (&mode)
+	  && REAL_MODE_FORMAT (mode)->ieee_bits > n
+	  && targetm.scalar_mode_supported_p (mode)
+	  && targetm.libgcc_floating_mode_supported_p (mode))
+	return cand1;
+      if (cand2.exists (&mode)
+	  && REAL_MODE_FORMAT (mode)->ieee_bits > n
+	  && targetm.scalar_mode_supported_p (mode)
+	  && targetm.libgcc_floating_mode_supported_p (mode))
+	return cand2;
+    }
+  else
+    {
+      opt_scalar_float_mode cand;
+      scalar_float_mode mode;
+      switch (n)
+	{
+	case 16:
+	  /* Always enable _Float16 if we have basic support for the mode.
+	     Targets can control the range and precision of operations on
+	     the _Float16 type using TARGET_C_EXCESS_PRECISION.  */
+#ifdef HAVE_HFmode
+	  cand = HFmode;
+#endif
+	  break;
+
+	case 32:
+#ifdef HAVE_SFmode
+	  cand = SFmode;
+#endif
+	  break;
+
+	case 64:
+#ifdef HAVE_DFmode
+	  cand = DFmode;
+#endif
+	  break;
+
+	case 128:
+#ifdef HAVE_TFmode
+	  cand = TFmode;
+#endif
+	  break;
+
+	default:
+	  break;
+	}
+      if (cand.exists (&mode)
+	  && REAL_MODE_FORMAT (mode)->ieee_bits == n
+	  && targetm.scalar_mode_supported_p (mode)
+	  && targetm.libgcc_floating_mode_supported_p (mode))
+	return cand;
+    }
+  return opt_scalar_float_mode ();
+}
+
+/* Define this to return true if the _Floatn and _Floatnx built-in functions
+   should implicitly enable the built-in function without the __builtin_ prefix
+   in addition to the normal built-in function with the __builtin_ prefix.  The
+   default is to only enable built-in functions without the __builtin_ prefix
+   for the GNU C langauge.  The argument FUNC is the enum builtin_in_function
+   id of the function to be enabled.  */
+
+bool
+default_floatn_builtin_p (int func ATTRIBUTE_UNUSED)
+{
+  static bool first_time_p = true;
+  static bool c_or_objective_c;
+
+  if (first_time_p)
+    {
+      first_time_p = false;
+      c_or_objective_c = lang_GNU_C () || lang_GNU_OBJC ();
+    }
+
+  return c_or_objective_c;
+}
+
+/* Make some target macros useable by target-independent code.  */
+bool
+targhook_words_big_endian (void)
+{
+  return !!WORDS_BIG_ENDIAN;
+}
+
+bool
+targhook_float_words_big_endian (void)
+{
+  return !!FLOAT_WORDS_BIG_ENDIAN;
+}
+
+/* True if the target supports floating-point exceptions and rounding
+   modes.  */
+
+bool
+default_float_exceptions_rounding_supported_p (void)
+{
+#ifdef HAVE_adddf3
+  return HAVE_adddf3;
+#else
+  return false;
+#endif
+}
+
+/* True if the target supports decimal floating point.  */
+
+bool
+default_decimal_float_supported_p (void)
+{
+  return ENABLE_DECIMAL_FLOAT;
+}
+
+/* True if the target supports fixed-point arithmetic.  */
+
+bool
+default_fixed_point_supported_p (void)
+{
+  return ENABLE_FIXED_POINT;
+}
+
+/* True if the target supports GNU indirect functions.  */
+
+bool
+default_has_ifunc_p (void)
+{
+  return HAVE_GNU_INDIRECT_FUNCTION;
+}
+
+/* Return true if we predict the loop LOOP will be transformed to a
+   low-overhead loop, otherwise return false.
+
+   By default, false is returned, as this hook's applicability should be
+   verified for each target.  Target maintainers should re-define the hook
+   if the target can take advantage of it.  */
+
+bool
+default_predict_doloop_p (class loop *loop ATTRIBUTE_UNUSED)
+{
+  return false;
+}
+
+/* NULL if INSN insn is valid within a low-overhead loop, otherwise returns
+   an error message.
+
+   This function checks whether a given INSN is valid within a low-overhead
+   loop.  If INSN is invalid it returns the reason for that, otherwise it
+   returns NULL. A called function may clobber any special registers required
+   for low-overhead looping. Additionally, some targets (eg, PPC) use the count
+   register for branch on table instructions. We reject the doloop pattern in
+   these cases.  */
+
+const char *
+default_invalid_within_doloop (const rtx_insn *insn)
+{
+  if (CALL_P (insn))
+    return "Function call in loop.";
+
+  if (tablejump_p (insn, NULL, NULL) || computed_jump_p (insn))
+    return "Computed branch in the loop.";
+
+  return NULL;
+}
+
+/* Mapping of builtin functions to vectorized variants.  */
+
+tree
+default_builtin_vectorized_function (unsigned int, tree, tree)
+{
+  return NULL_TREE;
+}
+
+/* Mapping of target builtin functions to vectorized variants.  */
+
+tree
+default_builtin_md_vectorized_function (tree, tree, tree)
+{
+  return NULL_TREE;
+}
+
+/* Default vectorizer cost model values.  */
+
+int
+default_builtin_vectorization_cost (enum vect_cost_for_stmt type_of_cost,
+                                    tree vectype,
+                                    int misalign ATTRIBUTE_UNUSED)
+{
+  switch (type_of_cost)
+    {
+      case scalar_stmt:
+      case scalar_load:
+      case scalar_store:
+      case vector_stmt:
+      case vector_load:
+      case vector_store:
+      case vec_to_scalar:
+      case scalar_to_vec:
+      case cond_branch_not_taken:
+      case vec_perm:
+      case vec_promote_demote:
+        return 1;
+
+      case unaligned_load:
+      case unaligned_store:
+        return 2;
+
+      case cond_branch_taken:
+        return 3;
+
+      case vec_construct:
+	return estimated_poly_value (TYPE_VECTOR_SUBPARTS (vectype)) - 1;
+
+      default:
+        gcc_unreachable ();
+    }
+}
+
+/* Reciprocal.  */
+
+tree
+default_builtin_reciprocal (tree)
+{
+  return NULL_TREE;
+}
+
+bool
+hook_bool_CUMULATIVE_ARGS_arg_info_false (cumulative_args_t,
+					  const function_arg_info &)
+{
+  return false;
+}
+
+bool
+hook_bool_CUMULATIVE_ARGS_arg_info_true (cumulative_args_t,
+					 const function_arg_info &)
+{
+  return true;
+}
+
+int
+hook_int_CUMULATIVE_ARGS_arg_info_0 (cumulative_args_t,
+				     const function_arg_info &)
+{
+  return 0;
+}
+
+void
+hook_void_CUMULATIVE_ARGS_tree (cumulative_args_t ca ATTRIBUTE_UNUSED,
+				tree ATTRIBUTE_UNUSED)
+{
+}
+
+void
+default_function_arg_advance (cumulative_args_t, const function_arg_info &)
+{
+  gcc_unreachable ();
+}
+
+/* Default implementation of TARGET_FUNCTION_ARG_OFFSET.  */
+
+HOST_WIDE_INT
+default_function_arg_offset (machine_mode, const_tree)
+{
+  return 0;
+}
+
+/* Default implementation of TARGET_FUNCTION_ARG_PADDING: usually pad
+   upward, but pad short args downward on big-endian machines.  */
+
+pad_direction
+default_function_arg_padding (machine_mode mode, const_tree type)
+{
+  if (!BYTES_BIG_ENDIAN)
+    return PAD_UPWARD;
+
+  unsigned HOST_WIDE_INT size;
+  if (mode == BLKmode)
+    {
+      if (!type || TREE_CODE (TYPE_SIZE (type)) != INTEGER_CST)
+	return PAD_UPWARD;
+      size = int_size_in_bytes (type);
+    }
+  else
+    /* Targets with variable-sized modes must override this hook
+       and handle variable-sized modes explicitly.  */
+    size = GET_MODE_SIZE (mode).to_constant ();
+
+  if (size < (PARM_BOUNDARY / BITS_PER_UNIT))
+    return PAD_DOWNWARD;
+
+  return PAD_UPWARD;
+}
+
+rtx
+default_function_arg (cumulative_args_t, const function_arg_info &)
+{
+  gcc_unreachable ();
+}
+
+rtx
+default_function_incoming_arg (cumulative_args_t, const function_arg_info &)
+{
+  gcc_unreachable ();
+}
+
+unsigned int
+default_function_arg_boundary (machine_mode mode ATTRIBUTE_UNUSED,
+			       const_tree type ATTRIBUTE_UNUSED)
+{
+  return PARM_BOUNDARY;
+}
+
+unsigned int
+default_function_arg_round_boundary (machine_mode mode ATTRIBUTE_UNUSED,
+				     const_tree type ATTRIBUTE_UNUSED)
+{
+  return PARM_BOUNDARY;
+}
+
+void
+hook_void_bitmap (bitmap regs ATTRIBUTE_UNUSED)
+{
+}
+
+const char *
+hook_invalid_arg_for_unprototyped_fn (
+	const_tree typelist ATTRIBUTE_UNUSED,
+	const_tree funcdecl ATTRIBUTE_UNUSED,
+	const_tree val ATTRIBUTE_UNUSED)
+{
+  return NULL;
+}
+
+/* Initialize the stack protection decls.  */
+
+/* Stack protection related decls living in libgcc.  */
+static GTY(()) tree stack_chk_guard_decl;
+
+tree
+default_stack_protect_guard (void)
+{
+  tree t = stack_chk_guard_decl;
+
+  if (t == NULL)
+    {
+      rtx x;
+
+      t = build_decl (UNKNOWN_LOCATION,
+		      VAR_DECL, get_identifier ("__stack_chk_guard"),
+		      ptr_type_node);
+      TREE_STATIC (t) = 1;
+      TREE_PUBLIC (t) = 1;
+      DECL_EXTERNAL (t) = 1;
+      TREE_USED (t) = 1;
+      TREE_THIS_VOLATILE (t) = 1;
+      DECL_ARTIFICIAL (t) = 1;
+      DECL_IGNORED_P (t) = 1;
+
+      /* Do not share RTL as the declaration is visible outside of
+	 current function.  */
+      x = DECL_RTL (t);
+      RTX_FLAG (x, used) = 1;
+
+      stack_chk_guard_decl = t;
+    }
+
+  return t;
+}
+
+static GTY(()) tree stack_chk_fail_decl;
+
+tree
+default_external_stack_protect_fail (void)
+{
+  tree t = stack_chk_fail_decl;
+
+  if (t == NULL_TREE)
+    {
+      t = build_function_type_list (void_type_node, NULL_TREE);
+      t = build_decl (UNKNOWN_LOCATION,
+		      FUNCTION_DECL, get_identifier ("__stack_chk_fail"), t);
+      TREE_STATIC (t) = 1;
+      TREE_PUBLIC (t) = 1;
+      DECL_EXTERNAL (t) = 1;
+      TREE_USED (t) = 1;
+      TREE_THIS_VOLATILE (t) = 1;
+      TREE_NOTHROW (t) = 1;
+      DECL_ARTIFICIAL (t) = 1;
+      DECL_IGNORED_P (t) = 1;
+      DECL_VISIBILITY (t) = VISIBILITY_DEFAULT;
+      DECL_VISIBILITY_SPECIFIED (t) = 1;
+
+      stack_chk_fail_decl = t;
+    }
+
+  return build_call_expr (t, 0);
+}
+
+tree
+default_hidden_stack_protect_fail (void)
+{
+#ifndef HAVE_GAS_HIDDEN
+  return default_external_stack_protect_fail ();
+#else
+  tree t = stack_chk_fail_decl;
+
+  if (!flag_pic)
+    return default_external_stack_protect_fail ();
+
+  if (t == NULL_TREE)
+    {
+      t = build_function_type_list (void_type_node, NULL_TREE);
+      t = build_decl (UNKNOWN_LOCATION, FUNCTION_DECL,
+		      get_identifier ("__stack_chk_fail_local"), t);
+      TREE_STATIC (t) = 1;
+      TREE_PUBLIC (t) = 1;
+      DECL_EXTERNAL (t) = 1;
+      TREE_USED (t) = 1;
+      TREE_THIS_VOLATILE (t) = 1;
+      TREE_NOTHROW (t) = 1;
+      DECL_ARTIFICIAL (t) = 1;
+      DECL_IGNORED_P (t) = 1;
+      DECL_VISIBILITY_SPECIFIED (t) = 1;
+      DECL_VISIBILITY (t) = VISIBILITY_HIDDEN;
+
+      stack_chk_fail_decl = t;
+    }
+
+  return build_call_expr (t, 0);
+#endif
+}
+
+bool
+hook_bool_const_rtx_commutative_p (const_rtx x,
+				   int outer_code ATTRIBUTE_UNUSED)
+{
+  return COMMUTATIVE_P (x);
+}
+
+rtx
+default_function_value (const_tree ret_type ATTRIBUTE_UNUSED,
+			const_tree fn_decl_or_type,
+			bool outgoing ATTRIBUTE_UNUSED)
+{
+  /* The old interface doesn't handle receiving the function type.  */
+  if (fn_decl_or_type
+      && !DECL_P (fn_decl_or_type))
+    fn_decl_or_type = NULL;
+
+#ifdef FUNCTION_VALUE
+  return FUNCTION_VALUE (ret_type, fn_decl_or_type);
+#else
+  gcc_unreachable ();
+#endif
+}
+
+rtx
+default_libcall_value (machine_mode mode ATTRIBUTE_UNUSED,
+		       const_rtx fun ATTRIBUTE_UNUSED)
+{
+#ifdef LIBCALL_VALUE
+  return LIBCALL_VALUE (MACRO_MODE (mode));
+#else
+  gcc_unreachable ();
+#endif
+}
+
+/* The default hook for TARGET_FUNCTION_VALUE_REGNO_P.  */
+
+bool
+default_function_value_regno_p (const unsigned int regno ATTRIBUTE_UNUSED)
+{
+#ifdef FUNCTION_VALUE_REGNO_P
+  return FUNCTION_VALUE_REGNO_P (regno);
+#else
+  gcc_unreachable ();
+#endif
+}
+
+/* The default hook for TARGET_ZERO_CALL_USED_REGS.  */
+
+HARD_REG_SET
+default_zero_call_used_regs (HARD_REG_SET need_zeroed_hardregs)
+{
+  gcc_assert (!hard_reg_set_empty_p (need_zeroed_hardregs));
+
+  for (unsigned int regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)
+    if (TEST_HARD_REG_BIT (need_zeroed_hardregs, regno))
+      {
+	rtx_insn *last_insn = get_last_insn ();
+	machine_mode mode = GET_MODE (regno_reg_rtx[regno]);
+	rtx zero = CONST0_RTX (mode);
+	rtx_insn *insn = emit_move_insn (regno_reg_rtx[regno], zero);
+	if (!valid_insn_p (insn))
+	  {
+	    static bool issued_error;
+	    if (!issued_error)
+	      {
+		issued_error = true;
+		sorry ("%qs not supported on this target",
+			"-fzero-call-used-regs");
+	      }
+	    delete_insns_since (last_insn);
+	  }
+      }
+  return need_zeroed_hardregs;
+}
+
+rtx
+default_internal_arg_pointer (void)
+{
+  /* If the reg that the virtual arg pointer will be translated into is
+     not a fixed reg or is the stack pointer, make a copy of the virtual
+     arg pointer, and address parms via the copy.  The frame pointer is
+     considered fixed even though it is not marked as such.  */
+  if ((ARG_POINTER_REGNUM == STACK_POINTER_REGNUM
+       || ! (fixed_regs[ARG_POINTER_REGNUM]
+	     || ARG_POINTER_REGNUM == FRAME_POINTER_REGNUM)))
+    return copy_to_reg (virtual_incoming_args_rtx);
+  else
+    return virtual_incoming_args_rtx;
+}
+
+rtx
+default_static_chain (const_tree ARG_UNUSED (fndecl_or_type), bool incoming_p)
+{
+  if (incoming_p)
+    {
+#ifdef STATIC_CHAIN_INCOMING_REGNUM
+      return gen_rtx_REG (Pmode, STATIC_CHAIN_INCOMING_REGNUM);
+#endif
+    }
+
+#ifdef STATIC_CHAIN_REGNUM
+  return gen_rtx_REG (Pmode, STATIC_CHAIN_REGNUM);
+#endif
+
+  {
+    static bool issued_error;
+    if (!issued_error)
+      {
+	issued_error = true;
+	sorry ("nested functions not supported on this target");
+      }
+
+    /* It really doesn't matter what we return here, so long at it
+       doesn't cause the rest of the compiler to crash.  */
+    return gen_rtx_MEM (Pmode, stack_pointer_rtx);
+  }
+}
+
+void
+default_trampoline_init (rtx ARG_UNUSED (m_tramp), tree ARG_UNUSED (t_func),
+			 rtx ARG_UNUSED (r_chain))
+{
+  sorry ("nested function trampolines not supported on this target");
+}
+
+poly_int64
+default_return_pops_args (tree, tree, poly_int64)
+{
+  return 0;
+}
+
+reg_class_t
+default_ira_change_pseudo_allocno_class (int regno ATTRIBUTE_UNUSED,
+					 reg_class_t cl,
+					 reg_class_t best_cl ATTRIBUTE_UNUSED)
+{
+  return cl;
+}
+
+extern bool
+default_lra_p (void)
+{
+  return true;
+}
+
+int
+default_register_priority (int hard_regno ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+extern bool
+default_register_usage_leveling_p (void)
+{
+  return false;
+}
+
+extern bool
+default_different_addr_displacement_p (void)
+{
+  return false;
+}
+
+reg_class_t
+default_secondary_reload (bool in_p ATTRIBUTE_UNUSED, rtx x ATTRIBUTE_UNUSED,
+			  reg_class_t reload_class_i ATTRIBUTE_UNUSED,
+			  machine_mode reload_mode ATTRIBUTE_UNUSED,
+			  secondary_reload_info *sri)
+{
+  enum reg_class rclass = NO_REGS;
+  enum reg_class reload_class = (enum reg_class) reload_class_i;
+
+  if (sri->prev_sri && sri->prev_sri->t_icode != CODE_FOR_nothing)
+    {
+      sri->icode = sri->prev_sri->t_icode;
+      return NO_REGS;
+    }
+#ifdef SECONDARY_INPUT_RELOAD_CLASS
+  if (in_p)
+    rclass = SECONDARY_INPUT_RELOAD_CLASS (reload_class,
+					   MACRO_MODE (reload_mode), x);
+#endif
+#ifdef SECONDARY_OUTPUT_RELOAD_CLASS
+  if (! in_p)
+    rclass = SECONDARY_OUTPUT_RELOAD_CLASS (reload_class,
+					    MACRO_MODE (reload_mode), x);
+#endif
+  if (rclass != NO_REGS)
+    {
+      enum insn_code icode
+	= direct_optab_handler (in_p ? reload_in_optab : reload_out_optab,
+				reload_mode);
+
+      if (icode != CODE_FOR_nothing
+	  && !insn_operand_matches (icode, in_p, x))
+	icode = CODE_FOR_nothing;
+      else if (icode != CODE_FOR_nothing)
+	{
+	  const char *insn_constraint, *scratch_constraint;
+	  enum reg_class insn_class, scratch_class;
+
+	  gcc_assert (insn_data[(int) icode].n_operands == 3);
+	  insn_constraint = insn_data[(int) icode].operand[!in_p].constraint;
+	  if (!*insn_constraint)
+	    insn_class = ALL_REGS;
+	  else
+	    {
+	      if (in_p)
+		{
+		  gcc_assert (*insn_constraint == '=');
+		  insn_constraint++;
+		}
+	      insn_class = (reg_class_for_constraint
+			    (lookup_constraint (insn_constraint)));
+	      gcc_assert (insn_class != NO_REGS);
+	    }
+
+	  scratch_constraint = insn_data[(int) icode].operand[2].constraint;
+	  /* The scratch register's constraint must start with "=&",
+	     except for an input reload, where only "=" is necessary,
+	     and where it might be beneficial to re-use registers from
+	     the input.  */
+	  gcc_assert (scratch_constraint[0] == '='
+		      && (in_p || scratch_constraint[1] == '&'));
+	  scratch_constraint++;
+	  if (*scratch_constraint == '&')
+	    scratch_constraint++;
+	  scratch_class = (reg_class_for_constraint
+			   (lookup_constraint (scratch_constraint)));
+
+	  if (reg_class_subset_p (reload_class, insn_class))
+	    {
+	      gcc_assert (scratch_class == rclass);
+	      rclass = NO_REGS;
+	    }
+	  else
+	    rclass = insn_class;
+
+        }
+      if (rclass == NO_REGS)
+	sri->icode = icode;
+      else
+	sri->t_icode = icode;
+    }
+  return rclass;
+}
+
+/* The default implementation of TARGET_SECONDARY_MEMORY_NEEDED_MODE.  */
+
+machine_mode
+default_secondary_memory_needed_mode (machine_mode mode)
+{
+  if (!targetm.lra_p ()
+      && known_lt (GET_MODE_BITSIZE (mode), BITS_PER_WORD)
+      && INTEGRAL_MODE_P (mode))
+    return mode_for_size (BITS_PER_WORD, GET_MODE_CLASS (mode), 0).require ();
+  return mode;
+}
+
+/* By default, if flag_pic is true, then neither local nor global relocs
+   should be placed in readonly memory.  */
+
+int
+default_reloc_rw_mask (void)
+{
+  return flag_pic ? 3 : 0;
+}
+
+/* By default, address diff vectors are generated
+for jump tables when flag_pic is true.  */
+
+bool
+default_generate_pic_addr_diff_vec (void)
+{
+  return flag_pic;
+}
+
+/* By default, do no modification. */
+tree default_mangle_decl_assembler_name (tree decl ATTRIBUTE_UNUSED,
+					 tree id)
+{
+   return id;
+}
+
+/* The default implementation of TARGET_STATIC_RTX_ALIGNMENT.  */
+
+HOST_WIDE_INT
+default_static_rtx_alignment (machine_mode mode)
+{
+  return GET_MODE_ALIGNMENT (mode);
+}
+
+/* The default implementation of TARGET_CONSTANT_ALIGNMENT.  */
+
+HOST_WIDE_INT
+default_constant_alignment (const_tree, HOST_WIDE_INT align)
+{
+  return align;
+}
+
+/* An implementation of TARGET_CONSTANT_ALIGNMENT that aligns strings
+   to at least BITS_PER_WORD but otherwise makes no changes.  */
+
+HOST_WIDE_INT
+constant_alignment_word_strings (const_tree exp, HOST_WIDE_INT align)
+{
+  if (TREE_CODE (exp) == STRING_CST)
+    return MAX (align, BITS_PER_WORD);
+  return align;
+}
+
+/* Default to natural alignment for vector types, bounded by
+   MAX_OFILE_ALIGNMENT.  */
+
+HOST_WIDE_INT
+default_vector_alignment (const_tree type)
+{
+  unsigned HOST_WIDE_INT align = MAX_OFILE_ALIGNMENT;
+  tree size = TYPE_SIZE (type);
+  if (tree_fits_uhwi_p (size))
+    align = tree_to_uhwi (size);
+  if (align >= MAX_OFILE_ALIGNMENT)
+    return MAX_OFILE_ALIGNMENT;
+  return MAX (align, GET_MODE_ALIGNMENT (TYPE_MODE (type)));
+}
+
+/* The default implementation of
+   TARGET_VECTORIZE_PREFERRED_VECTOR_ALIGNMENT.  */
+
+poly_uint64
+default_preferred_vector_alignment (const_tree type)
+{
+  return TYPE_ALIGN (type);
+}
+
+/* By default assume vectors of element TYPE require a multiple of the natural
+   alignment of TYPE.  TYPE is naturally aligned if IS_PACKED is false.  */
+bool
+default_builtin_vector_alignment_reachable (const_tree /*type*/, bool is_packed)
+{
+  return ! is_packed;
+}
+
+/* By default, assume that a target supports any factor of misalignment
+   memory access if it supports movmisalign patten.
+   is_packed is true if the memory access is defined in a packed struct.  */
+bool
+default_builtin_support_vector_misalignment (machine_mode mode,
+					     const_tree type
+					     ATTRIBUTE_UNUSED,
+					     int misalignment
+					     ATTRIBUTE_UNUSED,
+					     bool is_packed
+					     ATTRIBUTE_UNUSED)
+{
+  if (optab_handler (movmisalign_optab, mode) != CODE_FOR_nothing)
+    return true;
+  return false;
+}
+
+/* By default, only attempt to parallelize bitwise operations, and
+   possibly adds/subtracts using bit-twiddling.  */
+
+machine_mode
+default_preferred_simd_mode (scalar_mode)
+{
+  return word_mode;
+}
+
+/* By default do not split reductions further.  */
+
+machine_mode
+default_split_reduction (machine_mode mode)
+{
+  return mode;
+}
+
+/* By default only the preferred vector mode is tried.  */
+
+unsigned int
+default_autovectorize_vector_modes (vector_modes *, bool)
+{
+  return 0;
+}
+
+/* The default implementation of TARGET_VECTORIZE_RELATED_MODE.  */
+
+opt_machine_mode
+default_vectorize_related_mode (machine_mode vector_mode,
+				scalar_mode element_mode,
+				poly_uint64 nunits)
+{
+  machine_mode result_mode;
+  if ((maybe_ne (nunits, 0U)
+       || multiple_p (GET_MODE_SIZE (vector_mode),
+		      GET_MODE_SIZE (element_mode), &nunits))
+      && mode_for_vector (element_mode, nunits).exists (&result_mode)
+      && VECTOR_MODE_P (result_mode)
+      && targetm.vector_mode_supported_p (result_mode))
+    return result_mode;
+
+  return opt_machine_mode ();
+}
+
+/* By default a vector of integers is used as a mask.  */
+
+opt_machine_mode
+default_get_mask_mode (machine_mode mode)
+{
+  return related_int_vector_mode (mode);
+}
+
+/* By default consider masked stores to be expensive.  */
+
+bool
+default_empty_mask_is_expensive (unsigned ifn)
+{
+  return ifn == IFN_MASK_STORE;
+}
+
+/* By default, the cost model accumulates three separate costs (prologue,
+   loop body, and epilogue) for a vectorized loop or block.  So allocate an
+   array of three unsigned ints, set it to zero, and return its address.  */
+
+void *
+default_init_cost (class loop *loop_info ATTRIBUTE_UNUSED)
+{
+  unsigned *cost = XNEWVEC (unsigned, 3);
+  cost[vect_prologue] = cost[vect_body] = cost[vect_epilogue] = 0;
+  return cost;
+}
+
+/* By default, the cost model looks up the cost of the given statement
+   kind and mode, multiplies it by the occurrence count, accumulates
+   it into the cost specified by WHERE, and returns the cost added.  */
+
+unsigned
+default_add_stmt_cost (class vec_info *vinfo, void *data, int count,
+		       enum vect_cost_for_stmt kind,
+		       class _stmt_vec_info *stmt_info, tree vectype,
+		       int misalign,
+		       enum vect_cost_model_location where)
+{
+  unsigned *cost = (unsigned *) data;
+  unsigned retval = 0;
+  int stmt_cost = targetm.vectorize.builtin_vectorization_cost (kind, vectype,
+								misalign);
+   /* Statements in an inner loop relative to the loop being
+      vectorized are weighted more heavily.  The value here is
+      arbitrary and could potentially be improved with analysis.  */
+  if (where == vect_body && stmt_info
+      && stmt_in_inner_loop_p (vinfo, stmt_info))
+    count *= 50;  /* FIXME.  */
+
+  retval = (unsigned) (count * stmt_cost);
+  cost[where] += retval;
+
+  return retval;
+}
+
+/* By default, the cost model just returns the accumulated costs.  */
+
+void
+default_finish_cost (void *data, unsigned *prologue_cost,
+		     unsigned *body_cost, unsigned *epilogue_cost)
+{
+  unsigned *cost = (unsigned *) data;
+  *prologue_cost = cost[vect_prologue];
+  *body_cost     = cost[vect_body];
+  *epilogue_cost = cost[vect_epilogue];
+}
+
+/* Free the cost data.  */
+
+void
+default_destroy_cost_data (void *data)
+{
+  free (data);
+}
+
+/* Determine whether or not a pointer mode is valid. Assume defaults
+   of ptr_mode or Pmode - can be overridden.  */
+bool
+default_valid_pointer_mode (scalar_int_mode mode)
+{
+  return (mode == ptr_mode || mode == Pmode);
+}
+
+/* Determine whether the memory reference specified by REF may alias
+   the C libraries errno location.  */
+bool
+default_ref_may_alias_errno (ao_ref *ref)
+{
+  tree base = ao_ref_base (ref);
+  /* The default implementation assumes the errno location is
+     a declaration of type int or is always accessed via a
+     pointer to int.  We assume that accesses to errno are
+     not deliberately obfuscated (even in conforming ways).  */
+  if (TYPE_UNSIGNED (TREE_TYPE (base))
+      || TYPE_MODE (TREE_TYPE (base)) != TYPE_MODE (integer_type_node))
+    return false;
+  /* The default implementation assumes an errno location declaration
+     is never defined in the current compilation unit and may not be
+     aliased by a local variable.  */
+  if (DECL_P (base)
+      && DECL_EXTERNAL (base)
+      && !TREE_STATIC (base))
+    return true;
+  else if (TREE_CODE (base) == MEM_REF
+	   && TREE_CODE (TREE_OPERAND (base, 0)) == SSA_NAME)
+    {
+      struct ptr_info_def *pi = SSA_NAME_PTR_INFO (TREE_OPERAND (base, 0));
+      return !pi || pi->pt.anything || pi->pt.nonlocal;
+    }
+  return false;
+}
+
+/* Return the mode for a pointer to a given ADDRSPACE,
+   defaulting to ptr_mode for all address spaces.  */
+
+scalar_int_mode
+default_addr_space_pointer_mode (addr_space_t addrspace ATTRIBUTE_UNUSED)
+{
+  return ptr_mode;
+}
+
+/* Return the mode for an address in a given ADDRSPACE,
+   defaulting to Pmode for all address spaces.  */
+
+scalar_int_mode
+default_addr_space_address_mode (addr_space_t addrspace ATTRIBUTE_UNUSED)
+{
+  return Pmode;
+}
+
+/* Named address space version of valid_pointer_mode.
+   To match the above, the same modes apply to all address spaces.  */
+
+bool
+default_addr_space_valid_pointer_mode (scalar_int_mode mode,
+				       addr_space_t as ATTRIBUTE_UNUSED)
+{
+  return targetm.valid_pointer_mode (mode);
+}
+
+/* Some places still assume that all pointer or address modes are the
+   standard Pmode and ptr_mode.  These optimizations become invalid if
+   the target actually supports multiple different modes.  For now,
+   we disable such optimizations on such targets, using this function.  */
+
+bool
+target_default_pointer_address_modes_p (void)
+{
+  if (targetm.addr_space.address_mode != default_addr_space_address_mode)
+    return false;
+  if (targetm.addr_space.pointer_mode != default_addr_space_pointer_mode)
+    return false;
+
+  return true;
+}
+
+/* Named address space version of legitimate_address_p.
+   By default, all address spaces have the same form.  */
+
+bool
+default_addr_space_legitimate_address_p (machine_mode mode, rtx mem,
+					 bool strict,
+					 addr_space_t as ATTRIBUTE_UNUSED)
+{
+  return targetm.legitimate_address_p (mode, mem, strict);
+}
+
+/* Named address space version of LEGITIMIZE_ADDRESS.
+   By default, all address spaces have the same form.  */
+
+rtx
+default_addr_space_legitimize_address (rtx x, rtx oldx, machine_mode mode,
+				       addr_space_t as ATTRIBUTE_UNUSED)
+{
+  return targetm.legitimize_address (x, oldx, mode);
+}
+
+/* The default hook for determining if one named address space is a subset of
+   another and to return which address space to use as the common address
+   space.  */
+
+bool
+default_addr_space_subset_p (addr_space_t subset, addr_space_t superset)
+{
+  return (subset == superset);
+}
+
+/* The default hook for determining if 0 within a named address
+   space is a valid address.  */
+
+bool
+default_addr_space_zero_address_valid (addr_space_t as ATTRIBUTE_UNUSED)
+{
+  return false;
+}
+
+/* The default hook for debugging the address space is to return the
+   address space number to indicate DW_AT_address_class.  */
+int
+default_addr_space_debug (addr_space_t as)
+{
+  return as;
+}
+
+/* The default hook implementation for TARGET_ADDR_SPACE_DIAGNOSE_USAGE.
+   Don't complain about any address space.  */
+
+void
+default_addr_space_diagnose_usage (addr_space_t, location_t)
+{
+}
+	 
+
+/* The default hook for TARGET_ADDR_SPACE_CONVERT. This hook should never be
+   called for targets with only a generic address space.  */
+
+rtx
+default_addr_space_convert (rtx op ATTRIBUTE_UNUSED,
+			    tree from_type ATTRIBUTE_UNUSED,
+			    tree to_type ATTRIBUTE_UNUSED)
+{
+  gcc_unreachable ();
+}
+
+/* The defualt implementation of TARGET_HARD_REGNO_NREGS.  */
+
+unsigned int
+default_hard_regno_nregs (unsigned int, machine_mode mode)
+{
+  /* Targets with variable-sized modes must provide their own definition
+     of this hook.  */
+  return CEIL (GET_MODE_SIZE (mode).to_constant (), UNITS_PER_WORD);
+}
+
+bool
+default_hard_regno_scratch_ok (unsigned int regno ATTRIBUTE_UNUSED)
+{
+  return true;
+}
+
+/* The default implementation of TARGET_MODE_DEPENDENT_ADDRESS_P.  */
+
+bool
+default_mode_dependent_address_p (const_rtx addr ATTRIBUTE_UNUSED,
+				  addr_space_t addrspace ATTRIBUTE_UNUSED)
+{
+  return false;
+}
+
+extern bool default_new_address_profitable_p (rtx, rtx);
+
+
+/* The default implementation of TARGET_NEW_ADDRESS_PROFITABLE_P.  */
+
+bool
+default_new_address_profitable_p (rtx memref ATTRIBUTE_UNUSED,
+				  rtx_insn *insn ATTRIBUTE_UNUSED,
+				  rtx new_addr ATTRIBUTE_UNUSED)
+{
+  return true;
+}
+
+bool
+default_target_option_valid_attribute_p (tree ARG_UNUSED (fndecl),
+					 tree ARG_UNUSED (name),
+					 tree ARG_UNUSED (args),
+					 int ARG_UNUSED (flags))
+{
+  warning (OPT_Wattributes,
+	   "target attribute is not supported on this machine");
+
+  return false;
+}
+
+bool
+default_target_option_pragma_parse (tree ARG_UNUSED (args),
+				    tree ARG_UNUSED (pop_target))
+{
+  /* If args is NULL the caller is handle_pragma_pop_options ().  In that case,
+     emit no warning because "#pragma GCC pop_target" is valid on targets that
+     do not have the "target" pragma.  */
+  if (args)
+    warning (OPT_Wpragmas,
+	     "%<#pragma GCC target%> is not supported for this machine");
+
+  return false;
+}
+
+bool
+default_target_can_inline_p (tree caller, tree callee)
+{
+  tree callee_opts = DECL_FUNCTION_SPECIFIC_TARGET (callee);
+  tree caller_opts = DECL_FUNCTION_SPECIFIC_TARGET (caller);
+  if (! callee_opts)
+    callee_opts = target_option_default_node;
+  if (! caller_opts)
+    caller_opts = target_option_default_node;
+
+  /* If both caller and callee have attributes, assume that if the
+     pointer is different, the two functions have different target
+     options since build_target_option_node uses a hash table for the
+     options.  */
+  return callee_opts == caller_opts;
+}
+
+/* If the machine does not have a case insn that compares the bounds,
+   this means extra overhead for dispatch tables, which raises the
+   threshold for using them.  */
+
+unsigned int
+default_case_values_threshold (void)
+{
+  return (targetm.have_casesi () ? 4 : 5);
+}
+
+bool
+default_have_conditional_execution (void)
+{
+  return HAVE_conditional_execution;
+}
+
+/* By default we assume that c99 functions are present at the runtime,
+   but sincos is not.  */
+bool
+default_libc_has_function (enum function_class fn_class,
+			   tree type ATTRIBUTE_UNUSED)
+{
+  if (fn_class == function_c94
+      || fn_class == function_c99_misc
+      || fn_class == function_c99_math_complex)
+    return true;
+
+  return false;
+}
+
+/* By default assume that libc has not a fast implementation.  */
+
+bool
+default_libc_has_fast_function (int fcode ATTRIBUTE_UNUSED)
+{
+  return false;
+}
+
+bool
+gnu_libc_has_function (enum function_class fn_class ATTRIBUTE_UNUSED,
+		       tree type ATTRIBUTE_UNUSED)
+{
+  return true;
+}
+
+bool
+no_c99_libc_has_function (enum function_class fn_class ATTRIBUTE_UNUSED,
+			  tree type ATTRIBUTE_UNUSED)
+{
+  return false;
+}
+
+tree
+default_builtin_tm_load_store (tree ARG_UNUSED (type))
+{
+  return NULL_TREE;
+}
+
+/* Compute cost of moving registers to/from memory.  */
+
+int
+default_memory_move_cost (machine_mode mode ATTRIBUTE_UNUSED,
+			  reg_class_t rclass ATTRIBUTE_UNUSED,
+			  bool in ATTRIBUTE_UNUSED)
+{
+#ifndef MEMORY_MOVE_COST
+    return (4 + memory_move_secondary_cost (mode, (enum reg_class) rclass, in));
+#else
+    return MEMORY_MOVE_COST (MACRO_MODE (mode), (enum reg_class) rclass, in);
+#endif
+}
+
+/* Compute cost of moving data from a register of class FROM to one of
+   TO, using MODE.  */
+
+int
+default_register_move_cost (machine_mode mode ATTRIBUTE_UNUSED,
+                            reg_class_t from ATTRIBUTE_UNUSED,
+                            reg_class_t to ATTRIBUTE_UNUSED)
+{
+#ifndef REGISTER_MOVE_COST
+  return 2;
+#else
+  return REGISTER_MOVE_COST (MACRO_MODE (mode),
+			     (enum reg_class) from, (enum reg_class) to);
+#endif
+}
+
+/* The default implementation of TARGET_SLOW_UNALIGNED_ACCESS.  */
+
+bool
+default_slow_unaligned_access (machine_mode, unsigned int)
+{
+  return STRICT_ALIGNMENT;
+}
+
+/* The default implementation of TARGET_ESTIMATED_POLY_VALUE.  */
+
+HOST_WIDE_INT
+default_estimated_poly_value (poly_int64 x, poly_value_estimate_kind)
+{
+  return x.coeffs[0];
+}
+
+/* For hooks which use the MOVE_RATIO macro, this gives the legacy default
+   behavior.  SPEED_P is true if we are compiling for speed.  */
+
+unsigned int
+get_move_ratio (bool speed_p ATTRIBUTE_UNUSED)
+{
+  unsigned int move_ratio;
+#ifdef MOVE_RATIO
+  move_ratio = (unsigned int) MOVE_RATIO (speed_p);
+#else
+#if defined (HAVE_cpymemqi) || defined (HAVE_cpymemhi) || defined (HAVE_cpymemsi) || defined (HAVE_cpymemdi) || defined (HAVE_cpymemti)
+  move_ratio = 2;
+#else /* No cpymem patterns, pick a default.  */
+  move_ratio = ((speed_p) ? 15 : 3);
+#endif
+#endif
+  return move_ratio;
+}
+
+/* Return TRUE if the move_by_pieces/set_by_pieces infrastructure should be
+   used; return FALSE if the cpymem/setmem optab should be expanded, or
+   a call to memcpy emitted.  */
+
+bool
+default_use_by_pieces_infrastructure_p (unsigned HOST_WIDE_INT size,
+					unsigned int alignment,
+					enum by_pieces_operation op,
+					bool speed_p)
+{
+  unsigned int max_size = 0;
+  unsigned int ratio = 0;
+
+  switch (op)
+    {
+    case CLEAR_BY_PIECES:
+      max_size = STORE_MAX_PIECES;
+      ratio = CLEAR_RATIO (speed_p);
+      break;
+    case MOVE_BY_PIECES:
+      max_size = MOVE_MAX_PIECES;
+      ratio = get_move_ratio (speed_p);
+      break;
+    case SET_BY_PIECES:
+      max_size = STORE_MAX_PIECES;
+      ratio = SET_RATIO (speed_p);
+      break;
+    case STORE_BY_PIECES:
+      max_size = STORE_MAX_PIECES;
+      ratio = get_move_ratio (speed_p);
+      break;
+    case COMPARE_BY_PIECES:
+      max_size = COMPARE_MAX_PIECES;
+      /* Pick a likely default, just as in get_move_ratio.  */
+      ratio = speed_p ? 15 : 3;
+      break;
+    }
+
+  return by_pieces_ninsns (size, alignment, max_size + 1, op) < ratio;
+}
+
+/* This hook controls code generation for expanding a memcmp operation by
+   pieces.  Return 1 for the normal pattern of compare/jump after each pair
+   of loads, or a higher number to reduce the number of branches.  */
+
+int
+default_compare_by_pieces_branch_ratio (machine_mode)
+{
+  return 1;
+}
+
+/* Helper for default_print_patchable_function_entry and other
+   print_patchable_function_entry hook implementations.  */
+
+void
+default_print_patchable_function_entry_1 (FILE *file,
+					  unsigned HOST_WIDE_INT
+					  patch_area_size,
+					  bool record_p,
+					  unsigned int flags)
+{
+  const char *nop_templ = 0;
+  int code_num;
+  rtx_insn *my_nop = make_insn_raw (gen_nop ());
+
+  /* We use the template alone, relying on the (currently sane) assumption
+     that the NOP template does not have variable operands.  */
+  code_num = recog_memoized (my_nop);
+  nop_templ = get_insn_template (code_num, my_nop);
+
+  if (record_p && targetm_common.have_named_sections)
+    {
+      char buf[256];
+      static int patch_area_number;
+      section *previous_section = in_section;
+      const char *asm_op = integer_asm_op (POINTER_SIZE_UNITS, false);
+
+      gcc_assert (asm_op != NULL);
+      patch_area_number++;
+      ASM_GENERATE_INTERNAL_LABEL (buf, "LPFE", patch_area_number);
+
+      switch_to_section (get_section ("__patchable_function_entries",
+				      flags, current_function_decl));
+      assemble_align (POINTER_SIZE);
+      fputs (asm_op, file);
+      assemble_name_raw (file, buf);
+      fputc ('\n', file);
+
+      switch_to_section (previous_section);
+      ASM_OUTPUT_LABEL (file, buf);
+    }
+
+  unsigned i;
+  for (i = 0; i < patch_area_size; ++i)
+    output_asm_insn (nop_templ, NULL);
+}
+
+/* Write PATCH_AREA_SIZE NOPs into the asm outfile FILE around a function
+   entry.  If RECORD_P is true and the target supports named sections,
+   the location of the NOPs will be recorded in a special object section
+   called "__patchable_function_entries".  This routine may be called
+   twice per function to put NOPs before and after the function
+   entry.  */
+
+void
+default_print_patchable_function_entry (FILE *file,
+					unsigned HOST_WIDE_INT patch_area_size,
+					bool record_p)
+{
+  unsigned int flags = SECTION_WRITE | SECTION_RELRO;
+  if (HAVE_GAS_SECTION_LINK_ORDER)
+    flags |= SECTION_LINK_ORDER;
+  default_print_patchable_function_entry_1 (file, patch_area_size, record_p,
+					    flags);
+}
+
+bool
+default_profile_before_prologue (void)
+{
+#ifdef PROFILE_BEFORE_PROLOGUE
+  return true;
+#else
+  return false;
+#endif
+}
+
+/* The default implementation of TARGET_PREFERRED_RELOAD_CLASS.  */
+
+reg_class_t
+default_preferred_reload_class (rtx x ATTRIBUTE_UNUSED,
+			        reg_class_t rclass)
+{
+#ifdef PREFERRED_RELOAD_CLASS 
+  return (reg_class_t) PREFERRED_RELOAD_CLASS (x, (enum reg_class) rclass);
+#else
+  return rclass;
+#endif
+}
+
+/* The default implementation of TARGET_OUTPUT_PREFERRED_RELOAD_CLASS.  */
+
+reg_class_t
+default_preferred_output_reload_class (rtx x ATTRIBUTE_UNUSED,
+				       reg_class_t rclass)
+{
+  return rclass;
+}
+
+/* The default implementation of TARGET_PREFERRED_RENAME_CLASS.  */
+reg_class_t
+default_preferred_rename_class (reg_class_t rclass ATTRIBUTE_UNUSED)
+{
+  return NO_REGS;
+}
+
+/* The default implementation of TARGET_CLASS_LIKELY_SPILLED_P.  */
+
+bool
+default_class_likely_spilled_p (reg_class_t rclass)
+{
+  return (reg_class_size[(int) rclass] == 1);
+}
+
+/* The default implementation of TARGET_CLASS_MAX_NREGS.  */
+
+unsigned char
+default_class_max_nregs (reg_class_t rclass ATTRIBUTE_UNUSED,
+			 machine_mode mode ATTRIBUTE_UNUSED)
+{
+#ifdef CLASS_MAX_NREGS
+  return (unsigned char) CLASS_MAX_NREGS ((enum reg_class) rclass,
+					  MACRO_MODE (mode));
+#else
+  /* Targets with variable-sized modes must provide their own definition
+     of this hook.  */
+  unsigned int size = GET_MODE_SIZE (mode).to_constant ();
+  return (size + UNITS_PER_WORD - 1) / UNITS_PER_WORD;
+#endif
+}
+
+/* Determine the debugging unwind mechanism for the target.  */
+
+enum unwind_info_type
+default_debug_unwind_info (void)
+{
+  /* If the target wants to force the use of dwarf2 unwind info, let it.  */
+  /* ??? Change all users to the hook, then poison this.  */
+#ifdef DWARF2_FRAME_INFO
+  if (DWARF2_FRAME_INFO)
+    return UI_DWARF2;
+#endif
+
+  /* Otherwise, only turn it on if dwarf2 debugging is enabled.  */
+#ifdef DWARF2_DEBUGGING_INFO
+  if (write_symbols == DWARF2_DEBUG || write_symbols == VMS_AND_DWARF2_DEBUG)
+    return UI_DWARF2;
+#endif
+
+  return UI_NONE;
+}
+
+/* Targets that set NUM_POLY_INT_COEFFS to something greater than 1
+   must define this hook.  */
+
+unsigned int
+default_dwarf_poly_indeterminate_value (unsigned int, unsigned int *, int *)
+{
+  gcc_unreachable ();
+}
+
+/* Determine the correct mode for a Dwarf frame register that represents
+   register REGNO.  */
+
+machine_mode
+default_dwarf_frame_reg_mode (int regno)
+{
+  machine_mode save_mode = reg_raw_mode[regno];
+
+  if (targetm.hard_regno_call_part_clobbered (eh_edge_abi.id (),
+					      regno, save_mode))
+    save_mode = choose_hard_reg_mode (regno, 1, &eh_edge_abi);
+  return save_mode;
+}
+
+/* To be used by targets where reg_raw_mode doesn't return the right
+   mode for registers used in apply_builtin_return and apply_builtin_arg.  */
+
+fixed_size_mode
+default_get_reg_raw_mode (int regno)
+{
+  /* Targets must override this hook if the underlying register is
+     variable-sized.  */
+  return as_a <fixed_size_mode> (reg_raw_mode[regno]);
+}
+
+/* Return true if a leaf function should stay leaf even with profiling
+   enabled.  */
+
+bool
+default_keep_leaf_when_profiled ()
+{
+  return false;
+}
+
+/* Return true if the state of option OPTION should be stored in PCH files
+   and checked by default_pch_valid_p.  Store the option's current state
+   in STATE if so.  */
+
+static inline bool
+option_affects_pch_p (int option, struct cl_option_state *state)
+{
+  if ((cl_options[option].flags & CL_TARGET) == 0)
+    return false;
+  if ((cl_options[option].flags & CL_PCH_IGNORE) != 0)
+    return false;
+  if (option_flag_var (option, &global_options) == &target_flags)
+    if (targetm.check_pch_target_flags)
+      return false;
+  return get_option_state (&global_options, option, state);
+}
+
+/* Default version of get_pch_validity.
+   By default, every flag difference is fatal; that will be mostly right for
+   most targets, but completely right for very few.  */
+
+void *
+default_get_pch_validity (size_t *sz)
+{
+  struct cl_option_state state;
+  size_t i;
+  char *result, *r;
+
+  *sz = 2;
+  if (targetm.check_pch_target_flags)
+    *sz += sizeof (target_flags);
+  for (i = 0; i < cl_options_count; i++)
+    if (option_affects_pch_p (i, &state))
+      *sz += state.size;
+
+  result = r = XNEWVEC (char, *sz);
+  r[0] = flag_pic;
+  r[1] = flag_pie;
+  r += 2;
+  if (targetm.check_pch_target_flags)
+    {
+      memcpy (r, &target_flags, sizeof (target_flags));
+      r += sizeof (target_flags);
+    }
+
+  for (i = 0; i < cl_options_count; i++)
+    if (option_affects_pch_p (i, &state))
+      {
+	memcpy (r, state.data, state.size);
+	r += state.size;
+      }
+
+  return result;
+}
+
+/* Return a message which says that a PCH file was created with a different
+   setting of OPTION.  */
+
+static const char *
+pch_option_mismatch (const char *option)
+{
+  return xasprintf (_("created and used with differing settings of '%s'"),
+		    option);
+}
+
+/* Default version of pch_valid_p.  */
+
+const char *
+default_pch_valid_p (const void *data_p, size_t len)
+{
+  struct cl_option_state state;
+  const char *data = (const char *)data_p;
+  size_t i;
+
+  /* -fpic and -fpie also usually make a PCH invalid.  */
+  if (data[0] != flag_pic)
+    return _("created and used with different settings of %<-fpic%>");
+  if (data[1] != flag_pie)
+    return _("created and used with different settings of %<-fpie%>");
+  data += 2;
+
+  /* Check target_flags.  */
+  if (targetm.check_pch_target_flags)
+    {
+      int tf;
+      const char *r;
+
+      memcpy (&tf, data, sizeof (target_flags));
+      data += sizeof (target_flags);
+      len -= sizeof (target_flags);
+      r = targetm.check_pch_target_flags (tf);
+      if (r != NULL)
+	return r;
+    }
+
+  for (i = 0; i < cl_options_count; i++)
+    if (option_affects_pch_p (i, &state))
+      {
+	if (memcmp (data, state.data, state.size) != 0)
+	  return pch_option_mismatch (cl_options[i].opt_text);
+	data += state.size;
+	len -= state.size;
+      }
+
+  return NULL;
+}
+
+/* Default version of cstore_mode.  */
+
+scalar_int_mode
+default_cstore_mode (enum insn_code icode)
+{
+  return as_a <scalar_int_mode> (insn_data[(int) icode].operand[0].mode);
+}
+
+/* Default version of member_type_forces_blk.  */
+
+bool
+default_member_type_forces_blk (const_tree, machine_mode)
+{
+  return false;
+}
+
+rtx
+default_load_bounds_for_arg (rtx addr ATTRIBUTE_UNUSED,
+			     rtx ptr ATTRIBUTE_UNUSED,
+			     rtx bnd ATTRIBUTE_UNUSED)
+{
+  gcc_unreachable ();
+}
+
+void
+default_store_bounds_for_arg (rtx val ATTRIBUTE_UNUSED,
+			      rtx addr ATTRIBUTE_UNUSED,
+			      rtx bounds ATTRIBUTE_UNUSED,
+			      rtx to ATTRIBUTE_UNUSED)
+{
+  gcc_unreachable ();
+}
+
+rtx
+default_load_returned_bounds (rtx slot ATTRIBUTE_UNUSED)
+{
+  gcc_unreachable ();
+}
+
+void
+default_store_returned_bounds (rtx slot ATTRIBUTE_UNUSED,
+			       rtx bounds ATTRIBUTE_UNUSED)
+{
+  gcc_unreachable ();
+}
+
+/* Default version of canonicalize_comparison.  */
+
+void
+default_canonicalize_comparison (int *, rtx *, rtx *, bool)
+{
+}
+
+/* Default implementation of TARGET_ATOMIC_ASSIGN_EXPAND_FENV.  */
+
+void
+default_atomic_assign_expand_fenv (tree *, tree *, tree *)
+{
+}
+
+#ifndef PAD_VARARGS_DOWN
+#define PAD_VARARGS_DOWN BYTES_BIG_ENDIAN
+#endif
+
+/* Build an indirect-ref expression over the given TREE, which represents a
+   piece of a va_arg() expansion.  */
+tree
+build_va_arg_indirect_ref (tree addr)
+{
+  addr = build_simple_mem_ref_loc (EXPR_LOCATION (addr), addr);
+  return addr;
+}
+
+/* The "standard" implementation of va_arg: read the value from the
+   current (padded) address and increment by the (padded) size.  */
+
+tree
+std_gimplify_va_arg_expr (tree valist, tree type, gimple_seq *pre_p,
+			  gimple_seq *post_p)
+{
+  tree addr, t, type_size, rounded_size, valist_tmp;
+  unsigned HOST_WIDE_INT align, boundary;
+  bool indirect;
+
+  /* All of the alignment and movement below is for args-grow-up machines.
+     As of 2004, there are only 3 ARGS_GROW_DOWNWARD targets, and they all
+     implement their own specialized gimplify_va_arg_expr routines.  */
+  if (ARGS_GROW_DOWNWARD)
+    gcc_unreachable ();
+
+  indirect = pass_va_arg_by_reference (type);
+  if (indirect)
+    type = build_pointer_type (type);
+
+  if (targetm.calls.split_complex_arg
+      && TREE_CODE (type) == COMPLEX_TYPE
+      && targetm.calls.split_complex_arg (type))
+    {
+      tree real_part, imag_part;
+
+      real_part = std_gimplify_va_arg_expr (valist,
+					    TREE_TYPE (type), pre_p, NULL);
+      real_part = get_initialized_tmp_var (real_part, pre_p);
+
+      imag_part = std_gimplify_va_arg_expr (unshare_expr (valist),
+					    TREE_TYPE (type), pre_p, NULL);
+      imag_part = get_initialized_tmp_var (imag_part, pre_p);
+
+      return build2 (COMPLEX_EXPR, type, real_part, imag_part);
+   }
+
+  align = PARM_BOUNDARY / BITS_PER_UNIT;
+  boundary = targetm.calls.function_arg_boundary (TYPE_MODE (type), type);
+
+  /* When we align parameter on stack for caller, if the parameter
+     alignment is beyond MAX_SUPPORTED_STACK_ALIGNMENT, it will be
+     aligned at MAX_SUPPORTED_STACK_ALIGNMENT.  We will match callee
+     here with caller.  */
+  if (boundary > MAX_SUPPORTED_STACK_ALIGNMENT)
+    boundary = MAX_SUPPORTED_STACK_ALIGNMENT;
+
+  boundary /= BITS_PER_UNIT;
+
+  /* Hoist the valist value into a temporary for the moment.  */
+  valist_tmp = get_initialized_tmp_var (valist, pre_p);
+
+  /* va_list pointer is aligned to PARM_BOUNDARY.  If argument actually
+     requires greater alignment, we must perform dynamic alignment.  */
+  if (boundary > align
+      && !TYPE_EMPTY_P (type)
+      && !integer_zerop (TYPE_SIZE (type)))
+    {
+      t = build2 (MODIFY_EXPR, TREE_TYPE (valist), valist_tmp,
+		  fold_build_pointer_plus_hwi (valist_tmp, boundary - 1));
+      gimplify_and_add (t, pre_p);
+
+      t = build2 (MODIFY_EXPR, TREE_TYPE (valist), valist_tmp,
+		  fold_build2 (BIT_AND_EXPR, TREE_TYPE (valist),
+			       valist_tmp,
+			       build_int_cst (TREE_TYPE (valist), -boundary)));
+      gimplify_and_add (t, pre_p);
+    }
+  else
+    boundary = align;
+
+  /* If the actual alignment is less than the alignment of the type,
+     adjust the type accordingly so that we don't assume strict alignment
+     when dereferencing the pointer.  */
+  boundary *= BITS_PER_UNIT;
+  if (boundary < TYPE_ALIGN (type))
+    {
+      type = build_variant_type_copy (type);
+      SET_TYPE_ALIGN (type, boundary);
+    }
+
+  /* Compute the rounded size of the type.  */
+  type_size = arg_size_in_bytes (type);
+  rounded_size = round_up (type_size, align);
+
+  /* Reduce rounded_size so it's sharable with the postqueue.  */
+  gimplify_expr (&rounded_size, pre_p, post_p, is_gimple_val, fb_rvalue);
+
+  /* Get AP.  */
+  addr = valist_tmp;
+  if (PAD_VARARGS_DOWN && !integer_zerop (rounded_size))
+    {
+      /* Small args are padded downward.  */
+      t = fold_build2_loc (input_location, GT_EXPR, sizetype,
+		       rounded_size, size_int (align));
+      t = fold_build3 (COND_EXPR, sizetype, t, size_zero_node,
+		       size_binop (MINUS_EXPR, rounded_size, type_size));
+      addr = fold_build_pointer_plus (addr, t);
+    }
+
+  /* Compute new value for AP.  */
+  t = fold_build_pointer_plus (valist_tmp, rounded_size);
+  t = build2 (MODIFY_EXPR, TREE_TYPE (valist), valist, t);
+  gimplify_and_add (t, pre_p);
+
+  addr = fold_convert (build_pointer_type (type), addr);
+
+  if (indirect)
+    addr = build_va_arg_indirect_ref (addr);
+
+  return build_va_arg_indirect_ref (addr);
+}
+
+/* An implementation of TARGET_CAN_USE_DOLOOP_P for targets that do
+   not support nested low-overhead loops.  */
+
+bool
+can_use_doloop_if_innermost (const widest_int &, const widest_int &,
+			     unsigned int loop_depth, bool)
+{
+  return loop_depth == 1;
+}
+
+/* Default implementation of TARGET_OPTAB_SUPPORTED_P.  */
+
+bool
+default_optab_supported_p (int, machine_mode, machine_mode, optimization_type)
+{
+  return true;
+}
+
+/* Default implementation of TARGET_MAX_NOCE_IFCVT_SEQ_COST.  */
+
+unsigned int
+default_max_noce_ifcvt_seq_cost (edge e)
+{
+  bool predictable_p = predictable_edge_p (e);
+
+  if (predictable_p)
+    {
+      if (global_options_set.x_param_max_rtl_if_conversion_predictable_cost)
+	return param_max_rtl_if_conversion_predictable_cost;
+    }
+  else
+    {
+      if (global_options_set.x_param_max_rtl_if_conversion_unpredictable_cost)
+	return param_max_rtl_if_conversion_unpredictable_cost;
+    }
+
+  return BRANCH_COST (true, predictable_p) * COSTS_N_INSNS (3);
+}
+
+/* Default implementation of TARGET_MIN_ARITHMETIC_PRECISION.  */
+
+unsigned int
+default_min_arithmetic_precision (void)
+{
+  return WORD_REGISTER_OPERATIONS ? BITS_PER_WORD : BITS_PER_UNIT;
+}
+
+/* Default implementation of TARGET_C_EXCESS_PRECISION.  */
+
+enum flt_eval_method
+default_excess_precision (enum excess_precision_type ATTRIBUTE_UNUSED)
+{
+  return FLT_EVAL_METHOD_PROMOTE_TO_FLOAT;
+}
+
+/* Default implementation for
+  TARGET_STACK_CLASH_PROTECTION_ALLOCA_PROBE_RANGE.  */
+HOST_WIDE_INT
+default_stack_clash_protection_alloca_probe_range (void)
+{
+  return 0;
+}
+
+/* The default implementation of TARGET_EARLY_REMAT_MODES.  */
+
+void
+default_select_early_remat_modes (sbitmap)
+{
+}
+
+/* The default implementation of TARGET_PREFERRED_ELSE_VALUE.  */
+
+tree
+default_preferred_else_value (unsigned, tree type, unsigned, tree *)
+{
+  return build_zero_cst (type);
+}
+
+/* Default implementation of TARGET_HAVE_SPECULATION_SAFE_VALUE.  */
+bool
+default_have_speculation_safe_value (bool active ATTRIBUTE_UNUSED)
+{
+#ifdef HAVE_speculation_barrier
+  return active ? HAVE_speculation_barrier : true;
+#else
+  return false;
+#endif
+}
+/* Alternative implementation of TARGET_HAVE_SPECULATION_SAFE_VALUE
+   that can be used on targets that never have speculative execution.  */
+bool
+speculation_safe_value_not_needed (bool active)
+{
+  return !active;
+}
+
+/* Default implementation of the speculation-safe-load builtin.  This
+   implementation simply copies val to result and generates a
+   speculation_barrier insn, if such a pattern is defined.  */
+rtx
+default_speculation_safe_value (machine_mode mode ATTRIBUTE_UNUSED,
+				rtx result, rtx val,
+				rtx failval ATTRIBUTE_UNUSED)
+{
+  emit_move_insn (result, val);
+
+#ifdef HAVE_speculation_barrier
+  /* Assume the target knows what it is doing: if it defines a
+     speculation barrier, but it is not enabled, then assume that one
+     isn't needed.  */
+  if (HAVE_speculation_barrier)
+    emit_insn (gen_speculation_barrier ());
+#endif
+
+  return result;
+}
+
+/* How many bits to shift in order to access the tag bits.
+   The default is to store the tag in the top 8 bits of a 64 bit pointer, hence
+   shifting 56 bits will leave just the tag.  */
+#define HWASAN_SHIFT (GET_MODE_PRECISION (Pmode) - 8)
+#define HWASAN_SHIFT_RTX GEN_INT (HWASAN_SHIFT)
+
+bool
+default_memtag_can_tag_addresses ()
+{
+  return false;
+}
+
+uint8_t
+default_memtag_tag_size ()
+{
+  return 8;
+}
+
+uint8_t
+default_memtag_granule_size ()
+{
+  return 16;
+}
+
+/* The default implementation of TARGET_MEMTAG_INSERT_RANDOM_TAG.  */
+rtx
+default_memtag_insert_random_tag (rtx untagged, rtx target)
+{
+  gcc_assert (param_hwasan_instrument_stack);
+  if (param_hwasan_random_frame_tag)
+    {
+      rtx fn = init_one_libfunc ("__hwasan_generate_tag");
+      rtx new_tag = emit_library_call_value (fn, NULL_RTX, LCT_NORMAL, QImode);
+      return targetm.memtag.set_tag (untagged, new_tag, target);
+    }
+  else
+    {
+      /* NOTE: The kernel API does not have __hwasan_generate_tag exposed.
+	 In the future we may add the option emit random tags with inline
+	 instrumentation instead of function calls.  This would be the same
+	 between the kernel and userland.  */
+      return untagged;
+    }
+}
+
+/* The default implementation of TARGET_MEMTAG_ADD_TAG.  */
+rtx
+default_memtag_add_tag (rtx base, poly_int64 offset, uint8_t tag_offset)
+{
+  /* Need to look into what the most efficient code sequence is.
+     This is a code sequence that would be emitted *many* times, so we
+     want it as small as possible.
+
+     There are two places where tag overflow is a question:
+       - Tagging the shadow stack.
+	  (both tagging and untagging).
+       - Tagging addressable pointers.
+
+     We need to ensure both behaviors are the same (i.e. that the tag that
+     ends up in a pointer after "overflowing" the tag bits with a tag addition
+     is the same that ends up in the shadow space).
+
+     The aim is that the behavior of tag addition should follow modulo
+     wrapping in both instances.
+
+     The libhwasan code doesn't have any path that increments a pointer's tag,
+     which means it has no opinion on what happens when a tag increment
+     overflows (and hence we can choose our own behavior).  */
+
+  offset += ((uint64_t)tag_offset << HWASAN_SHIFT);
+  return plus_constant (Pmode, base, offset);
+}
+
+/* The default implementation of TARGET_MEMTAG_SET_TAG.  */
+rtx
+default_memtag_set_tag (rtx untagged, rtx tag, rtx target)
+{
+  gcc_assert (GET_MODE (untagged) == Pmode && GET_MODE (tag) == QImode);
+  tag = expand_simple_binop (Pmode, ASHIFT, tag, HWASAN_SHIFT_RTX, NULL_RTX,
+			     /* unsignedp = */1, OPTAB_WIDEN);
+  rtx ret = expand_simple_binop (Pmode, IOR, untagged, tag, target,
+				 /* unsignedp = */1, OPTAB_DIRECT);
+  gcc_assert (ret);
+  return ret;
+}
+
+/* The default implementation of TARGET_MEMTAG_EXTRACT_TAG.  */
+rtx
+default_memtag_extract_tag (rtx tagged_pointer, rtx target)
+{
+  rtx tag = expand_simple_binop (Pmode, LSHIFTRT, tagged_pointer,
+				 HWASAN_SHIFT_RTX, target,
+				 /* unsignedp = */0,
+				 OPTAB_DIRECT);
+  rtx ret = gen_lowpart (QImode, tag);
+  gcc_assert (ret);
+  return ret;
+}
+
+/* The default implementation of TARGET_MEMTAG_UNTAGGED_POINTER.  */
+rtx
+default_memtag_untagged_pointer (rtx tagged_pointer, rtx target)
+{
+  rtx tag_mask = gen_int_mode ((HOST_WIDE_INT_1U << HWASAN_SHIFT) - 1, Pmode);
+  rtx untagged_base = expand_simple_binop (Pmode, AND, tagged_pointer,
+					   tag_mask, target, true,
+					   OPTAB_DIRECT);
+  gcc_assert (untagged_base);
+  return untagged_base;
+}
+
+#include "gt-targhooks.h"
diff -uNr gcc-11.2.0.orig/gcc/targhooks.h gcc-11.2.0/gcc/targhooks.h
--- gcc-11.2.0.orig/gcc/targhooks.h	2021-07-28 01:55:07.504290371 -0500
+++ gcc-11.2.0/gcc/targhooks.h	2021-08-06 00:34:01.885709574 -0500
@@ -228,6 +228,8 @@
 						    unsigned int,
 						    enum by_pieces_operation,
 						    bool);
+extern rtx default_clear_by_pieces (void *, void *, HOST_WIDE_INT,
+				    scalar_int_mode);
 extern int default_compare_by_pieces_branch_ratio (machine_mode);
 
 extern void default_print_patchable_function_entry_1 (FILE *,
diff -uNr gcc-11.2.0.orig/gcc/targhooks.h.orig gcc-11.2.0/gcc/targhooks.h.orig
--- gcc-11.2.0.orig/gcc/targhooks.h.orig	1969-12-31 18:00:00.000000000 -0600
+++ gcc-11.2.0/gcc/targhooks.h.orig	2021-07-28 01:55:07.504290371 -0500
@@ -0,0 +1,303 @@
+/* Default target hook functions.
+   Copyright (C) 2003-2021 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 3, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING3.  If not see
+<http://www.gnu.org/licenses/>.  */
+
+#ifndef GCC_TARGHOOKS_H
+#define GCC_TARGHOOKS_H
+
+extern bool default_legitimate_address_p (machine_mode, rtx, bool);
+
+extern void default_external_libcall (rtx);
+extern rtx default_legitimize_address (rtx, rtx, machine_mode);
+extern bool default_legitimize_address_displacement (rtx *, rtx *,
+						     poly_int64, machine_mode);
+extern bool default_const_not_ok_for_debug_p (rtx);
+
+extern int default_unspec_may_trap_p (const_rtx, unsigned);
+extern machine_mode default_promote_function_mode (const_tree, machine_mode,
+							int *, const_tree, int);
+extern machine_mode default_promote_function_mode_always_promote
+			(const_tree, machine_mode, int *, const_tree, int);
+
+extern machine_mode default_cc_modes_compatible (machine_mode,
+						      machine_mode);
+
+extern bool default_return_in_memory (const_tree, const_tree);
+
+extern rtx default_expand_builtin_saveregs (void);
+extern void default_setup_incoming_varargs (cumulative_args_t,
+					    const function_arg_info &,
+					    int *, int);
+extern rtx default_builtin_setjmp_frame_value (void);
+extern bool default_pretend_outgoing_varargs_named (cumulative_args_t);
+
+extern scalar_int_mode default_eh_return_filter_mode (void);
+extern scalar_int_mode default_libgcc_cmp_return_mode (void);
+extern scalar_int_mode default_libgcc_shift_count_mode (void);
+extern scalar_int_mode default_unwind_word_mode (void);
+extern unsigned HOST_WIDE_INT default_shift_truncation_mask
+  (machine_mode);
+extern unsigned int default_min_divisions_for_recip_mul (machine_mode);
+extern int default_mode_rep_extended (scalar_int_mode, scalar_int_mode);
+
+extern tree default_stack_protect_guard (void);
+extern tree default_external_stack_protect_fail (void);
+extern tree default_hidden_stack_protect_fail (void);
+
+extern machine_mode default_mode_for_suffix (char);
+
+extern tree default_cxx_guard_type (void);
+extern tree default_cxx_get_cookie_size (tree);
+
+extern bool hook_pass_by_reference_must_pass_in_stack
+  (cumulative_args_t, const function_arg_info &);
+extern bool hook_callee_copies_named
+  (cumulative_args_t ca, const function_arg_info &);
+
+extern void default_print_operand (FILE *, rtx, int);
+extern void default_print_operand_address (FILE *, machine_mode, rtx);
+extern bool default_print_operand_punct_valid_p (unsigned char);
+extern tree default_mangle_assembler_name (const char *);
+
+extern machine_mode default_translate_mode_attribute (machine_mode);
+extern bool default_scalar_mode_supported_p (scalar_mode);
+extern bool default_libgcc_floating_mode_supported_p (scalar_float_mode);
+extern opt_scalar_float_mode default_floatn_mode (int, bool);
+extern bool default_floatn_builtin_p (int);
+extern bool targhook_words_big_endian (void);
+extern bool targhook_float_words_big_endian (void);
+extern bool default_float_exceptions_rounding_supported_p (void);
+extern bool default_decimal_float_supported_p (void);
+extern bool default_fixed_point_supported_p (void);
+
+extern bool default_has_ifunc_p (void);
+
+extern bool default_predict_doloop_p (class loop *);
+extern const char * default_invalid_within_doloop (const rtx_insn *);
+
+extern tree default_builtin_vectorized_function (unsigned int, tree, tree);
+extern tree default_builtin_md_vectorized_function (tree, tree, tree);
+
+extern int default_builtin_vectorization_cost (enum vect_cost_for_stmt, tree, int);
+
+extern tree default_builtin_reciprocal (tree);
+
+extern HOST_WIDE_INT default_static_rtx_alignment (machine_mode);
+extern HOST_WIDE_INT default_constant_alignment (const_tree, HOST_WIDE_INT);
+extern HOST_WIDE_INT constant_alignment_word_strings (const_tree,
+						      HOST_WIDE_INT);
+extern HOST_WIDE_INT default_vector_alignment (const_tree);
+
+extern poly_uint64 default_preferred_vector_alignment (const_tree);
+extern bool default_builtin_vector_alignment_reachable (const_tree, bool);
+extern bool
+default_builtin_support_vector_misalignment (machine_mode mode,
+					     const_tree,
+					     int, bool);
+extern machine_mode default_preferred_simd_mode (scalar_mode mode);
+extern machine_mode default_split_reduction (machine_mode);
+extern unsigned int default_autovectorize_vector_modes (vector_modes *, bool);
+extern opt_machine_mode default_vectorize_related_mode (machine_mode,
+							scalar_mode,
+							poly_uint64);
+extern opt_machine_mode default_get_mask_mode (machine_mode);
+extern bool default_empty_mask_is_expensive (unsigned);
+extern void *default_init_cost (class loop *);
+extern unsigned default_add_stmt_cost (class vec_info *, void *, int,
+				       enum vect_cost_for_stmt,
+				       class _stmt_vec_info *, tree, int,
+				       enum vect_cost_model_location);
+extern void default_finish_cost (void *, unsigned *, unsigned *, unsigned *);
+extern void default_destroy_cost_data (void *);
+
+/* OpenACC hooks.  */
+extern bool default_goacc_validate_dims (tree, int [], int, unsigned);
+extern int default_goacc_dim_limit (int);
+extern bool default_goacc_fork_join (gcall *, const int [], bool);
+extern void default_goacc_reduction (gcall *);
+
+/* These are here, and not in hooks.[ch], because not all users of
+   hooks.h include tm.h, and thus we don't have CUMULATIVE_ARGS.  */
+
+extern bool hook_bool_CUMULATIVE_ARGS_false (cumulative_args_t);
+extern bool hook_bool_CUMULATIVE_ARGS_true (cumulative_args_t);
+
+extern bool hook_bool_CUMULATIVE_ARGS_arg_info_false
+  (cumulative_args_t, const function_arg_info &);
+extern bool hook_bool_CUMULATIVE_ARGS_arg_info_true
+  (cumulative_args_t, const function_arg_info &);
+extern int hook_int_CUMULATIVE_ARGS_arg_info_0
+  (cumulative_args_t, const function_arg_info &);
+extern void hook_void_CUMULATIVE_ARGS_tree
+  (cumulative_args_t, tree);
+extern const char *hook_invalid_arg_for_unprototyped_fn
+  (const_tree, const_tree, const_tree);
+extern void default_function_arg_advance
+  (cumulative_args_t, const function_arg_info &);
+extern HOST_WIDE_INT default_function_arg_offset (machine_mode, const_tree);
+extern pad_direction default_function_arg_padding (machine_mode, const_tree);
+extern rtx default_function_arg (cumulative_args_t, const function_arg_info &);
+extern rtx default_function_incoming_arg (cumulative_args_t,
+					  const function_arg_info &);
+extern unsigned int default_function_arg_boundary (machine_mode,
+						   const_tree);
+extern unsigned int default_function_arg_round_boundary (machine_mode,
+							 const_tree);
+extern bool hook_bool_const_rtx_commutative_p (const_rtx, int);
+extern rtx default_function_value (const_tree, const_tree, bool);
+extern HARD_REG_SET default_zero_call_used_regs (HARD_REG_SET);
+extern rtx default_libcall_value (machine_mode, const_rtx);
+extern bool default_function_value_regno_p (const unsigned int);
+extern rtx default_internal_arg_pointer (void);
+extern rtx default_static_chain (const_tree, bool);
+extern void default_trampoline_init (rtx, tree, rtx);
+extern void default_emit_call_builtin___clear_cache (rtx, rtx);
+extern poly_int64 default_return_pops_args (tree, tree, poly_int64);
+extern reg_class_t default_ira_change_pseudo_allocno_class (int, reg_class_t,
+							    reg_class_t);
+extern bool default_lra_p (void);
+extern int default_register_priority (int);
+extern bool default_register_usage_leveling_p (void);
+extern bool default_different_addr_displacement_p (void);
+extern reg_class_t default_secondary_reload (bool, rtx, reg_class_t,
+					     machine_mode,
+					     secondary_reload_info *);
+extern machine_mode default_secondary_memory_needed_mode (machine_mode);
+extern void default_target_option_override (void);
+extern void hook_void_bitmap (bitmap);
+extern int default_reloc_rw_mask (void);
+extern bool default_generate_pic_addr_diff_vec (void);
+extern tree default_mangle_decl_assembler_name (tree, tree);
+extern tree default_emutls_var_fields (tree, tree *);
+extern tree default_emutls_var_init (tree, tree, tree);
+extern unsigned int default_hard_regno_nregs (unsigned int, machine_mode);
+extern bool default_hard_regno_scratch_ok (unsigned int);
+extern bool default_mode_dependent_address_p (const_rtx, addr_space_t);
+extern bool default_new_address_profitable_p (rtx, rtx_insn *, rtx);
+extern bool default_target_option_valid_attribute_p (tree, tree, tree, int);
+extern bool default_target_option_pragma_parse (tree, tree);
+extern bool default_target_can_inline_p (tree, tree);
+extern bool default_valid_pointer_mode (scalar_int_mode);
+extern bool default_ref_may_alias_errno (class ao_ref *);
+extern scalar_int_mode default_addr_space_pointer_mode (addr_space_t);
+extern scalar_int_mode default_addr_space_address_mode (addr_space_t);
+extern bool default_addr_space_valid_pointer_mode (scalar_int_mode,
+						   addr_space_t);
+extern bool default_addr_space_legitimate_address_p (machine_mode, rtx,
+						     bool, addr_space_t);
+extern rtx default_addr_space_legitimize_address (rtx, rtx, machine_mode,
+						  addr_space_t);
+extern bool default_addr_space_subset_p (addr_space_t, addr_space_t);
+extern bool default_addr_space_zero_address_valid (addr_space_t);
+extern int default_addr_space_debug (addr_space_t);
+extern void default_addr_space_diagnose_usage (addr_space_t, location_t);
+extern rtx default_addr_space_convert (rtx, tree, tree);
+extern unsigned int default_case_values_threshold (void);
+extern bool default_have_conditional_execution (void);
+
+extern bool default_libc_has_function (enum function_class, tree);
+extern bool default_libc_has_fast_function (int fcode);
+extern bool no_c99_libc_has_function (enum function_class, tree);
+extern bool gnu_libc_has_function (enum function_class, tree);
+
+extern tree default_builtin_tm_load_store (tree);
+
+extern int default_memory_move_cost (machine_mode, reg_class_t, bool);
+extern int default_register_move_cost (machine_mode, reg_class_t,
+				       reg_class_t);
+extern bool default_slow_unaligned_access (machine_mode, unsigned int);
+extern HOST_WIDE_INT default_estimated_poly_value (poly_int64,
+						   poly_value_estimate_kind);
+
+extern bool default_use_by_pieces_infrastructure_p (unsigned HOST_WIDE_INT,
+						    unsigned int,
+						    enum by_pieces_operation,
+						    bool);
+extern int default_compare_by_pieces_branch_ratio (machine_mode);
+
+extern void default_print_patchable_function_entry_1 (FILE *,
+						      unsigned HOST_WIDE_INT,
+						      bool, unsigned int);
+extern void default_print_patchable_function_entry (FILE *,
+						    unsigned HOST_WIDE_INT,
+						    bool);
+extern bool default_profile_before_prologue (void);
+extern reg_class_t default_preferred_reload_class (rtx, reg_class_t);
+extern reg_class_t default_preferred_output_reload_class (rtx, reg_class_t);
+extern reg_class_t default_preferred_rename_class (reg_class_t rclass);
+extern bool default_class_likely_spilled_p (reg_class_t);
+extern unsigned char default_class_max_nregs (reg_class_t, machine_mode);
+
+extern enum unwind_info_type default_debug_unwind_info (void);
+
+extern void default_canonicalize_comparison (int *, rtx *, rtx *, bool);
+
+extern section * default_function_section(tree decl, enum node_frequency freq,
+					  bool startup, bool exit);
+extern unsigned int default_dwarf_poly_indeterminate_value (unsigned int,
+							    unsigned int *,
+							    int *);
+extern machine_mode default_dwarf_frame_reg_mode (int);
+extern fixed_size_mode default_get_reg_raw_mode (int);
+extern bool default_keep_leaf_when_profiled ();
+
+extern void *default_get_pch_validity (size_t *);
+extern const char *default_pch_valid_p (const void *, size_t);
+
+extern void default_asm_output_ident_directive (const char*);
+
+extern scalar_int_mode default_cstore_mode (enum insn_code);
+extern bool default_member_type_forces_blk (const_tree, machine_mode);
+extern void default_atomic_assign_expand_fenv (tree *, tree *, tree *);
+extern tree build_va_arg_indirect_ref (tree);
+extern tree std_gimplify_va_arg_expr (tree, tree, gimple_seq *, gimple_seq *);
+extern bool can_use_doloop_if_innermost (const widest_int &,
+					 const widest_int &,
+					 unsigned int, bool);
+
+extern rtx default_load_bounds_for_arg (rtx, rtx, rtx);
+extern void default_store_bounds_for_arg (rtx, rtx, rtx, rtx);
+extern rtx default_load_returned_bounds (rtx);
+extern void default_store_returned_bounds (rtx,rtx);
+extern bool default_optab_supported_p (int, machine_mode, machine_mode,
+				       optimization_type);
+extern unsigned int default_max_noce_ifcvt_seq_cost (edge);
+extern bool default_noce_conversion_profitable_p (rtx_insn *,
+						  struct noce_if_info *);
+extern unsigned int default_min_arithmetic_precision (void);
+
+extern enum flt_eval_method
+default_excess_precision (enum excess_precision_type ATTRIBUTE_UNUSED);
+extern HOST_WIDE_INT default_stack_clash_protection_alloca_probe_range (void);
+extern void default_select_early_remat_modes (sbitmap);
+extern tree default_preferred_else_value (unsigned, tree, unsigned, tree *);
+
+extern bool default_have_speculation_safe_value (bool);
+extern bool speculation_safe_value_not_needed (bool);
+extern rtx default_speculation_safe_value (machine_mode, rtx, rtx, rtx);
+
+extern bool default_memtag_can_tag_addresses ();
+extern uint8_t default_memtag_tag_size ();
+extern uint8_t default_memtag_granule_size ();
+extern rtx default_memtag_insert_random_tag (rtx, rtx);
+extern rtx default_memtag_add_tag (rtx, poly_int64, uint8_t);
+extern rtx default_memtag_set_tag (rtx, rtx, rtx);
+extern rtx default_memtag_extract_tag (rtx, rtx);
+extern rtx default_memtag_untagged_pointer (rtx, rtx);
+
+#endif /* GCC_TARGHOOKS_H */
